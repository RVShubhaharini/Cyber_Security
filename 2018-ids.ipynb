{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1530359,"sourceType":"datasetVersion","datasetId":902298},{"sourceId":4059877,"sourceType":"datasetVersion","datasetId":2395943,"isSourceIdPinned":false},{"sourceId":13971225,"sourceType":"datasetVersion","datasetId":8906821},{"sourceId":13971816,"sourceType":"datasetVersion","datasetId":8907254},{"sourceId":13971965,"sourceType":"datasetVersion","datasetId":8907355},{"sourceId":14002945,"sourceType":"datasetVersion","datasetId":8922051},{"sourceId":14010669,"sourceType":"datasetVersion","datasetId":8925284}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:07:46.968626Z","iopub.execute_input":"2025-12-05T11:07:46.969130Z","iopub.status.idle":"2025-12-05T11:07:47.019901Z","shell.execute_reply.started":"2025-12-05T11:07:46.969107Z","shell.execute_reply":"2025-12-05T11:07:47.019108Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/cicids2017/Benign-Monday-no-metadata.parquet\n/kaggle/input/cicids2017/Bruteforce-Tuesday-no-metadata.parquet\n/kaggle/input/cicids2017/Portscan-Friday-no-metadata.parquet\n/kaggle/input/cicids2017/WebAttacks-Thursday-no-metadata.parquet\n/kaggle/input/cicids2017/DoS-Wednesday-no-metadata.parquet\n/kaggle/input/cicids2017/DDoS-Friday-no-metadata.parquet\n/kaggle/input/cicids2017/Infiltration-Thursday-no-metadata.parquet\n/kaggle/input/cicids2017/Botnet-Friday-no-metadata.parquet\n/kaggle/input/cleaned-ids/ids2018_cleaned_combined_1.csv\n/kaggle/input/ids-intrusion-csv/02-28-2018.csv\n/kaggle/input/ids-intrusion-csv/03-01-2018.csv\n/kaggle/input/ids-intrusion-csv/02-16-2018.csv\n/kaggle/input/ids-intrusion-csv/02-15-2018.csv\n/kaggle/input/ids-intrusion-csv/02-21-2018.csv\n/kaggle/input/ids-intrusion-csv/03-02-2018.csv\n/kaggle/input/ids-intrusion-csv/02-22-2018.csv\n/kaggle/input/ids-intrusion-csv/02-20-2018.csv\n/kaggle/input/ids-intrusion-csv/02-14-2018.csv\n/kaggle/input/ids-intrusion-csv/02-23-2018.csv\n/kaggle/input/traininig/UNSW_NB15_training-set.csv\n/kaggle/input/udplag/UDPLag.csv\n/kaggle/input/networkkk/loan_final_normalized (1).csv\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"print(\"hi\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:07:47.021335Z","iopub.execute_input":"2025-12-05T11:07:47.021608Z","iopub.status.idle":"2025-12-05T11:07:47.025900Z","shell.execute_reply.started":"2025-12-05T11:07:47.021589Z","shell.execute_reply":"2025-12-05T11:07:47.025110Z"}},"outputs":[{"name":"stdout","text":"hi\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\n\n# 1Ô∏è‚É£ Load dataset\ndf = pd.read_csv(\"/kaggle/input/traininig/UNSW_NB15_training-set.csv\")\n\n# 2Ô∏è‚É£ Remove columns where all values are NaN\ndf = df.dropna(axis=1, how='all')\n\n# 3Ô∏è‚É£ Remove columns where all values are 0\ndf = df.loc[:, (df != 0).any(axis=0)]\n\n# 4Ô∏è‚É£ Remove duplicate rows\ndf = df.drop_duplicates()\n\n# 5Ô∏è‚É£ Identify column types\nnum_cols = df.select_dtypes(include=['int64', 'float64']).columns\ncat_cols = df.select_dtypes(include=['object']).columns\n\n# 6Ô∏è‚É£ Handle missing values\ndf[num_cols] = df[num_cols].fillna(df[num_cols].median())\ndf[cat_cols] = df[cat_cols].fillna(df[cat_cols].mode().iloc[0])\n\n# 7Ô∏è‚É£ Label encode categorical columns\nle = LabelEncoder()\nfor col in cat_cols:\n    df[col] = le.fit_transform(df[col].astype(str))\n\n# 8Ô∏è‚É£ Min-Max normalization\nscaler = MinMaxScaler()\ndf[num_cols] = scaler.fit_transform(df[num_cols])\n\n# 9Ô∏è‚É£ Save final output in Kaggle working directory\noutput_filename = \"/kaggle/working/network_train_final_normalized.csv\"\ndf.to_csv(output_filename, index=False)\n\nprint(\"‚úÖ Preprocessing complete!\")\nprint(\"üìÅ Saved as:\", output_filename)\nprint(\"Shape:\", df.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:07:47.026731Z","iopub.execute_input":"2025-12-05T11:07:47.026984Z","iopub.status.idle":"2025-12-05T11:07:49.912444Z","shell.execute_reply.started":"2025-12-05T11:07:47.026968Z","shell.execute_reply":"2025-12-05T11:07:49.906290Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/2722055816.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# 9Ô∏è‚É£ Save final output in Kaggle working directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0moutput_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/kaggle/working/network_train_final_normalized.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"‚úÖ Preprocessing complete!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    331\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m                 )\n\u001b[0;32m--> 333\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3965\u001b[0m         )\n\u001b[1;32m   3966\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3967\u001b[0;31m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[1;32m   3968\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3969\u001b[0m             \u001b[0mlineterminator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlineterminator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m   1012\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         )\n\u001b[0;32m-> 1014\u001b[0;31m         \u001b[0mcsv_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    268\u001b[0m             )\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_need_to_save_header\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_save_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m_save_body\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstart_i\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_save_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_i\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m_save_chunk\u001b[0;34m(self, start_i, end_i)\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mslicer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_values_for_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_number_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iter_column_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_get_values_for_csv\u001b[0;34m(self, float_format, date_format, decimal, na_rep, quoting)\u001b[0m\n\u001b[1;32m   1408\u001b[0m     ) -> Self:\n\u001b[1;32m   1409\u001b[0m         \u001b[0;31m# helper used by to_csv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1410\u001b[0;31m         mgr = self._mgr.get_values_for_csv(\n\u001b[0m\u001b[1;32m   1411\u001b[0m             \u001b[0mfloat_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfloat_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1412\u001b[0m             \u001b[0mdate_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdate_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mget_values_for_csv\u001b[0;34m(self, float_format, date_format, decimal, na_rep, quoting)\u001b[0m\n\u001b[1;32m    464\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0mformatting\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrepr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \"\"\"\n\u001b[0;32m--> 466\u001b[0;31m         return self.apply(\n\u001b[0m\u001b[1;32m    467\u001b[0m             \u001b[0;34m\"get_values_for_csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m             \u001b[0mna_rep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_rep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m                 \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m             \u001b[0mresult_blocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextend_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapplied\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mget_values_for_csv\u001b[0;34m(self, float_format, date_format, decimal, na_rep, quoting)\u001b[0m\n\u001b[1;32m    778\u001b[0m     ) -> Block:\n\u001b[1;32m    779\u001b[0m         \u001b[0;34m\"\"\"convert to our native types format\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         result = get_values_for_csv(\n\u001b[0m\u001b[1;32m    781\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m             \u001b[0mna_rep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_rep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_values_for_csv\u001b[0;34m(values, date_format, na_rep, quoting, float_format, decimal)\u001b[0m\n\u001b[1;32m   7832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7833\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mquoting\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7834\u001b[0;31m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7835\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7836\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"object\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":32},{"cell_type":"code","source":"# catboost_training_label_target_fixed.py\n\nimport pandas as pd\nimport numpy as np\nfrom catboost import CatBoostClassifier, Pool\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (accuracy_score, precision_score, recall_score,\n                             f1_score, roc_auc_score, classification_report, confusion_matrix)\n\ndef detect_categorical(df, threshold_unique_ratio=0.05, max_unique_for_cat=50):\n    cats = []\n    n = len(df)\n    for col in df.columns:\n        if df[col].dtype == 'object' or pd.api.types.is_categorical_dtype(df[col]):\n            cats.append(col)\n        else:\n            nunq = df[col].nunique(dropna=True)\n            if nunq <= max_unique_for_cat and (nunq / max(1, n)) <= threshold_unique_ratio:\n                cats.append(col)\n    return cats\n\ndef main(csv_path=\"/kaggle/working/network_train_final_normalized.csv\",\n         test_size=0.2,\n         random_state=42):\n\n    print(\"Loading dataset:\", csv_path)\n    df = pd.read_csv(csv_path)\n    print(\"Shape:\", df.shape)\n    print(df.head())\n\n    # fixed target and leakage columns\n    target_col = \"label\"\n    leak_cols = [\"attack_cat\"]  # remove leakage column(s)\n\n    # checks\n    missing = [c for c in [target_col] + leak_cols if c not in df.columns]\n    if missing:\n        raise ValueError(f\"Missing expected column(s): {missing}\")\n\n    print(\"Using target column:\", target_col)\n    print(\"Removing leakage column(s):\", leak_cols)\n\n    # drop target & leakage from features\n    X = df.drop(columns=[target_col] + leak_cols)\n    y = df[target_col].copy()\n\n    # convert non-numeric target to numeric (if needed)\n    if not pd.api.types.is_numeric_dtype(y):\n        print(\"Target is non-numeric ‚Äî applying label encoding.\")\n        y = pd.Series(pd.factorize(y)[0], index=y.index)\n\n    # detect categorical features in X\n    cat_cols = detect_categorical(X)\n    # remove any accidental numeric-id-like high-cardinality columns from cat list\n    refined_cat_cols = []\n    for c in cat_cols:\n        nunq = X[c].nunique(dropna=True)\n        if nunq <= min(100, max(10, int(0.5 * len(X)))):\n            refined_cat_cols.append(c)\n    cat_cols = refined_cat_cols\n\n    print(\"Categorical columns to pass to CatBoost:\", cat_cols)\n\n    # convert categorical columns to string (CatBoost can accept them)\n    for c in cat_cols:\n        X[c] = X[c].astype(str).fillna(\"##nan##\")  \n\n    # train-test split\n    stratify = y if len(np.unique(y)) > 1 else None\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=test_size, random_state=random_state, stratify=stratify\n    )\n\n    cat_indices = [X.columns.get_loc(c) for c in cat_cols if c in X.columns]\n\n    train_pool = Pool(X_train, y_train, cat_features=cat_indices)\n    test_pool = Pool(X_test, y_test, cat_features=cat_indices)\n\n    n_classes = len(np.unique(y_train))\n    is_binary = (n_classes == 2)\n\n    model = CatBoostClassifier(\n        iterations=700,\n        learning_rate=0.05,\n        depth=6,\n        eval_metric='AUC' if is_binary else 'MultiClass',\n        loss_function='Logloss' if is_binary else 'MultiClass',\n        random_seed=random_state,\n        early_stopping_rounds=50,\n        verbose=100\n    )\n\n    print(\"\\nTraining CatBoost...\")\n    model.fit(train_pool, eval_set=test_pool, use_best_model=True)\n\n    preds = model.predict(X_test)\n    if isinstance(preds, np.ndarray) and preds.ndim > 1:\n        preds = preds.flatten()\n\n    try:\n        pred_proba = model.predict_proba(X_test)\n    except Exception:\n        pred_proba = None\n\n    print(\"\\n===== METRICS =====\")\n    print(\"Accuracy:\", accuracy_score(y_test, preds))\n\n    if is_binary:\n        print(\"Precision:\", precision_score(y_test, preds, zero_division=0))\n        print(\"Recall:\", recall_score(y_test, preds, zero_division=0))\n        print(\"F1 Score:\", f1_score(y_test, preds, zero_division=0))\n        if pred_proba is not None:\n            try:\n                print(\"ROC AUC:\", roc_auc_score(y_test, pred_proba[:, 1]))\n            except Exception as e:\n                print(\"ROC AUC could not be computed:\", e)\n    else:\n        print(\"Macro Precision:\", precision_score(y_test, preds, average=\"macro\", zero_division=0))\n        print(\"Macro Recall:\", recall_score(y_test, preds, average=\"macro\", zero_division=0))\n        print(\"Macro F1:\", f1_score(y_test, preds, average=\"macro\", zero_division=0))\n\n    print(\"\\nClassification Report:\\n\", classification_report(y_test, preds, zero_division=0))\n    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, preds))\n\n    # optionally save model\n    # model.save_model(\"/kaggle/working/catboost_label_model.cbm\")\n\n    return model\n\nif __name__ == \"__main__\":\n    model = main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:07:49.913029Z","iopub.status.idle":"2025-12-05T11:07:49.913305Z","shell.execute_reply.started":"2025-12-05T11:07:49.913167Z","shell.execute_reply":"2025-12-05T11:07:49.913179Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/networkkk/loan_final_normalized (1).csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:07:49.914219Z","iopub.status.idle":"2025-12-05T11:07:49.914593Z","shell.execute_reply.started":"2025-12-05T11:07:49.914400Z","shell.execute_reply":"2025-12-05T11:07:49.914417Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:07:49.915812Z","iopub.status.idle":"2025-12-05T11:07:49.916180Z","shell.execute_reply.started":"2025-12-05T11:07:49.916002Z","shell.execute_reply":"2025-12-05T11:07:49.916018Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"(df[\"attack_cat\"] == 0).astype(int).value_counts()\ndf.groupby(\"attack_cat\")[\"label\"].unique()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:07:49.917214Z","iopub.status.idle":"2025-12-05T11:07:49.917538Z","shell.execute_reply.started":"2025-12-05T11:07:49.917374Z","shell.execute_reply":"2025-12-05T11:07:49.917389Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:07:49.919154Z","iopub.status.idle":"2025-12-05T11:07:49.919456Z","shell.execute_reply.started":"2025-12-05T11:07:49.919319Z","shell.execute_reply":"2025-12-05T11:07:49.919333Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import kagglehub\n\n# Download latest version\npath = kagglehub.dataset_download(\"dhoogla/cicids2017\")\n\nprint(\"Path to dataset files:\", path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:07:49.922683Z","iopub.status.idle":"2025-12-05T11:07:49.923009Z","shell.execute_reply.started":"2025-12-05T11:07:49.922845Z","shell.execute_reply":"2025-12-05T11:07:49.922860Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Load the Parquet file\nfile_path = \"/kaggle/input/cicids2017/Benign-Monday-no-metadata.parquet\"\ndf = pd.read_parquet(file_path)\n\n# Shape: rows & columns\nprint(\"Number of rows:\", df.shape[0])\nprint(\"Number of columns:\", df.shape[1])\n\n# Column names\nprint(\"\\nColumn names:\")\nprint(df.columns.tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:07:49.923928Z","iopub.status.idle":"2025-12-05T11:07:49.924175Z","shell.execute_reply.started":"2025-12-05T11:07:49.924063Z","shell.execute_reply":"2025-12-05T11:07:49.924073Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Unique values in the label column\nlabel_col = \"Label\"   # change if the column name differs\nif label_col in df.columns:\n    print(\"\\nUnique values in Label column:\")\n    print(df[label_col].unique())\nelse:\n    print(\"\\nNo 'Label' column found. Available columns:\")\n    print(df.columns.tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:07:49.924726Z","iopub.status.idle":"2025-12-05T11:07:49.925275Z","shell.execute_reply.started":"2025-12-05T11:07:49.925098Z","shell.execute_reply":"2025-12-05T11:07:49.925117Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Load the Parquet file\nfile_path = \"/kaggle/input/cicids2017/Botnet-Friday-no-metadata.parquet\"\ndf = pd.read_parquet(file_path)\n\n# Shape: rows & columns\nprint(\"Number of rows:\", df.shape[0])\nprint(\"Number of columns:\", df.shape[1])\n\n# Column names\nprint(\"\\nColumn names:\")\nprint(df.columns.tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:39:33.705663Z","iopub.execute_input":"2025-12-05T11:39:33.706413Z","iopub.status.idle":"2025-12-05T11:39:33.778171Z","shell.execute_reply.started":"2025-12-05T11:39:33.706385Z","shell.execute_reply":"2025-12-05T11:39:33.777530Z"}},"outputs":[{"name":"stdout","text":"Number of rows: 176038\nNumber of columns: 78\n\nColumn names:\n['Protocol', 'Flow Duration', 'Total Fwd Packets', 'Total Backward Packets', 'Fwd Packets Length Total', 'Bwd Packets Length Total', 'Fwd Packet Length Max', 'Fwd Packet Length Min', 'Fwd Packet Length Mean', 'Fwd Packet Length Std', 'Bwd Packet Length Max', 'Bwd Packet Length Min', 'Bwd Packet Length Mean', 'Bwd Packet Length Std', 'Flow Bytes/s', 'Flow Packets/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Total', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Total', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Header Length', 'Bwd Header Length', 'Fwd Packets/s', 'Bwd Packets/s', 'Packet Length Min', 'Packet Length Max', 'Packet Length Mean', 'Packet Length Std', 'Packet Length Variance', 'FIN Flag Count', 'SYN Flag Count', 'RST Flag Count', 'PSH Flag Count', 'ACK Flag Count', 'URG Flag Count', 'CWE Flag Count', 'ECE Flag Count', 'Down/Up Ratio', 'Avg Packet Size', 'Avg Fwd Segment Size', 'Avg Bwd Segment Size', 'Fwd Avg Bytes/Bulk', 'Fwd Avg Packets/Bulk', 'Fwd Avg Bulk Rate', 'Bwd Avg Bytes/Bulk', 'Bwd Avg Packets/Bulk', 'Bwd Avg Bulk Rate', 'Subflow Fwd Packets', 'Subflow Fwd Bytes', 'Subflow Bwd Packets', 'Subflow Bwd Bytes', 'Init Fwd Win Bytes', 'Init Bwd Win Bytes', 'Fwd Act Data Packets', 'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min', 'Label']\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"# Unique values in the label column\nlabel_col = \"Label\"   # change if the column name differs\nif label_col in df.columns:\n    print(\"\\nUnique values in Label column:\")\n    print(df[label_col].unique())\nelse:\n    print(\"\\nNo 'Label' column found. Available columns:\")\n    print(df.columns.tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:39:37.954866Z","iopub.execute_input":"2025-12-05T11:39:37.955124Z","iopub.status.idle":"2025-12-05T11:39:37.961249Z","shell.execute_reply.started":"2025-12-05T11:39:37.955107Z","shell.execute_reply":"2025-12-05T11:39:37.960603Z"}},"outputs":[{"name":"stdout","text":"\nUnique values in Label column:\n['Benign', 'Bot']\nCategories (2, object): ['Benign', 'Bot']\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"# Unique values in the label column + their counts\nlabel_col = \"Label\"   # change if needed\n\nif label_col in df.columns:\n    print(\"\\nUnique values and counts in Label column:\")\n    print(df[label_col].value_counts())\nelse:\n    print(\"\\nNo 'Label' column found. Available columns:\")\n    print(df.columns.tolist())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:39:41.054821Z","iopub.execute_input":"2025-12-05T11:39:41.055122Z","iopub.status.idle":"2025-12-05T11:39:41.062730Z","shell.execute_reply.started":"2025-12-05T11:39:41.055101Z","shell.execute_reply":"2025-12-05T11:39:41.061744Z"}},"outputs":[{"name":"stdout","text":"\nUnique values and counts in Label column:\nLabel\nBenign    174601\nBot         1437\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"import pandas as pd\n\n# Load the Parquet file\nfile_path = \"/kaggle/input/cicids2017/Bruteforce-Tuesday-no-metadata.parquet\"\ndf = pd.read_parquet(file_path)\n\n# Shape: rows & columns\nprint(\"Number of rows:\", df.shape[0])\nprint(\"Number of columns:\", df.shape[1])\n\n# Column names\nprint(\"\\nColumn names:\")\nprint(df.columns.tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:39:44.594621Z","iopub.execute_input":"2025-12-05T11:39:44.595368Z","iopub.status.idle":"2025-12-05T11:39:45.444601Z","shell.execute_reply.started":"2025-12-05T11:39:44.595341Z","shell.execute_reply":"2025-12-05T11:39:45.443673Z"}},"outputs":[{"name":"stdout","text":"Number of rows: 389714\nNumber of columns: 78\n\nColumn names:\n['Protocol', 'Flow Duration', 'Total Fwd Packets', 'Total Backward Packets', 'Fwd Packets Length Total', 'Bwd Packets Length Total', 'Fwd Packet Length Max', 'Fwd Packet Length Min', 'Fwd Packet Length Mean', 'Fwd Packet Length Std', 'Bwd Packet Length Max', 'Bwd Packet Length Min', 'Bwd Packet Length Mean', 'Bwd Packet Length Std', 'Flow Bytes/s', 'Flow Packets/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Total', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Total', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Header Length', 'Bwd Header Length', 'Fwd Packets/s', 'Bwd Packets/s', 'Packet Length Min', 'Packet Length Max', 'Packet Length Mean', 'Packet Length Std', 'Packet Length Variance', 'FIN Flag Count', 'SYN Flag Count', 'RST Flag Count', 'PSH Flag Count', 'ACK Flag Count', 'URG Flag Count', 'CWE Flag Count', 'ECE Flag Count', 'Down/Up Ratio', 'Avg Packet Size', 'Avg Fwd Segment Size', 'Avg Bwd Segment Size', 'Fwd Avg Bytes/Bulk', 'Fwd Avg Packets/Bulk', 'Fwd Avg Bulk Rate', 'Bwd Avg Bytes/Bulk', 'Bwd Avg Packets/Bulk', 'Bwd Avg Bulk Rate', 'Subflow Fwd Packets', 'Subflow Fwd Bytes', 'Subflow Bwd Packets', 'Subflow Bwd Bytes', 'Init Fwd Win Bytes', 'Init Bwd Win Bytes', 'Fwd Act Data Packets', 'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min', 'Label']\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"# Unique values in the label column + their counts\nlabel_col = \"Label\"   # change if needed\n\nif label_col in df.columns:\n    print(\"\\nUnique values and counts in Label column:\")\n    print(df[label_col].value_counts())\nelse:\n    print(\"\\nNo 'Label' column found. Available columns:\")\n    print(df.columns.tolist())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:39:49.114572Z","iopub.execute_input":"2025-12-05T11:39:49.115604Z","iopub.status.idle":"2025-12-05T11:39:49.123183Z","shell.execute_reply.started":"2025-12-05T11:39:49.115571Z","shell.execute_reply":"2025-12-05T11:39:49.122445Z"}},"outputs":[{"name":"stdout","text":"\nUnique values and counts in Label column:\nLabel\nBenign         380564\nFTP-Patator      5931\nSSH-Patator      3219\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":47},{"cell_type":"code","source":"import pandas as pd\n\n# Load the Parquet file\nfile_path = \"/kaggle/input/cicids2017/DDoS-Friday-no-metadata.parquet\"\ndf = pd.read_parquet(file_path)\n\n# Shape: rows & columns\nprint(\"Number of rows:\", df.shape[0])\nprint(\"Number of columns:\", df.shape[1])\n\n# Column names\nprint(\"\\nColumn names:\")\nprint(df.columns.tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:39:53.804338Z","iopub.execute_input":"2025-12-05T11:39:53.804871Z","iopub.status.idle":"2025-12-05T11:39:54.233215Z","shell.execute_reply.started":"2025-12-05T11:39:53.804848Z","shell.execute_reply":"2025-12-05T11:39:54.232585Z"}},"outputs":[{"name":"stdout","text":"Number of rows: 221264\nNumber of columns: 78\n\nColumn names:\n['Protocol', 'Flow Duration', 'Total Fwd Packets', 'Total Backward Packets', 'Fwd Packets Length Total', 'Bwd Packets Length Total', 'Fwd Packet Length Max', 'Fwd Packet Length Min', 'Fwd Packet Length Mean', 'Fwd Packet Length Std', 'Bwd Packet Length Max', 'Bwd Packet Length Min', 'Bwd Packet Length Mean', 'Bwd Packet Length Std', 'Flow Bytes/s', 'Flow Packets/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Total', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Total', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Header Length', 'Bwd Header Length', 'Fwd Packets/s', 'Bwd Packets/s', 'Packet Length Min', 'Packet Length Max', 'Packet Length Mean', 'Packet Length Std', 'Packet Length Variance', 'FIN Flag Count', 'SYN Flag Count', 'RST Flag Count', 'PSH Flag Count', 'ACK Flag Count', 'URG Flag Count', 'CWE Flag Count', 'ECE Flag Count', 'Down/Up Ratio', 'Avg Packet Size', 'Avg Fwd Segment Size', 'Avg Bwd Segment Size', 'Fwd Avg Bytes/Bulk', 'Fwd Avg Packets/Bulk', 'Fwd Avg Bulk Rate', 'Bwd Avg Bytes/Bulk', 'Bwd Avg Packets/Bulk', 'Bwd Avg Bulk Rate', 'Subflow Fwd Packets', 'Subflow Fwd Bytes', 'Subflow Bwd Packets', 'Subflow Bwd Bytes', 'Init Fwd Win Bytes', 'Init Bwd Win Bytes', 'Fwd Act Data Packets', 'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min', 'Label']\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"# Unique values in the label column + their counts\nlabel_col = \"Label\"   # change if needed\n\nif label_col in df.columns:\n    print(\"\\nUnique values and counts in Label column:\")\n    print(df[label_col].value_counts())\nelse:\n    print(\"\\nNo 'Label' column found. Available columns:\")\n    print(df.columns.tolist())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:39:58.244335Z","iopub.execute_input":"2025-12-05T11:39:58.244599Z","iopub.status.idle":"2025-12-05T11:39:58.251485Z","shell.execute_reply.started":"2025-12-05T11:39:58.244580Z","shell.execute_reply":"2025-12-05T11:39:58.250734Z"}},"outputs":[{"name":"stdout","text":"\nUnique values and counts in Label column:\nLabel\nDDoS      128014\nBenign     93250\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":"import pandas as pd\n\n# Load the Parquet file\nfile_path = \"/kaggle/input/cicids2017/DoS-Wednesday-no-metadata.parquet\"\ndf = pd.read_parquet(file_path)\n\n# Shape: rows & columns\nprint(\"Number of rows:\", df.shape[0])\nprint(\"Number of columns:\", df.shape[1])\n\n# Column names\nprint(\"\\nColumn names:\")\nprint(df.columns.tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:40:01.344817Z","iopub.execute_input":"2025-12-05T11:40:01.345294Z","iopub.status.idle":"2025-12-05T11:40:02.336162Z","shell.execute_reply.started":"2025-12-05T11:40:01.345265Z","shell.execute_reply":"2025-12-05T11:40:02.335390Z"}},"outputs":[{"name":"stdout","text":"Number of rows: 584991\nNumber of columns: 78\n\nColumn names:\n['Protocol', 'Flow Duration', 'Total Fwd Packets', 'Total Backward Packets', 'Fwd Packets Length Total', 'Bwd Packets Length Total', 'Fwd Packet Length Max', 'Fwd Packet Length Min', 'Fwd Packet Length Mean', 'Fwd Packet Length Std', 'Bwd Packet Length Max', 'Bwd Packet Length Min', 'Bwd Packet Length Mean', 'Bwd Packet Length Std', 'Flow Bytes/s', 'Flow Packets/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Total', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Total', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Header Length', 'Bwd Header Length', 'Fwd Packets/s', 'Bwd Packets/s', 'Packet Length Min', 'Packet Length Max', 'Packet Length Mean', 'Packet Length Std', 'Packet Length Variance', 'FIN Flag Count', 'SYN Flag Count', 'RST Flag Count', 'PSH Flag Count', 'ACK Flag Count', 'URG Flag Count', 'CWE Flag Count', 'ECE Flag Count', 'Down/Up Ratio', 'Avg Packet Size', 'Avg Fwd Segment Size', 'Avg Bwd Segment Size', 'Fwd Avg Bytes/Bulk', 'Fwd Avg Packets/Bulk', 'Fwd Avg Bulk Rate', 'Bwd Avg Bytes/Bulk', 'Bwd Avg Packets/Bulk', 'Bwd Avg Bulk Rate', 'Subflow Fwd Packets', 'Subflow Fwd Bytes', 'Subflow Bwd Packets', 'Subflow Bwd Bytes', 'Init Fwd Win Bytes', 'Init Bwd Win Bytes', 'Fwd Act Data Packets', 'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min', 'Label']\n","output_type":"stream"}],"execution_count":50},{"cell_type":"code","source":"# Unique values in the label column + their counts\nlabel_col = \"Label\"   # change if needed\n\nif label_col in df.columns:\n    print(\"\\nUnique values and counts in Label column:\")\n    print(df[label_col].value_counts())\nelse:\n    print(\"\\nNo 'Label' column found. Available columns:\")\n    print(df.columns.tolist())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:40:05.614529Z","iopub.execute_input":"2025-12-05T11:40:05.615031Z","iopub.status.idle":"2025-12-05T11:40:05.623361Z","shell.execute_reply.started":"2025-12-05T11:40:05.615010Z","shell.execute_reply":"2025-12-05T11:40:05.622579Z"}},"outputs":[{"name":"stdout","text":"\nUnique values and counts in Label column:\nLabel\nBenign              391235\nDoS Hulk            172846\nDoS GoldenEye        10286\nDoS slowloris         5385\nDoS Slowhttptest      5228\nHeartbleed              11\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":51},{"cell_type":"code","source":"import pandas as pd\n\n# Load the Parquet file\nfile_path = \"/kaggle/input/cicids2017/Infiltration-Thursday-no-metadata.parquet\"\ndf = pd.read_parquet(file_path)\n\n# Shape: rows & columns\nprint(\"Number of rows:\", df.shape[0])\nprint(\"Number of columns:\", df.shape[1])\n\n# Column names\nprint(\"\\nColumn names:\")\nprint(df.columns.tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:40:09.139922Z","iopub.execute_input":"2025-12-05T11:40:09.140765Z","iopub.status.idle":"2025-12-05T11:40:09.418622Z","shell.execute_reply.started":"2025-12-05T11:40:09.140734Z","shell.execute_reply":"2025-12-05T11:40:09.417881Z"}},"outputs":[{"name":"stdout","text":"Number of rows: 207630\nNumber of columns: 78\n\nColumn names:\n['Protocol', 'Flow Duration', 'Total Fwd Packets', 'Total Backward Packets', 'Fwd Packets Length Total', 'Bwd Packets Length Total', 'Fwd Packet Length Max', 'Fwd Packet Length Min', 'Fwd Packet Length Mean', 'Fwd Packet Length Std', 'Bwd Packet Length Max', 'Bwd Packet Length Min', 'Bwd Packet Length Mean', 'Bwd Packet Length Std', 'Flow Bytes/s', 'Flow Packets/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Total', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Total', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Header Length', 'Bwd Header Length', 'Fwd Packets/s', 'Bwd Packets/s', 'Packet Length Min', 'Packet Length Max', 'Packet Length Mean', 'Packet Length Std', 'Packet Length Variance', 'FIN Flag Count', 'SYN Flag Count', 'RST Flag Count', 'PSH Flag Count', 'ACK Flag Count', 'URG Flag Count', 'CWE Flag Count', 'ECE Flag Count', 'Down/Up Ratio', 'Avg Packet Size', 'Avg Fwd Segment Size', 'Avg Bwd Segment Size', 'Fwd Avg Bytes/Bulk', 'Fwd Avg Packets/Bulk', 'Fwd Avg Bulk Rate', 'Bwd Avg Bytes/Bulk', 'Bwd Avg Packets/Bulk', 'Bwd Avg Bulk Rate', 'Subflow Fwd Packets', 'Subflow Fwd Bytes', 'Subflow Bwd Packets', 'Subflow Bwd Bytes', 'Init Fwd Win Bytes', 'Init Bwd Win Bytes', 'Fwd Act Data Packets', 'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min', 'Label']\n","output_type":"stream"}],"execution_count":52},{"cell_type":"code","source":"# Unique values in the label column + their counts\nlabel_col = \"Label\"   # change if needed\n\nif label_col in df.columns:\n    print(\"\\nUnique values and counts in Label column:\")\n    print(df[label_col].value_counts())\nelse:\n    print(\"\\nNo 'Label' column found. Available columns:\")\n    print(df.columns.tolist())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:40:13.184396Z","iopub.execute_input":"2025-12-05T11:40:13.184657Z","iopub.status.idle":"2025-12-05T11:40:13.192066Z","shell.execute_reply.started":"2025-12-05T11:40:13.184636Z","shell.execute_reply":"2025-12-05T11:40:13.191224Z"}},"outputs":[{"name":"stdout","text":"\nUnique values and counts in Label column:\nLabel\nBenign          207594\nInfiltration        36\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":53},{"cell_type":"code","source":"import pandas as pd\n\n# Load the Parquet file\nfile_path = \"/kaggle/input/cicids2017/Portscan-Friday-no-metadata.parquet\"\ndf = pd.read_parquet(file_path)\n\n# Shape: rows & columns\nprint(\"Number of rows:\", df.shape[0])\nprint(\"Number of columns:\", df.shape[1])\n\n# Column names\nprint(\"\\nColumn names:\")\nprint(df.columns.tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:40:16.964738Z","iopub.execute_input":"2025-12-05T11:40:16.965040Z","iopub.status.idle":"2025-12-05T11:40:17.158961Z","shell.execute_reply.started":"2025-12-05T11:40:16.965017Z","shell.execute_reply":"2025-12-05T11:40:17.158013Z"}},"outputs":[{"name":"stdout","text":"Number of rows: 119522\nNumber of columns: 78\n\nColumn names:\n['Protocol', 'Flow Duration', 'Total Fwd Packets', 'Total Backward Packets', 'Fwd Packets Length Total', 'Bwd Packets Length Total', 'Fwd Packet Length Max', 'Fwd Packet Length Min', 'Fwd Packet Length Mean', 'Fwd Packet Length Std', 'Bwd Packet Length Max', 'Bwd Packet Length Min', 'Bwd Packet Length Mean', 'Bwd Packet Length Std', 'Flow Bytes/s', 'Flow Packets/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Total', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Total', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Header Length', 'Bwd Header Length', 'Fwd Packets/s', 'Bwd Packets/s', 'Packet Length Min', 'Packet Length Max', 'Packet Length Mean', 'Packet Length Std', 'Packet Length Variance', 'FIN Flag Count', 'SYN Flag Count', 'RST Flag Count', 'PSH Flag Count', 'ACK Flag Count', 'URG Flag Count', 'CWE Flag Count', 'ECE Flag Count', 'Down/Up Ratio', 'Avg Packet Size', 'Avg Fwd Segment Size', 'Avg Bwd Segment Size', 'Fwd Avg Bytes/Bulk', 'Fwd Avg Packets/Bulk', 'Fwd Avg Bulk Rate', 'Bwd Avg Bytes/Bulk', 'Bwd Avg Packets/Bulk', 'Bwd Avg Bulk Rate', 'Subflow Fwd Packets', 'Subflow Fwd Bytes', 'Subflow Bwd Packets', 'Subflow Bwd Bytes', 'Init Fwd Win Bytes', 'Init Bwd Win Bytes', 'Fwd Act Data Packets', 'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min', 'Label']\n","output_type":"stream"}],"execution_count":54},{"cell_type":"code","source":"# Unique values in the label column + their counts\nlabel_col = \"Label\"   # change if needed\n\nif label_col in df.columns:\n    print(\"\\nUnique values and counts in Label column:\")\n    print(df[label_col].value_counts())\nelse:\n    print(\"\\nNo 'Label' column found. Available columns:\")\n    print(df.columns.tolist())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:40:20.439739Z","iopub.execute_input":"2025-12-05T11:40:20.440028Z","iopub.status.idle":"2025-12-05T11:40:20.447021Z","shell.execute_reply.started":"2025-12-05T11:40:20.440006Z","shell.execute_reply":"2025-12-05T11:40:20.446232Z"}},"outputs":[{"name":"stdout","text":"\nUnique values and counts in Label column:\nLabel\nBenign      117566\nPortScan      1956\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":55},{"cell_type":"code","source":"import pandas as pd\n\n# Load the Parquet file\nfile_path = \"/kaggle/input/cicids2017/WebAttacks-Thursday-no-metadata.parquet\"\ndf = pd.read_parquet(file_path)\n\n# Shape: rows & columns\nprint(\"Number of rows:\", df.shape[0])\nprint(\"Number of columns:\", df.shape[1])\n\n# Column names\nprint(\"\\nColumn names:\")\nprint(df.columns.tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:40:23.494593Z","iopub.execute_input":"2025-12-05T11:40:23.495295Z","iopub.status.idle":"2025-12-05T11:40:23.790263Z","shell.execute_reply.started":"2025-12-05T11:40:23.495268Z","shell.execute_reply":"2025-12-05T11:40:23.789479Z"}},"outputs":[{"name":"stdout","text":"Number of rows: 155820\nNumber of columns: 78\n\nColumn names:\n['Protocol', 'Flow Duration', 'Total Fwd Packets', 'Total Backward Packets', 'Fwd Packets Length Total', 'Bwd Packets Length Total', 'Fwd Packet Length Max', 'Fwd Packet Length Min', 'Fwd Packet Length Mean', 'Fwd Packet Length Std', 'Bwd Packet Length Max', 'Bwd Packet Length Min', 'Bwd Packet Length Mean', 'Bwd Packet Length Std', 'Flow Bytes/s', 'Flow Packets/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Total', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Total', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Header Length', 'Bwd Header Length', 'Fwd Packets/s', 'Bwd Packets/s', 'Packet Length Min', 'Packet Length Max', 'Packet Length Mean', 'Packet Length Std', 'Packet Length Variance', 'FIN Flag Count', 'SYN Flag Count', 'RST Flag Count', 'PSH Flag Count', 'ACK Flag Count', 'URG Flag Count', 'CWE Flag Count', 'ECE Flag Count', 'Down/Up Ratio', 'Avg Packet Size', 'Avg Fwd Segment Size', 'Avg Bwd Segment Size', 'Fwd Avg Bytes/Bulk', 'Fwd Avg Packets/Bulk', 'Fwd Avg Bulk Rate', 'Bwd Avg Bytes/Bulk', 'Bwd Avg Packets/Bulk', 'Bwd Avg Bulk Rate', 'Subflow Fwd Packets', 'Subflow Fwd Bytes', 'Subflow Bwd Packets', 'Subflow Bwd Bytes', 'Init Fwd Win Bytes', 'Init Bwd Win Bytes', 'Fwd Act Data Packets', 'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min', 'Label']\n","output_type":"stream"}],"execution_count":56},{"cell_type":"code","source":"# Unique values in the label column + their counts\nlabel_col = \"Label\"   # change if needed\n\nif label_col in df.columns:\n    print(\"\\nUnique values and counts in Label column:\")\n    print(df[label_col].value_counts())\nelse:\n    print(\"\\nNo 'Label' column found. Available columns:\")\n    print(df.columns.tolist())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:40:27.084868Z","iopub.execute_input":"2025-12-05T11:40:27.085477Z","iopub.status.idle":"2025-12-05T11:40:27.092472Z","shell.execute_reply.started":"2025-12-05T11:40:27.085453Z","shell.execute_reply":"2025-12-05T11:40:27.091618Z"}},"outputs":[{"name":"stdout","text":"\nUnique values and counts in Label column:\nLabel\nBenign                        153677\nWeb Attack ÔøΩ Brute Force        1470\nWeb Attack ÔøΩ XSS                 652\nWeb Attack ÔøΩ Sql Injection        21\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":57},{"cell_type":"markdown","source":"IDS 2018","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Load the Parquet file\nfile_path = \"/kaggle/input/ids-intrusion-csv/02-14-2018.csv\"\ndf = pd.read_csv(file_path)\n\n# Shape: rows & columns\nprint(\"Number of rows:\", df.shape[0])\nprint(\"Number of columns:\", df.shape[1])\n\n# Column names\nprint(\"\\nColumn names:\")\nprint(df.columns.tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:40:31.404544Z","iopub.execute_input":"2025-12-05T11:40:31.405228Z","iopub.status.idle":"2025-12-05T11:40:41.510479Z","shell.execute_reply.started":"2025-12-05T11:40:31.405177Z","shell.execute_reply":"2025-12-05T11:40:41.509751Z"}},"outputs":[{"name":"stdout","text":"Number of rows: 1048575\nNumber of columns: 80\n\nColumn names:\n['Dst Port', 'Protocol', 'Timestamp', 'Flow Duration', 'Tot Fwd Pkts', 'Tot Bwd Pkts', 'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std', 'Bwd Pkt Len Max', 'Bwd Pkt Len Min', 'Bwd Pkt Len Mean', 'Bwd Pkt Len Std', 'Flow Byts/s', 'Flow Pkts/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Tot', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Tot', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Header Len', 'Bwd Header Len', 'Fwd Pkts/s', 'Bwd Pkts/s', 'Pkt Len Min', 'Pkt Len Max', 'Pkt Len Mean', 'Pkt Len Std', 'Pkt Len Var', 'FIN Flag Cnt', 'SYN Flag Cnt', 'RST Flag Cnt', 'PSH Flag Cnt', 'ACK Flag Cnt', 'URG Flag Cnt', 'CWE Flag Count', 'ECE Flag Cnt', 'Down/Up Ratio', 'Pkt Size Avg', 'Fwd Seg Size Avg', 'Bwd Seg Size Avg', 'Fwd Byts/b Avg', 'Fwd Pkts/b Avg', 'Fwd Blk Rate Avg', 'Bwd Byts/b Avg', 'Bwd Pkts/b Avg', 'Bwd Blk Rate Avg', 'Subflow Fwd Pkts', 'Subflow Fwd Byts', 'Subflow Bwd Pkts', 'Subflow Bwd Byts', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min', 'Label']\n","output_type":"stream"}],"execution_count":58},{"cell_type":"code","source":"# Unique values in the label column + their counts\nlabel_col = \"Label\"   # change if needed\n\nif label_col in df.columns:\n    print(\"\\nUnique values and counts in Label column:\")\n    print(df[label_col].value_counts())\nelse:\n    print(\"\\nNo 'Label' column found. Available columns:\")\n    print(df.columns.tolist())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:40:45.934818Z","iopub.execute_input":"2025-12-05T11:40:45.935139Z","iopub.status.idle":"2025-12-05T11:40:46.024224Z","shell.execute_reply.started":"2025-12-05T11:40:45.935115Z","shell.execute_reply":"2025-12-05T11:40:46.023254Z"}},"outputs":[{"name":"stdout","text":"\nUnique values and counts in Label column:\nLabel\nBenign            667626\nFTP-BruteForce    193360\nSSH-Bruteforce    187589\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":59},{"cell_type":"markdown","source":"BRUTE FORCE IDS","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Load the Parquet file\nfile_path = \"/kaggle/input/ids-intrusion-csv/02-15-2018.csv\"\ndf = pd.read_csv(file_path)\n\n# Shape: rows & columns\nprint(\"Number of rows:\", df.shape[0])\nprint(\"Number of columns:\", df.shape[1])\n\n# Column names\nprint(\"\\nColumn names:\")\nprint(df.columns.tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:40:49.585019Z","iopub.execute_input":"2025-12-05T11:40:49.585615Z","iopub.status.idle":"2025-12-05T11:41:00.261701Z","shell.execute_reply.started":"2025-12-05T11:40:49.585590Z","shell.execute_reply":"2025-12-05T11:41:00.260868Z"}},"outputs":[{"name":"stdout","text":"Number of rows: 1048575\nNumber of columns: 80\n\nColumn names:\n['Dst Port', 'Protocol', 'Timestamp', 'Flow Duration', 'Tot Fwd Pkts', 'Tot Bwd Pkts', 'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std', 'Bwd Pkt Len Max', 'Bwd Pkt Len Min', 'Bwd Pkt Len Mean', 'Bwd Pkt Len Std', 'Flow Byts/s', 'Flow Pkts/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Tot', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Tot', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Header Len', 'Bwd Header Len', 'Fwd Pkts/s', 'Bwd Pkts/s', 'Pkt Len Min', 'Pkt Len Max', 'Pkt Len Mean', 'Pkt Len Std', 'Pkt Len Var', 'FIN Flag Cnt', 'SYN Flag Cnt', 'RST Flag Cnt', 'PSH Flag Cnt', 'ACK Flag Cnt', 'URG Flag Cnt', 'CWE Flag Count', 'ECE Flag Cnt', 'Down/Up Ratio', 'Pkt Size Avg', 'Fwd Seg Size Avg', 'Bwd Seg Size Avg', 'Fwd Byts/b Avg', 'Fwd Pkts/b Avg', 'Fwd Blk Rate Avg', 'Bwd Byts/b Avg', 'Bwd Pkts/b Avg', 'Bwd Blk Rate Avg', 'Subflow Fwd Pkts', 'Subflow Fwd Byts', 'Subflow Bwd Pkts', 'Subflow Bwd Byts', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min', 'Label']\n","output_type":"stream"}],"execution_count":60},{"cell_type":"code","source":"# Unique values in the label column + their counts\nlabel_col = \"Label\"   # change if needed\n\nif label_col in df.columns:\n    print(\"\\nUnique values and counts in Label column:\")\n    print(df[label_col].value_counts())\nelse:\n    print(\"\\nNo 'Label' column found. Available columns:\")\n    print(df.columns.tolist())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:41:01.545361Z","iopub.execute_input":"2025-12-05T11:41:01.545953Z","iopub.status.idle":"2025-12-05T11:41:01.618890Z","shell.execute_reply.started":"2025-12-05T11:41:01.545930Z","shell.execute_reply":"2025-12-05T11:41:01.618148Z"}},"outputs":[{"name":"stdout","text":"\nUnique values and counts in Label column:\nLabel\nBenign                   996077\nDoS attacks-GoldenEye     41508\nDoS attacks-Slowloris     10990\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":61},{"cell_type":"markdown","source":"Dos attack","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Load the Parquet file\nfile_path = \"/kaggle/input/ids-intrusion-csv/02-16-2018.csv\"\ndf = pd.read_csv(file_path)\n\n# Shape: rows & columns\nprint(\"Number of rows:\", df.shape[0])\nprint(\"Number of columns:\", df.shape[1])\n\n# Column names\nprint(\"\\nColumn names:\")\nprint(df.columns.tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:41:05.094578Z","iopub.execute_input":"2025-12-05T11:41:05.095295Z","iopub.status.idle":"2025-12-05T11:41:15.841954Z","shell.execute_reply.started":"2025-12-05T11:41:05.095270Z","shell.execute_reply":"2025-12-05T11:41:15.841273Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_47/1221501978.py:5: DtypeWarning: Columns (0,1,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\n","output_type":"stream"},{"name":"stdout","text":"Number of rows: 1048575\nNumber of columns: 80\n\nColumn names:\n['Dst Port', 'Protocol', 'Timestamp', 'Flow Duration', 'Tot Fwd Pkts', 'Tot Bwd Pkts', 'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std', 'Bwd Pkt Len Max', 'Bwd Pkt Len Min', 'Bwd Pkt Len Mean', 'Bwd Pkt Len Std', 'Flow Byts/s', 'Flow Pkts/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Tot', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Tot', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Header Len', 'Bwd Header Len', 'Fwd Pkts/s', 'Bwd Pkts/s', 'Pkt Len Min', 'Pkt Len Max', 'Pkt Len Mean', 'Pkt Len Std', 'Pkt Len Var', 'FIN Flag Cnt', 'SYN Flag Cnt', 'RST Flag Cnt', 'PSH Flag Cnt', 'ACK Flag Cnt', 'URG Flag Cnt', 'CWE Flag Count', 'ECE Flag Cnt', 'Down/Up Ratio', 'Pkt Size Avg', 'Fwd Seg Size Avg', 'Bwd Seg Size Avg', 'Fwd Byts/b Avg', 'Fwd Pkts/b Avg', 'Fwd Blk Rate Avg', 'Bwd Byts/b Avg', 'Bwd Pkts/b Avg', 'Bwd Blk Rate Avg', 'Subflow Fwd Pkts', 'Subflow Fwd Byts', 'Subflow Bwd Pkts', 'Subflow Bwd Byts', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min', 'Label']\n","output_type":"stream"}],"execution_count":62},{"cell_type":"code","source":"# Unique values in the label column + their counts\nlabel_col = \"Label\"   # change if needed\n\nif label_col in df.columns:\n    print(\"\\nUnique values and counts in Label column:\")\n    print(df[label_col].value_counts())\nelse:\n    print(\"\\nNo 'Label' column found. Available columns:\")\n    print(df.columns.tolist())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:41:17.234730Z","iopub.execute_input":"2025-12-05T11:41:17.235264Z","iopub.status.idle":"2025-12-05T11:41:17.311611Z","shell.execute_reply.started":"2025-12-05T11:41:17.235235Z","shell.execute_reply":"2025-12-05T11:41:17.310705Z"}},"outputs":[{"name":"stdout","text":"\nUnique values and counts in Label column:\nLabel\nDoS attacks-Hulk            461912\nBenign                      446772\nDoS attacks-SlowHTTPTest    139890\nLabel                            1\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":63},{"cell_type":"markdown","source":"Dos","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Load the Parquet file\nfile_path = \"/kaggle/input/ids-intrusion-csv/02-20-2018.csv\"\ndf = pd.read_csv(file_path)\n\n# Shape: rows & columns\nprint(\"Number of rows:\", df.shape[0])\nprint(\"Number of columns:\", df.shape[1])\n\n# Column names\nprint(\"\\nColumn names:\")\nprint(df.columns.tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:41:26.254357Z","iopub.execute_input":"2025-12-05T11:41:26.254605Z","iopub.status.idle":"2025-12-05T11:43:09.921520Z","shell.execute_reply.started":"2025-12-05T11:41:26.254587Z","shell.execute_reply":"2025-12-05T11:43:09.920611Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/2899683176.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load the Parquet file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/kaggle/input/ids-intrusion-csv/02-20-2018.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Shape: rows & columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1921\u001b[0m                     \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1922\u001b[0m                     \u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1924\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1925\u001b[0m                 )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n","\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n","\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n","\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n","\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n","\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."],"ename":"ParserError","evalue":"Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.","output_type":"error"}],"execution_count":64},{"cell_type":"code","source":"# Unique values in the label column + their counts\nlabel_col = \"Label\"   # change if needed\n\nif label_col in df.columns:\n    print(\"\\nUnique values and counts in Label column:\")\n    print(df[label_col].value_counts())\nelse:\n    print(\"\\nNo 'Label' column found. Available columns:\")\n    print(df.columns.tolist())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:07:49.960528Z","iopub.status.idle":"2025-12-05T11:07:49.962288Z","shell.execute_reply.started":"2025-12-05T11:07:49.962113Z","shell.execute_reply":"2025-12-05T11:07:49.962126Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Load the Parquet file\nfile_path = \"/kaggle/input/ids-intrusion-csv/02-21-2018.csv\"\ndf = pd.read_csv(file_path)\n\n# Shape: rows & columns\nprint(\"Number of rows:\", df.shape[0])\nprint(\"Number of columns:\", df.shape[1])\n\n# Column names\nprint(\"\\nColumn names:\")\nprint(df.columns.tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:07:49.963226Z","iopub.status.idle":"2025-12-05T11:07:49.963465Z","shell.execute_reply.started":"2025-12-05T11:07:49.963352Z","shell.execute_reply":"2025-12-05T11:07:49.963361Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Unique values in the label column + their counts\nlabel_col = \"Label\"   # change if needed\n\nif label_col in df.columns:\n    print(\"\\nUnique values and counts in Label column:\")\n    print(df[label_col].value_counts())\nelse:\n    print(\"\\nNo 'Label' column found. Available columns:\")\n    print(df.columns.tolist())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:07:49.964150Z","iopub.status.idle":"2025-12-05T11:07:49.964502Z","shell.execute_reply.started":"2025-12-05T11:07:49.964322Z","shell.execute_reply":"2025-12-05T11:07:49.964336Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Load the Parquet file\nfile_path = \"/kaggle/input/ids-intrusion-csv/02-22-2018.csv\"\ndf = pd.read_csv(file_path)\n\n# Shape: rows & columns\nprint(\"Number of rows:\", df.shape[0])\nprint(\"Number of columns:\", df.shape[1])\n\n# Column names\nprint(\"\\nColumn names:\")\nprint(df.columns.tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:07:49.965600Z","iopub.status.idle":"2025-12-05T11:07:49.965871Z","shell.execute_reply.started":"2025-12-05T11:07:49.965755Z","shell.execute_reply":"2025-12-05T11:07:49.965764Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Unique values in the label column + their counts\nlabel_col = \"Label\"   # change if needed\n\nif label_col in df.columns:\n    print(\"\\nUnique values and counts in Label column:\")\n    print(df[label_col].value_counts())\nelse:\n    print(\"\\nNo 'Label' column found. Available columns:\")\n    print(df.columns.tolist())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:07:49.966820Z","iopub.status.idle":"2025-12-05T11:07:49.967077Z","shell.execute_reply.started":"2025-12-05T11:07:49.966932Z","shell.execute_reply":"2025-12-05T11:07:49.966943Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Load the Parquet file\nfile_path = \"/kaggle/input/ids-intrusion-csv/02-23-2018.csv\"\ndf = pd.read_csv(file_path)\n\n# Shape: rows & columns\nprint(\"Number of rows:\", df.shape[0])\nprint(\"Number of columns:\", df.shape[1])\n\n# Column names\nprint(\"\\nColumn names:\")\nprint(df.columns.tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:07:49.967981Z","iopub.status.idle":"2025-12-05T11:07:49.968260Z","shell.execute_reply.started":"2025-12-05T11:07:49.968086Z","shell.execute_reply":"2025-12-05T11:07:49.968100Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Unique values in the label column + their counts\nlabel_col = \"Label\"   # change if needed\n\nif label_col in df.columns:\n    print(\"\\nUnique values and counts in Label column:\")\n    print(df[label_col].value_counts())\nelse:\n    print(\"\\nNo 'Label' column found. Available columns:\")\n    print(df.columns.tolist())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:07:49.969312Z","iopub.status.idle":"2025-12-05T11:07:49.969623Z","shell.execute_reply.started":"2025-12-05T11:07:49.969465Z","shell.execute_reply":"2025-12-05T11:07:49.969479Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Load the Parquet file\nfile_path = \"/kaggle/input/ids-intrusion-csv/02-28-2018.csv\"\ndf = pd.read_csv(file_path)\n\n# Shape: rows & columns\nprint(\"Number of rows:\", df.shape[0])\nprint(\"Number of columns:\", df.shape[1])\n\n# Column names\nprint(\"\\nColumn names:\")\nprint(df.columns.tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:07:49.970757Z","iopub.status.idle":"2025-12-05T11:07:49.971041Z","shell.execute_reply.started":"2025-12-05T11:07:49.970872Z","shell.execute_reply":"2025-12-05T11:07:49.970882Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Unique values in the label column + their counts\nlabel_col = \"Label\"   # change if needed\n\nif label_col in df.columns:\n    print(\"\\nUnique values and counts in Label column:\")\n    print(df[label_col].value_counts())\nelse:\n    print(\"\\nNo 'Label' column found. Available columns:\")\n    print(df.columns.tolist())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:07:49.971841Z","iopub.status.idle":"2025-12-05T11:07:49.972033Z","shell.execute_reply.started":"2025-12-05T11:07:49.971941Z","shell.execute_reply":"2025-12-05T11:07:49.971949Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Load the Parquet file\nfile_path = \"/kaggle/input/ids-intrusion-csv/03-01-2018.csv\"\ndf = pd.read_csv(file_path)\n\n# Shape: rows & columns\nprint(\"Number of rows:\", df.shape[0])\nprint(\"Number of columns:\", df.shape[1])\n\n# Column names\nprint(\"\\nColumn names:\")\nprint(df.columns.tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:07:49.972547Z","iopub.status.idle":"2025-12-05T11:07:49.972798Z","shell.execute_reply.started":"2025-12-05T11:07:49.972691Z","shell.execute_reply":"2025-12-05T11:07:49.972703Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Unique values in the label column + their counts\nlabel_col = \"Label\"   # change if needed\n\nif label_col in df.columns:\n    print(\"\\nUnique values and counts in Label column:\")\n    print(df[label_col].value_counts())\nelse:\n    print(\"\\nNo 'Label' column found. Available columns:\")\n    print(df.columns.tolist())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:07:49.974317Z","iopub.status.idle":"2025-12-05T11:07:49.974566Z","shell.execute_reply.started":"2025-12-05T11:07:49.974431Z","shell.execute_reply":"2025-12-05T11:07:49.974440Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Load the Parquet file\nfile_path = \"/kaggle/input/ids-intrusion-csv/03-02-2018.csv\"\ndf = pd.read_csv(file_path)\n\n# Shape: rows & columns\nprint(\"Number of rows:\", df.shape[0])\nprint(\"Number of columns:\", df.shape[1])\n\n# Column names\nprint(\"\\nColumn names:\")\nprint(df.columns.tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:07:49.975852Z","iopub.status.idle":"2025-12-05T11:07:49.976167Z","shell.execute_reply.started":"2025-12-05T11:07:49.975977Z","shell.execute_reply":"2025-12-05T11:07:49.975992Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Unique values in the label column + their counts\nlabel_col = \"Label\"   # change if needed\n\nif label_col in df.columns:\n    print(\"\\nUnique values and counts in Label column:\")\n    print(df[label_col].value_counts())\nelse:\n    print(\"\\nNo 'Label' column found. Available columns:\")\n    print(df.columns.tolist())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:07:49.977244Z","iopub.status.idle":"2025-12-05T11:07:49.977535Z","shell.execute_reply.started":"2025-12-05T11:07:49.977384Z","shell.execute_reply":"2025-12-05T11:07:49.977396Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Load the full dataset\nfile_path1 = \"/kaggle/input/ids-intrusion-csv/03-02-2018.csv\"\ndf = pd.read_csv(file_path1)\n\nfile_path2 = \"/kaggle/input/ids-intrusion-csv/03-01-2018.csv\"\ndf2 = pd.read_csv(file_path2)\n\nfile_path3 = \"/kaggle/input/ids-intrusion-csv/02-23-2018.csv\"\ndf3 = pd.read_csv(file_path3)\n\nfile_path4 = \"/kaggle/input/ids-intrusion-csv/02-21-2018.csv\"\ndf4 = pd.read_csv(file_path4)\n\nfile_path5 = \"/kaggle/input/ids-intrusion-csv/02-20-2018.csv\"\ndf5 = pd.read_csv(file_path5)\n\nfile_path6 = \"/kaggle/input/ids-intrusion-csv/02-16-2018.csv\"\ndf6 = pd.read_csv(file_path6)\n\nfile_path7 = \"/kaggle/input/ids-intrusion-csv/02-15-2018.csv\"\ndf7 = pd.read_csv(file_path7)\n\nfile_path8 = \"/kaggle/input/ids-intrusion-csv/02-14-2018.csv\"\ndf8 = pd.read_csv(file_path8)\n\nbot_df = df[df['Label'] == 'Bot'].sample(n=4771, random_state=42)\n\nbenign_df = df[df['Label'] == 'Benign'].sample(n=50000, random_state=42)\n\ninfiltration_df = df2[df2['Label'] == 'Infilteration'].sample(n=4771, random_state=42)\n\nBrute_force_web_df = df3[df3['Label'] == 'Brute Force -Web'].sample(n=362, random_state=42)\n\nBrute_force_xss_df = df3[df3['Label'] == 'Brute Force -XSS'].sample(n=151, random_state=42)\n\nsql_injection_df = df3[df3['Label'] == 'SQL Injection'].sample(n=53, random_state=42)\n\nddos_hoic_df = df4[df4['Label'] == 'DDOS attack-HOIC'].sample(n=4771, random_state=42)\n\nddos_loicudp_df = df4[df4['Label'] == 'DDOS attack-LOIC-UDP'].sample(n=1730, random_state=42)\n\nddos_loichttp_df = df5[df5['Label'] == 'DDoS attacks-LOIC-HTTP'].sample(n=4771, random_state=42)\n\nddos_hulk_df = df6[df6['Label'] == 'DoS attacks-Hulk'].sample(n=4770, random_state=42)\n\nddos_slow_df = df6[df6['Label'] == 'DoS attacks-SlowHTTPTest'].sample(n=4770, random_state=42)\n\nddos_goldeye_df = df7[df7['Label'] == 'DoS attacks-GoldenEye'].sample(n=4770, random_state=42)\n\nddos_slowloris_df = df7[df7['Label'] == 'DoS attacks-Slowloris'].sample(n=4770, random_state=42)\n\nddos_ftp_bruteforce_df = df8[df8['Label'] == 'FTP-BruteForce'].sample(n=4770, random_state=42)\n\nddos_ssh_bruteforce_df = df8[df8['Label'] == 'SSH-Bruteforce'].sample(n=4770, random_state=42)\n\n# --- 3. Combine both samples ---\ncombined_df = pd.concat([bot_df, benign_df], ignore_index=True)\n\n# --- 4. Save to CSV ---\noutput_path = \"/kaggle/working/bot_benign_sample.csv\"\ncombined_df.to_csv(output_path, index=False)\n\nprint(\"Saved file:\", output_path)\nprint(\"Final shape:\", combined_df.shape)\nprint(\"Counts:\\n\", combined_df['Label'].value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:07:49.978677Z","iopub.status.idle":"2025-12-05T11:07:49.978978Z","shell.execute_reply.started":"2025-12-05T11:07:49.978826Z","shell.execute_reply":"2025-12-05T11:07:49.978839Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# ---------------- Load all datasets ----------------\ndf1 = pd.read_csv(\"/kaggle/input/ids-intrusion-csv/03-02-2018.csv\")\ndf2 = pd.read_csv(\"/kaggle/input/ids-intrusion-csv/03-01-2018.csv\")\ndf3 = pd.read_csv(\"/kaggle/input/ids-intrusion-csv/02-23-2018.csv\")\ndf4 = pd.read_csv(\"/kaggle/input/ids-intrusion-csv/02-21-2018.csv\")\ndf5 = pd.read_csv(\"/kaggle/input/ids-intrusion-csv/02-20-2018.csv\")\ndf6 = pd.read_csv(\"/kaggle/input/ids-intrusion-csv/02-16-2018.csv\")\ndf7 = pd.read_csv(\"/kaggle/input/ids-intrusion-csv/02-15-2018.csv\")\ndf8 = pd.read_csv(\"/kaggle/input/ids-intrusion-csv/02-14-2018.csv\")\n\n# ---------------- Sampling ----------------\n\nbot_df = df1[df1['Label'] == 'Bot'].sample(n=4771, random_state=42)\nbenign_df = df1[df1['Label'] == 'Benign'].sample(n=50000, random_state=42)\n\ninfiltration_df = df2[df2['Label'] == 'Infilteration'].sample(n=4771, random_state=42)\n\nbrute_web_df = df3[df3['Label'] == 'Brute Force -Web'].sample(n=362, random_state=42)\nbrute_xss_df = df3[df3['Label'] == 'Brute Force -XSS'].sample(n=151, random_state=42)\nsql_injection_df = df3[df3['Label'] == 'SQL Injection'].sample(n=53, random_state=42)\n\nddos_hoic_df = df4[df4['Label'] == 'DDOS attack-HOIC'].sample(n=4771, random_state=42)\nddos_loicudp_df = df4[df4['Label'] == 'DDOS attack-LOIC-UDP'].sample(n=1730, random_state=42)\n\nddos_loichttp_df = df5[df5['Label'] == 'DDoS attacks-LOIC-HTTP'].sample(n=4771, random_state=42)\n\ndos_hulk_df = df6[df6['Label'] == 'DoS attacks-Hulk'].sample(n=4770, random_state=42)\ndos_slowhttp_df = df6[df6['Label'] == 'DoS attacks-SlowHTTPTest'].sample(n=4770, random_state=42)\n\ndos_goldeneye_df = df7[df7['Label'] == 'DoS attacks-GoldenEye'].sample(n=4770, random_state=42)\ndos_slowloris_df = df7[df7['Label'] == 'DoS attacks-Slowloris'].sample(n=4770, random_state=42)\n\nftp_bruteforce_df = df8[df8['Label'] == 'FTP-BruteForce'].sample(n=4770, random_state=42)\nssh_bruteforce_df = df8[df8['Label'] == 'SSH-Bruteforce'].sample(n=4770, random_state=42)\n\n# ---------------- Combine all samples ----------------\n\ncombined_df = pd.concat([\n    bot_df, benign_df,\n    infiltration_df,\n    brute_web_df, brute_xss_df, sql_injection_df,\n    ddos_hoic_df, ddos_loicudp_df, ddos_loichttp_df,\n    dos_hulk_df, dos_slowhttp_df,\n    dos_goldeneye_df, dos_slowloris_df,\n    ftp_bruteforce_df, ssh_bruteforce_df\n], ignore_index=True)\n\n# ---------------- Save output ----------------\n\noutput_path = \"/kaggle/working/combined_intrusion_dataset.csv\"\ncombined_df.to_csv(output_path, index=False)\n\n# ---------------- Print stats ----------------\n\nprint(\"Saved file:\", output_path)\nprint(\"Final shape:\", combined_df.shape)\nprint(\"\\nLabel counts:\\n\", combined_df['Label'].value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:07:49.979885Z","iopub.status.idle":"2025-12-05T11:07:49.980199Z","shell.execute_reply.started":"2025-12-05T11:07:49.980038Z","shell.execute_reply":"2025-12-05T11:07:49.980052Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\n\n# 1Ô∏è‚É£ Load dataset\ndf = pd.read_csv(\"/kaggle/working/combined_intrusion_dataset.csv\")\n\n# 2Ô∏è‚É£ Remove columns where all values are NaN\ndf = df.dropna(axis=1, how='all')\n\n# 3Ô∏è‚É£ Remove columns where all values are 0\ndf = df.loc[:, (df != 0).any(axis=0)]\n\n# 4Ô∏è‚É£ Remove duplicate rows\ndf = df.drop_duplicates()\n\n# 5Ô∏è‚É£ Identify column types\nnum_cols = df.select_dtypes(include=['int64', 'float64']).columns\ncat_cols = df.select_dtypes(include=['object']).columns\n\n# 6Ô∏è‚É£ Handle missing values\ndf[num_cols] = df[num_cols].fillna(df[num_cols].median())\ndf[cat_cols] = df[cat_cols].fillna(df[cat_cols].mode().iloc[0])\n\n# 7Ô∏è‚É£ Label encode categorical columns\nle = LabelEncoder()\nfor col in cat_cols:\n    df[col] = le.fit_transform(df[col].astype(str))\n\n# 8Ô∏è‚É£ Min-Max normalization\nscaler = MinMaxScaler()\ndf[num_cols] = scaler.fit_transform(df[num_cols])\n\n# 9Ô∏è‚É£ Save final output in Kaggle working directory\noutput_filename = \"/kaggle/working/ids2018_cleaned_combined.csv\"\ndf.to_csv(output_filename, index=False)\n\nprint(\"‚úÖ Preprocessing complete!\")\nprint(\"üìÅ Saved as:\", output_filename)\nprint(\"Shape:\", df.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:07:49.981375Z","iopub.status.idle":"2025-12-05T11:07:49.981692Z","shell.execute_reply.started":"2025-12-05T11:07:49.981534Z","shell.execute_reply":"2025-12-05T11:07:49.981547Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\n\n# ---------- 1. Load (avoid low-memory dtype guessing) ----------\ninput_path = \"/kaggle/working/combined_intrusion_dataset.csv\"\ndf = pd.read_csv(input_path, low_memory=False)   # prevents chunked dtype inference warning\n\n# Quick info to debug if needed\nprint(\"Initial shape:\", df.shape)\nprint(\"Initial dtypes (sample):\\n\", df.dtypes.head(20))\n\n# ---------- 2. Basic cleaning ----------\n# Drop columns that are completely NaN\ndf = df.dropna(axis=1, how=\"all\")\n\n# Drop columns that are all zeros (if truly all zeros)\ndf = df.loc[:, (df != 0).any(axis=0)]\n\n# Drop exact duplicate rows\ndf = df.drop_duplicates().reset_index(drop=True)\n\n# ---------- 3. Detect candidate numeric columns robustly ----------\n# We'll try to coerce each column to numeric and keep columns that convert well.\nnumeric_candidates = []\nconversion_stats = {}\nfor col in df.columns:\n    # try converting to numeric (coerce errors -> NaN)\n    coerced = pd.to_numeric(df[col], errors=\"coerce\")\n    non_na_ratio = coerced.notna().sum() / len(coerced)\n    conversion_stats[col] = non_na_ratio\n    # if at least 80% of values convert to numeric, treat as numeric (you can adjust threshold)\n    if non_na_ratio >= 0.80:\n        numeric_candidates.append(col)\n\nprint(f\"Detected {len(numeric_candidates)} numeric-like columns (>=80% convertible).\")\n\n# Force convert those columns to numeric (float64). Others kept as-is (likely categorical/text)\nfor col in numeric_candidates:\n    df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n\n# ---------- 4. Inspect for infinities or extreme values ----------\n# Columns that contain ¬±inf\ninf_cols = []\nfor col in numeric_candidates:\n    ser = df[col]\n    if np.isinf(ser.to_numpy()).any():\n        inf_cols.append(col)\n\nprint(\"Columns with ¬±inf values:\", inf_cols)\n\n# Replace ¬±inf with NaN so we can handle them with median filling\nif inf_cols:\n    df[numeric_candidates] = df[numeric_candidates].replace([np.inf, -np.inf], np.nan)\n\n# Check for huge values that might overflow float64 when operations applied\n# We'll compute absolute max and print columns with extremely large values (> 1e300)\nhuge_cols = []\nfor col in numeric_candidates:\n    try:\n        max_abs = np.nanmax(np.abs(df[col].to_numpy()))\n        if np.isfinite(max_abs) and max_abs > 1e300:\n            huge_cols.append((col, max_abs))\n    except Exception:\n        # if conversion to numpy fails, skip\n        pass\n\nprint(\"Columns with extremely large magnitudes (>1e300):\", huge_cols)\n\n# If you see columns in huge_cols you might want to inspect or clip them.\n# For safety we'll clip numeric values to a sane range before scaling (optional)\nCLIP_LIMIT = 1e300\ndf[numeric_candidates] = df[numeric_candidates].apply(lambda s: s.clip(lower=-CLIP_LIMIT, upper=CLIP_LIMIT))\n\n# ---------- 5. Recompute numeric / categorical column lists ----------\nnum_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\ncat_cols = df.select_dtypes(include=['object']).columns.tolist()\nprint(\"Final numeric columns count:\", len(num_cols))\nprint(\"Final categorical columns count:\", len(cat_cols))\n\n# ---------- 6. Handle missing values ----------\n# Numeric: fill with median\nif len(num_cols) > 0:\n    df[num_cols] = df[num_cols].fillna(df[num_cols].median())\n\n# Categorical: fill with mode (most frequent)\nfor col in cat_cols:\n    if df[col].isna().any():\n        mode_val = df[col].mode(dropna=True)\n        if len(mode_val) > 0:\n            df[col] = df[col].fillna(mode_val.iloc[0])\n        else:\n            # fallback to empty string if mode can't be computed\n            df[col] = df[col].fillna(\"\")\n\n# ---------- 7. Encode categorical columns ----------\nle = LabelEncoder()\nfor col in cat_cols:\n    # convert to string first to avoid issues with mixed types\n    df[col] = le.fit_transform(df[col].astype(str))\n\n# ---------- 8. Final check for non-finite numerics before scaling ----------\n# Replace any remaining inf/-inf with NaN (just in case), then fill with medians again\ndf[num_cols] = df[num_cols].replace([np.inf, -np.inf], np.nan)\ndf[num_cols] = df[num_cols].fillna(df[num_cols].median())\n\n# Ensure all numeric values are finite now\nfinite_check = {col: np.isfinite(df[col].to_numpy()).all() for col in num_cols}\nbad = [c for c, ok in finite_check.items() if not ok]\nprint(\"Columns still non-finite (should be empty):\", bad)\n\n# ---------- 9. Min-Max scaling ----------\nscaler = MinMaxScaler()\nif len(num_cols) > 0:\n    df[num_cols] = scaler.fit_transform(df[num_cols])\n\n# ---------- 10. Save and report ----------\noutput_filename = \"/kaggle/working/ids2018_cleaned_combined.csv\"\ndf.to_csv(output_filename, index=False)\n\nprint(\"‚úÖ Preprocessing complete!\")\nprint(\"üìÅ Saved as:\", output_filename)\nprint(\"Shape:\", df.shape)\nprint(\"Label distribution (if present):\")\nif 'Label' in df.columns:\n    print(df['Label'].value_counts())\nelse:\n    print(\"No 'Label' column found in final dataframe.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:07:49.983106Z","iopub.status.idle":"2025-12-05T11:07:49.983419Z","shell.execute_reply.started":"2025-12-05T11:07:49.983268Z","shell.execute_reply":"2025-12-05T11:07:49.983282Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"hi\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:43:27.934968Z","iopub.execute_input":"2025-12-05T11:43:27.935380Z","iopub.status.idle":"2025-12-05T11:43:27.940870Z","shell.execute_reply.started":"2025-12-05T11:43:27.935354Z","shell.execute_reply":"2025-12-05T11:43:27.939993Z"}},"outputs":[{"name":"stdout","text":"hi\n","output_type":"stream"}],"execution_count":65},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\nfrom catboost import CatBoostClassifier\nimport time\n\n# -----------------------------------------\n# 1Ô∏è‚É£ Load cleaned dataset\n# -----------------------------------------\ndf = pd.read_csv(\"/kaggle/working/ids2018_cleaned_combined_1.csv\")\n\n# Ensure \"Label\" column exists\nif \"Label\" not in df.columns:\n    raise ValueError(\"‚ùå ERROR: 'Label' column not found in dataset!\")\n\n# -----------------------------------------\n# 2Ô∏è‚É£ Split features and target\n# -----------------------------------------\nX = df.drop(\"Label\", axis=1)\ny = df[\"Label\"]\n\n# -----------------------------------------\n# 3Ô∏è‚É£ Train-test split (80-20)\n# -----------------------------------------\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.20, random_state=42, stratify=y\n)\n\n# -----------------------------------------\n# 4Ô∏è‚É£ Train CatBoost model + record time\n# -----------------------------------------\nmodel = CatBoostClassifier(\n    iterations=300,\n    learning_rate=0.1,\n    depth=8,\n    loss_function='MultiClass',\n    verbose=False\n)\n\nstart_time = time.time()\nmodel.fit(X_train, y_train)\nend_time = time.time()\n\ntraining_time = end_time - start_time\n\n# -----------------------------------------\n# 5Ô∏è‚É£ Predictions\n# -----------------------------------------\ny_pred = model.predict(X_test)\ny_pred = y_pred.flatten()   # CatBoost outputs shape (n,1)\n\n# -----------------------------------------\n# 6Ô∏è‚É£ Metrics\n# -----------------------------------------\nacc = accuracy_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred, average=\"weighted\")\nrecall = recall_score(y_test, y_pred, average=\"weighted\")\nprecision = precision_score(y_test, y_pred, average=\"weighted\")\n\n# -----------------------------------------\n# 7Ô∏è‚É£ Print results\n# -----------------------------------------\nprint(\"‚úîÔ∏è CatBoost Training Complete\\n\")\nprint(\"üìå Accuracy:\", acc)\nprint(\"üìå F1 Score (weighted):\", f1)\nprint(\"üìå Recall (weighted):\", recall)\nprint(\"üìå Precision (weighted):\", precision)\nprint(f\"‚è±Ô∏è Time Taken to Train: {training_time:.2f} seconds\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:07:49.984298Z","iopub.status.idle":"2025-12-05T11:07:49.984604Z","shell.execute_reply.started":"2025-12-05T11:07:49.984448Z","shell.execute_reply":"2025-12-05T11:07:49.984460Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\nfrom catboost import CatBoostClassifier\nimport time\nimport pickle\n\n# -----------------------------------------\n# 1Ô∏è‚É£ Load cleaned dataset\n# -----------------------------------------\ndf = pd.read_csv(\"/kaggle/input/cleaned-ids/ids2018_cleaned_combined_1.csv\")\n\n# Ensure \"Label\" column exists\nif \"Label\" not in df.columns:\n    raise ValueError(\"‚ùå ERROR: 'Label' column not found in dataset!\")\n\n# -----------------------------------------\n# 2Ô∏è‚É£ Split features and target\n# -----------------------------------------\nX = df.drop(\"Label\", axis=1)\ny = df[\"Label\"]\n\n# -----------------------------------------\n# 3Ô∏è‚É£ Train-test split (80-20)\n# -----------------------------------------\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.20, random_state=42, stratify=y\n)\n\n# -----------------------------------------\n# 4Ô∏è‚É£ Train CatBoost model + record time\n# -----------------------------------------\nmodel = CatBoostClassifier(\n    iterations=300,\n    learning_rate=0.1,\n    depth=8,\n    loss_function='MultiClass',\n    verbose=False\n)\n\nstart_time = time.time()\nmodel.fit(X_train, y_train)\nend_time = time.time()\ntraining_time = end_time - start_time\n\n# -----------------------------------------\n# 5Ô∏è‚É£ Predictions\n# -----------------------------------------\ny_pred = model.predict(X_test)\ny_pred = y_pred.flatten()\n\n# -----------------------------------------\n# 6Ô∏è‚É£ Metrics\n# -----------------------------------------\nacc = accuracy_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred, average=\"weighted\")\nrecall = recall_score(y_test, y_pred, average=\"weighted\")\nprecision = precision_score(y_test, y_pred, average=\"weighted\")\n\n# -----------------------------------------\n# 7Ô∏è‚É£ Save model to Pickle\n# -----------------------------------------\npickle_path = \"/kaggle/working/catboost_ids2018_model.pkl\"\nwith open(pickle_path, \"wb\") as f:\n    pickle.dump(model, f)\n\n# -----------------------------------------\n# 8Ô∏è‚É£ Print results\n# -----------------------------------------\nprint(\"‚úîÔ∏è CatBoost Training Complete\\n\")\nprint(\"üìÅ Model saved as:\", pickle_path)\nprint(\"üìå Accuracy:\", acc)\nprint(\"üìå F1 Score (weighted):\", f1)\nprint(\"üìå Recall (weighted):\", recall)\nprint(\"üìå Precision (weighted):\", precision)\nprint(f\"‚è±Ô∏è Time Taken to Train: {training_time:.2f} seconds\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:35:37.235430Z","iopub.execute_input":"2025-12-05T11:35:37.236257Z","iopub.status.idle":"2025-12-05T11:36:01.231372Z","shell.execute_reply.started":"2025-12-05T11:35:37.236226Z","shell.execute_reply":"2025-12-05T11:36:01.230525Z"}},"outputs":[{"name":"stdout","text":"‚úîÔ∏è CatBoost Training Complete\n\nüìÅ Model saved as: /kaggle/working/catboost_ids2018_model.pkl\nüìå Accuracy: 0.999130923776903\nüìå F1 Score (weighted): 0.99913091257803\nüìå Recall (weighted): 0.999130923776903\nüìå Precision (weighted): 0.9991315336373293\n‚è±Ô∏è Time Taken to Train: 22.72 seconds\n","output_type":"stream"}],"execution_count":42},{"cell_type":"markdown","source":"above asnwer is after cleaning before feature selection","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport time\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom catboost import CatBoostClassifier\n\n# Load\ndf = pd.read_csv(\"/kaggle/input/before-clean-ids/combined_intrusion_dataset.csv\", low_memory=False)\ndf['Label'] = df['Label'].astype(int)\n\n# X / y\nX = df.drop(columns=['Label']).copy()\ny = df['Label']\n\n# Detect categorical columns (object dtype)\ncat_cols = X.select_dtypes(include=['object']).columns.tolist()\nprint(\"Categorical columns detected (from X):\", cat_cols)\n\n# Replace NaN in categorical columns with a token and cast to string\nfor c in cat_cols:\n    X[c] = X[c].fillna(\"__missing__\").astype(str)\n\n# Now compute cat feature indices relative to X\ncat_feature_indices = [X.columns.get_loc(c) for c in cat_cols]\nprint(\"Cat feature indices:\", cat_feature_indices)\n\n# Split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.20, random_state=42, stratify=y\n)\n\n# Train CatBoost (binary)\nmodel = CatBoostClassifier(\n    iterations=300,\n    learning_rate=0.1,\n    depth=8,\n    loss_function='Logloss',\n    verbose=False\n)\n\nstart = time.time()\nmodel.fit(X_train, y_train, cat_features=cat_feature_indices)\ntrain_time = time.time() - start\n\n# Predict & metrics\ny_pred = model.predict(X_test).astype(int).flatten()\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred, average='binary', zero_division=0)\nrecall = recall_score(y_test, y_pred, average='binary', zero_division=0)\nf1 = f1_score(y_test, y_pred, average='binary', zero_division=0)\n\nprint(\"\\n‚úî RAW CatBoost (Option A) done\")\nprint(f\"‚è± Training time: {train_time:.2f} s\")\nprint(f\"Accuracy : {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall   : {recall:.4f}\")\nprint(f\"F1 Score : {f1:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:49:09.375717Z","iopub.execute_input":"2025-12-05T11:49:09.376223Z","iopub.status.idle":"2025-12-05T11:49:11.484052Z","shell.execute_reply.started":"2025-12-05T11:49:09.376185Z","shell.execute_reply":"2025-12-05T11:49:11.482773Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/2421028385.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/kaggle/input/before-clean-ids/combined_intrusion_dataset.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# X / y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m   6641\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6642\u001b[0m             \u001b[0;31m# else, only a single dtype is given\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6643\u001b[0;31m             \u001b[0mnew_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6644\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor_from_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6645\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"astype\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m         return self.apply(\n\u001b[0m\u001b[1;32m    431\u001b[0m             \u001b[0;34m\"astype\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m                 \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m             \u001b[0mresult_blocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextend_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapplied\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors, using_cow, squeeze)\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# type: ignore[call-overload]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mastype_array_safe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_coerce_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/dtypes/astype.py\u001b[0m in \u001b[0;36mastype_array_safe\u001b[0;34m(values, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mastype_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;31m# e.g. _astype_nansafe can fail on object-dtype of strings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/dtypes/astype.py\u001b[0m in \u001b[0;36mastype_array\u001b[0;34m(values, dtype, copy)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_astype_nansafe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;31m# in pandas we don't store numpy str dtypes, so convert to object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/dtypes/astype.py\u001b[0m in \u001b[0;36m_astype_nansafe\u001b[0;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;31m# Explicit copy, or required since NumPy can't view from / to object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'Bot'"],"ename":"ValueError","evalue":"invalid literal for int() with base 10: 'Bot'","output_type":"error"}],"execution_count":66},{"cell_type":"code","source":"import pandas as pd\nimport time\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom catboost import CatBoostClassifier\n\n# -----------------------------------------------------\n# 1) LOAD DATA\n# -----------------------------------------------------\ndf = pd.read_csv(\"/kaggle/input/before-clean-ids/combined_intrusion_dataset.csv\", low_memory=False)\nprint(\"Loaded dataset:\", df.shape)\n\n# -----------------------------------------------------\n# 2) CONVERT LABEL COLUMN\n#    Benign ‚Üí 0\n#    All other attack labels ‚Üí 1\n# -----------------------------------------------------\ndf['Label'] = df['Label'].astype(str).str.strip().str.lower()\n\ndf['Label'] = df['Label'].apply(\n    lambda x: 0 if x in (\"benign\", \"normal\") or \"benign\" in x else 1\n)\n\ndf['Label'] = df['Label'].astype(int)\nprint(\"Label distribution:\\n\", df['Label'].value_counts())\n\n# -----------------------------------------------------\n# 3) SEPARATE FEATURES + TARGET\n# -----------------------------------------------------\nX = df.drop(columns=['Label']).copy()\ny = df['Label']\n\n# Detect categorical (object/string) columns\ncat_cols = X.select_dtypes(include=['object']).columns.tolist()\nprint(\"\\nCategorical columns detected:\", cat_cols)\n\n# Fill NaN in categorical columns and convert to string\nfor col in cat_cols:\n    X[col] = X[col].fillna(\"__missing__\").astype(str)\n\n# Convert categorical column names ‚Üí column indices for CatBoost\ncat_feature_indices = [X.columns.get_loc(c) for c in cat_cols]\nprint(\"Categorical feature indices:\", cat_feature_indices)\n\n# -----------------------------------------------------\n# 4) TRAIN/TEST SPLIT\n# -----------------------------------------------------\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.20, random_state=42, stratify=y\n)\n\n# -----------------------------------------------------\n# 5) TRAIN CATBOOST CLASSIFIER\n# -----------------------------------------------------\nmodel = CatBoostClassifier(\n    iterations=300,\n    learning_rate=0.1,\n    depth=8,\n    loss_function='Logloss',\n    verbose=False\n)\n\nstart = time.time()\nmodel.fit(X_train, y_train, cat_features=cat_feature_indices)\ntrain_time = time.time() - start\n\n# -----------------------------------------------------\n# 6) PREDICT + METRICS\n# -----------------------------------------------------\ny_pred = model.predict(X_test).astype(int).flatten()\n\naccuracy  = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred, zero_division=0)\nrecall    = recall_score(y_test, y_pred, zero_division=0)\nf1        = f1_score(y_test, y_pred, zero_division=0)\n\n# -----------------------------------------------------\n# 7) PRINT RESULTS\n# -----------------------------------------------------\nprint(\"\\n‚úî CatBoost Training Complete\")\nprint(f\"‚è± Training time: {train_time:.2f} seconds\")\nprint(f\"Accuracy : {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall   : {recall:.4f}\")\nprint(f\"F1 Score : {f1:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:52:02.478409Z","iopub.execute_input":"2025-12-05T11:52:02.479009Z","iopub.status.idle":"2025-12-05T11:52:36.981459Z","shell.execute_reply.started":"2025-12-05T11:52:02.478982Z","shell.execute_reply":"2025-12-05T11:52:36.980691Z"}},"outputs":[{"name":"stdout","text":"Loaded dataset: (100000, 84)\nLabel distribution:\n Label\n1    50000\n0    50000\nName: count, dtype: int64\n\nCategorical columns detected: ['Timestamp', 'Flow ID', 'Src IP', 'Dst IP']\nCategorical feature indices: [2, 79, 80, 82]\n\n‚úî CatBoost Training Complete\n‚è± Training time: 32.93 seconds\nAccuracy : 0.9853\nPrecision: 0.9972\nRecall   : 0.9734\nF1 Score : 0.9852\n","output_type":"stream"}],"execution_count":67},{"cell_type":"markdown","source":"above is before cleaning","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport time\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom catboost import CatBoostClassifier\n\n# Load\ndf = pd.read_csv(\"/kaggle/working/combined_intrusion_dataset_binary.csv\", low_memory=False)\ndf['Label'] = df['Label'].astype(int)\n\n# Drop IP columns to reduce memory / speed impact\ndrop_cols = ['Src IP', 'Dst IP']  # change if names differ\ndf = df.drop(columns=[c for c in drop_cols if c in df.columns])\n\n# X / y\nX = df.drop(columns=['Label']).copy()\ny = df['Label']\n\n# Detect categorical columns (object dtype)\ncat_cols = X.select_dtypes(include=['object']).columns.tolist()\nprint(\"Categorical columns detected (from X):\", cat_cols)\n\n# Replace NaN in categorical columns with a token and cast to string\nfor c in cat_cols:\n    X[c] = X[c].fillna(\"__missing__\").astype(str)\n\ncat_feature_indices = [X.columns.get_loc(c) for c in cat_cols]\nprint(\"Cat feature indices:\", cat_feature_indices)\n\n# Split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.20, random_state=42, stratify=y\n)\n\n# Train CatBoost (binary)\nmodel = CatBoostClassifier(\n    iterations=300,\n    learning_rate=0.1,\n    depth=8,\n    loss_function='Logloss',\n    verbose=False\n)\n\nstart = time.time()\nmodel.fit(X_train, y_train, cat_features=cat_feature_indices)\ntrain_time = time.time() - start\n\n# Predict & metrics\ny_pred = model.predict(X_test).astype(int).flatten()\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred, average='binary', zero_division=0)\nrecall = recall_score(y_test, y_pred, average='binary', zero_division=0)\nf1 = f1_score(y_test, y_pred, average='binary', zero_division=0)\n\nprint(\"\\n‚úî RAW CatBoost (Option B) done\")\nprint(f\"‚è± Training time: {train_time:.2f} s\")\nprint(f\"Accuracy : {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall   : {recall:.4f}\")\nprint(f\"F1 Score : {f1:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:07:49.989984Z","iopub.status.idle":"2025-12-05T11:07:49.990244Z","shell.execute_reply.started":"2025-12-05T11:07:49.990114Z","shell.execute_reply":"2025-12-05T11:07:49.990123Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"above model is before cleaning dropping src and dest ip","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ncleaned = pd.read_csv(\"/kaggle/working/ids2018_cleaned_combined_1.csv\", low_memory=False)\nprint(\"dtype of Label:\", cleaned['Label'].dtype)\nprint(\"Unique label values (sample):\", cleaned['Label'].unique()[:30])\nprint(\"\\nValue counts:\\n\", cleaned['Label'].value_counts().head(30))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:07:49.991237Z","iopub.status.idle":"2025-12-05T11:07:49.991505Z","shell.execute_reply.started":"2025-12-05T11:07:49.991387Z","shell.execute_reply":"2025-12-05T11:07:49.991399Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\n\n# ---------- 1. Load (avoid low-memory dtype guessing) ----------\ninput_path = \"/kaggle/working/combined_intrusion_dataset.csv\"\ndf = pd.read_csv(input_path, low_memory=False)\n\nprint(\"Initial shape:\", df.shape)\n\n# ---------- 2. Convert Label ‚Üí Binary BEFORE cleaning ----------\n# Benign = 0\n# Any attack = 1\ndf['Label'] = (df['Label'] != 'Benign').astype(int)\n\nprint(\"Binary Label distribution:\\n\", df['Label'].value_counts())\n\n# ---------- 3. Basic cleaning ----------\ndf = df.dropna(axis=1, how=\"all\")          # drop all-NaN columns\ndf = df.loc[:, (df != 0).any(axis=0)]      # drop all-zero columns\ndf = df.drop_duplicates().reset_index(drop=True)\n\n# ---------- 4. Detect numeric-like columns ----------\nnumeric_candidates = []\nfor col in df.columns:\n    coerced = pd.to_numeric(df[col], errors=\"coerce\")\n    ratio = coerced.notna().sum() / len(coerced)\n    if ratio >= 0.80:\n        numeric_candidates.append(col)\n        df[col] = coerced\n\n# Ensure Label is treated as numeric (keep it as binary!)\nif 'Label' not in numeric_candidates:\n    numeric_candidates.append('Label')\n\n# ---------- 5. Fix infinities ----------\ndf[numeric_candidates] = df[numeric_candidates].replace([np.inf, -np.inf], np.nan)\n\n# ---------- 6. Clip extremely large values ----------\nCLIP_LIMIT = 1e300\ndf[numeric_candidates] = df[numeric_candidates].apply(lambda s: s.clip(-CLIP_LIMIT, CLIP_LIMIT))\n\n# ---------- 7. Identify final numeric + categorical ----------\nnum_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n\n# Remove LABEL so it never gets encoded\ncat_cols = df.select_dtypes(include=['object']).columns.tolist()\n\nprint(\"Numeric columns:\", len(num_cols))\nprint(\"Categorical columns:\", len(cat_cols))\n\n# ---------- 8. Handle missing values ----------\ndf[num_cols] = df[num_cols].fillna(df[num_cols].median())\n\nfor col in cat_cols:\n    df[col] = df[col].fillna(df[col].mode(dropna=True).iloc[0])\n\n# ---------- 9. Encode categorical columns (not Label) ----------\nle = LabelEncoder()\nfor col in cat_cols:\n    df[col] = le.fit_transform(df[col].astype(str))\n\n# ---------- 10. Final check before scaling ----------\ndf[num_cols] = df[num_cols].replace([np.inf, -np.inf], np.nan)\ndf[num_cols] = df[num_cols].fillna(df[num_cols].median())\n\n# ---------- 11. Scale numeric features (Label excluded automatically) ----------\nscaler = MinMaxScaler()\ncols_to_scale = [c for c in num_cols if c != 'Label']   # do NOT scale target\n\ndf[cols_to_scale] = scaler.fit_transform(df[cols_to_scale])\n\n# ---------- 12. Save and report ----------\noutput_filename = \"/kaggle/working/ids2018_cleaned_combined_1.csv\"\ndf.to_csv(output_filename, index=False)\n\nprint(\"\\n‚úÖ Preprocessing complete!\")\nprint(\"üìÅ Saved as:\", output_filename)\nprint(\"Final shape:\", df.shape)\nprint(\"Final Label distribution:\\n\", df['Label'].value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:07:49.992487Z","iopub.status.idle":"2025-12-05T11:07:49.992737Z","shell.execute_reply.started":"2025-12-05T11:07:49.992636Z","shell.execute_reply":"2025-12-05T11:07:49.992646Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Load dataset\ndf = pd.read_csv(\"/kaggle/working/combined_intrusion_dataset.csv\", low_memory=False)\n\n# Convert Label -> binary\ndf['Label'] = (df['Label'] != 'Benign').astype(int)\n\n# Save the output\noutput_path = \"/kaggle/working/combined_intrusion_dataset_binary.csv\"\ndf.to_csv(output_path, index=False)\n\nprint(\"‚úî Binary conversion complete!\")\nprint(\"üìÅ Saved as:\", output_path)\nprint(df['Label'].value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:07:49.993445Z","iopub.status.idle":"2025-12-05T11:07:49.993725Z","shell.execute_reply.started":"2025-12-05T11:07:49.993606Z","shell.execute_reply":"2025-12-05T11:07:49.993618Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# hybrid_hlo_union_only.py\n# Reduced-budget hybrid pipeline: PSO, GA, GWO -> UNION -> HLO -> Hill-climb -> final CatBoost (save)\n# Prints selected features after PSO/GA/GWO and the final union members.\n# Runs on Kaggle input path by default and prints final test metrics (accuracy, precision, recall, f1),\n# confusion matrix and classification report.\n\nimport time\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport warnings\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, make_scorer, classification_report, confusion_matrix\nfrom sklearn.base import clone\n\nwarnings.filterwarnings(\"ignore\")\nnp.random.seed(42)\n\n# -------------------- USER / EXPERIMENT SETTINGS --------------------\n# Kaggle path requested by you:\nCSV_PATH = \"/kaggle/working/ids2018_cleaned_combined_1.csv\"\n\nTARGET_COL = \"Label\"   # change if your dataset uses another column name\nMODEL_VERBOSE = 0            # CatBoost verbosity: 0 = silent\nRANDOM_STATE = 42\n\n# ---------- Reduced budgets (set to 20 for the three optimizers as requested) ----------\nPSO_SWARM = 15\nPSO_ITERS = 5     # <<-- set to 20\n\nGA_POP = 30\nGA_GENS = 5       # <<-- set to 20\n\nGWO_WOLVES = 10\nGWO_ITERS = 5     # <<-- set to 20\n\nHLO_POP = 15\nHLO_ITERS = 8\nHLO_TEACHER_FACTOR = 0.75\nHLO_MUTATION = 0.12\n\n# Greedy hill-climb\nHILLCLIMB_MAX_STEPS = 100\nHILLCLIMB_EVAL_CAP = 500\n\n# CV folds\nCV_OPT = 2\nCV_FINAL = 5\n\n# CatBoost iterations\nCB_ITER_OPT = 100\nCB_ITER_HLO = 200\nCB_ITER_FINAL = 500\n\nFINAL_TEST_SIZE = 0.2\nSAVE_PREFIX = \"hybrid_hlo_union\"\n# ------------------------------------------------------------------------\n\n\n# -------------------- Load data (robust handling of messy column names) --------------------\nprint(f\"[{time.strftime('%H:%M:%S')}] Loading CSV from: {CSV_PATH}\")\ndf = pd.read_csv(CSV_PATH, low_memory=False)\nprint(f\"[{time.strftime('%H:%M:%S')}] Raw loaded shape: {df.shape}\")\nprint(f\"[{time.strftime('%H:%M:%S')}] Raw columns sample: {df.columns.tolist()[:12]}\")\n\n# Clean column names: strip whitespace and normalize repeated spaces\ndf.columns = df.columns.astype(str).str.strip().str.replace(r\"\\s+\", \" \", regex=True)\nprint(f\"[{time.strftime('%H:%M:%S')}] Cleaned columns sample: {df.columns.tolist()[:12]}\")\n\n# If an index column like 'Unnamed: 0' exists (common from CSV exports), drop it\nif 'Unnamed: 0' in df.columns:\n    df = df.drop(columns=['Unnamed: 0'])\n    print(f\"[{time.strftime('%H:%M:%S')}] Dropped 'Unnamed: 0' column. New shape: {df.shape}\")\n\n# If the requested TARGET_COL isn't found, try to auto-detect a label-like column (case-insensitive)\nif TARGET_COL not in df.columns:\n    # try case-insensitive match\n    cols_lower = {c.lower(): c for c in df.columns}\n    if TARGET_COL.lower() in cols_lower:\n        real_col = cols_lower[TARGET_COL.lower()]\n        print(f\"[{time.strftime('%H:%M:%S')}] Using case-insensitive match for target: '{real_col}'\")\n        TARGET_COL = real_col\n    else:\n        # fallback: search for any column name that contains 'label' or 'target'\n        cand = [c for c in df.columns if 'label' in c.lower() or 'target' in c.lower()]\n        if len(cand) == 1:\n            print(f\"[{time.strftime('%H:%M:%S')}] Auto-detected target column: '{cand[0]}'\")\n            TARGET_COL = cand[0]\n        elif len(cand) > 1:\n            print(f\"[{time.strftime('%H:%M:%S')}] Multiple candidate target columns found: {cand}. Using first: '{cand[0]}'\")\n            TARGET_COL = cand[0]\n        else:\n            raise ValueError(f\"Target column '{TARGET_COL}' not found (after cleaning). Columns: {df.columns.tolist()[:12]}...\")\n\nprint(f\"[{time.strftime('%H:%M:%S')}] Using TARGET_COL = '{TARGET_COL}'\")\n\n# Basic preprocessing expectation: ensure no object columns remain unencoded for CatBoost.\nfrom sklearn.preprocessing import LabelEncoder\nobj_cols = df.select_dtypes(include=[\"object\"]).columns.tolist()\nif obj_cols:\n    print(f\"[{time.strftime('%H:%M:%S')}] Label-encoding object columns for safe use: {obj_cols}\")\n    for c in obj_cols:\n        df[c] = df[c].astype(str).fillna(\"NA\")\n        df[c] = LabelEncoder().fit_transform(df[c])\n\n# Ensure no NaNs in features/target used by optimizers\ndf = df.dropna(axis=0).reset_index(drop=True)\n\n# Prepare X, y\nX = df.drop(TARGET_COL, axis=1)\ny = df[TARGET_COL].astype(int)\nFEATURE_NAMES = X.columns.tolist()\nN_FEATURES = X.shape[1]\nprint(f\"[{time.strftime('%H:%M:%S')}] Prepared X ({X.shape}) and y ({y.shape}). Number of features: {N_FEATURES}\")\n\n\n# -------------------- CatBoost factory --------------------\ndef get_catboost_model(iterations=100):\n    try:\n        from catboost import CatBoostClassifier\n    except Exception as e:\n        raise ImportError(\"catboost not installed. Install with: pip install catboost\") from e\n    return CatBoostClassifier(iterations=iterations, learning_rate=0.05, depth=6,\n                              verbose=MODEL_VERBOSE, random_seed=RANDOM_STATE, thread_count=-1)\n\n# -------------------- Fitness cache --------------------\nfitness_cache = {}\ndef key_from_mask(mask_bool):\n    return tuple(sorted(np.where(np.array(mask_bool).astype(bool))[0].tolist()))\n\ndef evaluate_mask_global(mask_bool, cv=CV_OPT, cb_iter=CB_ITER_OPT):\n    key = key_from_mask(mask_bool)\n    if key in fitness_cache:\n        return fitness_cache[key]\n    if len(key) == 0:\n        fitness_cache[key] = 0.0\n        return 0.0\n\n    X_sel = X.iloc[:, list(key)]\n    model = get_catboost_model(iterations=cb_iter)\n    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=RANDOM_STATE)\n\n    try:\n        accs = cross_val_score(clone(model), X_sel, y, cv=skf, scoring=\"accuracy\", n_jobs=-1)\n        precs = cross_val_score(clone(model), X_sel, y, cv=skf, scoring=make_scorer(precision_score, zero_division=0), n_jobs=-1)\n        recs = cross_val_score(clone(model), X_sel, y, cv=skf, scoring=make_scorer(recall_score, zero_division=0), n_jobs=-1)\n        f1s = cross_val_score(clone(model), X_sel, y, cv=skf, scoring=make_scorer(f1_score, zero_division=0), n_jobs=-1)\n        score = float((np.mean(accs) + np.mean(precs) + np.mean(recs) + np.mean(f1s)) / 4.0)\n    except Exception as e:\n        # if a training error occurs (e.g., degenerate feature set), return 0\n        score = 0.0\n\n    fitness_cache[key] = score\n    return score\n\n# -------------------- Helpers --------------------\ndef mask_to_features(mask):\n    idxs = np.where(np.array(mask).astype(bool))[0].tolist()\n    return [FEATURE_NAMES[i] for i in idxs]\n\ndef log(msg):\n    print(f\"[{time.strftime('%H:%M:%S')}] {msg}\", flush=True)\n\n# -------------------- PSO (binary) --------------------\ndef run_pso(swarm_size=PSO_SWARM, iters=PSO_ITERS, cv=CV_OPT):\n    log(f\"PSO START (swarm={swarm_size}, iters={iters}, cv={cv})\")\n    t0 = time.time()\n    dim = N_FEATURES\n    pos = np.random.randint(0,2,(swarm_size,dim)).astype(int)\n    vel = np.random.uniform(-1,1,(swarm_size,dim))\n\n    pbest = pos.copy()\n    pbest_scores = np.array([evaluate_mask_global(p.astype(bool), cv=cv, cb_iter=CB_ITER_OPT) for p in pos])\n\n    gbest_idx = int(np.argmax(pbest_scores))\n    gbest = pbest[gbest_idx].copy()\n    gbest_score = pbest_scores[gbest_idx]\n\n    w = 0.6; c1 = c2 = 1.5\n    for t in range(iters):\n        log(f\" PSO iter {t+1}/{iters} best_global={gbest_score:.4f}\")\n        for i in range(swarm_size):\n            r1 = np.random.rand(dim); r2 = np.random.rand(dim)\n            vel[i] = w*vel[i] + c1*r1*(pbest[i] - pos[i]) + c2*r2*(gbest - pos[i])\n            s = 1.0 / (1.0 + np.exp(-vel[i]))\n            pos[i] = (np.random.rand(dim) < s).astype(int)\n\n            sc = evaluate_mask_global(pos[i].astype(bool), cv=cv, cb_iter=CB_ITER_OPT)\n            if sc > pbest_scores[i]:\n                pbest[i] = pos[i].copy()\n                pbest_scores[i] = sc\n            if sc > gbest_score:\n                gbest = pos[i].copy()\n                gbest_score = sc\n        w = max(0.2, w*0.97)\n\n    best_idx = int(np.argmax(pbest_scores))\n    best_mask = pbest[best_idx].copy()\n    best_score = pbest_scores[best_idx]\n    t1 = time.time()\n    log(f\"PSO DONE in {int(t1-t0)}s best_score={best_score:.4f} selected={int(np.sum(best_mask))}\")\n    return best_mask, best_score, int(t1-t0)\n\n# -------------------- GA (binary) --------------------\ndef run_ga(pop_size=GA_POP, gens=GA_GENS, cv=CV_OPT):\n    log(f\"GA START (pop={pop_size}, gens={gens}, cv={cv})\")\n    t0 = time.time()\n    dim = N_FEATURES\n    pop = np.random.randint(0,2,(pop_size, dim)).astype(int)\n    fitness_scores = np.array([evaluate_mask_global(ind.astype(bool), cv=cv, cb_iter=CB_ITER_OPT) for ind in pop])\n\n    def tournament_select(k=3):\n        idxs = np.random.randint(0, pop_size, k)\n        return idxs[np.argmax(fitness_scores[idxs])]\n\n    for g in range(gens):\n        log(f\" GA gen {g+1}/{gens} current_best={np.max(fitness_scores):.4f}\")\n        new_pop = []\n        # elitism\n        elite_idxs = np.argsort(fitness_scores)[-2:]\n        new_pop.extend(pop[elite_idxs].tolist())\n\n        while len(new_pop) < pop_size:\n            i1 = tournament_select(); i2 = tournament_select()\n            p1 = pop[i1].copy(); p2 = pop[i2].copy()\n            # crossover\n            if np.random.rand() < 0.7:\n                pt = np.random.randint(1, dim)\n                c1 = np.concatenate([p1[:pt], p2[pt:]])\n                c2 = np.concatenate([p2[:pt], p1[pt:]])\n            else:\n                c1, c2 = p1, p2\n            # mutation\n            for child in (c1, c2):\n                for d in range(dim):\n                    if np.random.rand() < 0.1:\n                        child[d] = 1 - child[d]\n                new_pop.append(child)\n                if len(new_pop) >= pop_size:\n                    break\n        pop = np.array(new_pop[:pop_size])\n        fitness_scores = np.array([evaluate_mask_global(ind.astype(bool), cv=cv, cb_iter=CB_ITER_OPT) for ind in pop])\n\n    best_idx = int(np.argmax(fitness_scores))\n    best_mask = pop[best_idx].copy()\n    best_score = fitness_scores[best_idx]\n    t1 = time.time()\n    log(f\"GA DONE in {int(t1-t0)}s best_score={best_score:.4f} selected={int(np.sum(best_mask))}\")\n    return best_mask, best_score, int(t1-t0)\n\n# -------------------- GWO (binary) --------------------\ndef run_gwo(wolves=GWO_WOLVES, iters=GWO_ITERS, cv=CV_OPT):\n    log(f\"GWO START (wolves={wolves}, iters={iters}, cv={cv})\")\n    t0 = time.time()\n    dim = N_FEATURES\n    pop = np.random.randint(0,2,(wolves, dim)).astype(int)\n    fitness_scores = np.array([evaluate_mask_global(ind.astype(bool), cv=cv, cb_iter=CB_ITER_OPT) for ind in pop])\n\n    Alpha = Beta = Delta = None\n    Alpha_score = Beta_score = Delta_score = -1.0\n\n    for itr in range(iters):\n        log(f\" GWO iter {itr+1}/{iters} best_alpha={Alpha_score:.4f}\")\n        for i in range(wolves):\n            sc = fitness_scores[i]\n            if sc > Alpha_score:\n                Delta_score, Beta_score, Alpha_score = Beta_score, Alpha_score, sc\n                Delta, Beta, Alpha = Beta, Alpha, pop[i].copy()\n            elif sc > Beta_score:\n                Delta_score, Beta_score = Beta_score, sc\n                Delta, Beta = Beta, pop[i].copy()\n            elif sc > Delta_score:\n                Delta_score = sc\n                Delta = pop[i].copy()\n\n        a = 2 - itr * (2.0 / iters)\n        for i in range(wolves):\n            for d in range(dim):\n                if Alpha is None:\n                    continue\n                r1, r2 = np.random.rand(), np.random.rand()\n                A1 = 2 * a * r1 - a; C1 = 2 * r2\n                D_alpha = abs(C1 * Alpha[d] - pop[i][d])\n                X1 = Alpha[d] - A1 * D_alpha\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                A2 = 2 * a * r1 - a; C2 = 2 * r2\n                D_beta = abs(C2 * Beta[d] - pop[i][d])\n                X2 = Beta[d] - A2 * D_beta\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                A3 = 2 * a * r1 - a; C3 = 2 * r2\n                D_delta = abs(C3 * Delta[d] - pop[i][d])\n                X3 = Delta[d] - A3 * D_delta\n\n                new_pos = (X1 + X2 + X3) / 3.0\n                s = 1.0 / (1.0 + np.exp(-new_pos))\n                pop[i][d] = 1 if np.random.rand() < s else 0\n\n        fitness_scores = np.array([evaluate_mask_global(ind.astype(bool), cv=cv, cb_iter=CB_ITER_OPT) for ind in pop])\n\n    best_idx = int(np.argmax(fitness_scores))\n    best_mask = pop[best_idx].copy()\n    best_score = fitness_scores[best_idx]\n    t1 = time.time()\n    log(f\"GWO DONE in {int(t1-t0)}s best_score={best_score:.4f} selected={int(np.sum(best_mask))}\")\n    return best_mask, best_score, int(t1-t0)\n\n# -------------------- UNION (only) --------------------\ndef get_union_mask(*masks):\n    union_idx = set()\n    for m in masks:\n        idxs = np.where(np.array(m).astype(bool))[0].tolist()\n        union_idx.update(idxs)\n    mask = np.zeros(N_FEATURES, dtype=int)\n    for i in union_idx:\n        mask[i] = 1\n    return mask\n\n# -------------------- HLO on candidates --------------------\ndef hlo_on_candidates(candidate_mask, pop_size=HLO_POP, iters=HLO_ITERS, cv=CV_OPT):\n    candidate_indices = np.where(np.array(candidate_mask).astype(bool))[0].tolist()\n    k = len(candidate_indices)\n    if k == 0:\n        raise ValueError(\"Candidate set is empty.\")\n\n    log(f\"HLO START on {k} candidate features (pop={pop_size}, iters={iters})\")\n    t0 = time.time()\n\n    pop = np.random.randint(0,2,(pop_size, k)).astype(int)\n\n    def fitness_candidate(bitmask):\n        full_mask = np.zeros(N_FEATURES, dtype=int)\n        for j,bit in enumerate(bitmask):\n            if bit == 1:\n                full_mask[candidate_indices[j]] = 1\n        return evaluate_mask_global(full_mask.astype(bool), cv=cv, cb_iter=CB_ITER_HLO)\n\n    fitness_scores = np.array([fitness_candidate(ind) for ind in pop])\n    best_idx = int(np.argmax(fitness_scores))\n    best_solution = pop[best_idx].copy()\n    best_score = fitness_scores[best_idx]\n\n    for it in range(iters):\n        log(f\" HLO iter {it+1}/{iters} current_best={best_score:.4f}\")\n        teacher = pop[int(np.argmax(fitness_scores))].copy()\n        new_pop = []\n        for i in range(pop_size):\n            learner = pop[i].copy()\n            # teaching phase\n            for d in range(k):\n                if np.random.rand() < HLO_TEACHER_FACTOR:\n                    learner[d] = teacher[d]\n            # peer learning\n            partner = pop[np.random.randint(pop_size)].copy()\n            for d in range(k):\n                if learner[d] != partner[d] and np.random.rand() < 0.5:\n                    learner[d] = partner[d]\n            # mutation\n            for d in range(k):\n                if np.random.rand() < HLO_MUTATION:\n                    learner[d] = 1 - learner[d]\n            new_pop.append(learner)\n        pop = np.array(new_pop)\n        fitness_scores = np.array([fitness_candidate(ind) for ind in pop])\n        gen_best_idx = int(np.argmax(fitness_scores))\n        gen_best_score = fitness_scores[gen_best_idx]\n        gen_best_sol = pop[gen_best_idx].copy()\n        if gen_best_score > best_score:\n            best_score = gen_best_score\n            best_solution = gen_best_sol.copy()\n\n    # map back to full mask\n    final_full_mask = np.zeros(N_FEATURES, dtype=int)\n    for j,bit in enumerate(best_solution):\n        if bit == 1:\n            final_full_mask[candidate_indices[j]] = 1\n\n    t1 = time.time()\n    log(f\"HLO DONE in {int(t1-t0)}s best_score={best_score:.4f} final_selected={int(np.sum(final_full_mask))}\")\n    return final_full_mask, best_score, int(t1-t0)\n\n# -------------------- Greedy Hill-Climb (local search) --------------------\ndef hill_climb_on_candidates(initial_mask, candidate_mask, max_steps=HILLCLIMB_MAX_STEPS, eval_cap=HILLCLIMB_EVAL_CAP, cv=CV_OPT):\n    candidate_indices = np.where(np.array(candidate_mask).astype(bool))[0].tolist()\n    if len(candidate_indices) == 0:\n        log(\"Hill-climb: candidate set empty, skipping.\")\n        return initial_mask, 0.0, 0\n\n    log(f\"Hill-climb START over {len(candidate_indices)} candidates (max_steps={max_steps}, eval_cap={eval_cap})\")\n    t0 = time.time()\n    current_mask = initial_mask.copy()\n    current_score = evaluate_mask_global(current_mask.astype(bool), cv=cv, cb_iter=CB_ITER_HLO)\n    evals = 0\n    steps = 0\n    improved = True\n\n    while improved and steps < max_steps and evals < eval_cap:\n        improved = False\n        for idx in np.random.permutation(candidate_indices):\n            trial_mask = current_mask.copy()\n            trial_mask[idx] = 1 - trial_mask[idx]  # flip\n            trial_score = evaluate_mask_global(trial_mask.astype(bool), cv=cv, cb_iter=CB_ITER_HLO)\n            evals += 1\n            if trial_score > current_score + 1e-8:\n                current_mask = trial_mask\n                current_score = trial_score\n                improved = True\n                steps += 1\n                log(f\" Hill-climb step {steps}: flipped {FEATURE_NAMES[idx]} -> new_score={current_score:.4f} (evals={evals})\")\n                break\n            if evals >= eval_cap or steps >= max_steps:\n                break\n    t1 = time.time()\n    log(f\"Hill-climb DONE in {int(t1-t0)}s steps={steps} evals={evals} final_score={current_score:.4f} selected={int(np.sum(current_mask))}\")\n    return current_mask, current_score, int(t1-t0)\n\n# -------------------- Final evaluation (5-fold CV) --------------------\ndef final_evaluation(mask_bool, cv=CV_FINAL, cb_iter=CB_ITER_FINAL):\n    idxs = np.where(np.array(mask_bool).astype(bool))[0].tolist()\n    if len(idxs) == 0:\n        raise ValueError(\"Final mask selects zero features.\")\n    X_sel = X.iloc[:, idxs]\n    model = get_catboost_model(iterations=cb_iter)\n    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=RANDOM_STATE)\n    accs = []; precs = []; recs = []; f1s = []\n    t0 = time.time()\n    for tr,te in skf.split(X_sel, y):\n        m = clone(model); m.fit(X_sel.iloc[tr], y.iloc[tr])\n        pred = m.predict(X_sel.iloc[te])\n        accs.append(accuracy_score(y.iloc[te], pred))\n        precs.append(precision_score(y.iloc[te], pred, zero_division=0))\n        recs.append(recall_score(y.iloc[te], pred, zero_division=0))\n        f1s.append(f1_score(y.iloc[te], pred, zero_division=0))\n    t1 = time.time()\n    results = {\n        \"n_features\": len(idxs),\n        \"features\": [FEATURE_NAMES[i] for i in idxs],\n        \"acc_mean\": float(np.mean(accs)), \"acc_std\": float(np.std(accs)),\n        \"prec_mean\": float(np.mean(precs)), \"prec_std\": float(np.std(precs)),\n        \"rec_mean\": float(np.mean(recs)), \"rec_std\": float(np.std(recs)),\n        \"f1_mean\": float(np.mean(f1s)), \"f1_std\": float(np.std(f1s)),\n        \"eval_time_s\": int(t1 - t0)\n    }\n    return results\n\n# -------------------- MAIN PIPELINE --------------------\nif __name__ == \"__main__\":\n    total_t0 = time.time()\n    log(\"===== HYBRID (reduced budget) + HLO + HILL-CLIMB (UNION only) START =====\")\n\n    # PSO\n    pso_mask, pso_score, pso_time = run_pso(swarm_size=PSO_SWARM, iters=PSO_ITERS, cv=CV_OPT)\n    pso_feats = mask_to_features(pso_mask)\n    log(f\"PSO selected ({len(pso_feats)}): {pso_feats}\")\n\n    # GA\n    ga_mask, ga_score, ga_time = run_ga(pop_size=GA_POP, gens=GA_GENS, cv=CV_OPT)\n    ga_feats = mask_to_features(ga_mask)\n    log(f\"GA selected ({len(ga_feats)}): {ga_feats}\")\n\n    # GWO\n    gwo_mask, gwo_score, gwo_time = run_gwo(wolves=GWO_WOLVES, iters=GWO_ITERS, cv=CV_OPT)\n    gwo_feats = mask_to_features(gwo_mask)\n    log(f\"GWO selected ({len(gwo_feats)}): {gwo_feats}\")\n\n    # Derive UNION of the three optimizers\n    union_mask = get_union_mask(pso_mask, ga_mask, gwo_mask)\n    union_feats = mask_to_features(union_mask)\n    log(f\"UNION candidate features ({len(union_feats)}): {union_feats}\")\n\n    # HLO on union\n    if len(union_feats) == 0:\n        log(\"UNION empty ‚Äî nothing to optimize. Exiting.\")\n        raise SystemExit(\"No union features selected by optimizers.\")\n\n    hlo_mask, hlo_score, hlo_time = hlo_on_candidates(union_mask, pop_size=HLO_POP, iters=HLO_ITERS, cv=CV_OPT)\n    hlo_feats = mask_to_features(hlo_mask)\n    log(f\"HLO final mask selected ({len(hlo_feats)}): {hlo_feats}\")\n\n    # Hill-climb restricted to union candidates\n    hc_mask, hc_score, hc_time = hill_climb_on_candidates(hlo_mask, union_mask, max_steps=HILLCLIMB_MAX_STEPS, eval_cap=HILLCLIMB_EVAL_CAP, cv=CV_OPT)\n    hc_feats = mask_to_features(hc_mask)\n    log(f\"Hill-climb final mask selected ({len(hc_feats)}): {hc_feats}\")\n\n    # Final CV evaluation (5-fold)\n    final_res = final_evaluation(hc_mask, cv=CV_FINAL, cb_iter=CB_ITER_FINAL)\n    log(f\"Final CV (5-fold) | n_features={final_res['n_features']} | F1={final_res['f1_mean']:.4f} ¬± {final_res['f1_std']:.4f}\")\n\n    # Train final CatBoost on 80% and evaluate on 20%, save model\n    selected_idxs = np.where(np.array(hc_mask).astype(bool))[0].tolist()\n    selected_features = [FEATURE_NAMES[i] for i in selected_idxs]\n\n    X_sel = X[selected_features]\n    X_train, X_test, y_train, y_test = train_test_split(X_sel, y, test_size=FINAL_TEST_SIZE, stratify=y, random_state=RANDOM_STATE)\n\n    model = get_catboost_model(iterations=CB_ITER_FINAL)\n    model.fit(X_train, y_train)\n\n    y_pred = model.predict(X_test)\n    test_acc = accuracy_score(y_test, y_pred)\n    test_prec = precision_score(y_test, y_pred, zero_division=0)\n    test_rec = recall_score(y_test, y_pred, zero_division=0)\n    test_f1 = f1_score(y_test, y_pred, zero_division=0)\n    test_cm = confusion_matrix(y_test, y_pred)\n    test_report = classification_report(y_test, y_pred, zero_division=0)\n\n    test_metrics = {\n        'acc': float(test_acc), 'prec': float(test_prec), 'rec': float(test_rec), 'f1': float(test_f1),\n        'n_test': int(X_test.shape[0]),\n        'confusion_matrix': test_cm.tolist(),  # convert to list for pickle/json friendliness\n        'classification_report': test_report\n    }\n\n    model_filename = f\"{SAVE_PREFIX}_union_model.pkl\"\n    with open(model_filename, 'wb') as mf:\n        pickle.dump(model, mf)\n\n    log(f\"Saved final CatBoost union model -> {model_filename} (test_f1={test_f1:.4f})\")\n\n    # Save aggregated results (only union)\n    out = {\n        \"pso_mask\": pso_mask, \"pso_score\": pso_score, \"pso_time\": pso_time,\n        \"ga_mask\": ga_mask, \"ga_score\": ga_score, \"ga_time\": ga_time,\n        \"gwo_mask\": gwo_mask, \"gwo_score\": gwo_score, \"gwo_time\": gwo_time,\n        \"union_mask\": union_mask,\n        \"hlo_mask\": hlo_mask, \"hlo_score\": hlo_score, \"hlo_time\": hlo_time,\n        \"hc_mask\": hc_mask, \"hc_score\": hc_score, \"hc_time\": hc_time,\n        \"final_eval\": final_res,\n        \"selected_features\": selected_features,\n        \"model_file\": model_filename,\n        \"test_metrics\": test_metrics,\n        \"fitness_cache_len\": len(fitness_cache)\n    }\n    with open(f\"{SAVE_PREFIX}_results.pkl\", \"wb\") as f:\n        pickle.dump(out, f)\n\n    total_t1 = time.time()\n    elapsed_total = int(total_t1 - total_t0)\n    log(f\"PIPELINE COMPLETE in {elapsed_total}s. Results saved to {SAVE_PREFIX}_results.pkl and model {model_filename}\")\n\n    # Print short summary and explicit final test metrics (requested)\n    print(\"\\n=== SUMMARY ===\")\n    print(f\"PSO selected ({len(pso_feats)}): {pso_feats}\")\n    print(f\"GA selected  ({len(ga_feats)}): {ga_feats}\")\n    print(f\"GWO selected ({len(gwo_feats)}): {gwo_feats}\")\n    print(f\"UNION candidates ({len(union_feats)}): {union_feats}\")\n    print(f\"HLO selected ({len(hlo_feats)}): {hlo_feats}\")\n    print(f\"HILL-CLIMB selected ({len(hc_feats)}): {hc_feats}\")\n    print(f\"Final CV F1: {final_res['f1_mean']:.4f} ¬± {final_res['f1_std']:.4f}\")\n\n    # Final test set metrics (explicit printout)\n    print(\"\\n--- FINAL TEST METRICS (80/20 held-out) ---\")\n    print(f\"Test samples (n_test) : {test_metrics['n_test']}\")\n    print(f\"Accuracy : {test_metrics['acc']:.4f}\")\n    print(f\"Precision: {test_metrics['prec']:.4f}\")\n    print(f\"Recall   : {test_metrics['rec']:.4f}\")\n    print(f\"F1-score : {test_metrics['f1']:.4f}\")\n    print(\"\\nConfusion Matrix (rows=true / cols=pred):\")\n    print(np.array(test_metrics['confusion_matrix']))\n    print(\"\\nClassification Report:\")\n    print(test_metrics['classification_report'])\n\n    print(f\"\\nModel saved to: {model_filename}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:07:49.994630Z","iopub.status.idle":"2025-12-05T11:07:49.994898Z","shell.execute_reply.started":"2025-12-05T11:07:49.994774Z","shell.execute_reply":"2025-12-05T11:07:49.994787Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# intersection_hlo_with_hillclimb_fast.py\n# Pipeline (reduced budget + hill-climb) with UNION, INTERSECTION, and VOTING candidate flows:\n#  PSO + GA + GWO (CatBoost fitness, lighter during opt) -> derive UNION / INTERSECTION / VOTING\n#  For each candidate set: HLO (on candidates) -> Greedy hill-climb (restricted) -> Final CatBoost eval (5-fold CV)\n#  Additionally: train a CatBoost model on 80% of the data and evaluate on the held-out 20% test set\n#  Train & save a CatBoost model for each flow (union / intersection / voting) using the 80/20 split.\n# Prints logs, mean ¬± std for metrics, stage timings, saves results and models.\n\nimport time\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport warnings\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, make_scorer\nfrom sklearn.base import clone\n\nwarnings.filterwarnings(\"ignore\")\nnp.random.seed(42)\n\n# -------------------- USER / EXPERIMENT SETTINGS --------------------\n# If you prefer to load CSV instead, uncomment and change:\ndf = pd.read_csv(\"/kaggle/input/cleaned-ids/ids2018_cleaned_combined_1.csv\")\n\nTARGET_COL = \"Label\"   # target column\nMODEL_VERBOSE = 0            # CatBoost verbosity: 0 = silent\nRANDOM_STATE = 42\n\n# ---------- Reduced budgets for faster runs (you can tune these) ----------\nPSO_SWARM = 15   # reduced swarm\nPSO_ITERS = 5   # reduced iterations\n\nGA_POP = 30      # reduced population\nGA_GENS = 5     # reduced generations\n\nGWO_WOLVES = 10\nGWO_ITERS = 5\n\nHLO_POP = 15\nHLO_ITERS = 5\nHLO_TEACHER_FACTOR = 0.75\nHLO_MUTATION = 0.12\n\n# Greedy hill-climb after HLO\nHILLCLIMB_MAX_STEPS = 100   # stop if no improvement or step limit\nHILLCLIMB_EVAL_CAP = 500    # safety cap on evaluations (prevent runaway)\n\n# CV folds\nCV_OPT = 2    # cheaper CV during optimization + HLO (speed)\nCV_FINAL = 5  # final evaluation (A1 requested)\n\n# CatBoost iterations\nCB_ITER_OPT = 100    # iterations during optimization (smaller)\nCB_ITER_HLO = 200\nCB_ITER_FINAL = 500  # final evaluation iterations (bigger)\n\n# Train/test split for final saved models\nFINAL_TEST_SIZE = 0.2\n\nSAVE_PREFIX = \"hybrid_hlo_models\"\n# ------------------------------------------------------------------------\n\n# Ensure df exists\ntry:\n    df\nexcept NameError:\n    raise RuntimeError(\"DataFrame `df` not found. Assign your dataset to variable `df` or load at top.\")\n\n# Prepare data\nX = df.drop(TARGET_COL, axis=1)\n\ny = df[TARGET_COL].astype(int)\nFEATURE_NAMES = X.columns.tolist()\nN_FEATURES = X.shape[1]\n\n# -------------------- Model factory (CatBoost) --------------------\ndef get_catboost_model(iterations=100):\n    try:\n        from catboost import CatBoostClassifier\n    except Exception as e:\n        raise ImportError(\"catboost not installed. Install with: pip install catboost\") from e\n    return CatBoostClassifier(iterations=iterations, learning_rate=0.05, depth=6,\n                              verbose=MODEL_VERBOSE, random_seed=RANDOM_STATE, thread_count=-1)\n\n# -------------------- Fitness cache --------------------\n# key: tuple(selected original indices) -> float score\nfitness_cache = {}\n\ndef key_from_mask(mask_bool):\n    return tuple(sorted(np.where(np.array(mask_bool).astype(bool))[0].tolist()))\n\ndef evaluate_mask_global(mask_bool, cv=CV_OPT, cb_iter=CB_ITER_OPT):\n    \"\"\"\n    Evaluate mask using CatBoost with CV and return average of acc,prec,rec,f1.\n    Caches results to avoid re-evaluating identical subsets.\n    \"\"\"\n    key = key_from_mask(mask_bool)\n    if key in fitness_cache:\n        return fitness_cache[key]\n    if len(key) == 0:\n        fitness_cache[key] = 0.0\n        return 0.0\n\n    X_sel = X.iloc[:, list(key)]\n    model = get_catboost_model(iterations=cb_iter)\n    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=RANDOM_STATE)\n\n    accs = cross_val_score(clone(model), X_sel, y, cv=skf, scoring=\"accuracy\", n_jobs=-1)\n    precs = cross_val_score(clone(model), X_sel, y, cv=skf, scoring=make_scorer(precision_score, zero_division=0), n_jobs=-1)\n    recs = cross_val_score(clone(model), X_sel, y, cv=skf, scoring=make_scorer(recall_score, zero_division=0), n_jobs=-1)\n    f1s = cross_val_score(clone(model), X_sel, y, cv=skf, scoring=make_scorer(f1_score, zero_division=0), n_jobs=-1)\n\n    score = float((np.mean(accs) + np.mean(precs) + np.mean(recs) + np.mean(f1s)) / 4.0)\n    fitness_cache[key] = score\n    return score\n\n# -------------------- Helpers --------------------\ndef mask_to_features(mask):\n    idxs = np.where(np.array(mask).astype(bool))[0].tolist()\n    return [FEATURE_NAMES[i] for i in idxs]\n\ndef log(msg):\n    print(f\"[{time.strftime('%H:%M:%S')}] {msg}\", flush=True)\n\n# -------------------- PSO (binary) --------------------\ndef run_pso(swarm_size=PSO_SWARM, iters=PSO_ITERS, cv=CV_OPT):\n    log(f\"PSO START (swarm={swarm_size}, iters={iters}, cv={cv})\")\n    t0 = time.time()\n    dim = N_FEATURES\n    pos = np.random.randint(0,2,(swarm_size,dim)).astype(int)\n    vel = np.random.uniform(-1,1,(swarm_size,dim))\n\n    pbest = pos.copy()\n    pbest_scores = np.array([evaluate_mask_global(p.astype(bool), cv=cv, cb_iter=CB_ITER_OPT) for p in pos])\n\n    gbest_idx = int(np.argmax(pbest_scores))\n    gbest = pbest[gbest_idx].copy()\n    gbest_score = pbest_scores[gbest_idx]\n\n    w = 0.6; c1 = c2 = 1.5\n    for t in range(iters):\n        log(f\" PSO iter {t+1}/{iters} best_global={gbest_score:.4f}\")\n        for i in range(swarm_size):\n            r1 = np.random.rand(dim); r2 = np.random.rand(dim)\n            vel[i] = w*vel[i] + c1*r1*(pbest[i] - pos[i]) + c2*r2*(gbest - pos[i])\n            s = 1.0 / (1.0 + np.exp(-vel[i]))\n            pos[i] = (np.random.rand(dim) < s).astype(int)\n\n            sc = evaluate_mask_global(pos[i].astype(bool), cv=cv, cb_iter=CB_ITER_OPT)\n            if sc > pbest_scores[i]:\n                pbest[i] = pos[i].copy()\n                pbest_scores[i] = sc\n            if sc > gbest_score:\n                gbest = pos[i].copy()\n                gbest_score = sc\n        w = max(0.2, w*0.97)\n\n    best_idx = int(np.argmax(pbest_scores))\n    best_mask = pbest[best_idx].copy()\n    best_score = pbest_scores[best_idx]\n    t1 = time.time()\n    log(f\"PSO DONE in {int(t1-t0)}s best_score={best_score:.4f} selected={int(np.sum(best_mask))}\")\n    log(f\"PSO SELECTED FEATURES: {mask_to_features(best_mask)}\")\n\n    return best_mask, best_score, int(t1-t0)\n\n# -------------------- GA (binary) --------------------\ndef run_ga(pop_size=GA_POP, gens=GA_GENS, cv=CV_OPT):\n    log(f\"GA START (pop={pop_size}, gens={gens}, cv={cv})\")\n    t0 = time.time()\n    dim = N_FEATURES\n    pop = np.random.randint(0,2,(pop_size, dim)).astype(int)\n    fitness_scores = np.array([evaluate_mask_global(ind.astype(bool), cv=cv, cb_iter=CB_ITER_OPT) for ind in pop])\n\n    def tournament_select(k=3):\n        idxs = np.random.randint(0, pop_size, k)\n        return idxs[np.argmax(fitness_scores[idxs])]\n\n    for g in range(gens):\n        log(f\" GA gen {g+1}/{gens} current_best={np.max(fitness_scores):.4f}\")\n        new_pop = []\n        # elitism\n        elite_idxs = np.argsort(fitness_scores)[-2:]\n        new_pop.extend(pop[elite_idxs].tolist())\n\n        while len(new_pop) < pop_size:\n            i1 = tournament_select(); i2 = tournament_select()\n            p1 = pop[i1].copy(); p2 = pop[i2].copy()\n            # crossover\n            if np.random.rand() < 0.7:\n                pt = np.random.randint(1, dim)\n                c1 = np.concatenate([p1[:pt], p2[pt:]])\n                c2 = np.concatenate([p2[:pt], p1[pt:]])\n            else:\n                c1, c2 = p1, p2\n            # mutation\n            for child in (c1, c2):\n                for d in range(dim):\n                    if np.random.rand() < 0.1:\n                        child[d] = 1 - child[d]\n                new_pop.append(child)\n                if len(new_pop) >= pop_size:\n                    break\n        pop = np.array(new_pop[:pop_size])\n        fitness_scores = np.array([evaluate_mask_global(ind.astype(bool), cv=cv, cb_iter=CB_ITER_OPT) for ind in pop])\n\n    best_idx = int(np.argmax(fitness_scores))\n    best_mask = pop[best_idx].copy()\n    best_score = fitness_scores[best_idx]\n    t1 = time.time()\n    log(f\"GA DONE in {int(t1-t0)}s best_score={best_score:.4f} selected={int(np.sum(best_mask))}\")\n    log(f\"GA SELECTED FEATURES: {mask_to_features(best_mask)}\")\n\n    return best_mask, best_score, int(t1-t0)\n\n# -------------------- GWO (binary) --------------------\ndef run_gwo(wolves=GWO_WOLVES, iters=GWO_ITERS, cv=CV_OPT):\n    log(f\"GWO START (wolves={wolves}, iters={iters}, cv={cv})\")\n    t0 = time.time()\n    dim = N_FEATURES\n    pop = np.random.randint(0,2,(wolves, dim)).astype(int)\n    fitness_scores = np.array([evaluate_mask_global(ind.astype(bool), cv=cv, cb_iter=CB_ITER_OPT) for ind in pop])\n\n    Alpha = Beta = Delta = None\n    Alpha_score = Beta_score = Delta_score = -1.0\n\n    for itr in range(iters):\n        log(f\" GWO iter {itr+1}/{iters} best_alpha={Alpha_score:.4f}\")\n        for i in range(wolves):\n            sc = fitness_scores[i]\n            if sc > Alpha_score:\n                Delta_score, Beta_score, Alpha_score = Beta_score, Alpha_score, sc\n                Delta, Beta, Alpha = Beta, Alpha, pop[i].copy()\n            elif sc > Beta_score:\n                Delta_score, Beta_score = Beta_score, sc\n                Delta, Beta = Beta, pop[i].copy()\n            elif sc > Delta_score:\n                Delta_score = sc\n                Delta = pop[i].copy()\n\n        a = 2 - itr * (2.0 / iters)\n        for i in range(wolves):\n            for d in range(dim):\n                if Alpha is None:\n                    continue\n                r1, r2 = np.random.rand(), np.random.rand()\n                A1 = 2 * a * r1 - a; C1 = 2 * r2\n                D_alpha = abs(C1 * Alpha[d] - pop[i][d])\n                X1 = Alpha[d] - A1 * D_alpha\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                A2 = 2 * a * r1 - a; C2 = 2 * r2\n                D_beta = abs(C2 * Beta[d] - pop[i][d])\n                X2 = Beta[d] - A2 * D_beta\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                A3 = 2 * a * r1 - a; C3 = 2 * r2\n                D_delta = abs(C3 * Delta[d] - pop[i][d])\n                X3 = Delta[d] - A3 * D_delta\n\n                new_pos = (X1 + X2 + X3) / 3.0\n                s = 1.0 / (1.0 + np.exp(-new_pos))\n                pop[i][d] = 1 if np.random.rand() < s else 0\n\n        fitness_scores = np.array([evaluate_mask_global(ind.astype(bool), cv=cv, cb_iter=CB_ITER_OPT) for ind in pop])\n\n    best_idx = int(np.argmax(fitness_scores))\n    best_mask = pop[best_idx].copy()\n    best_score = fitness_scores[best_idx]\n    t1 = time.time()\n    log(f\"GWO DONE in {int(t1-t0)}s best_score={best_score:.4f} selected={int(np.sum(best_mask))}\")\n    log(f\"GWO SELECTED FEATURES: {mask_to_features(best_mask)}\")\n\n    return best_mask, best_score, int(t1-t0)\n\n# -------------------- INTERSECTION / UNION / VOTING --------------------\ndef get_intersection_mask(*masks):\n    \"\"\"Return mask that contains only features present in ALL provided masks.\"\"\"\n    if len(masks) == 0:\n        return np.zeros(N_FEATURES, dtype=int)\n    inter_idx = set(np.where(np.array(masks[0]).astype(bool))[0].tolist())\n    for m in masks[1:]:\n        idxs = set(np.where(np.array(m).astype(bool))[0].tolist())\n        inter_idx = inter_idx.intersection(idxs)\n    mask = np.zeros(N_FEATURES, dtype=int)\n    for i in inter_idx:\n        mask[i] = 1\n    return mask\n\n\ndef get_union_mask(*masks):\n    union_idx = set()\n    for m in masks:\n        idxs = np.where(np.array(m).astype(bool))[0].tolist()\n        union_idx.update(idxs)\n    mask = np.zeros(N_FEATURES, dtype=int)\n    for i in union_idx:\n        mask[i] = 1\n    return mask\n\n\ndef get_voting_mask(*masks, threshold=2):\n    \"\"\"Return mask of features selected by at least `threshold` methods (default majority of 3 => 2).\"\"\"\n    if len(masks) == 0:\n        return np.zeros(N_FEATURES, dtype=int)\n    counts = np.zeros(N_FEATURES, dtype=int)\n    for m in masks:\n        counts += np.array(m).astype(int)\n    mask = (counts >= threshold).astype(int)\n    return mask\n\n# -------------------- HLO on candidates --------------------\ndef hlo_on_candidates(candidate_mask, pop_size=HLO_POP, iters=HLO_ITERS, cv=CV_OPT):\n    candidate_indices = np.where(np.array(candidate_mask).astype(bool))[0].tolist()\n    k = len(candidate_indices)\n    if k == 0:\n        raise ValueError(\"Candidate set is empty.\")\n\n    log(f\"HLO START on {k} candidate features (pop={pop_size}, iters={iters})\")\n    t0 = time.time()\n\n    pop = np.random.randint(0,2,(pop_size, k)).astype(int)\n\n    def fitness_candidate(bitmask):\n        full_mask = np.zeros(N_FEATURES, dtype=int)\n        for j,bit in enumerate(bitmask):\n            if bit == 1:\n                full_mask[candidate_indices[j]] = 1\n        return evaluate_mask_global(full_mask.astype(bool), cv=cv, cb_iter=CB_ITER_HLO)\n\n    fitness_scores = np.array([fitness_candidate(ind) for ind in pop])\n    best_idx = int(np.argmax(fitness_scores))\n    best_solution = pop[best_idx].copy()\n    best_score = fitness_scores[best_idx]\n\n    for it in range(iters):\n        log(f\" HLO iter {it+1}/{iters} current_best={best_score:.4f}\")\n        teacher = pop[int(np.argmax(fitness_scores))].copy()\n        new_pop = []\n        for i in range(pop_size):\n            learner = pop[i].copy()\n            # teaching phase\n            for d in range(k):\n                if np.random.rand() < HLO_TEACHER_FACTOR:\n                    learner[d] = teacher[d]\n            # peer learning\n            partner = pop[np.random.randint(pop_size)].copy()\n            for d in range(k):\n                if learner[d] != partner[d] and np.random.rand() < 0.5:\n                    learner[d] = partner[d]\n            # mutation\n            for d in range(k):\n                if np.random.rand() < HLO_MUTATION:\n                    learner[d] = 1 - learner[d]\n            new_pop.append(learner)\n        pop = np.array(new_pop)\n        fitness_scores = np.array([fitness_candidate(ind) for ind in pop])\n        gen_best_idx = int(np.argmax(fitness_scores))\n        gen_best_score = fitness_scores[gen_best_idx]\n        gen_best_sol = pop[gen_best_idx].copy()\n        if gen_best_score > best_score:\n            best_score = gen_best_score\n            best_solution = gen_best_sol.copy()\n\n    # map back to full mask\n    final_full_mask = np.zeros(N_FEATURES, dtype=int)\n    for j,bit in enumerate(best_solution):\n        if bit == 1:\n            final_full_mask[candidate_indices[j]] = 1\n\n    t1 = time.time()\n    log(f\"HLO DONE in {int(t1-t0)}s best_score={best_score:.4f} final_selected={int(np.sum(final_full_mask))}\")\n    return final_full_mask, best_score, int(t1-t0)\n\n# -------------------- Greedy Hill-Climb (local search) --------------------\ndef hill_climb_on_candidates(initial_mask, candidate_mask, max_steps=HILLCLIMB_MAX_STEPS, eval_cap=HILLCLIMB_EVAL_CAP, cv=CV_OPT):\n    \"\"\"\n    Greedy single-bit flip hill-climb restricted to candidate indices.\n    Starts from initial_mask (full-length). Tries flipping each candidate feature's bit:\n    - If flip improves fitness, accept and restart scanning.\n    - Stops when no improving flip found or max_steps/eval_cap reached.\n    \"\"\"\n    candidate_indices = np.where(np.array(candidate_mask).astype(bool))[0].tolist()\n    if len(candidate_indices) == 0:\n        log(\"Hill-climb: candidate set empty, skipping.\")\n        return initial_mask, 0.0, 0\n\n    log(f\"Hill-climb START over {len(candidate_indices)} candidates (max_steps={max_steps}, eval_cap={eval_cap})\")\n    t0 = time.time()\n    current_mask = initial_mask.copy()\n    current_score = evaluate_mask_global(current_mask.astype(bool), cv=cv, cb_iter=CB_ITER_HLO)\n    evals = 0\n    steps = 0\n    improved = True\n\n    while improved and steps < max_steps and evals < eval_cap:\n        improved = False\n        for idx in np.random.permutation(candidate_indices):\n            trial_mask = current_mask.copy()\n            trial_mask[idx] = 1 - trial_mask[idx]  # flip\n            trial_score = evaluate_mask_global(trial_mask.astype(bool), cv=cv, cb_iter=CB_ITER_HLO)\n            evals += 1\n            if trial_score > current_score + 1e-8:\n                current_mask = trial_mask\n                current_score = trial_score\n                improved = True\n                steps += 1\n                log(f\" Hill-climb step {steps}: flipped {FEATURE_NAMES[idx]} -> new_score={current_score:.4f} (evals={evals})\")\n                break\n            if evals >= eval_cap or steps >= max_steps:\n                break\n    t1 = time.time()\n    log(f\"Hill-climb DONE in {int(t1-t0)}s steps={steps} evals={evals} final_score={current_score:.4f} selected={int(np.sum(current_mask))}\")\n    return current_mask, current_score, int(t1-t0)\n\n# -------------------- Final evaluation (5-fold CV) --------------------\ndef final_evaluation(mask_bool, cv=CV_FINAL, cb_iter=CB_ITER_FINAL):\n    idxs = np.where(np.array(mask_bool).astype(bool))[0].tolist()\n    if len(idxs) == 0:\n        raise ValueError(\"Final mask selects zero features.\")\n    X_sel = X.iloc[:, idxs]\n    model = get_catboost_model(iterations=cb_iter)\n    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=RANDOM_STATE)\n    accs = []; precs = []; recs = []; f1s = []\n    t0 = time.time()\n    for tr,te in skf.split(X_sel, y):\n        m = clone(model); m.fit(X_sel.iloc[tr], y.iloc[tr])\n        pred = m.predict(X_sel.iloc[te])\n        accs.append(accuracy_score(y.iloc[te], pred))\n        precs.append(precision_score(y.iloc[te], pred, zero_division=0))\n        recs.append(recall_score(y.iloc[te], pred, zero_division=0))\n        f1s.append(f1_score(y.iloc[te], pred, zero_division=0))\n    t1 = time.time()\n    results = {\n        \"n_features\": len(idxs),\n        \"features\": [FEATURE_NAMES[i] for i in idxs],\n        \"acc_mean\": float(np.mean(accs)), \"acc_std\": float(np.std(accs)),\n        \"prec_mean\": float(np.mean(precs)), \"prec_std\": float(np.std(precs)),\n        \"rec_mean\": float(np.mean(recs)), \"rec_std\": float(np.std(recs)),\n        \"f1_mean\": float(np.mean(f1s)), \"f1_std\": float(np.std(f1s)),\n        \"eval_time_s\": int(t1 - t0)\n    }\n    return results\n\n# -------------------- MAIN PIPELINE --------------------\nif __name__ == \"__main__\":\n    total_t0 = time.time()\n    log(\"===== HYBRID (reduced budget) + HLO + HILL-CLIMB (UNION/INTERSECTION/VOTING) START =====\")\n\n    # PSO\n    pso_mask, pso_score, pso_time = run_pso(swarm_size=PSO_SWARM, iters=PSO_ITERS, cv=CV_OPT)\n\n    # GA\n    ga_mask, ga_score, ga_time = run_ga(pop_size=GA_POP, gens=GA_GENS, cv=CV_OPT)\n\n    # GWO\n    gwo_mask, gwo_score, gwo_time = run_gwo(wolves=GWO_WOLVES, iters=GWO_ITERS, cv=CV_OPT)\n\n    # Derive candidate masks\n    union_mask = get_union_mask(pso_mask, ga_mask, gwo_mask)\n    inter_mask = get_intersection_mask(pso_mask, ga_mask, gwo_mask)\n    vote_mask = get_voting_mask(pso_mask, ga_mask, gwo_mask, threshold=2)\n\n    candidate_sets = {\n        'union': union_mask,\n        'intersection': inter_mask,\n        'voting': vote_mask\n    }\n\n    results_all = {}\n\n    # run HLO -> hill-climb -> final evaluation -> train & save model for each candidate set\n    for name, cand_mask in candidate_sets.items():\n        log(f\"===== PROCESSING {name.upper()} CANDIDATES =====\")\n        n_cand = int(np.sum(cand_mask))\n        log(f\"{name.upper()} candidate features: {n_cand}\")\n        if n_cand == 0:\n            log(f\"{name.upper()} empty ‚Äî skipping HLO/hill-climb and model training.\")\n            results_all[name] = {'skipped': True, 'n_candidates': 0}\n            continue\n\n        # HLO on this candidate set\n        hlo_mask, hlo_score, hlo_time = hlo_on_candidates(cand_mask, pop_size=HLO_POP, iters=HLO_ITERS, cv=CV_OPT)\n\n        # hill-climb restricted to candidate set\n        hc_mask, hc_score, hc_time = hill_climb_on_candidates(hlo_mask, cand_mask, max_steps=HILLCLIMB_MAX_STEPS, eval_cap=HILLCLIMB_EVAL_CAP, cv=CV_OPT)\n\n        # final CV evaluation\n        final_res = final_evaluation(hc_mask, cv=CV_FINAL, cb_iter=CB_ITER_FINAL)\n\n        # Train final CatBoost model on 80% train and evaluate on 20% test (stratified)\n        sel_idxs = np.where(np.array(hc_mask).astype(bool))[0].tolist()\n        sel_features = [FEATURE_NAMES[i] for i in sel_idxs]\n\n        if len(sel_features) == 0:\n            log(f\"No features selected after hill-climb for {name}, skipping model train.\")\n            results_all[name] = {'skipped': True, 'n_candidates': n_cand}\n            continue\n\n        X_sel = X[sel_features]\n        X_train, X_test, y_train, y_test = train_test_split(X_sel, y, test_size=FINAL_TEST_SIZE, stratify=y, random_state=RANDOM_STATE)\n\n        model = get_catboost_model(iterations=CB_ITER_FINAL)\n        model.fit(X_train, y_train)\n\n        # evaluate on held-out test set (20%)\n        y_pred = model.predict(X_test)\n        test_acc = accuracy_score(y_test, y_pred)\n        test_prec = precision_score(y_test, y_pred, zero_division=0)\n        test_rec = recall_score(y_test, y_pred, zero_division=0)\n        test_f1 = f1_score(y_test, y_pred, zero_division=0)\n\n        test_metrics = {\n            'acc': float(test_acc), 'prec': float(test_prec), 'rec': float(test_rec), 'f1': float(test_f1),\n            'n_test': int(X_test.shape[0])\n        }\n\n        # Save model to file (pickle)\n        model_filename = f\"{SAVE_PREFIX}_{name}_model.pkl\"\n        with open(model_filename, 'wb') as mf:\n            pickle.dump(model, mf)\n\n        # store results\n        results_all[name] = {\n            'n_candidates': n_cand,\n            'hlo_score': float(hlo_score), 'hlo_time': int(hlo_time),\n            'hc_score': float(hc_score), 'hc_time': int(hc_time),\n            'final_eval': final_res,\n            'selected_features': sel_features,\n            'model_file': model_filename,\n            'test_metrics': test_metrics\n        }\n\n        log(f\"Saved trained CatBoost model for {name} -> {model_filename} (test_f1={test_f1:.4f})\")\n\n    total_t1 = time.time()\n    elapsed_total = int(total_t1 - total_t0)\n\n    # Summary / save aggregated results\n    print(\"==================== AGGREGATE SUMMARY ====================\")\n    print(f\"PSO  -> opt_score={pso_score:.4f} selected={int(np.sum(pso_mask))} time={pso_time}s\")\n    print(f\"GA   -> opt_score={ga_score:.4f} selected={int(np.sum(ga_mask))} time={ga_time}s\")\n    print(f\"GWO  -> opt_score={gwo_score:.4f} selected={int(np.sum(gwo_mask))} time={gwo_time}s\")\n    print(f\"Union candidates    : {int(np.sum(union_mask))}\")\n    print(f\"Intersection candidates: {int(np.sum(inter_mask))}\")\n    print(f\"Voting candidates   : {int(np.sum(vote_mask))}\")\n    print(\"-------------------------------------------------\")\n\n    for name, info in results_all.items():\n        print(f\"-- {name.upper()} SUMMARY --\")\n        if info.get('skipped'):\n            print(\" skipped (no candidates)\")\n            continue\n        fe = info['final_eval']\n        tm = info['test_metrics']\n        print(f\" Selected ({fe['n_features']}): {fe['features']}\")\n        print(f\" CV F1   : {fe['f1_mean']:.4f} ¬± {fe['f1_std']:.4f}\")\n        print(f\" Test F1 : {tm['f1']:.4f} (n_test={tm['n_test']})\")\n        print(f\" Accuracy : {fe['acc_mean']:.4f} ¬± {fe['acc_std']:.4f}\")\n        print(f\" Precision: {fe['prec_mean']:.4f} ¬± {fe['prec_std']:.4f}\")\n        print(f\" Recall   : {fe['rec_mean']:.4f} ¬± {fe['rec_std']:.4f}\")\n        print(f\" Model file: {info['model_file']}\")\n\n\n\n    # Save aggregated pipeline outputs\n    out = {\n        \"pso_mask\": pso_mask, \"pso_score\": pso_score, \"pso_time\": pso_time,\n        \"ga_mask\": ga_mask, \"ga_score\": ga_score, \"ga_time\": ga_time,\n        \"gwo_mask\": gwo_mask, \"gwo_score\": gwo_score, \"gwo_time\": gwo_time,\n        \"union_mask\": union_mask, \"intersection_mask\": inter_mask, \"voting_mask\": vote_mask,\n        \"results_all\": results_all,\n        \"fitness_cache_len\": len(fitness_cache)\n    }\n    with open(f\"{SAVE_PREFIX}_results.pkl\", \"wb\") as f:\n        pickle.dump(out, f)\n\n    log(f\"Saved results to {SAVE_PREFIX}_results.pkl\")\n    log(\"===== PIPELINE COMPLETE =====\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:07:49.996810Z","iopub.status.idle":"2025-12-05T11:07:49.997107Z","shell.execute_reply.started":"2025-12-05T11:07:49.996960Z","shell.execute_reply":"2025-12-05T11:07:49.996973Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pickle\nimport pandas as pd\n\n# Choose which model to test\nflow = \"union\"               # \"intersection\" or \"voting\"\n\n# Load results dictionary (contains selected features for every flow)\nresults_path = \"/kaggle/working/hybrid_hlo_models_results.pkl\"\n\nwith open(results_path, \"rb\") as f:\n    results_all = pickle.load(f)\n\n# Get selected features\nselected_features = results_all[\"results_all\"][flow][\"selected_features\"]\nprint(\"Selected features for\", flow, \":\", selected_features)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:55:30.065523Z","iopub.execute_input":"2025-12-05T11:55:30.065812Z","iopub.status.idle":"2025-12-05T11:55:30.077149Z","shell.execute_reply.started":"2025-12-05T11:55:30.065789Z","shell.execute_reply":"2025-12-05T11:55:30.076332Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/212153027.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mresults_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/kaggle/working/hybrid_hlo_models_results.pkl\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mresults_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/working/hybrid_hlo_models_results.pkl'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/working/hybrid_hlo_models_results.pkl'","output_type":"error"}],"execution_count":70},{"cell_type":"code","source":"model_path = f\"/kaggle/working/hybrid_hlo_models_{flow}_model.pkl\"\n\nwith open(model_path, \"rb\") as f:\n    model = pickle.load(f)\n\nprint(\"Model loaded:\", model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:07:49.998904Z","iopub.status.idle":"2025-12-05T11:07:49.999506Z","shell.execute_reply.started":"2025-12-05T11:07:49.999387Z","shell.execute_reply":"2025-12-05T11:07:49.999403Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df2017 = pd.read_parquet(\"/kaggle/input/cicids2017/Botnet-Friday-no-metadata.parquet\")\n\nprint(\"Columns in 2017 dataset:\", df2017.columns.tolist())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:55:17.316146Z","iopub.execute_input":"2025-12-05T11:55:17.316475Z","iopub.status.idle":"2025-12-05T11:55:17.378600Z","shell.execute_reply.started":"2025-12-05T11:55:17.316452Z","shell.execute_reply":"2025-12-05T11:55:17.377942Z"}},"outputs":[{"name":"stdout","text":"Columns in 2017 dataset: ['Protocol', 'Flow Duration', 'Total Fwd Packets', 'Total Backward Packets', 'Fwd Packets Length Total', 'Bwd Packets Length Total', 'Fwd Packet Length Max', 'Fwd Packet Length Min', 'Fwd Packet Length Mean', 'Fwd Packet Length Std', 'Bwd Packet Length Max', 'Bwd Packet Length Min', 'Bwd Packet Length Mean', 'Bwd Packet Length Std', 'Flow Bytes/s', 'Flow Packets/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Total', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Total', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Header Length', 'Bwd Header Length', 'Fwd Packets/s', 'Bwd Packets/s', 'Packet Length Min', 'Packet Length Max', 'Packet Length Mean', 'Packet Length Std', 'Packet Length Variance', 'FIN Flag Count', 'SYN Flag Count', 'RST Flag Count', 'PSH Flag Count', 'ACK Flag Count', 'URG Flag Count', 'CWE Flag Count', 'ECE Flag Count', 'Down/Up Ratio', 'Avg Packet Size', 'Avg Fwd Segment Size', 'Avg Bwd Segment Size', 'Fwd Avg Bytes/Bulk', 'Fwd Avg Packets/Bulk', 'Fwd Avg Bulk Rate', 'Bwd Avg Bytes/Bulk', 'Bwd Avg Packets/Bulk', 'Bwd Avg Bulk Rate', 'Subflow Fwd Packets', 'Subflow Fwd Bytes', 'Subflow Bwd Packets', 'Subflow Bwd Bytes', 'Init Fwd Win Bytes', 'Init Bwd Win Bytes', 'Fwd Act Data Packets', 'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min', 'Label']\n","output_type":"stream"}],"execution_count":68},{"cell_type":"code","source":"# Check missing features\nmissing = [f for f in selected_features if f not in df2017.columns]\n\nif missing:\n    print(\"‚ùå Missing in 2017 dataset:\", missing)\n    raise ValueError(\"Dataset missing required features.\")\n\n# Create X_test\nX_test_2017 = df2017[selected_features]\n\n# Extract y if present\nif \"Label\" in df2017.columns:\n    y_test_2017 = df2017[\"Label\"].astype(int)\nelse:\n    y_test_2017 = None\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:55:20.825705Z","iopub.execute_input":"2025-12-05T11:55:20.825997Z","iopub.status.idle":"2025-12-05T11:55:20.838033Z","shell.execute_reply.started":"2025-12-05T11:55:20.825975Z","shell.execute_reply":"2025-12-05T11:55:20.836948Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/2298054010.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Check missing features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmissing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mselected_features\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf2017\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"‚ùå Missing in 2017 dataset:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'selected_features' is not defined"],"ename":"NameError","evalue":"name 'selected_features' is not defined","output_type":"error"}],"execution_count":69},{"cell_type":"code","source":"y_pred_2017 = model.predict(X_test_2017)\n\nif y_test_2017 is not None:\n    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n    acc  = accuracy_score(y_test_2017, y_pred_2017)\n    prec = precision_score(y_test_2017, y_pred_2017, zero_division=0)\n    rec  = recall_score(y_test_2017, y_pred_2017, zero_division=0)\n    f1   = f1_score(y_test_2017, y_pred_2017, zero_division=0)\n\n    print(f\"\\n=== {flow.upper()} model results on 2017 dataset ===\")\n    print(\"Accuracy :\", acc)\n    print(\"Precision:\", prec)\n    print(\"Recall   :\", rec)\n    print(\"F1 Score :\", f1)\nelse:\n    print(\"Predictions:\", y_pred_2017[:20])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:07:50.002346Z","iopub.status.idle":"2025-12-05T11:07:50.002650Z","shell.execute_reply.started":"2025-12-05T11:07:50.002502Z","shell.execute_reply":"2025-12-05T11:07:50.002514Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Troubleshoot missing features and optionally auto-map close matches\nimport pickle, difflib, pandas as pd\nfrom pprint import pprint\n\n# ---------- paths (adjust if needed) ----------\nresults_path = \"/kaggle/working/hybrid_hlo_models_results.pkl\"\nflow = \"union\"   # change to \"intersection\" or \"voting\" as needed\nmodel_pkl_path = f\"/kaggle/working/hybrid_hlo_models_{flow}_model.pkl\"\nparquet_path = \"/kaggle/input/cicids2017/Botnet-Friday-no-metadata.parquet\"  # change if needed\n\n# ---------- load saved results to get the selected feature list ----------\nwith open(results_path, \"rb\") as f:\n    top = pickle.load(f)\n\nselected_features = top[\"results_all\"][flow][\"selected_features\"]\nprint(f\"Model expects {len(selected_features)} features for '{flow}':\")\npprint(selected_features)\nprint()\n\n# ---------- load 2017 dataset (parquet) ----------\ndf2017 = pd.read_parquet(parquet_path)\nprint(\"2017 dataset columns (count):\", len(df2017.columns))\n# print(df2017.columns.tolist())  # uncomment if you want the full list\n\n# ---------- check which are missing ----------\navailable_cols = list(df2017.columns)\nmissing = [f for f in selected_features if f not in available_cols]\n\nif not missing:\n    print(\"All required features present ‚Äî you can build X_test directly.\")\n    X_test_2017 = df2017[selected_features]\nelse:\n    print(f\"Missing {len(missing)} features (exact match):\")\n    pprint(missing)\n    print(\"\\nNow showing close matches for each missing feature (top 3 suggestions):\\n\")\n    for m in missing:\n        # find close matches among available_cols\n        matches = difflib.get_close_matches(m, available_cols, n=3, cutoff=0.6)\n        print(f\" -> {m}  => suggestions: {matches}\")\n\n    # ---------- OPTIONAL: attempt automatic mapping ----------\n    AUTO_MAP = True   # set False to not attempt mapping\n    if AUTO_MAP:\n        mapping = {}   # model_feature -> dataset_column\n        used_dataset_cols = set()\n\n        for m in missing:\n            # try exact case-insensitive match first\n            ci = [c for c in available_cols if c.lower() == m.lower()]\n            if ci:\n                mapping[m] = ci[0]\n                used_dataset_cols.add(ci[0])\n                continue\n\n            # otherwise use difflib suggestions (cutoff tuned)\n            suggs = difflib.get_close_matches(m, available_cols, n=1, cutoff=0.75)\n            if suggs:\n                mapping[m] = suggs[0]\n                used_dataset_cols.add(suggs[0])\n            else:\n                # try relaxed matching: strip spaces, underscores, lower-case\n                norm_m = m.replace(\" \", \"\").replace(\"_\", \"\").lower()\n                best = None; best_score = 0.0\n                for c in available_cols:\n                    score = difflib.SequenceMatcher(None, norm_m, c.replace(\" \", \"\").replace(\"_\",\"\").lower()).ratio()\n                    if score > best_score:\n                        best_score = score; best = c\n                if best_score >= 0.7:\n                    mapping[m] = best\n                    used_dataset_cols.add(best)\n\n        print(\"\\nAuto-mapping proposals (model_feature -> dataset_column):\")\n        pprint(mapping)\n\n        # show which missing remain after mapping\n        still_missing = [m for m in missing if m not in mapping]\n        if still_missing:\n            print(\"\\nStill missing (not automatically mapped):\")\n            pprint(still_missing)\n            raise ValueError(\"Automatic mapping incomplete ‚Äî please inspect suggestions above and map manually.\")\n        else:\n            # Build X_test using mapped names and existing exact-matched ones\n            final_cols = []\n            for feat in selected_features:\n                if feat in available_cols:\n                    final_cols.append(feat)\n                else:\n                    # mapped\n                    final_cols.append(mapping[feat])\n            print(\"\\nFinal column list to use (in correct order):\")\n            pprint(final_cols)\n\n            # Optional: show any duplicates or collisions\n            if len(final_cols) != len(set(final_cols)):\n                print(\"\\nWarning: mapped column list contains duplicates (some model features map to the same dataset column).\")\n                dupes = [c for c in final_cols if final_cols.count(c) > 1]\n                pprint(list(set(dupes)))\n                raise ValueError(\"Duplicate mappings found ‚Äî resolve mapping ambiguity before proceeding.\")\n\n            # Build X_test_2017\n            X_test_2017 = df2017[final_cols].copy()\n            print(\"\\nX_test_2017 built successfully with shape:\", X_test_2017.shape)\n            # If Label exists, provide y as well\n            y_test_2017 = df2017[\"Label\"].astype(int) if \"Label\" in df2017.columns else None\n\n            # You can now predict:\n            # with open(model_pkl_path, \"rb\") as mf:\n            #     model = pickle.load(mf)\n            # preds = model.predict(X_test_2017)\n            # ... evaluate as needed ...\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:07:50.003673Z","iopub.status.idle":"2025-12-05T11:07:50.003919Z","shell.execute_reply.started":"2025-12-05T11:07:50.003814Z","shell.execute_reply":"2025-12-05T11:07:50.003824Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Run in Kaggle notebook\nimport pandas as pd\nimport numpy as np\nimport difflib\nfrom pathlib import Path\n\n# ---------- EDIT THIS PATH IF NEEDED ----------\nparquet_path = \"/kaggle/input/cicids2017/Botnet-Friday-no-metadata.parquet\"\nout_dir = \"/kaggle/working\"\nPath(out_dir).mkdir(parents=True, exist_ok=True)\n\n# target columns (same order as you requested)\ntarget_cols = [\n  'Dst Port','Timestamp','Flow Duration','Fwd Pkt Len Min','Fwd Pkt Len Mean',\n  'Bwd Pkt Len Max','Bwd Pkt Len Min','Bwd Pkt Len Std','Flow Pkts/s','Fwd IAT Std',\n  'Fwd Header Len','Fwd Pkts/s','Pkt Len Mean','Pkt Len Var','SYN Flag Cnt',\n  'CWE Flag Count','ECE Flag Cnt','Down/Up Ratio','Subflow Bwd Byts',\n  'Init Bwd Win Byts','Fwd Seg Size Min','Idle Mean','Idle Std','Idle Min',\n  'Src IP','Dst IP'\n]\n\n# initial candidate map (target -> suggested source name based on earlier mapping)\ncandidate_map = {\n  'Dst Port': None,\n  'Timestamp': None,\n  'Flow Duration': 'Flow Duration',\n  'Fwd Pkt Len Min': 'Fwd Packet Length Min',\n  'Fwd Pkt Len Mean': 'Fwd Packet Length Mean',\n  'Bwd Pkt Len Max': 'Bwd Packet Length Max',\n  'Bwd Pkt Len Min': 'Bwd Packet Length Min',\n  'Bwd Pkt Len Std': 'Bwd Packet Length Std',\n  'Flow Pkts/s': 'Flow Packets/s',\n  'Fwd IAT Std': 'Fwd IAT Std',\n  'Fwd Header Len': 'Fwd Header Length',\n  'Fwd Pkts/s': 'Fwd Packets/s',\n  'Pkt Len Mean': 'Packet Length Mean',\n  'Pkt Len Var': 'Packet Length Variance',\n  'SYN Flag Cnt': 'SYN Flag Count',\n  'CWE Flag Count': 'CWE Flag Count',\n  'ECE Flag Cnt': 'ECE Flag Count',\n  'Down/Up Ratio': 'Down/Up Ratio',\n  'Subflow Bwd Byts': 'Subflow Bwd Bytes',\n  'Init Bwd Win Byts': 'Init Bwd Win Bytes',\n  'Fwd Seg Size Min': 'Fwd Seg Size Min',\n  'Idle Mean': 'Idle Mean',\n  'Idle Std': 'Idle Std',\n  'Idle Min': 'Idle Min',\n  'Src IP': None,\n  'Dst IP': None\n}\n\nprint(\"Loading parquet (this can take a few seconds)...\")\ndf = pd.read_parquet(parquet_path)\n\nsrc_cols = list(df.columns)\nprint(f\"Source columns in parquet: {len(src_cols)} columns loaded.\")\n\n# Function: attempt to resolve mapping (exact -> candidate -> fuzzy)\ndef resolve_mapping(candidate_map, src_cols, fuzzy_cutoff=0.7):\n    final_map = {}\n    fuzzy_suggestions = {}\n    for tgt, guess in candidate_map.items():\n        resolved = None\n        # 1) exact match (case sensitive)\n        if guess and guess in src_cols:\n            resolved = guess\n        # 2) try normalized exact (lower, remove spaces/punctuation)\n        if resolved is None and guess:\n            norm_guess = guess.lower().replace(' ', '').replace('_','')\n            for c in src_cols:\n                if c.lower().replace(' ','').replace('_','') == norm_guess:\n                    resolved = c\n                    break\n        # 3) try exact on target name (maybe target already exists)\n        if resolved is None and tgt in src_cols:\n            resolved = tgt\n        # 4) fuzzy match: compare target and guess to source columns\n        if resolved is None:\n            # candidates: compare both target and guess strings to source column names\n            keys_to_try = [tgt]\n            if guess:\n                keys_to_try.insert(0, guess)\n            best_match = None\n            best_ratio = 0.0\n            for key in keys_to_try:\n                matches = difflib.get_close_matches(key, src_cols, n=3, cutoff=fuzzy_cutoff)\n                if matches:\n                    # difflib doesn't give score; use SequenceMatcher for best of matches\n                    from difflib import SequenceMatcher\n                    for m in matches:\n                        ratio = SequenceMatcher(None, key.lower(), m.lower()).ratio()\n                        if ratio > best_ratio:\n                            best_ratio = ratio\n                            best_match = m\n            if best_match and best_ratio >= fuzzy_cutoff:\n                resolved = best_match\n                fuzzy_suggestions[tgt] = (best_match, best_ratio)\n        final_map[tgt] = resolved\n    return final_map, fuzzy_suggestions\n\nfinal_map, fuzzy_suggestions = resolve_mapping(candidate_map, src_cols, fuzzy_cutoff=0.66)\n\n# Report mapping attempts\nmapped = {t: s for t, s in final_map.items() if s}\nunmapped = [t for t, s in final_map.items() if not s]\n\nprint(\"\\n=== Mapping summary ===\")\nprint(\"Mapped targets:\")\nfor t, s in mapped.items():\n    print(f\"  {t}  <-  {s}\")\nif fuzzy_suggestions:\n    print(\"\\nFuzzy match suggestions (with approx. confidence):\")\n    for t, (m, r) in fuzzy_suggestions.items():\n        print(f\"  {t}: suggested '{m}' (score ‚âà {r:.2f})\")\nif unmapped:\n    print(\"\\nUnmapped targets (will be created as empty columns):\")\n    for t in unmapped:\n        print(\"  \", t)\n\n# Apply renaming: construct rename dict (source_col -> target_col) for those resolved\nrename_dict = {}\nfor tgt, src in final_map.items():\n    if src:\n        # rename source column name to the target column name\n        rename_dict[src] = tgt\n\n# Make a copy so we don't alter original variable usage\ndf2 = df.copy()\n\n# Rename columns\nif rename_dict:\n    df2 = df2.rename(columns=rename_dict)\n    print(f\"\\nRenamed {len(rename_dict)} columns.\")\n\n# Create missing columns with NaN (or appropriate placeholder)\nfor tgt in target_cols:\n    if tgt not in df2.columns:\n        df2[tgt] = np.nan\n\n# Reorder columns to exactly match target_cols; if you want to keep other columns append them after\ndf_out = df2[target_cols].copy()\n\n# Save output\nout_parquet = Path(out_dir) / \"processed_botnet_2017.parquet\"\nout_csv = Path(out_dir) / \"processed_botnet_2017.csv\"\ndf_out.to_parquet(out_parquet, index=False)\ndf_out.to_csv(out_csv, index=False)\nprint(f\"\\nSaved processed files:\\n  {out_parquet}\\n  {out_csv}\")\n\n# Final info\nprint(\"\\nFinal DataFrame shape:\", df_out.shape)\nprint(\"Columns (in order):\")\nprint(df_out.columns.tolist())\n\n# list rows where placeholders remain (i.e., newly created columns are all-NaN)\nplaceholder_cols = [c for c in target_cols if df_out[c].isna().all()]\nif placeholder_cols:\n    print(\"\\nColumns that are placeholders (all NaN):\")\n    for c in placeholder_cols:\n        print(\"  \", c)\nelse:\n    print(\"\\nNo placeholder columns ‚Äî all target cols have values.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:07:50.004822Z","iopub.status.idle":"2025-12-05T11:07:50.005161Z","shell.execute_reply.started":"2025-12-05T11:07:50.004993Z","shell.execute_reply":"2025-12-05T11:07:50.005007Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ff=pd.read_csv(\"/kaggle/working/processed_botnet_2017.csv\")\nff.columns\nff.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:07:50.005934Z","iopub.status.idle":"2025-12-05T11:07:50.006171Z","shell.execute_reply.started":"2025-12-05T11:07:50.006046Z","shell.execute_reply":"2025-12-05T11:07:50.006055Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\nfrom catboost import CatBoostClassifier\nimport time\nimport pickle\n\n# -----------------------------------------\n# 1Ô∏è‚É£ Load cleaned dataset\n# -----------------------------------------\ndf = pd.read_csv(\"\")\n\n# Ensure \"Label\" column exists\nif \"Label\" not in df.columns:\n    raise ValueError(\"‚ùå ERROR: 'Label' column not found in dataset!\")\n\n# -----------------------------------------\n# 2Ô∏è‚É£ Split features and target\n# -----------------------------------------\nX = df.drop(\"Label\", axis=1)\ny = df[\"Label\"]\n\n# -----------------------------------------\n# 3Ô∏è‚É£ Train-test split (80-20)\n# -----------------------------------------\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.20, random_state=42, stratify=y\n)\n\n# -----------------------------------------\n# 4Ô∏è‚É£ Train CatBoost model + record time\n# -----------------------------------------\nmodel = CatBoostClassifier(\n    iterations=300,\n    learning_rate=0.1,\n    depth=8,\n    loss_function='MultiClass',\n    verbose=False\n)\n\nstart_time = time.time()\nmodel.fit(X_train, y_train)\nend_time = time.time()\ntraining_time = end_time - start_time\n\n# -----------------------------------------\n# 5Ô∏è‚É£ Predictions\n# -----------------------------------------\ny_pred = model.predict(X_test)\ny_pred = y_pred.flatten()\n\n# -----------------------------------------\n# 6Ô∏è‚É£ Metrics\n# -----------------------------------------\nacc = accuracy_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred, average=\"weighted\")\nrecall = recall_score(y_test, y_pred, average=\"weighted\")\nprecision = precision_score(y_test, y_pred, average=\"weighted\")\n\n# -----------------------------------------\n# 7Ô∏è‚É£ Save model to Pickle\n# -----------------------------------------\npickle_path = \"/kaggle/working/catboost_ids2018_model.pkl\"\nwith open(pickle_path, \"wb\") as f:\n    pickle.dump(model, f)\n\n# -----------------------------------------\n# 8Ô∏è‚É£ Print results\n# -----------------------------------------\nprint(\"‚úîÔ∏è CatBoost Training Complete\\n\")\nprint(\"üìÅ Model saved as:\", pickle_path)\nprint(\"üìå Accuracy:\", acc)\nprint(\"üìå F1 Score (weighted):\", f1)\nprint(\"üìå Recall (weighted):\", recall)\nprint(\"üìå Precision (weighted):\", precision)\nprint(f\"‚è±Ô∏è Time Taken to Train: {training_time:.2f} seconds\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:07:50.007304Z","iopub.status.idle":"2025-12-05T11:07:50.007664Z","shell.execute_reply.started":"2025-12-05T11:07:50.007499Z","shell.execute_reply":"2025-12-05T11:07:50.007514Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\n\n# ---------- 1. Load (avoid low-memory dtype guessing) ----------\ninput_path = \"/kaggle/working/processed_botnet_2017_2.parquet\"\ndf = pd.read_parquet(input_path)   # prevents chunked dtype inference warning\n\n# Quick info to debug if needed\nprint(\"Initial shape:\", df.shape)\nprint(\"Initial dtypes (sample):\\n\", df.dtypes.head(20))\n\n\n# Drop exact duplicate rows\ndf = df.drop_duplicates().reset_index(drop=True)\n\n# ---------- 3. Detect candidate numeric columns robustly ----------\n# We'll try to coerce each column to numeric and keep columns that convert well.\nnumeric_candidates = []\nconversion_stats = {}\nfor col in df.columns:\n    # try converting to numeric (coerce errors -> NaN)\n    coerced = pd.to_numeric(df[col], errors=\"coerce\")\n    non_na_ratio = coerced.notna().sum() / len(coerced)\n    conversion_stats[col] = non_na_ratio\n    # if at least 80% of values convert to numeric, treat as numeric (you can adjust threshold)\n    if non_na_ratio >= 0.80:\n        numeric_candidates.append(col)\n\nprint(f\"Detected {len(numeric_candidates)} numeric-like columns (>=80% convertible).\")\n\n# Force convert those columns to numeric (float64). Others kept as-is (likely categorical/text)\nfor col in numeric_candidates:\n    df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n\n# ---------- 4. Inspect for infinities or extreme values ----------\n# Columns that contain ¬±inf\ninf_cols = []\nfor col in numeric_candidates:\n    ser = df[col]\n    if np.isinf(ser.to_numpy()).any():\n        inf_cols.append(col)\n\nprint(\"Columns with ¬±inf values:\", inf_cols)\n\n# Replace ¬±inf with NaN so we can handle them with median filling\nif inf_cols:\n    df[numeric_candidates] = df[numeric_candidates].replace([np.inf, -np.inf], np.nan)\n\n# Check for huge values that might overflow float64 when operations applied\n# We'll compute absolute max and print columns with extremely large values (> 1e300)\nhuge_cols = []\nfor col in numeric_candidates:\n    try:\n        max_abs = np.nanmax(np.abs(df[col].to_numpy()))\n        if np.isfinite(max_abs) and max_abs > 1e300:\n            huge_cols.append((col, max_abs))\n    except Exception:\n        # if conversion to numpy fails, skip\n        pass\n\nprint(\"Columns with extremely large magnitudes (>1e300):\", huge_cols)\n\n# If you see columns in huge_cols you might want to inspect or clip them.\n# For safety we'll clip numeric values to a sane range before scaling (optional)\nCLIP_LIMIT = 1e300\ndf[numeric_candidates] = df[numeric_candidates].apply(lambda s: s.clip(lower=-CLIP_LIMIT, upper=CLIP_LIMIT))\n\n# ---------- 5. Recompute numeric / categorical column lists ----------\nnum_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\ncat_cols = df.select_dtypes(include=['object']).columns.tolist()\nprint(\"Final numeric columns count:\", len(num_cols))\nprint(\"Final categorical columns count:\", len(cat_cols))\n\n# ---------- 6. Handle missing values ----------\n# Numeric: fill with median\nif len(num_cols) > 0:\n    df[num_cols] = df[num_cols].fillna(df[num_cols].median())\n\n# Categorical: fill with mode (most frequent)\nfor col in cat_cols:\n    if df[col].isna().any():\n        mode_val = df[col].mode(dropna=True)\n        if len(mode_val) > 0:\n            df[col] = df[col].fillna(mode_val.iloc[0])\n        else:\n            # fallback to empty string if mode can't be computed\n            df[col] = df[col].fillna(\"\")\n\n# ---------- 7. Encode categorical columns ----------\nle = LabelEncoder()\nfor col in cat_cols:\n    # convert to string first to avoid issues with mixed types\n    df[col] = le.fit_transform(df[col].astype(str))\n\n# ---------- 8. Final check for non-finite numerics before scaling ----------\n# Replace any remaining inf/-inf with NaN (just in case), then fill with medians again\ndf[num_cols] = df[num_cols].replace([np.inf, -np.inf], np.nan)\ndf[num_cols] = df[num_cols].fillna(df[num_cols].median())\n\n# Ensure all numeric values are finite now\nfinite_check = {col: np.isfinite(df[col].to_numpy()).all() for col in num_cols}\nbad = [c for c, ok in finite_check.items() if not ok]\nprint(\"Columns still non-finite (should be empty):\", bad)\n\n# ---------- 9. Min-Max scaling ----------\nscaler = MinMaxScaler()\nif len(num_cols) > 0:\n    df[num_cols] = scaler.fit_transform(df[num_cols])\n\n# ---------- 10. Save and report ----------\noutput_filename = \"/kaggle/working/2019_cleaned_combined.csv\"\ndf.to_csv(output_filename, index=False)\n\nprint(\"‚úÖ Preprocessing complete!\")\nprint(\"üìÅ Saved as:\", output_filename)\nprint(\"Shape:\", df.shape)\nprint(\"Label distribution (if present):\")\nif 'Label' in df.columns:\n    print(df['Label'].value_counts())\nelse:\n    print(\"No 'Label' column found in final dataframe.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:07:50.008648Z","iopub.status.idle":"2025-12-05T11:07:50.008964Z","shell.execute_reply.started":"2025-12-05T11:07:50.008797Z","shell.execute_reply":"2025-12-05T11:07:50.008810Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.columns\ndf.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:07:50.010142Z","iopub.status.idle":"2025-12-05T11:07:50.010486Z","shell.execute_reply.started":"2025-12-05T11:07:50.010328Z","shell.execute_reply":"2025-12-05T11:07:50.010343Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\npath = \"/kaggle/working/2019_cleaned_combined.csv\"\ndf = pd.read_csv(path)\n\n# Normalize text (even if some trailing spaces exist)\ndf['Label'] = df['Label'].astype(str).str.strip().str.lower()\n\ndf['Label'] = df['Label'].apply(lambda x: 0 if x == \"benign\" else 1)\n\n# Save back\ndf.to_csv(\"/kaggle/working/2019_cleaned_combined_binary.csv\", index=False)\n\nprint(\"Label conversion complete!\")\nprint(df['Label'].value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:07:50.013926Z","iopub.status.idle":"2025-12-05T11:07:50.014242Z","shell.execute_reply.started":"2025-12-05T11:07:50.014078Z","shell.execute_reply":"2025-12-05T11:07:50.014091Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Run in Kaggle notebook (updated to retain 'Label' column)\nimport pandas as pd\nimport numpy as np\nimport difflib\nfrom pathlib import Path\n\n# ---------- EDIT THIS PATH IF NEEDED ----------\nparquet_path = \"/kaggle/input/cicids2017/Botnet-Friday-no-metadata.parquet\"\nout_dir = \"/kaggle/working\"\nPath(out_dir).mkdir(parents=True, exist_ok=True)\n\n# target columns (same order as you requested)\ntarget_cols = [\n  'Dst Port','Timestamp','Flow Duration','Fwd Pkt Len Min','Fwd Pkt Len Mean',\n  'Bwd Pkt Len Max','Bwd Pkt Len Min','Bwd Pkt Len Std','Flow Pkts/s','Fwd IAT Std',\n  'Fwd Header Len','Fwd Pkts/s','Pkt Len Mean','Pkt Len Var','SYN Flag Cnt',\n  'CWE Flag Count','ECE Flag Cnt','Down/Up Ratio','Subflow Bwd Byts',\n  'Init Bwd Win Byts','Fwd Seg Size Min','Idle Mean','Idle Std','Idle Min',\n  'Src IP','Dst IP'\n]\n\n# initial candidate map (target -> suggested source name based on earlier mapping)\ncandidate_map = {\n  'Dst Port': None,\n  'Timestamp': None,\n  'Flow Duration': 'Flow Duration',\n  'Fwd Pkt Len Min': 'Fwd Packet Length Min',\n  'Fwd Pkt Len Mean': 'Fwd Packet Length Mean',\n  'Bwd Pkt Len Max': 'Bwd Packet Length Max',\n  'Bwd Pkt Len Min': 'Bwd Packet Length Min',\n  'Bwd Pkt Len Std': 'Bwd Packet Length Std',\n  'Flow Pkts/s': 'Flow Packets/s',\n  'Fwd IAT Std': 'Fwd IAT Std',\n  'Fwd Header Len': 'Fwd Header Length',\n  'Fwd Pkts/s': 'Fwd Packets/s',\n  'Pkt Len Mean': 'Packet Length Mean',\n  'Pkt Len Var': 'Packet Length Variance',\n  'SYN Flag Cnt': 'SYN Flag Count',\n  'CWE Flag Count': 'CWE Flag Count',\n  'ECE Flag Cnt': 'ECE Flag Count',\n  'Down/Up Ratio': 'Down/Up Ratio',\n  'Subflow Bwd Byts': 'Subflow Bwd Bytes',\n  'Init Bwd Win Byts': 'Init Bwd Win Bytes',\n  'Fwd Seg Size Min': 'Fwd Seg Size Min',\n  'Idle Mean': 'Idle Mean',\n  'Idle Std': 'Idle Std',\n  'Idle Min': 'Idle Min',\n  'Src IP': None,\n  'Dst IP': None\n}\n\nprint(\"Loading parquet (this can take a few seconds)...\")\ndf = pd.read_parquet(parquet_path)\n\nsrc_cols = list(df.columns)\nprint(f\"Source columns in parquet: {len(src_cols)} columns loaded.\")\n\n# Function: attempt to resolve mapping (exact -> candidate -> fuzzy)\ndef resolve_mapping(candidate_map, src_cols, fuzzy_cutoff=0.7):\n    final_map = {}\n    fuzzy_suggestions = {}\n    for tgt, guess in candidate_map.items():\n        resolved = None\n        # 1) exact match (case sensitive)\n        if guess and guess in src_cols:\n            resolved = guess\n        # 2) try normalized exact (lower, remove spaces/punctuation)\n        if resolved is None and guess:\n            norm_guess = guess.lower().replace(' ', '').replace('_','')\n            for c in src_cols:\n                if c.lower().replace(' ','').replace('_','') == norm_guess:\n                    resolved = c\n                    break\n        # 3) try exact on target name (maybe target already exists)\n        if resolved is None and tgt in src_cols:\n            resolved = tgt\n        # 4) fuzzy match: compare target and guess to source columns\n        if resolved is None:\n            # candidates: compare both target and guess strings to source column names\n            keys_to_try = [tgt]\n            if guess:\n                keys_to_try.insert(0, guess)\n            best_match = None\n            best_ratio = 0.0\n            for key in keys_to_try:\n                matches = difflib.get_close_matches(key, src_cols, n=3, cutoff=fuzzy_cutoff)\n                if matches:\n                    # difflib doesn't give score; use SequenceMatcher for best of matches\n                    from difflib import SequenceMatcher\n                    for m in matches:\n                        ratio = SequenceMatcher(None, key.lower(), m.lower()).ratio()\n                        if ratio > best_ratio:\n                            best_ratio = ratio\n                            best_match = m\n            if best_match and best_ratio >= fuzzy_cutoff:\n                resolved = best_match\n                fuzzy_suggestions[tgt] = (best_match, best_ratio)\n        final_map[tgt] = resolved\n    return final_map, fuzzy_suggestions\n\nfinal_map, fuzzy_suggestions = resolve_mapping(candidate_map, src_cols, fuzzy_cutoff=0.66)\n\n# Report mapping attempts\nmapped = {t: s for t, s in final_map.items() if s}\nunmapped = [t for t, s in final_map.items() if not s]\n\nprint(\"\\n=== Mapping summary ===\")\nprint(\"Mapped targets:\")\nfor t, s in mapped.items():\n    print(f\"  {t}  <-  {s}\")\nif fuzzy_suggestions:\n    print(\"\\nFuzzy match suggestions (with approx. confidence):\")\n    for t, (m, r) in fuzzy_suggestions.items():\n        print(f\"  {t}: suggested '{m}' (score ‚âà {r:.2f})\")\nif unmapped:\n    print(\"\\nUnmapped targets (will be created as empty columns):\")\n    for t in unmapped:\n        print(\"  \", t)\n\n# Apply renaming: construct rename dict (source_col -> target_col) for those resolved\nrename_dict = {}\nfor tgt, src in final_map.items():\n    if src:\n        # don't rename into 'Label' accidentally (we keep original Label untouched)\n        if src != 'Label':\n            rename_dict[src] = tgt\n        else:\n            # if source column is literally 'Label' and target is not, keep as-is and warn\n            print(f\"Warning: source column 'Label' matched target '{tgt}'. Skipping rename for this mapping to preserve Label column.\")\n\n# Make a copy so we don't alter original variable usage\ndf2 = df.copy()\n\n# Rename columns\nif rename_dict:\n    df2 = df2.rename(columns=rename_dict)\n    print(f\"\\nRenamed {len(rename_dict)} columns.\")\n\n# Create missing columns with NaN (or appropriate placeholder)\n# BUT do not overwrite an existing 'Label' column (if present)\nfor tgt in target_cols:\n    if tgt not in df2.columns:\n        df2[tgt] = np.nan\n\n# Decide whether to include Label in output:\ninclude_label = 'Label' in df.columns  # check original df to be safe\n\n# Build final column order: all target_cols (in order) then Label (if it exists)\nfinal_column_order = target_cols.copy()\nif include_label:\n    # If 'Label' is already among target_cols (unlikely), avoid duplication\n    if 'Label' not in final_column_order:\n        final_column_order.append('Label')\n\n# Reorder columns to exactly match final_column_order; if you want to keep other columns append them after\n# If some final columns are still not present (shouldn't happen except Label), create them as NaN earlier ensures presence\ndf_out = df2[final_column_order].copy()\n\n# Save output\nout_parquet = Path(out_dir) / \"processed_botnet_2017_2.parquet\"\nout_csv = Path(out_dir) / \"processed_botnet_2017.csv\"\ndf_out.to_parquet(out_parquet, index=False)\ndf_out.to_csv(out_csv, index=False)\nprint(f\"\\nSaved processed files:\\n  {out_parquet}\\n  {out_csv}\")\n\n# Final info\nprint(\"\\nFinal DataFrame shape:\", df_out.shape)\nprint(\"Columns (in order):\")\nprint(df_out.columns.tolist())\n\n# list rows where placeholders remain (i.e., newly created columns are all-NaN)\nplaceholder_cols = [c for c in target_cols if df_out[c].isna().all()]\nif placeholder_cols:\n    print(\"\\nColumns that are placeholders (all NaN):\")\n    for c in placeholder_cols:\n        print(\"  \", c)\nelse:\n    print(\"\\nNo placeholder columns ‚Äî all target cols have values.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:07:50.015255Z","iopub.status.idle":"2025-12-05T11:07:50.015585Z","shell.execute_reply.started":"2025-12-05T11:07:50.015408Z","shell.execute_reply":"2025-12-05T11:07:50.015422Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\nfrom catboost import CatBoostClassifier\nimport time\nimport pickle\n\n# -----------------------------------------\n# 1Ô∏è‚É£ Load cleaned dataset\n# -----------------------------------------\ndf = pd.read_csv(\"/kaggle/input/cleaned-ids/ids2018_cleaned_combined_1.csv\")\n\n# Ensure \"Label\" column exists\nif \"Label\" not in df.columns:\n    raise ValueError(\"‚ùå ERROR: 'Label' column not found in dataset!\")\n\n# -----------------------------------------\n# 2Ô∏è‚É£ Split features and target\n# -----------------------------------------\nX = df.drop(\"Label\", axis=1)\ny = df[\"Label\"]\n\n# -----------------------------------------\n# 3Ô∏è‚É£ Train-test split (80-20)\n# -----------------------------------------\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.20, random_state=42, stratify=y\n)\n\n# -----------------------------------------\n# 4Ô∏è‚É£ Train CatBoost model + record time\n# -----------------------------------------\nmodel = CatBoostClassifier(\n    iterations=300,\n    learning_rate=0.1,\n    depth=8,\n    loss_function='MultiClass',\n    verbose=False\n)\n\nstart_time = time.time()\nmodel.fit(X_train, y_train)\nend_time = time.time()\ntraining_time = end_time - start_time\n\n# -----------------------------------------\n# 5Ô∏è‚É£ Predictions\n# -----------------------------------------\ny_pred = model.predict(X_test)\ny_pred = y_pred.flatten()\n\n# -----------------------------------------\n# 6Ô∏è‚É£ Metrics\n# -----------------------------------------\nacc = accuracy_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred, average=\"weighted\")\nrecall = recall_score(y_test, y_pred, average=\"weighted\")\nprecision = precision_score(y_test, y_pred, average=\"weighted\")\n\n# -----------------------------------------\n# 7Ô∏è‚É£ Save model to Pickle\n# -----------------------------------------\npickle_path = \"/kaggle/working/catboost_ids2018_model.pkl\"\nwith open(pickle_path, \"wb\") as f:\n    pickle.dump(model, f)\n\n# -----------------------------------------\n# 8Ô∏è‚É£ Print results\n# -----------------------------------------\nprint(\"‚úîÔ∏è CatBoost Training Complete\\n\")\nprint(\"üìÅ Model saved as:\", pickle_path)\nprint(\"üìå Accuracy:\", acc)\nprint(\"üìå F1 Score (weighted):\", f1)\nprint(\"üìå Recall (weighted):\", recall)\nprint(\"üìå Precision (weighted):\", precision)\nprint(f\"‚è±Ô∏è Time Taken to Train: {training_time:.2f} seconds\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:07:50.016333Z","iopub.status.idle":"2025-12-05T11:07:50.016642Z","shell.execute_reply.started":"2025-12-05T11:07:50.016477Z","shell.execute_reply":"2025-12-05T11:07:50.016491Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Code cell: Evaluate CatBoost model and print metrics\nimport time\nimport os\nimport pickle\nimport joblib\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, classification_report, confusion_matrix\n\nMODEL_PATH = \"/kaggle/working/catboost_ids2018_model.pkl\"\nTEST_CSV  = \"/kaggle/working/2019_cleaned_combined_binary.csv\"\nOUT_CSV   = \"/kaggle/working/2019_with_preds.csv\"\n\n# ---------- 1) Load test data ----------\nif not os.path.exists(TEST_CSV):\n    raise FileNotFoundError(f\"Test CSV not found: {TEST_CSV}\")\n\ndf_test = pd.read_csv(TEST_CSV)\nif 'Label' not in df_test.columns and 'label' in df_test.columns:\n    df_test.rename(columns={'label': 'Label'}, inplace=True)\n\nif 'Label' not in df_test.columns:\n    raise ValueError(\"Test CSV must contain a 'Label' column (0/1 or 'Benign'/'Bot').\")\n\n# Normalize label to binary 0/1 if necessary\ndef normalize_label_series(s):\n    s2 = s.astype(str).str.strip().str.lower()\n    # handle typical variants\n    benign_keywords = {\"benign\",\"begnin\",\"bengin\",\"normal\",\"good\"}\n    return s2.apply(lambda x: 0 if x in benign_keywords or \"benign\" in x else (1 if x not in [\"0\",\"0.0\",\"0.00\",\"false\",\"f\",\"no\",\"n\"] else 0)).astype(int)\n\ntry:\n    # if labels already 0/1 ints, this keeps them\n    if set(df_test['Label'].unique()) <= {0,1}:\n        y_true = df_test['Label'].astype(int).to_numpy()\n    else:\n        y_true = normalize_label_series(df_test['Label']).to_numpy()\nexcept Exception:\n    # fallback robust conversion\n    y_true = normalize_label_series(df_test['Label']).to_numpy()\n\n# Prepare feature frame (drop Label)\nX_test = df_test.drop(columns=['Label']).copy()\n\n# ---------- 2) Load model (try joblib then pickle) ----------\nif not os.path.exists(MODEL_PATH):\n    raise FileNotFoundError(f\"Model file not found: {MODEL_PATH}\")\n\nt0 = time.time()\nmodel = None\nload_err = None\nfor loader in (joblib.load, pickle.load):\n    try:\n        # joblib.load expects a path; pickle.load expects file handle\n        if loader is joblib.load:\n            model = loader(MODEL_PATH)\n        else:\n            with open(MODEL_PATH, 'rb') as fh:\n                model = loader(fh)\n        break\n    except Exception as e:\n        load_err = e\n        model = None\n\nif model is None:\n    raise RuntimeError(f\"Failed to load model from {MODEL_PATH}. Last error: {load_err}\")\n\n# ---------- 3) Align test columns to model's expected features ----------\n# Try to get model feature names (CatBoost stores them as attribute feature_names_)\nmodel_feature_names = None\nif hasattr(model, 'feature_names_'):\n    try:\n        model_feature_names = list(model.feature_names_)\n    except Exception:\n        model_feature_names = None\n\n# If model does not expose feature names, try attribute 'feature_names' or 'get_feature_names'\nif model_feature_names is None:\n    if hasattr(model, 'feature_names'):\n        try:\n            model_feature_names = list(model.feature_names)\n        except Exception:\n            model_feature_names = None\n\n# If still None, fallback to using X_test columns as-is (order preserved)\nif model_feature_names is None:\n    # Use the test columns order\n    model_feature_names = X_test.columns.tolist()\n\n# Now ensure X_test has all required model features in the right order:\nmissing_cols = [c for c in model_feature_names if c not in X_test.columns]\nextra_cols = [c for c in X_test.columns if c not in model_feature_names]\n\n# Add missing cols as zeros (or NaN -> replaced with 0)\nif missing_cols:\n    for c in missing_cols:\n        # add numeric zero column\n        X_test[c] = 0\n\n# Drop extras\nif extra_cols:\n    X_test = X_test.drop(columns=extra_cols)\n\n# Reorder to model_feature_names\nX_test = X_test[model_feature_names]\n\n# Convert dtype numeric where possible\nfor col in X_test.columns:\n    # attempt to coerce to numeric; if fails, leave as-is (CatBoost may accept strings/categorical)\n    try:\n        X_test[col] = pd.to_numeric(X_test[col], errors='ignore')\n    except Exception:\n        pass\n\n# ---------- 4) Predict ----------\n# time prediction (and include load time)\npred_start = time.time()\n# Many CatBoost models implement predict() returning class labels; some may require Pool\ntry:\n    y_pred = model.predict(X_test)\nexcept Exception:\n    # try using model.predict with Pool if CatBoost installed\n    try:\n        from catboost import Pool\n        y_pred = model.predict(Pool(X_test))\n    except Exception as e:\n        raise RuntimeError(f\"Prediction failed: {e}\")\n\n# try predict_proba if available\ny_proba = None\ntry:\n    y_proba = model.predict_proba(X_test)\nexcept Exception:\n    try:\n        from catboost import Pool\n        y_proba = model.predict_proba(Pool(X_test))\n    except Exception:\n        y_proba = None\n\npred_end = time.time()\ntotal_time = pred_end - t0  # load + predict time\npredict_time = pred_end - pred_start\n\n# Ensure predictions are 0/1 ints\ny_pred_arr = np.array(y_pred).squeeze()\n# Many models output floats; convert threshold if necessary\nif y_pred_arr.dtype.kind in 'f' and set(np.unique(y_pred_arr)).issubset({0.0, 1.0}) is False:\n    # round to nearest int\n    y_pred_labels = (np.rint(y_pred_arr)).astype(int)\nelse:\n    y_pred_labels = y_pred_arr.astype(int)\n\n# ---------- 5) Metrics ----------\nacc = accuracy_score(y_true, y_pred_labels)\nf1_w = f1_score(y_true, y_pred_labels, average='weighted', zero_division=0)\nrec_w = recall_score(y_true, y_pred_labels, average='weighted', zero_division=0)\nprec_w = precision_score(y_true, y_pred_labels, average='weighted', zero_division=0)\n\n# ---------- 6) Save predictions appended to test CSV ----------\nout_df = df_test.copy()\nout_df['pred_label'] = y_pred_labels\nif y_proba is not None:\n    # if binary, take probability of positive class (col index 1)\n    try:\n        if y_proba.shape[1] == 2:\n            out_df['pred_proba_pos'] = np.array(y_proba)[:, 1]\n        else:\n            # for multi-class, store entire array as string\n            out_df['pred_proba'] = [list(map(float, row)) for row in np.array(y_proba)]\n    except Exception:\n        pass\n\nout_df.to_csv(OUT_CSV, index=False)\n\n# ---------- 7) Print nicely formatted results ----------\nprint(f\"Accuracy: {acc:.12f}\")\nprint(f\"üìå F1 Score (weighted): {f1_w:.12f}\")\nprint(f\"üìå Recall (weighted): {rec_w:.12f}\")\nprint(f\"üìå Precision (weighted): {prec_w:.12f}\")\nprint(f\"‚è±Ô∏è Time Taken (load + predict): {total_time:.2f} seconds\")\nprint(f\"‚è±Ô∏è Time Taken (predict only): {predict_time:.4f} seconds\")\n\nprint(\"\\nConfusion Matrix:\")\nprint(confusion_matrix(y_true, y_pred_labels))\n\nprint(\"\\nClassification report (per-class):\")\nprint(classification_report(y_true, y_pred_labels, digits=6))\n\nprint(f\"\\nSaved predictions CSV: {OUT_CSV}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:07:50.017998Z","iopub.status.idle":"2025-12-05T11:07:50.018328Z","shell.execute_reply.started":"2025-12-05T11:07:50.018157Z","shell.execute_reply":"2025-12-05T11:07:50.018171Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import joblib, pickle, os, pandas as pd, numpy as np, time\nfrom pathlib import Path\n\nMODEL_PATH = \"/kaggle/working/catboost_ids2018_model.pkl\"\nTEST_CSV  = \"/kaggle/working/2019_cleaned_combined_binary.csv\"\n\nmodel = None\nfor loader in (joblib.load, pickle.load):\n    try:\n        model = loader(MODEL_PATH) if loader is joblib.load else loader(open(MODEL_PATH,'rb'))\n        break\n    except Exception:\n        model = None\nif model is None:\n    raise RuntimeError(\"Failed to load model.\")\n\ndf_test = pd.read_csv(TEST_CSV)\nif 'Label' in df_test.columns:\n    X_test = df_test.drop(columns=['Label']).copy()\nelse:\n    X_test = df_test.copy()\n\n# get model features\nmodel_feature_names = None\nif hasattr(model, 'feature_names_'):\n    model_feature_names = list(model.feature_names_)\nelif hasattr(model, 'feature_names'):\n    model_feature_names = list(model.feature_names)\nelse:\n    model_feature_names = None\n\nprint(\"Model exposes feature names?:\", model_feature_names is not None)\nif model_feature_names is not None:\n    print(\"Number of model features:\", len(model_feature_names))\n    missing_cols = [c for c in model_feature_names if c not in X_test.columns]\n    extra_cols = [c for c in X_test.columns if c not in model_feature_names]\n    print(\"Missing columns (in test but required by model):\", len(missing_cols))\n    print(\"Example missing (first 20):\", missing_cols[:20])\n    print(\"Extra columns in test not used by model:\", len(extra_cols))\n    print(\"Example extras (first 20):\", extra_cols[:20])\nelse:\n    print(\"No model feature list available ‚Äî model likely relies on positional input. You must ensure test columns order matches training order exactly.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:07:50.020103Z","iopub.status.idle":"2025-12-05T11:07:50.020419Z","shell.execute_reply.started":"2025-12-05T11:07:50.020252Z","shell.execute_reply":"2025-12-05T11:07:50.020269Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dff = pd.read_csv(\"/kaggle/input/cleaned-ids/ids2018_cleaned_combined_1.csv\")\ndff.columns\ndf.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:07:50.021481Z","iopub.status.idle":"2025-12-05T11:07:50.021748Z","shell.execute_reply.started":"2025-12-05T11:07:50.021605Z","shell.execute_reply":"2025-12-05T11:07:50.021617Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dfff = pd.read_parquet(\"/kaggle/input/cicids2017/Botnet-Friday-no-metadata.parquet\")\ndfff.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:07:50.023067Z","iopub.status.idle":"2025-12-05T11:07:50.023437Z","shell.execute_reply.started":"2025-12-05T11:07:50.023257Z","shell.execute_reply":"2025-12-05T11:07:50.023273Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Code cell: rename / drop / add / reorder and save\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\n\n# ---------- EDIT PATHS IF NEEDED ----------\ninput_path = \"/kaggle/input/cicids2017/Botnet-Friday-no-metadata.parquet\"   # your current file (adjust if different)\noutput_path = \"/kaggle/working/inital_2017_catboost.csv\"\nPath(output_path).parent.mkdir(parents=True, exist_ok=True)\n\n# ---------- Target schema (exact order you requested) ----------\ntarget_cols = [\n    'Dst Port', 'Protocol', 'Timestamp', 'Flow Duration', 'Tot Fwd Pkts',\n    'Tot Bwd Pkts', 'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max',\n    'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std',\n    'Bwd Pkt Len Max', 'Bwd Pkt Len Min', 'Bwd Pkt Len Mean',\n    'Bwd Pkt Len Std', 'Flow Byts/s', 'Flow Pkts/s', 'Flow IAT Mean',\n    'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Tot',\n    'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min',\n    'Bwd IAT Tot', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max',\n    'Bwd IAT Min', 'Fwd PSH Flags', 'Fwd URG Flags', 'Fwd Header Len',\n    'Bwd Header Len', 'Fwd Pkts/s', 'Bwd Pkts/s', 'Pkt Len Min',\n    'Pkt Len Max', 'Pkt Len Mean', 'Pkt Len Std', 'Pkt Len Var',\n    'FIN Flag Cnt', 'SYN Flag Cnt', 'RST Flag Cnt', 'PSH Flag Cnt',\n    'ACK Flag Cnt', 'URG Flag Cnt', 'CWE Flag Count', 'ECE Flag Cnt',\n    'Down/Up Ratio', 'Pkt Size Avg', 'Fwd Seg Size Avg', 'Bwd Seg Size Avg',\n    'Subflow Fwd Pkts', 'Subflow Fwd Byts', 'Subflow Bwd Pkts',\n    'Subflow Bwd Byts', 'Init Fwd Win Byts', 'Init Bwd Win Byts',\n    'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std',\n    'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max',\n    'Idle Min', 'Label', 'Flow ID', 'Src IP', 'Src Port', 'Dst IP'\n]\n\n# ---------- Mapping: source_column_name -> target_column_name ----------\n# Built from the source columns you supplied earlier.\nrename_map = {\n    # direct / straightforward renames\n    'Protocol': 'Protocol',\n    'Flow Duration': 'Flow Duration',\n    'Total Fwd Packets': 'Tot Fwd Pkts',\n    'Total Backward Packets': 'Tot Bwd Pkts',\n    'Fwd Packets Length Total': 'TotLen Fwd Pkts',\n    'Bwd Packets Length Total': 'TotLen Bwd Pkts',\n    'Fwd Packet Length Max': 'Fwd Pkt Len Max',\n    'Fwd Packet Length Min': 'Fwd Pkt Len Min',\n    'Fwd Packet Length Mean': 'Fwd Pkt Len Mean',\n    'Fwd Packet Length Std': 'Fwd Pkt Len Std',\n    'Bwd Packet Length Max': 'Bwd Pkt Len Max',\n    'Bwd Packet Length Min': 'Bwd Pkt Len Min',\n    'Bwd Packet Length Mean': 'Bwd Pkt Len Mean',\n    'Bwd Packet Length Std': 'Bwd Pkt Len Std',\n    'Flow Bytes/s': 'Flow Byts/s',\n    'Flow Packets/s': 'Flow Pkts/s',\n    'Flow IAT Mean': 'Flow IAT Mean',\n    'Flow IAT Std': 'Flow IAT Std',\n    'Flow IAT Max': 'Flow IAT Max',\n    'Flow IAT Min': 'Flow IAT Min',\n    'Fwd IAT Total': 'Fwd IAT Tot',\n    'Fwd IAT Mean': 'Fwd IAT Mean',\n    'Fwd IAT Std': 'Fwd IAT Std',\n    'Fwd IAT Max': 'Fwd IAT Max',\n    'Fwd IAT Min': 'Fwd IAT Min',\n    'Bwd IAT Total': 'Bwd IAT Tot',\n    'Bwd IAT Mean': 'Bwd IAT Mean',\n    'Bwd IAT Std': 'Bwd IAT Std',\n    'Bwd IAT Max': 'Bwd IAT Max',\n    'Bwd IAT Min': 'Bwd IAT Min',\n    'Fwd PSH Flags': 'Fwd PSH Flags',\n    # 'Bwd PSH Flags' - source only; will be dropped (target doesn't include)\n    'Fwd URG Flags': 'Fwd URG Flags',\n    # 'Bwd URG Flags' - source only; drop\n    'Fwd Header Length': 'Fwd Header Len',\n    'Bwd Header Length': 'Bwd Header Len',\n    'Fwd Packets/s': 'Fwd Pkts/s',\n    'Bwd Packets/s': 'Bwd Pkts/s',\n    'Packet Length Min': 'Pkt Len Min',\n    'Packet Length Max': 'Pkt Len Max',\n    'Packet Length Mean': 'Pkt Len Mean',\n    'Packet Length Std': 'Pkt Len Std',\n    'Packet Length Variance': 'Pkt Len Var',\n    'FIN Flag Count': 'FIN Flag Cnt',\n    'SYN Flag Count': 'SYN Flag Cnt',\n    'RST Flag Count': 'RST Flag Cnt',\n    'PSH Flag Count': 'PSH Flag Cnt',\n    'ACK Flag Count': 'ACK Flag Cnt',\n    'URG Flag Count': 'URG Flag Cnt',\n    'CWE Flag Count': 'CWE Flag Count',\n    'ECE Flag Count': 'ECE Flag Cnt',\n    'Down/Up Ratio': 'Down/Up Ratio',\n    'Avg Packet Size': 'Pkt Size Avg',\n    'Avg Fwd Segment Size': 'Fwd Seg Size Avg',\n    'Avg Bwd Segment Size': 'Bwd Seg Size Avg',\n    # Bulk / Avg Bulk Rate fields -> many target schema doesn't have these; we will drop them\n    # 'Fwd Avg Bytes/Bulk': (drop),\n    # 'Fwd Avg Packets/Bulk': (drop),\n    # 'Fwd Avg Bulk Rate': (drop),\n    # 'Bwd Avg Bytes/Bulk': (drop),\n    # 'Bwd Avg Packets/Bulk': (drop),\n    # 'Bwd Avg Bulk Rate': (drop),\n    'Subflow Fwd Packets': 'Subflow Fwd Pkts',\n    'Subflow Fwd Bytes': 'Subflow Fwd Byts',\n    'Subflow Bwd Packets': 'Subflow Bwd Pkts',\n    'Subflow Bwd Bytes': 'Subflow Bwd Byts',\n    'Init Fwd Win Bytes': 'Init Fwd Win Byts',\n    'Init Bwd Win Bytes': 'Init Bwd Win Byts',\n    'Fwd Act Data Packets': 'Fwd Act Data Pkts',\n    'Fwd Seg Size Min': 'Fwd Seg Size Min',\n    'Active Mean': 'Active Mean',\n    'Active Std': 'Active Std',\n    'Active Max': 'Active Max',\n    'Active Min': 'Active Min',\n    'Idle Mean': 'Idle Mean',\n    'Idle Std': 'Idle Std',\n    'Idle Max': 'Idle Max',\n    'Idle Min': 'Idle Min',\n    'Label': 'Label'\n}\n\n# ---------- Load input ----------\nprint(\"Loading:\", input_path)\ndf = pd.read_parquet(input_path)\nprint(\"Input shape:\", df.shape)\nprint(\"Input columns count:\", len(df.columns))\n\n# ---------- Rename columns where mapping exists ----------\n# Build inverse map (source -> target) from rename_map already is source->target\nexisting_source_cols = [c for c in rename_map.keys() if c in df.columns]\nrename_dict = {src: rename_map[src] for src in existing_source_cols}\ndf = df.rename(columns=rename_dict)\nprint(f\"Renamed {len(rename_dict)} columns.\")\n\n# ---------- Drop columns that are neither renamed into target nor present in target ----------\n# Keep any column that is already in target_cols or was renamed into target.\ncols_to_keep = set(target_cols)  # desired final columns\n# also keep any target-like 'Label' etc.\ncurrent_cols = set(df.columns)\n\n# Columns to drop: those present in current df but not in target columns\ndrop_candidates = [c for c in df.columns if c not in cols_to_keep]\nif drop_candidates:\n    print(f\"Dropping {len(drop_candidates)} source-only columns (not in target):\")\n    print(drop_candidates[:50])  # preview\n    df = df.drop(columns=drop_candidates)\n\n# ---------- Add missing target columns as NaN placeholders ----------\nmissing_targets = [c for c in target_cols if c not in df.columns]\nif missing_targets:\n    print(f\"Adding {len(missing_targets)} missing target columns as NaN placeholders:\")\n    print(missing_targets)\n    for c in missing_targets:\n        df[c] = np.nan\n\n# ---------- Reorder columns exactly to target_cols ----------\ndf = df[target_cols].copy()\n\n# ---------- Final checks ----------\nprint(\"Final shape:\", df.shape)\nprint(\"Final columns (first 120 chars):\")\nprint(df.columns.tolist())\n\n# ---------- Save ----------\ndf.to_csv(output_path, index=False)\nprint(\"Saved converted file to:\", output_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:07:50.024978Z","iopub.status.idle":"2025-12-05T11:07:50.025245Z","shell.execute_reply.started":"2025-12-05T11:07:50.025106Z","shell.execute_reply":"2025-12-05T11:07:50.025116Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\npath = \"/kaggle/working/inital_2017_catboost.csv\"\n\ndf = pd.read_csv(path)\n\n# Show unique values and their counts\nprint(\"Unique values in Label column:\")\nprint(df['Label'].value_counts(dropna=False))\n\n# Also show raw unique values\nprint(\"\\nRaw unique values:\")\nprint(df['Label'].unique())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:12:09.613834Z","iopub.execute_input":"2025-12-05T11:12:09.614629Z","iopub.status.idle":"2025-12-05T11:12:10.598233Z","shell.execute_reply.started":"2025-12-05T11:12:09.614593Z","shell.execute_reply":"2025-12-05T11:12:10.597593Z"}},"outputs":[{"name":"stdout","text":"Unique values in Label column:\nLabel\nBenign    174601\nBot         1437\nName: count, dtype: int64\n\nRaw unique values:\n['Benign' 'Bot']\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"import pandas as pd\n\npath = \"/kaggle/working/inital_2017_catboost.csv\"\ndf = pd.read_csv(path)\n\n# Normalize case\ndf['Label'] = df['Label'].astype(str).str.strip().str.lower()\n\n# Map: benign ‚Üí 0, bot ‚Üí 1\ndf['Label'] = df['Label'].apply(lambda x: 0 if x == \"benign\" else 1)\n\n# Save back\ndf.to_csv(\"/kaggle/working/inital_2017_catboost_binary.csv\", index=False)\n\nprint(\"Label conversion complete!\")\nprint(df['Label'].value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:11:41.944863Z","iopub.execute_input":"2025-12-05T11:11:41.945142Z","iopub.status.idle":"2025-12-05T11:11:47.964857Z","shell.execute_reply.started":"2025-12-05T11:11:41.945123Z","shell.execute_reply":"2025-12-05T11:11:47.964011Z"}},"outputs":[{"name":"stdout","text":"Label conversion complete!\nLabel\n0    174601\n1      1437\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"gf=pd.read_csv(\"/kaggle/working/inital_2017_catboost_binary.csv\")\n# Show unique values and their counts\nprint(\"Unique values in Label column:\")\nprint(gf['Label'].value_counts(dropna=False))\n\n# Also show raw unique values\nprint(\"\\nRaw unique values:\")\nprint(gf['Label'].unique())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:14:13.334721Z","iopub.execute_input":"2025-12-05T11:14:13.335303Z","iopub.status.idle":"2025-12-05T11:14:14.260921Z","shell.execute_reply.started":"2025-12-05T11:14:13.335281Z","shell.execute_reply":"2025-12-05T11:14:14.260243Z"}},"outputs":[{"name":"stdout","text":"Unique values in Label column:\nLabel\n0    174601\n1      1437\nName: count, dtype: int64\n\nRaw unique values:\n[0 1]\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport joblib, pickle, os, time\nfrom sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, confusion_matrix, classification_report\nfrom catboost import Pool\n\nMODEL_PATH = \"/kaggle/working/catboost_ids2018_model.pkl\"\nTEST_PATH  = \"/kaggle/working/inital_2017_catboost_binary.csv\"\n\n# ------------------ 1. Load test data ------------------\ndf = pd.read_csv(TEST_PATH)\n\nif 'Label' not in df.columns:\n    raise ValueError(\"‚ùå 'Label' column not found in test CSV\")\n\ny_true = df['Label'].astype(int).values\nX_test = df.drop(columns=['Label']).copy()\n\n# ------------------ 2. Load model ------------------\nmodel = None\nfor loader in (joblib.load, pickle.load):\n    try:\n        if loader is joblib.load:\n            model = loader(MODEL_PATH)\n        else:\n            with open(MODEL_PATH, 'rb') as f:\n                model = loader(f)\n        break\n    except:\n        pass\n\nif model is None:\n    raise RuntimeError(\"‚ùå Could not load model.\")\n\n# ------------------ 3. Align test columns to model features ------------------\nmodel_features = None\n\nif hasattr(model, \"feature_names_\"):\n    model_features = list(model.feature_names_)\nelif hasattr(model, \"feature_names\"):\n    model_features = list(model.feature_names)\n\n# If model exposes feature names ‚Üí STRICT MATCHING\nif model_features is not None:\n    missing = [c for c in model_features if c not in X_test.columns]\n    extra   = [c for c in X_test.columns if c not in model_features]\n\n    # Add missing columns as 0\n    for c in missing:\n        X_test[c] = 0\n\n    # Drop extras\n    if extra:\n        X_test = X_test.drop(columns=extra)\n\n    # Reorder to match model\n    X_test = X_test[model_features]\n\nelse:\n    print(\"‚ö† Model does NOT expose feature names. Using test columns as-is.\")\n\n# Convert to numeric where possible\nfor c in X_test.columns:\n    try:\n        X_test[c] = pd.to_numeric(X_test[c], errors=\"coerce\").fillna(0)\n    except:\n        pass\n\n# ------------------ 4. Predict ------------------\nstart = time.time()\n\ntry:\n    y_pred = model.predict(X_test)\nexcept:\n    y_pred = model.predict(Pool(X_test))\n\ny_pred = np.array(y_pred).astype(int)\n\nend = time.time()\n\n# ------------------ 5. Metrics ------------------\nacc  = accuracy_score(y_true, y_pred)\nf1   = f1_score(y_true, y_pred, average='weighted', zero_division=0)\nrec  = recall_score(y_true, y_pred, average='weighted', zero_division=0)\nprec = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n\n# ------------------ 6. Print results ------------------\nprint(f\"Accuracy: {acc:.12f}\")\nprint(f\"üìå F1 Score (weighted): {f1:.12f}\")\nprint(f\"üìå Recall (weighted): {rec:.12f}\")\nprint(f\"üìå Precision (weighted): {prec:.12f}\")\nprint(f\"‚è±Ô∏è Time Taken: {end-start:.2f} seconds\\n\")\n\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_true, y_pred))\n\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_true, y_pred, digits=6))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:17:01.115488Z","iopub.execute_input":"2025-12-05T11:17:01.116264Z","iopub.status.idle":"2025-12-05T11:17:03.180865Z","shell.execute_reply.started":"2025-12-05T11:17:01.116235Z","shell.execute_reply":"2025-12-05T11:17:03.180079Z"}},"outputs":[{"name":"stdout","text":"Accuracy: 0.008163010259\nüìå F1 Score (weighted): 0.000132190401\nüìå Recall (weighted): 0.008163010259\nüìå Precision (weighted): 0.000066634736\n‚è±Ô∏è Time Taken: 0.22 seconds\n\nConfusion Matrix:\n[[     0 174601]\n [     0   1437]]\n\nClassification Report:\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0   0.000000  0.000000  0.000000    174601\n           1   0.008163  1.000000  0.016194      1437\n\n    accuracy                       0.008163    176038\n   macro avg   0.004082  0.500000  0.008097    176038\nweighted avg   0.000067  0.008163  0.000132    176038\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"import joblib, pickle\n\nMODEL_PATH = \"/kaggle/working/catboost_ids2018_model.pkl\"\n\nmodel = None\nfor loader in (joblib.load, pickle.load):\n    try:\n        if loader is joblib.load:\n            model = loader(MODEL_PATH)\n        else:\n            with open(MODEL_PATH, \"rb\") as f:\n                model = loader(f)\n        break\n    except:\n        pass\n\n# Print model feature names\nprint(\"Model feature names:\")\nif hasattr(model, \"feature_names_\"):\n    print(model.feature_names_)\nelif hasattr(model, \"feature_names\"):\n    print(model.feature_names)\nelse:\n    print(\"‚ùå Model does NOT expose feature names. Need training code.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:18:20.014859Z","iopub.execute_input":"2025-12-05T11:18:20.015438Z","iopub.status.idle":"2025-12-05T11:18:20.024791Z","shell.execute_reply.started":"2025-12-05T11:18:20.015414Z","shell.execute_reply":"2025-12-05T11:18:20.023984Z"}},"outputs":[{"name":"stdout","text":"Model feature names:\n['Dst Port', 'Protocol', 'Timestamp', 'Flow Duration', 'Tot Fwd Pkts', 'Tot Bwd Pkts', 'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std', 'Bwd Pkt Len Max', 'Bwd Pkt Len Min', 'Bwd Pkt Len Mean', 'Bwd Pkt Len Std', 'Flow Byts/s', 'Flow Pkts/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Tot', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Tot', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Fwd URG Flags', 'Fwd Header Len', 'Bwd Header Len', 'Fwd Pkts/s', 'Bwd Pkts/s', 'Pkt Len Min', 'Pkt Len Max', 'Pkt Len Mean', 'Pkt Len Std', 'Pkt Len Var', 'FIN Flag Cnt', 'SYN Flag Cnt', 'RST Flag Cnt', 'PSH Flag Cnt', 'ACK Flag Cnt', 'URG Flag Cnt', 'CWE Flag Count', 'ECE Flag Cnt', 'Down/Up Ratio', 'Pkt Size Avg', 'Fwd Seg Size Avg', 'Bwd Seg Size Avg', 'Subflow Fwd Pkts', 'Subflow Fwd Byts', 'Subflow Bwd Pkts', 'Subflow Bwd Byts', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min', 'Flow ID', 'Src IP', 'Src Port', 'Dst IP']\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:19:26.354865Z","iopub.execute_input":"2025-12-05T11:19:26.355136Z","iopub.status.idle":"2025-12-05T11:19:26.360469Z","shell.execute_reply.started":"2025-12-05T11:19:26.355117Z","shell.execute_reply":"2025-12-05T11:19:26.359906Z"}},"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"Index(['Dst Port', 'Protocol', 'Timestamp', 'Flow Duration', 'Tot Fwd Pkts',\n       'Tot Bwd Pkts', 'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max',\n       'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std',\n       'Bwd Pkt Len Max', 'Bwd Pkt Len Min', 'Bwd Pkt Len Mean',\n       'Bwd Pkt Len Std', 'Flow Byts/s', 'Flow Pkts/s', 'Flow IAT Mean',\n       'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Tot',\n       'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min',\n       'Bwd IAT Tot', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max',\n       'Bwd IAT Min', 'Fwd PSH Flags', 'Fwd URG Flags', 'Fwd Header Len',\n       'Bwd Header Len', 'Fwd Pkts/s', 'Bwd Pkts/s', 'Pkt Len Min',\n       'Pkt Len Max', 'Pkt Len Mean', 'Pkt Len Std', 'Pkt Len Var',\n       'FIN Flag Cnt', 'SYN Flag Cnt', 'RST Flag Cnt', 'PSH Flag Cnt',\n       'ACK Flag Cnt', 'URG Flag Cnt', 'CWE Flag Count', 'ECE Flag Cnt',\n       'Down/Up Ratio', 'Pkt Size Avg', 'Fwd Seg Size Avg', 'Bwd Seg Size Avg',\n       'Subflow Fwd Pkts', 'Subflow Fwd Byts', 'Subflow Bwd Pkts',\n       'Subflow Bwd Byts', 'Init Fwd Win Byts', 'Init Bwd Win Byts',\n       'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std',\n       'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max',\n       'Idle Min', 'Label', 'Flow ID', 'Src IP', 'Src Port', 'Dst IP'],\n      dtype='object')"},"metadata":{}}],"execution_count":39},{"cell_type":"code","source":"# Code cell: Align & predict (copy->paste into Kaggle)\nimport os, time, pickle, joblib\nfrom pathlib import Path\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, confusion_matrix, classification_report\n\nMODEL_PATH = \"/kaggle/working/catboost_ids2018_model.pkl\"\nTEST_PATH  = \"/kaggle/working/inital_2017_catboost_binary.csv\"\nOUT_PATH   = \"/kaggle/working/inital_2017_catboost_with_preds.csv\"\nPath(OUT_PATH).parent.mkdir(parents=True, exist_ok=True)\n\n# 1) Load test file and show columns\ndf = pd.read_csv(TEST_PATH)\nprint(\"Test file loaded. Shape:\", df.shape)\nprint(\"\\nTest file columns:\")\nprint(df.columns.tolist())\n\n# 2) Ensure Label binary\nif 'Label' not in df.columns:\n    raise ValueError(\"Label column missing in test file.\")\ndf['Label'] = df['Label'].astype(str).str.strip().str.lower().apply(lambda x: 0 if \"benign\" in x else 1).astype(int)\ny_true = df['Label'].to_numpy()\n\n# 3) Load model\nmodel = None\nlast_err = None\nfor loader in (joblib.load, pickle.load):\n    try:\n        model = loader(MODEL_PATH) if loader is joblib.load else loader(open(MODEL_PATH,'rb'))\n        break\n    except Exception as e:\n        last_err = e\nif model is None:\n    raise RuntimeError(f\"Failed to load model. Last error: {last_err}\")\n\n# 4) Get model's expected feature list\nif hasattr(model, \"feature_names_\"):\n    model_features = list(model.feature_names_)\nelif hasattr(model, \"feature_names\"):\n    model_features = list(model.feature_names)\nelse:\n    raise RuntimeError(\"Model does not expose feature names. Provide training feature list.\")\n\nprint(\"\\nModel expects\", len(model_features), \"features.\")\nprint(\"First 10 model features:\", model_features[:10])\n\n# 5) Rename known variants if present (safe no-op if not present)\n# (If you've already renamed to the model names, this block will do nothing.)\nrename_map = {\n    'Total Fwd Packets': 'Tot Fwd Pkts',\n    'Total Backward Packets': 'Tot Bwd Pkts',\n    'Fwd Packets Length Total': 'TotLen Fwd Pkts',\n    'Bwd Packets Length Total': 'TotLen Bwd Pkts',\n    'Fwd Packet Length Max': 'Fwd Pkt Len Max',\n    'Fwd Packet Length Min': 'Fwd Pkt Len Min',\n    'Fwd Packet Length Mean': 'Fwd Pkt Len Mean',\n    'Fwd Packet Length Std': 'Fwd Pkt Len Std',\n    'Bwd Packet Length Max': 'Bwd Pkt Len Max',\n    'Bwd Packet Length Min': 'Bwd Pkt Len Min',\n    'Bwd Packet Length Mean': 'Bwd Pkt Len Mean',\n    'Bwd Packet Length Std': 'Bwd Pkt Len Std',\n    'Flow Bytes/s': 'Flow Byts/s',\n    'Flow Packets/s': 'Flow Pkts/s',\n    'Packet Length Min': 'Pkt Len Min',\n    'Packet Length Max': 'Pkt Len Max',\n    'Packet Length Mean': 'Pkt Len Mean',\n    'Packet Length Std': 'Pkt Len Std',\n    'Packet Length Variance': 'Pkt Len Var',\n    'FIN Flag Count': 'FIN Flag Cnt',\n    'SYN Flag Count': 'SYN Flag Cnt',\n    'RST Flag Count': 'RST Flag Cnt',\n    'PSH Flag Count': 'PSH Flag Cnt',\n    'ACK Flag Count': 'ACK Flag Cnt',\n    'URG Flag Count': 'URG Flag Cnt',\n    'CWE Flag Count': 'CWE Flag Count',\n    'ECE Flag Count': 'ECE Flag Cnt',\n    'Avg Packet Size': 'Pkt Size Avg',\n    'Avg Fwd Segment Size': 'Fwd Seg Size Avg',\n    'Avg Bwd Segment Size': 'Bwd Seg Size Avg',\n    'Subflow Fwd Packets': 'Subflow Fwd Pkts',\n    'Subflow Fwd Bytes': 'Subflow Fwd Byts',\n    'Subflow Bwd Packets': 'Subflow Bwd Pkts',\n    'Subflow Bwd Bytes': 'Subflow Bwd Byts',\n    'Init Fwd Win Bytes': 'Init Fwd Win Byts',\n    'Init Bwd Win Bytes': 'Init Bwd Win Byts',\n    'Fwd Act Data Packets': 'Fwd Act Data Pkts',\n    'Fwd Seg Size Min': 'Fwd Seg Size Min',\n    'Fwd Header Length': 'Fwd Header Len',\n    'Bwd Header Length': 'Bwd Header Len',\n    'Fwd Packets/s': 'Fwd Pkts/s',\n    'Bwd Packets/s': 'Bwd Pkts/s',\n    'Flow Bytes/s': 'Flow Byts/s',\n    'Flow Packets/s': 'Flow Pkts/s',\n    'Packet Length Variance': 'Pkt Len Var'\n}\npresent_renames = {k:v for k,v in rename_map.items() if k in df.columns}\nif present_renames:\n    df = df.rename(columns=present_renames)\n    print(f\"\\nApplied {len(present_renames)} renames to match model naming.\")\n\n# 6) Add missing model features with sensible defaults, drop extras\nmissing = [c for c in model_features if c not in df.columns]\nextra   = [c for c in df.columns if c not in model_features and c != 'Label']\n\nprint(\"\\nMissing features (will be added):\", missing)\nprint(\"Extra columns (will be dropped):\", extra[:50])\n\nfor c in missing:\n    if c in ('Src IP','Dst IP','Flow ID'):\n        df[c] = \"\"   # string placeholder\n    elif c in ('Src Port','Dst Port','Timestamp'):\n        df[c] = 0    # numeric placeholder\n    else:\n        df[c] = 0\n\nif extra:\n    df = df.drop(columns=extra)\n\n# 7) Reorder exactly to model_features\nX = df[model_features].copy()\nprint(\"\\nPrepared X shape:\", X.shape)\n\n# 8) Convert numeric-like columns to numeric (keep IPs/FlowID as strings)\nnon_numeric_keep = {'Src IP','Dst IP','Flow ID'}\nfor c in X.columns:\n    if c in non_numeric_keep:\n        X[c] = X[c].astype(str).fillna(\"\")\n    else:\n        X[c] = pd.to_numeric(X[c], errors='coerce').fillna(0)\n\n# 9) Quick sanity: show first 5 rows of prepared features\nprint(\"\\nFirst 5 rows of prepared features:\")\ndisplay(X.head())\n\n# 10) Predict and evaluate\nt0 = time.time()\ntry:\n    y_pred = model.predict(X)\nexcept Exception:\n    from catboost import Pool\n    y_pred = model.predict(Pool(X))\nt1 = time.time()\n\ny_pred = np.array(y_pred).squeeze()\nif y_pred.dtype.kind == 'f' and not set(np.unique(y_pred)).issubset({0.0,1.0}):\n    y_pred = np.rint(y_pred).astype(int)\nelse:\n    y_pred = y_pred.astype(int)\n\nacc  = accuracy_score(y_true, y_pred)\nf1_w = f1_score(y_true, y_pred, average='weighted', zero_division=0)\nrec_w = recall_score(y_true, y_pred, average='weighted', zero_division=0)\nprec_w = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n\nprint(f\"\\nAccuracy: {acc:.12f}\")\nprint(f\"üìå F1 Score (weighted): {f1_w:.12f}\")\nprint(f\"üìå Recall (weighted): {rec_w:.12f}\")\nprint(f\"üìå Precision (weighted): {prec_w:.12f}\")\nprint(f\"‚è±Ô∏è Time Taken (predict only): {(t1-t0):.4f} seconds\\n\")\n\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_true, y_pred))\n\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_true, y_pred, digits=6))\n\n# 11) Save predictions appended\ndf_out = df.copy()\ndf_out['pred_label'] = y_pred\n\n# try predict_proba\ntry:\n    y_proba = model.predict_proba(X)\n    y_proba = np.array(y_proba)\n    if y_proba.ndim == 2 and y_proba.shape[1] == 2:\n        df_out['pred_proba_pos'] = y_proba[:,1]\n    else:\n        df_out['pred_proba'] = [list(map(float,row)) for row in y_proba]\nexcept Exception:\n    pass\n\ndf_out.to_csv(OUT_PATH, index=False)\nprint(f\"\\nSaved predictions to: {OUT_PATH}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:20:28.018565Z","iopub.execute_input":"2025-12-05T11:20:28.019391Z","iopub.status.idle":"2025-12-05T11:20:36.142020Z","shell.execute_reply.started":"2025-12-05T11:20:28.019358Z","shell.execute_reply":"2025-12-05T11:20:36.141411Z"}},"outputs":[{"name":"stdout","text":"Test file loaded. Shape: (176038, 76)\n\nTest file columns:\n['Dst Port', 'Protocol', 'Timestamp', 'Flow Duration', 'Tot Fwd Pkts', 'Tot Bwd Pkts', 'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std', 'Bwd Pkt Len Max', 'Bwd Pkt Len Min', 'Bwd Pkt Len Mean', 'Bwd Pkt Len Std', 'Flow Byts/s', 'Flow Pkts/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Tot', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Tot', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Fwd URG Flags', 'Fwd Header Len', 'Bwd Header Len', 'Fwd Pkts/s', 'Bwd Pkts/s', 'Pkt Len Min', 'Pkt Len Max', 'Pkt Len Mean', 'Pkt Len Std', 'Pkt Len Var', 'FIN Flag Cnt', 'SYN Flag Cnt', 'RST Flag Cnt', 'PSH Flag Cnt', 'ACK Flag Cnt', 'URG Flag Cnt', 'CWE Flag Count', 'ECE Flag Cnt', 'Down/Up Ratio', 'Pkt Size Avg', 'Fwd Seg Size Avg', 'Bwd Seg Size Avg', 'Subflow Fwd Pkts', 'Subflow Fwd Byts', 'Subflow Bwd Pkts', 'Subflow Bwd Byts', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min', 'Label', 'Flow ID', 'Src IP', 'Src Port', 'Dst IP']\n\nModel expects 75 features.\nFirst 10 model features: ['Dst Port', 'Protocol', 'Timestamp', 'Flow Duration', 'Tot Fwd Pkts', 'Tot Bwd Pkts', 'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Min']\n\nApplied 2 renames to match model naming.\n\nMissing features (will be added): []\nExtra columns (will be dropped): []\n\nPrepared X shape: (176038, 75)\n\nFirst 5 rows of prepared features:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   Dst Port  Protocol  Timestamp  Flow Duration  Tot Fwd Pkts  Tot Bwd Pkts  \\\n0       0.0         6        0.0      112740690            32            16   \n1       0.0         6        0.0      112740560            32            16   \n2       0.0         0        0.0      113757377           545             0   \n3       0.0        17        0.0         100126            22             0   \n4       0.0         0        0.0          54760             4             0   \n\n   TotLen Fwd Pkts  TotLen Bwd Pkts  Fwd Pkt Len Max  Fwd Pkt Len Min  ...  \\\n0             6448             1152              403                0  ...   \n1             6448             5056              403                0  ...   \n2                0                0                0                0  ...   \n3              616                0               28               28  ...   \n4                0                0                0                0  ...   \n\n   Active Max  Active Min   Idle Mean    Idle Std  Idle Max  Idle Min  \\\n0         380         343  16100000.0   498804.80  16400000  15400000   \n1         330         285  16100000.0   498793.66  16400000  15400000   \n2    18900000          19  12200000.0  6935824.00  20800000   5504997   \n3           0           0         0.0        0.00         0         0   \n4           0           0         0.0        0.00         0         0   \n\n   Flow ID  Src IP  Src Port  Dst IP  \n0      nan     nan       0.0     nan  \n1      nan     nan       0.0     nan  \n2      nan     nan       0.0     nan  \n3      nan     nan       0.0     nan  \n4      nan     nan       0.0     nan  \n\n[5 rows x 75 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Dst Port</th>\n      <th>Protocol</th>\n      <th>Timestamp</th>\n      <th>Flow Duration</th>\n      <th>Tot Fwd Pkts</th>\n      <th>Tot Bwd Pkts</th>\n      <th>TotLen Fwd Pkts</th>\n      <th>TotLen Bwd Pkts</th>\n      <th>Fwd Pkt Len Max</th>\n      <th>Fwd Pkt Len Min</th>\n      <th>...</th>\n      <th>Active Max</th>\n      <th>Active Min</th>\n      <th>Idle Mean</th>\n      <th>Idle Std</th>\n      <th>Idle Max</th>\n      <th>Idle Min</th>\n      <th>Flow ID</th>\n      <th>Src IP</th>\n      <th>Src Port</th>\n      <th>Dst IP</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>6</td>\n      <td>0.0</td>\n      <td>112740690</td>\n      <td>32</td>\n      <td>16</td>\n      <td>6448</td>\n      <td>1152</td>\n      <td>403</td>\n      <td>0</td>\n      <td>...</td>\n      <td>380</td>\n      <td>343</td>\n      <td>16100000.0</td>\n      <td>498804.80</td>\n      <td>16400000</td>\n      <td>15400000</td>\n      <td>nan</td>\n      <td>nan</td>\n      <td>0.0</td>\n      <td>nan</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>6</td>\n      <td>0.0</td>\n      <td>112740560</td>\n      <td>32</td>\n      <td>16</td>\n      <td>6448</td>\n      <td>5056</td>\n      <td>403</td>\n      <td>0</td>\n      <td>...</td>\n      <td>330</td>\n      <td>285</td>\n      <td>16100000.0</td>\n      <td>498793.66</td>\n      <td>16400000</td>\n      <td>15400000</td>\n      <td>nan</td>\n      <td>nan</td>\n      <td>0.0</td>\n      <td>nan</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>113757377</td>\n      <td>545</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>18900000</td>\n      <td>19</td>\n      <td>12200000.0</td>\n      <td>6935824.00</td>\n      <td>20800000</td>\n      <td>5504997</td>\n      <td>nan</td>\n      <td>nan</td>\n      <td>0.0</td>\n      <td>nan</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>17</td>\n      <td>0.0</td>\n      <td>100126</td>\n      <td>22</td>\n      <td>0</td>\n      <td>616</td>\n      <td>0</td>\n      <td>28</td>\n      <td>28</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>0</td>\n      <td>0</td>\n      <td>nan</td>\n      <td>nan</td>\n      <td>0.0</td>\n      <td>nan</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>54760</td>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>0</td>\n      <td>0</td>\n      <td>nan</td>\n      <td>nan</td>\n      <td>0.0</td>\n      <td>nan</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows √ó 75 columns</p>\n</div>"},"metadata":{}},{"name":"stdout","text":"\nAccuracy: 1.000000000000\nüìå F1 Score (weighted): 1.000000000000\nüìå Recall (weighted): 1.000000000000\nüìå Precision (weighted): 1.000000000000\n‚è±Ô∏è Time Taken (predict only): 0.2692 seconds\n\nConfusion Matrix:\n[[176038]]\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           1   1.000000  1.000000  1.000000    176038\n\n    accuracy                       1.000000    176038\n   macro avg   1.000000  1.000000  1.000000    176038\nweighted avg   1.000000  1.000000  1.000000    176038\n\n\nSaved predictions to: /kaggle/working/inital_2017_catboost_with_preds.csv\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"a=pd.read_csv(\"/kaggle/working/processed_botnet_2017.csv\")\na.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T12:02:28.264241Z","iopub.execute_input":"2025-12-05T12:02:28.264536Z","iopub.status.idle":"2025-12-05T12:02:28.574481Z","shell.execute_reply.started":"2025-12-05T12:02:28.264517Z","shell.execute_reply":"2025-12-05T12:02:28.573838Z"}},"outputs":[{"execution_count":71,"output_type":"execute_result","data":{"text/plain":"Index(['Dst Port', 'Timestamp', 'Flow Duration', 'Fwd Pkt Len Min',\n       'Fwd Pkt Len Mean', 'Bwd Pkt Len Max', 'Bwd Pkt Len Min',\n       'Bwd Pkt Len Std', 'Flow Pkts/s', 'Fwd IAT Std', 'Fwd Header Len',\n       'Fwd Pkts/s', 'Pkt Len Mean', 'Pkt Len Var', 'SYN Flag Cnt',\n       'CWE Flag Count', 'ECE Flag Cnt', 'Down/Up Ratio', 'Subflow Bwd Byts',\n       'Init Bwd Win Byts', 'Fwd Seg Size Min', 'Idle Mean', 'Idle Std',\n       'Idle Min', 'Src IP', 'Dst IP', 'Label'],\n      dtype='object')"},"metadata":{}}],"execution_count":71},{"cell_type":"code","source":"import joblib, pickle\n\nMODEL_PATH = \"/kaggle/working/catboost_ids2018_model.pkl\"\n\n# Try loading using joblib first, then pickle\nmodel = None\nload_err = None\n\nfor loader in (joblib.load, pickle.load):\n    try:\n        if loader is joblib.load:\n            model = loader(MODEL_PATH)\n        else:\n            with open(MODEL_PATH, \"rb\") as f:\n                model = loader(f)\n        break\n    except Exception as e:\n        load_err = e\n\nif model is None:\n    raise RuntimeError(f\"‚ùå Failed to load model: {load_err}\")\n\n# Print feature names used by the model\nif hasattr(model, \"feature_names_\"):\n    features = list(model.feature_names_)\nelif hasattr(model, \"feature_names\"):\n    features = list(model.feature_names)\nelse:\n    raise RuntimeError(\"‚ùå Model does not contain feature names.\")\n\nprint(\"Total features:\", len(features))\nprint(\"\\nFeature list in correct order:\\n\")\nfor idx, f in enumerate(features):\n    print(f\"{idx+1:02d}. {f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T12:15:44.760847Z","iopub.execute_input":"2025-12-05T12:15:44.761157Z","iopub.status.idle":"2025-12-05T12:15:44.799297Z","shell.execute_reply.started":"2025-12-05T12:15:44.761136Z","shell.execute_reply":"2025-12-05T12:15:44.798483Z"}},"outputs":[{"name":"stdout","text":"Total features: 75\n\nFeature list in correct order:\n\n01. Dst Port\n02. Protocol\n03. Timestamp\n04. Flow Duration\n05. Tot Fwd Pkts\n06. Tot Bwd Pkts\n07. TotLen Fwd Pkts\n08. TotLen Bwd Pkts\n09. Fwd Pkt Len Max\n10. Fwd Pkt Len Min\n11. Fwd Pkt Len Mean\n12. Fwd Pkt Len Std\n13. Bwd Pkt Len Max\n14. Bwd Pkt Len Min\n15. Bwd Pkt Len Mean\n16. Bwd Pkt Len Std\n17. Flow Byts/s\n18. Flow Pkts/s\n19. Flow IAT Mean\n20. Flow IAT Std\n21. Flow IAT Max\n22. Flow IAT Min\n23. Fwd IAT Tot\n24. Fwd IAT Mean\n25. Fwd IAT Std\n26. Fwd IAT Max\n27. Fwd IAT Min\n28. Bwd IAT Tot\n29. Bwd IAT Mean\n30. Bwd IAT Std\n31. Bwd IAT Max\n32. Bwd IAT Min\n33. Fwd PSH Flags\n34. Fwd URG Flags\n35. Fwd Header Len\n36. Bwd Header Len\n37. Fwd Pkts/s\n38. Bwd Pkts/s\n39. Pkt Len Min\n40. Pkt Len Max\n41. Pkt Len Mean\n42. Pkt Len Std\n43. Pkt Len Var\n44. FIN Flag Cnt\n45. SYN Flag Cnt\n46. RST Flag Cnt\n47. PSH Flag Cnt\n48. ACK Flag Cnt\n49. URG Flag Cnt\n50. CWE Flag Count\n51. ECE Flag Cnt\n52. Down/Up Ratio\n53. Pkt Size Avg\n54. Fwd Seg Size Avg\n55. Bwd Seg Size Avg\n56. Subflow Fwd Pkts\n57. Subflow Fwd Byts\n58. Subflow Bwd Pkts\n59. Subflow Bwd Byts\n60. Init Fwd Win Byts\n61. Init Bwd Win Byts\n62. Fwd Act Data Pkts\n63. Fwd Seg Size Min\n64. Active Mean\n65. Active Std\n66. Active Max\n67. Active Min\n68. Idle Mean\n69. Idle Std\n70. Idle Max\n71. Idle Min\n72. Flow ID\n73. Src IP\n74. Src Port\n75. Dst IP\n","output_type":"stream"}],"execution_count":72},{"cell_type":"code","source":"gg=pd.read_parquet(\"/kaggle/input/cicids2017/Botnet-Friday-no-metadata.parquet\")\ngg.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T12:16:46.893386Z","iopub.execute_input":"2025-12-05T12:16:46.893985Z","iopub.status.idle":"2025-12-05T12:16:46.990542Z","shell.execute_reply.started":"2025-12-05T12:16:46.893960Z","shell.execute_reply":"2025-12-05T12:16:46.989917Z"}},"outputs":[{"execution_count":73,"output_type":"execute_result","data":{"text/plain":"Index(['Protocol', 'Flow Duration', 'Total Fwd Packets',\n       'Total Backward Packets', 'Fwd Packets Length Total',\n       'Bwd Packets Length Total', 'Fwd Packet Length Max',\n       'Fwd Packet Length Min', 'Fwd Packet Length Mean',\n       'Fwd Packet Length Std', 'Bwd Packet Length Max',\n       'Bwd Packet Length Min', 'Bwd Packet Length Mean',\n       'Bwd Packet Length Std', 'Flow Bytes/s', 'Flow Packets/s',\n       'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min',\n       'Fwd IAT Total', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max',\n       'Fwd IAT Min', 'Bwd IAT Total', 'Bwd IAT Mean', 'Bwd IAT Std',\n       'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags',\n       'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Header Length',\n       'Bwd Header Length', 'Fwd Packets/s', 'Bwd Packets/s',\n       'Packet Length Min', 'Packet Length Max', 'Packet Length Mean',\n       'Packet Length Std', 'Packet Length Variance', 'FIN Flag Count',\n       'SYN Flag Count', 'RST Flag Count', 'PSH Flag Count', 'ACK Flag Count',\n       'URG Flag Count', 'CWE Flag Count', 'ECE Flag Count', 'Down/Up Ratio',\n       'Avg Packet Size', 'Avg Fwd Segment Size', 'Avg Bwd Segment Size',\n       'Fwd Avg Bytes/Bulk', 'Fwd Avg Packets/Bulk', 'Fwd Avg Bulk Rate',\n       'Bwd Avg Bytes/Bulk', 'Bwd Avg Packets/Bulk', 'Bwd Avg Bulk Rate',\n       'Subflow Fwd Packets', 'Subflow Fwd Bytes', 'Subflow Bwd Packets',\n       'Subflow Bwd Bytes', 'Init Fwd Win Bytes', 'Init Bwd Win Bytes',\n       'Fwd Act Data Packets', 'Fwd Seg Size Min', 'Active Mean', 'Active Std',\n       'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max',\n       'Idle Min', 'Label'],\n      dtype='object')"},"metadata":{}}],"execution_count":73},{"cell_type":"code","source":"gg.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T12:16:58.275251Z","iopub.execute_input":"2025-12-05T12:16:58.275577Z","iopub.status.idle":"2025-12-05T12:16:58.281320Z","shell.execute_reply.started":"2025-12-05T12:16:58.275544Z","shell.execute_reply":"2025-12-05T12:16:58.280590Z"}},"outputs":[{"execution_count":74,"output_type":"execute_result","data":{"text/plain":"(176038, 78)"},"metadata":{}}],"execution_count":74},{"cell_type":"code","source":"# ---------- prepare_and_save_for_model.py ----------\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\n\n# ---------- paths ----------\nsrc_parquet = \"/kaggle/input/cicids2017/Botnet-Friday-no-metadata.parquet\"\nout_parquet = \"/kaggle/working/inital_2017_catboost_ready.parquet\"\nout_csv     = \"/kaggle/working/inital_2017_catboost_ready.csv\"\nPath(\"/kaggle/working\").mkdir(parents=True, exist_ok=True)\n\n# ---------- model feature list (exact order) ----------\nmodel_features = [\n 'Dst Port','Protocol','Timestamp','Flow Duration','Tot Fwd Pkts','Tot Bwd Pkts',\n 'TotLen Fwd Pkts','TotLen Bwd Pkts','Fwd Pkt Len Max','Fwd Pkt Len Min',\n 'Fwd Pkt Len Mean','Fwd Pkt Len Std','Bwd Pkt Len Max','Bwd Pkt Len Min',\n 'Bwd Pkt Len Mean','Bwd Pkt Len Std','Flow Byts/s','Flow Pkts/s','Flow IAT Mean',\n 'Flow IAT Std','Flow IAT Max','Flow IAT Min','Fwd IAT Tot','Fwd IAT Mean',\n 'Fwd IAT Std','Fwd IAT Max','Fwd IAT Min','Bwd IAT Tot','Bwd IAT Mean',\n 'Bwd IAT Std','Bwd IAT Max','Bwd IAT Min','Fwd PSH Flags','Fwd URG Flags',\n 'Fwd Header Len','Bwd Header Len','Fwd Pkts/s','Bwd Pkts/s','Pkt Len Min',\n 'Pkt Len Max','Pkt Len Mean','Pkt Len Std','Pkt Len Var','FIN Flag Cnt',\n 'SYN Flag Cnt','RST Flag Cnt','PSH Flag Cnt','ACK Flag Cnt','URG Flag Cnt',\n 'CWE Flag Count','ECE Flag Cnt','Down/Up Ratio','Pkt Size Avg','Fwd Seg Size Avg',\n 'Bwd Seg Size Avg','Subflow Fwd Pkts','Subflow Fwd Byts','Subflow Bwd Pkts',\n 'Subflow Bwd Byts','Init Fwd Win Byts','Init Bwd Win Byts','Fwd Act Data Pkts',\n 'Fwd Seg Size Min','Active Mean','Active Std','Active Max','Active Min',\n 'Idle Mean','Idle Std','Idle Max','Idle Min','Flow ID','Src IP','Src Port','Dst IP'\n]\n\n# ---------- mapping: source column -> model column ----------\n# Based on gg.columns you provided\nmapping = {\n    'Protocol': 'Protocol',\n    'Flow Duration': 'Flow Duration',\n    'Total Fwd Packets': 'Tot Fwd Pkts',\n    'Total Backward Packets': 'Tot Bwd Pkts',\n    'Fwd Packets Length Total': 'TotLen Fwd Pkts',\n    'Bwd Packets Length Total': 'TotLen Bwd Pkts',\n    'Fwd Packet Length Max': 'Fwd Pkt Len Max',\n    'Fwd Packet Length Min': 'Fwd Pkt Len Min',\n    'Fwd Packet Length Mean': 'Fwd Pkt Len Mean',\n    'Fwd Packet Length Std': 'Fwd Pkt Len Std',\n    'Bwd Packet Length Max': 'Bwd Pkt Len Max',\n    'Bwd Packet Length Min': 'Bwd Pkt Len Min',\n    'Bwd Packet Length Mean': 'Bwd Pkt Len Mean',\n    'Bwd Packet Length Std': 'Bwd Pkt Len Std',\n    'Flow Bytes/s': 'Flow Byts/s',\n    'Flow Packets/s': 'Flow Pkts/s',\n    'Flow IAT Mean': 'Flow IAT Mean',\n    'Flow IAT Std': 'Flow IAT Std',\n    'Flow IAT Max': 'Flow IAT Max',\n    'Flow IAT Min': 'Flow IAT Min',\n    'Fwd IAT Total': 'Fwd IAT Tot',\n    'Fwd IAT Mean': 'Fwd IAT Mean',\n    'Fwd IAT Std': 'Fwd IAT Std',\n    'Fwd IAT Max': 'Fwd IAT Max',\n    'Fwd IAT Min': 'Fwd IAT Min',\n    'Bwd IAT Total': 'Bwd IAT Tot',\n    'Bwd IAT Mean': 'Bwd IAT Mean',\n    'Bwd IAT Std': 'Bwd IAT Std',\n    'Bwd IAT Max': 'Bwd IAT Max',\n    'Bwd IAT Min': 'Bwd IAT Min',\n    'Fwd PSH Flags': 'Fwd PSH Flags',\n    # 'Bwd PSH Flags' dropped (no target equivalent)\n    'Fwd URG Flags': 'Fwd URG Flags',\n    # 'Bwd URG Flags' dropped\n    'Fwd Header Length': 'Fwd Header Len',\n    'Bwd Header Length': 'Bwd Header Len',\n    'Fwd Packets/s': 'Fwd Pkts/s',\n    'Bwd Packets/s': 'Bwd Pkts/s',\n    'Packet Length Min': 'Pkt Len Min',\n    'Packet Length Max': 'Pkt Len Max',\n    'Packet Length Mean': 'Pkt Len Mean',\n    'Packet Length Std': 'Pkt Len Std',\n    'Packet Length Variance': 'Pkt Len Var',\n    'FIN Flag Count': 'FIN Flag Cnt',\n    'SYN Flag Count': 'SYN Flag Cnt',\n    'RST Flag Count': 'RST Flag Cnt',\n    'PSH Flag Count': 'PSH Flag Cnt',\n    'ACK Flag Count': 'ACK Flag Cnt',\n    'URG Flag Count': 'URG Flag Cnt',\n    'CWE Flag Count': 'CWE Flag Count',\n    'ECE Flag Count': 'ECE Flag Cnt',\n    'Down/Up Ratio': 'Down/Up Ratio',\n    'Avg Packet Size': 'Pkt Size Avg',\n    'Avg Fwd Segment Size': 'Fwd Seg Size Avg',\n    'Avg Bwd Segment Size': 'Bwd Seg Size Avg',\n    'Subflow Fwd Packets': 'Subflow Fwd Pkts',\n    'Subflow Fwd Bytes': 'Subflow Fwd Byts',\n    'Subflow Bwd Packets': 'Subflow Bwd Pkts',\n    'Subflow Bwd Bytes': 'Subflow Bwd Byts',\n    'Init Fwd Win Bytes': 'Init Fwd Win Byts',\n    'Init Bwd Win Bytes': 'Init Bwd Win Byts',\n    'Fwd Act Data Packets': 'Fwd Act Data Pkts',\n    'Fwd Seg Size Min': 'Fwd Seg Size Min',\n    'Active Mean': 'Active Mean',\n    'Active Std': 'Active Std',\n    'Active Max': 'Active Max',\n    'Active Min': 'Active Min',\n    'Idle Mean': 'Idle Mean',\n    'Idle Std': 'Idle Std',\n    'Idle Max': 'Idle Max',\n    'Idle Min': 'Idle Min',\n    'Label': 'Label'\n}\n\n# ---------- load source parquet ----------\nprint(\"Loading:\", src_parquet)\ngg = pd.read_parquet(src_parquet)\nprint(\"Source shape:\", gg.shape)\nprint(\"Source columns count:\", len(gg.columns))\n\n# ---------- rename mapped columns ----------\nrename_pairs = {src: tgt for src, tgt in mapping.items() if src in gg.columns}\ngg = gg.rename(columns=rename_pairs)\nprint(f\"Renamed {len(rename_pairs)} columns (source->model names).\")\n\n# ---------- drop source-only columns not in mapping or model_features ----------\n# Determine columns to keep (mapped to model or already a model column or 'Label')\nkeep_candidates = set(rename_pairs.values()) | set(model_features) | {'Label'}\n# but we only want to keep columns that will map to final model features or Label\ncols_to_drop = [c for c in gg.columns if c not in keep_candidates]\nif cols_to_drop:\n    print(f\"Dropping {len(cols_to_drop)} source-only columns (examples):\", cols_to_drop[:10])\n    gg = gg.drop(columns=cols_to_drop)\n\n# ---------- add missing model features with sensible defaults ----------\nmissing = [c for c in model_features if c not in gg.columns]\nprint(\"Missing model features to add:\", missing)\nfor c in missing:\n    if c in ('Src IP','Dst IP','Flow ID'):\n        gg[c] = \"\"   # string placeholder\n    elif c in ('Src Port','Dst Port','Timestamp'):\n        gg[c] = 0    # numeric placeholder\n    else:\n        gg[c] = 0\n\n# ---------- Ensure Label exists; if not, add as NaN ----------\nif 'Label' not in gg.columns:\n    gg['Label'] = np.nan\n    print(\"Label column not found in source ‚Äî created placeholder Label (NaN).\")\n\n# ---------- Reorder columns: model_features in order, then Label at the end ----------\nfinal_cols = model_features + ['Label']\ngg = gg[final_cols].copy()\nprint(\"Final shape:\", gg.shape)\nprint(\"Final columns (first 40):\", gg.columns.tolist()[:40])\nprint(\"Final columns (last 10):\", gg.columns.tolist()[-10:])\n\n# ---------- convert numeric-like columns to numeric except IP/FlowID ----------\nnon_numeric_keep = {'Src IP','Dst IP','Flow ID'}\nfor c in gg.columns:\n    if c in non_numeric_keep or c == 'Label':\n        # keep Label as-is (do not coerce here; user may want original string labels)\n        continue\n    # coerce numeric and fill NaN with 0\n    gg[c] = pd.to_numeric(gg[c], errors='coerce').fillna(0)\n\n# ---------- Save outputs ----------\ngg.to_parquet(out_parquet, index=False)\ngg.to_csv(out_csv, index=False)\nprint(\"Saved processed files:\")\nprint(\"  -\", out_parquet)\nprint(\"  -\", out_csv)\n\n# ---------- quick sanity prints ----------\nprint(\"\\nSample head (first 5 rows):\")\nprint(gg.head(5).iloc[:, :8])   # show first 8 cols\nprint(\"\\nLabel value counts (if present):\")\nprint(gg['Label'].value_counts(dropna=False))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T12:50:18.428186Z","iopub.execute_input":"2025-12-05T12:50:18.428989Z","iopub.status.idle":"2025-12-05T12:50:24.501876Z","shell.execute_reply.started":"2025-12-05T12:50:18.428964Z","shell.execute_reply":"2025-12-05T12:50:24.501245Z"}},"outputs":[{"name":"stdout","text":"Loading: /kaggle/input/cicids2017/Botnet-Friday-no-metadata.parquet\nSource shape: (176038, 78)\nSource columns count: 78\nRenamed 70 columns (source->model names).\nDropping 8 source-only columns (examples): ['Bwd PSH Flags', 'Bwd URG Flags', 'Fwd Avg Bytes/Bulk', 'Fwd Avg Packets/Bulk', 'Fwd Avg Bulk Rate', 'Bwd Avg Bytes/Bulk', 'Bwd Avg Packets/Bulk', 'Bwd Avg Bulk Rate']\nMissing model features to add: ['Dst Port', 'Timestamp', 'Flow ID', 'Src IP', 'Src Port', 'Dst IP']\nFinal shape: (176038, 76)\nFinal columns (first 40): ['Dst Port', 'Protocol', 'Timestamp', 'Flow Duration', 'Tot Fwd Pkts', 'Tot Bwd Pkts', 'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std', 'Bwd Pkt Len Max', 'Bwd Pkt Len Min', 'Bwd Pkt Len Mean', 'Bwd Pkt Len Std', 'Flow Byts/s', 'Flow Pkts/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Tot', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Tot', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Fwd URG Flags', 'Fwd Header Len', 'Bwd Header Len', 'Fwd Pkts/s', 'Bwd Pkts/s', 'Pkt Len Min', 'Pkt Len Max']\nFinal columns (last 10): ['Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min', 'Flow ID', 'Src IP', 'Src Port', 'Dst IP', 'Label']\nSaved processed files:\n  - /kaggle/working/inital_2017_catboost_ready.parquet\n  - /kaggle/working/inital_2017_catboost_ready.csv\n\nSample head (first 5 rows):\n   Dst Port  Protocol  Timestamp  Flow Duration  Tot Fwd Pkts  Tot Bwd Pkts  \\\n0         0         6          0      112740690            32            16   \n1         0         6          0      112740560            32            16   \n2         0         0          0      113757377           545             0   \n3         0        17          0         100126            22             0   \n4         0         0          0          54760             4             0   \n\n   TotLen Fwd Pkts  TotLen Bwd Pkts  \n0             6448             1152  \n1             6448             5056  \n2                0                0  \n3              616                0  \n4                0                0  \n\nLabel value counts (if present):\nLabel\nBenign    174601\nBot         1437\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":75},{"cell_type":"code","source":"import pandas as pd\n\npath = \"/kaggle/working/inital_2017_catboost_ready.csv\"\n\n# Load file\ndf = pd.read_csv(path)\n\n# Normalize text form\ndf['Label'] = df['Label'].astype(str).str.strip().str.lower()\n\n# Convert: benign -> 0, all other labels -> 1\ndf['Label'] = df['Label'].apply(lambda x: 0 if \"benign\" in x else 1)\n\n# Save back\ndf.to_csv(\"/kaggle/working/inital_2017_catboost_ready_binary.csv\", index=False)\n\nprint(\"‚úî Label column converted successfully!\")\nprint(df['Label'].value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T12:53:53.807010Z","iopub.execute_input":"2025-12-05T12:53:53.807962Z","iopub.status.idle":"2025-12-05T12:54:00.098882Z","shell.execute_reply.started":"2025-12-05T12:53:53.807933Z","shell.execute_reply":"2025-12-05T12:54:00.098124Z"}},"outputs":[{"name":"stdout","text":"‚úî Label column converted successfully!\nLabel\n0    174601\n1      1437\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":76},{"cell_type":"code","source":"sa=pd.read_csv(\"/kaggle/working/inital_2017_catboost_ready_binary.csv\")\nsa.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T12:56:22.307086Z","iopub.execute_input":"2025-12-05T12:56:22.307458Z","iopub.status.idle":"2025-12-05T12:56:23.256856Z","shell.execute_reply.started":"2025-12-05T12:56:22.307432Z","shell.execute_reply":"2025-12-05T12:56:23.256128Z"}},"outputs":[{"execution_count":79,"output_type":"execute_result","data":{"text/plain":"Index(['Dst Port', 'Protocol', 'Timestamp', 'Flow Duration', 'Tot Fwd Pkts',\n       'Tot Bwd Pkts', 'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max',\n       'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std',\n       'Bwd Pkt Len Max', 'Bwd Pkt Len Min', 'Bwd Pkt Len Mean',\n       'Bwd Pkt Len Std', 'Flow Byts/s', 'Flow Pkts/s', 'Flow IAT Mean',\n       'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Tot',\n       'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min',\n       'Bwd IAT Tot', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max',\n       'Bwd IAT Min', 'Fwd PSH Flags', 'Fwd URG Flags', 'Fwd Header Len',\n       'Bwd Header Len', 'Fwd Pkts/s', 'Bwd Pkts/s', 'Pkt Len Min',\n       'Pkt Len Max', 'Pkt Len Mean', 'Pkt Len Std', 'Pkt Len Var',\n       'FIN Flag Cnt', 'SYN Flag Cnt', 'RST Flag Cnt', 'PSH Flag Cnt',\n       'ACK Flag Cnt', 'URG Flag Cnt', 'CWE Flag Count', 'ECE Flag Cnt',\n       'Down/Up Ratio', 'Pkt Size Avg', 'Fwd Seg Size Avg', 'Bwd Seg Size Avg',\n       'Subflow Fwd Pkts', 'Subflow Fwd Byts', 'Subflow Bwd Pkts',\n       'Subflow Bwd Byts', 'Init Fwd Win Byts', 'Init Bwd Win Byts',\n       'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std',\n       'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max',\n       'Idle Min', 'Flow ID', 'Src IP', 'Src Port', 'Dst IP', 'Label'],\n      dtype='object')"},"metadata":{}}],"execution_count":79},{"cell_type":"code","source":"sa.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T12:56:32.508734Z","iopub.execute_input":"2025-12-05T12:56:32.509495Z","iopub.status.idle":"2025-12-05T12:56:32.515433Z","shell.execute_reply.started":"2025-12-05T12:56:32.509455Z","shell.execute_reply":"2025-12-05T12:56:32.514794Z"}},"outputs":[{"execution_count":80,"output_type":"execute_result","data":{"text/plain":"(176038, 76)"},"metadata":{}}],"execution_count":80},{"cell_type":"code","source":"# === Evaluate CatBoost model on provided CSV ===\nimport os, time, joblib, pickle\nfrom pathlib import Path\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, confusion_matrix, classification_report\n\nMODEL_PATH = \"/kaggle/working/catboost_ids2018_model.pkl\"\nTEST_CSV   = \"/kaggle/working/inital_2017_catboost_ready_binary.csv\"\nOUT_CSV    = \"/kaggle/working/inital_2017_catboost_ready_with_preds.csv\"\nPath(OUT_CSV).parent.mkdir(parents=True, exist_ok=True)\n\n# 1) Load test CSV\nif not os.path.exists(TEST_CSV):\n    raise FileNotFoundError(f\"Test CSV not found: {TEST_CSV}\")\nsa = pd.read_csv(TEST_CSV)\nprint(\"Loaded test file. Shape:\", sa.shape)\n\n# 2) Ensure Label exists and create y_true (binary)\nif 'Label' not in sa.columns:\n    raise ValueError(\"Test CSV must contain a 'Label' column.\")\n\n# Normalize common label variants to binary 0/1\ndef normalize_label_column(s):\n    s2 = s.astype(str).str.strip().str.lower()\n    benign_set = {\"benign\",\"begnin\",\"normal\",\"0\",\"no\",\"false\",\"n\",\"benign\\r\"}\n    return s2.apply(lambda x: 0 if any(k == x or k in x for k in benign_set) else 1).astype(int)\n\ntry:\n    # If already numeric 0/1, this keeps them\n    if set(sa['Label'].dropna().unique()) <= {0, 1}:\n        y_true = sa['Label'].astype(int).to_numpy()\n    else:\n        y_true = normalize_label_column(sa['Label']).to_numpy()\nexcept Exception:\n    y_true = normalize_label_column(sa['Label']).to_numpy()\n\n# 3) Load model (joblib then pickle)\nmodel = None\nlast_err = None\nfor loader in (joblib.load, pickle.load):\n    try:\n        model = loader(MODEL_PATH) if loader is joblib.load else loader(open(MODEL_PATH, 'rb'))\n        break\n    except Exception as e:\n        last_err = e\nif model is None:\n    raise RuntimeError(f\"Failed to load model. Last error: {last_err}\")\n\n# 4) Get model feature list\nif hasattr(model, \"feature_names_\"):\n    model_features = list(model.feature_names_)\nelif hasattr(model, \"feature_names\"):\n    model_features = list(model.feature_names)\nelse:\n    raise RuntimeError(\"Model does not expose feature names. Provide the model feature list.\")\n\nprint(\"Model expects\", len(model_features), \"features.\")\n\n# 5) Prepare X_test: drop Label, add missing features, drop extras, reorder\nX_df = sa.drop(columns=['Label']).copy()\n\nmissing = [c for c in model_features if c not in X_df.columns]\nextras  = [c for c in X_df.columns if c not in model_features]\n\nif missing:\n    print(\"Adding missing model features with sensible defaults:\", missing)\n    for c in missing:\n        if c in ('Src IP','Dst IP','Flow ID'):\n            X_df[c] = \"\"      # string placeholders\n        elif c in ('Src Port','Dst Port','Timestamp'):\n            X_df[c] = 0       # numeric placeholders\n        else:\n            X_df[c] = 0       # numeric default\n\nif extras:\n    print(\"Dropping extra columns not used by the model (examples):\", extras[:10])\n    X_df = X_df.drop(columns=extras)\n\n# Reorder exactly\nX_df = X_df[model_features].copy()\nprint(\"Prepared X_df shape:\", X_df.shape)\n\n# 6) Coerce numeric-like columns (preserve IP/FlowID as strings)\nnon_numeric_keep = {'Src IP','Dst IP','Flow ID'}\nfor c in X_df.columns:\n    if c in non_numeric_keep:\n        X_df[c] = X_df[c].astype(str).fillna(\"\")\n    else:\n        X_df[c] = pd.to_numeric(X_df[c], errors='coerce').fillna(0)\n\n# 7) Predict (time measured)\nt0 = time.time()\ntry:\n    y_pred_raw = model.predict(X_df)\nexcept Exception:\n    # try CatBoost Pool fallback\n    try:\n        from catboost import Pool\n        y_pred_raw = model.predict(Pool(X_df))\n    except Exception as e:\n        raise RuntimeError(f\"Prediction failed: {e}\")\nt1 = time.time()\n\ny_pred = np.array(y_pred_raw).squeeze()\n# if model returned probabilities or floats, convert to 0/1\nif y_pred.dtype.kind == 'f' and not set(np.unique(y_pred)).issubset({0.0,1.0}):\n    # rounding is a safe fallback\n    y_pred = np.rint(y_pred).astype(int)\nelse:\n    y_pred = y_pred.astype(int)\n\n# 8) Metrics\nacc   = accuracy_score(y_true, y_pred)\nf1_w  = f1_score(y_true, y_pred, average='weighted', zero_division=0)\nrec_w = recall_score(y_true, y_pred, average='weighted', zero_division=0)\nprec_w= precision_score(y_true, y_pred, average='weighted', zero_division=0)\n\n# 9) Print results (clean)\nprint(f\"\\nAccuracy: {acc:.12f}\")\nprint(f\"üìå F1 Score (weighted): {f1_w:.12f}\")\nprint(f\"üìå Recall (weighted): {rec_w:.12f}\")\nprint(f\"üìå Precision (weighted): {prec_w:.12f}\")\nprint(f\"‚è±Ô∏è Time Taken (predict only): {(t1-t0):.4f} seconds\\n\")\n\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_true, y_pred))\n\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_true, y_pred, digits=6))\n\n# 10) Save predictions\nsa_out = sa.copy()\nsa_out['pred_label'] = y_pred\ntry:\n    y_proba = model.predict_proba(X_df)\n    y_proba = np.array(y_proba)\n    if y_proba.ndim == 2 and y_proba.shape[1] == 2:\n        sa_out['pred_proba_pos'] = y_proba[:,1]\n    else:\n        sa_out['pred_proba'] = [list(map(float,row)) for row in y_proba]\nexcept Exception:\n    pass\n\nsa_out.to_csv(OUT_CSV, index=False)\nprint(f\"\\nSaved predictions to: {OUT_CSV}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T12:59:52.184853Z","iopub.execute_input":"2025-12-05T12:59:52.185616Z","iopub.status.idle":"2025-12-05T12:59:59.641115Z","shell.execute_reply.started":"2025-12-05T12:59:52.185576Z","shell.execute_reply":"2025-12-05T12:59:59.640213Z"}},"outputs":[{"name":"stdout","text":"Loaded test file. Shape: (176038, 76)\nModel expects 75 features.\nPrepared X_df shape: (176038, 75)\n\nAccuracy: 0.008163010259\nüìå F1 Score (weighted): 0.000132190401\nüìå Recall (weighted): 0.008163010259\nüìå Precision (weighted): 0.000066634736\n‚è±Ô∏è Time Taken (predict only): 0.2823 seconds\n\nConfusion Matrix:\n[[     0 174601]\n [     0   1437]]\n\nClassification Report:\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0   0.000000  0.000000  0.000000    174601\n           1   0.008163  1.000000  0.016194      1437\n\n    accuracy                       0.008163    176038\n   macro avg   0.004082  0.500000  0.008097    176038\nweighted avg   0.000067  0.008163  0.000132    176038\n\n\nSaved predictions to: /kaggle/working/inital_2017_catboost_ready_with_preds.csv\n","output_type":"stream"}],"execution_count":81},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\ndf = pd.read_csv(\"/kaggle/working/inital_2017_catboost_ready_binary.csv\")\n\nprint(\"Shape:\", df.shape)\n\n# Remove label\nfeatures = df.drop(columns=[\"Label\"])\n\n# -----------------------------\n# 1) Check for constant columns\n# -----------------------------\nconstant_cols = []\nzero_cols = []\nempty_cols = []\n\nfor col in features.columns:\n    unique_vals = features[col].dropna().unique()\n    if len(unique_vals) == 1:\n        constant_cols.append(col)\n    if pd.to_numeric(features[col], errors='coerce').fillna(0).eq(0).all():\n        zero_cols.append(col)\n    if features[col].astype(str).str.strip().eq(\"\").all():\n        empty_cols.append(col)\n\nprint(\"\\nüîç CONSTANT columns (single unique value):\")\nprint(constant_cols)\n\nprint(\"\\nüîç ZERO columns (all numeric zeros):\")\nprint(zero_cols)\n\nprint(\"\\nüîç EMPTY STRING columns (possibly IP/FlowID placeholders):\")\nprint(empty_cols)\n\n# --------------------------------\n# 2) Summary statistics (numerical)\n# --------------------------------\ndesc = features.describe(include='all').transpose()\ndesc.to_csv(\"/kaggle/working/feature_debug_stats.csv\")\n\nprint(\"\\nüìÅ Saved numeric summary to: feature_debug_stats.csv\")\n\n# --------------------------------\n# 3) Show first row to verify actual values\n# --------------------------------\nprint(\"\\nüîç First row of data:\")\nprint(features.head(1).transpose())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T13:00:56.427214Z","iopub.execute_input":"2025-12-05T13:00:56.427562Z","iopub.status.idle":"2025-12-05T13:01:05.312629Z","shell.execute_reply.started":"2025-12-05T13:00:56.427537Z","shell.execute_reply":"2025-12-05T13:01:05.311735Z"}},"outputs":[{"name":"stdout","text":"Shape: (176038, 76)\n\nüîç CONSTANT columns (single unique value):\n['Dst Port', 'Timestamp', 'Fwd URG Flags', 'CWE Flag Count', 'Src Port']\n\nüîç ZERO columns (all numeric zeros):\n['Dst Port', 'Timestamp', 'Fwd URG Flags', 'CWE Flag Count', 'Flow ID', 'Src IP', 'Src Port', 'Dst IP']\n\nüîç EMPTY STRING columns (possibly IP/FlowID placeholders):\n[]\n\nüìÅ Saved numeric summary to: feature_debug_stats.csv\n\nüîç First row of data:\n                         0\nDst Port               0.0\nProtocol               6.0\nTimestamp              0.0\nFlow Duration  112740690.0\nTot Fwd Pkts          32.0\n...                    ...\nIdle Min        15400000.0\nFlow ID                NaN\nSrc IP                 NaN\nSrc Port               0.0\nDst IP                 NaN\n\n[75 rows x 1 columns]\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n","output_type":"stream"}],"execution_count":82},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom pathlib import Path\n\npath = \"/kaggle/working/inital_2017_catboost_ready_binary.csv\"\ndf = pd.read_csv(path)\n\nprint(\"Loaded:\", df.shape)\n\n# ------------------------------\n# FIX 1 ‚Äî Dst Port\n# ------------------------------\nnp.random.seed(42)\n\ncommon_ports = [80, 443, 53, 22]\nn = len(df)\n\ndf[\"Dst Port\"] = np.where(\n    np.random.rand(n) < 0.80,              # 80% common ports\n    np.random.choice(common_ports, size=n),\n    np.random.randint(1024, 65535, size=n) # 20% random ephemeral\n)\n\n# ------------------------------\n# FIX 2 ‚Äî Src Port\n# ------------------------------\ndf[\"Src Port\"] = np.random.randint(1024, 65535, size=n)\n\n# ------------------------------\n# FIX 3 ‚Äî Timestamp (synthetic)\n# ------------------------------\navg_flow = df[\"Flow Duration\"].mean()\ndf[\"Timestamp\"] = np.arange(n) * avg_flow * 0.0001\n\n# ------------------------------\n# FIX 4 ‚Äî Synthetic IPs\n# ------------------------------\n\ndef random_ip_block(prefix):\n    return prefix + \".\" + str(np.random.randint(1, 255)) + \".\" + str(np.random.randint(1, 255))\n\ndf[\"Src IP\"] = [\n    random_ip_block(\"192.168\") for _ in range(n)\n]\n\ndf[\"Dst IP\"] = [\n    random_ip_block(\"10.0\") for _ in range(n)\n]\n\n# ------------------------------\n# FIX 5 ‚Äî Fwd URG Flags, CWE Flag Count\n# ------------------------------\ndf[\"Fwd URG Flags\"] = 0      # correct for 99% flows\ndf[\"CWE Flag Count\"] = 0     # normal for 2018 training data\n\n# ------------------------------\n# FIX 6 ‚Äî Flow ID reconstruction\n# ------------------------------\ndf[\"Flow ID\"] = (\n    df[\"Src IP\"].astype(str) + \"-\" +\n    df[\"Dst IP\"].astype(str) + \"-\" +\n    df[\"Protocol\"].astype(str) + \"-\" +\n    df[\"Src Port\"].astype(str) + \"-\" +\n    df[\"Dst Port\"].astype(str)\n)\n\n# ------------------------------\n# Save cleaned file\n# ------------------------------\nout = \"/kaggle/working/inital_2017_catboost_fixed.csv\"\ndf.to_csv(out, index=False)\n\nprint(\"\\n‚úî FIXED dataset saved:\", out)\nprint(\"New shape:\", df.shape)\nprint(\"\\nLabel counts:\")\nprint(df[\"Label\"].value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T13:03:43.088281Z","iopub.execute_input":"2025-12-05T13:03:43.089090Z","iopub.status.idle":"2025-12-05T13:03:51.988375Z","shell.execute_reply.started":"2025-12-05T13:03:43.089051Z","shell.execute_reply":"2025-12-05T13:03:51.987513Z"}},"outputs":[{"name":"stdout","text":"Loaded: (176038, 76)\n\n‚úî FIXED dataset saved: /kaggle/working/inital_2017_catboost_fixed.csv\nNew shape: (176038, 76)\n\nLabel counts:\nLabel\n0    174601\n1      1437\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":83},{"cell_type":"code","source":"# === Evaluate CatBoost model on provided CSV ===\nimport os, time, joblib, pickle\nfrom pathlib import Path\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, confusion_matrix, classification_report\n\nMODEL_PATH = \"/kaggle/working/catboost_ids2018_model.pkl\"\nTEST_CSV   = \"/kaggle/working/inital_2017_catboost_fixed.csv\"\nOUT_CSV    = \"/kaggle/working/inital_2017_catboost_ready_with_preds.csv\"\nPath(OUT_CSV).parent.mkdir(parents=True, exist_ok=True)\n\n# 1) Load test CSV\nif not os.path.exists(TEST_CSV):\n    raise FileNotFoundError(f\"Test CSV not found: {TEST_CSV}\")\nsa = pd.read_csv(TEST_CSV)\nprint(\"Loaded test file. Shape:\", sa.shape)\n\n# 2) Ensure Label exists and create y_true (binary)\nif 'Label' not in sa.columns:\n    raise ValueError(\"Test CSV must contain a 'Label' column.\")\n\n# Normalize common label variants to binary 0/1\ndef normalize_label_column(s):\n    s2 = s.astype(str).str.strip().str.lower()\n    benign_set = {\"benign\",\"begnin\",\"normal\",\"0\",\"no\",\"false\",\"n\",\"benign\\r\"}\n    return s2.apply(lambda x: 0 if any(k == x or k in x for k in benign_set) else 1).astype(int)\n\ntry:\n    # If already numeric 0/1, this keeps them\n    if set(sa['Label'].dropna().unique()) <= {0, 1}:\n        y_true = sa['Label'].astype(int).to_numpy()\n    else:\n        y_true = normalize_label_column(sa['Label']).to_numpy()\nexcept Exception:\n    y_true = normalize_label_column(sa['Label']).to_numpy()\n\n# 3) Load model (joblib then pickle)\nmodel = None\nlast_err = None\nfor loader in (joblib.load, pickle.load):\n    try:\n        model = loader(MODEL_PATH) if loader is joblib.load else loader(open(MODEL_PATH, 'rb'))\n        break\n    except Exception as e:\n        last_err = e\nif model is None:\n    raise RuntimeError(f\"Failed to load model. Last error: {last_err}\")\n\n# 4) Get model feature list\nif hasattr(model, \"feature_names_\"):\n    model_features = list(model.feature_names_)\nelif hasattr(model, \"feature_names\"):\n    model_features = list(model.feature_names)\nelse:\n    raise RuntimeError(\"Model does not expose feature names. Provide the model feature list.\")\n\nprint(\"Model expects\", len(model_features), \"features.\")\n\n# 5) Prepare X_test: drop Label, add missing features, drop extras, reorder\nX_df = sa.drop(columns=['Label']).copy()\n\nmissing = [c for c in model_features if c not in X_df.columns]\nextras  = [c for c in X_df.columns if c not in model_features]\n\nif missing:\n    print(\"Adding missing model features with sensible defaults:\", missing)\n    for c in missing:\n        if c in ('Src IP','Dst IP','Flow ID'):\n            X_df[c] = \"\"      # string placeholders\n        elif c in ('Src Port','Dst Port','Timestamp'):\n            X_df[c] = 0       # numeric placeholders\n        else:\n            X_df[c] = 0       # numeric default\n\nif extras:\n    print(\"Dropping extra columns not used by the model (examples):\", extras[:10])\n    X_df = X_df.drop(columns=extras)\n\n# Reorder exactly\nX_df = X_df[model_features].copy()\nprint(\"Prepared X_df shape:\", X_df.shape)\n\n# 6) Coerce numeric-like columns (preserve IP/FlowID as strings)\nnon_numeric_keep = {'Src IP','Dst IP','Flow ID'}\nfor c in X_df.columns:\n    if c in non_numeric_keep:\n        X_df[c] = X_df[c].astype(str).fillna(\"\")\n    else:\n        X_df[c] = pd.to_numeric(X_df[c], errors='coerce').fillna(0)\n\n# 7) Predict (time measured)\nt0 = time.time()\ntry:\n    y_pred_raw = model.predict(X_df)\nexcept Exception:\n    # try CatBoost Pool fallback\n    try:\n        from catboost import Pool\n        y_pred_raw = model.predict(Pool(X_df))\n    except Exception as e:\n        raise RuntimeError(f\"Prediction failed: {e}\")\nt1 = time.time()\n\ny_pred = np.array(y_pred_raw).squeeze()\n# if model returned probabilities or floats, convert to 0/1\nif y_pred.dtype.kind == 'f' and not set(np.unique(y_pred)).issubset({0.0,1.0}):\n    # rounding is a safe fallback\n    y_pred = np.rint(y_pred).astype(int)\nelse:\n    y_pred = y_pred.astype(int)\n\n# 8) Metrics\nacc   = accuracy_score(y_true, y_pred)\nf1_w  = f1_score(y_true, y_pred, average='weighted', zero_division=0)\nrec_w = recall_score(y_true, y_pred, average='weighted', zero_division=0)\nprec_w= precision_score(y_true, y_pred, average='weighted', zero_division=0)\n\n# 9) Print results (clean)\nprint(f\"\\nAccuracy: {acc:.12f}\")\nprint(f\"üìå F1 Score (weighted): {f1_w:.12f}\")\nprint(f\"üìå Recall (weighted): {rec_w:.12f}\")\nprint(f\"üìå Precision (weighted): {prec_w:.12f}\")\nprint(f\"‚è±Ô∏è Time Taken (predict only): {(t1-t0):.4f} seconds\\n\")\n\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_true, y_pred))\n\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_true, y_pred, digits=6))\n\n# 10) Save predictions\nsa_out = sa.copy()\nsa_out['pred_label'] = y_pred\ntry:\n    y_proba = model.predict_proba(X_df)\n    y_proba = np.array(y_proba)\n    if y_proba.ndim == 2 and y_proba.shape[1] == 2:\n        sa_out['pred_proba_pos'] = y_proba[:,1]\n    else:\n        sa_out['pred_proba'] = [list(map(float,row)) for row in y_proba]\nexcept Exception:\n    pass\n\nsa_out.to_csv(OUT_CSV, index=False)\nprint(f\"\\nSaved predictions to: {OUT_CSV}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T13:04:37.825886Z","iopub.execute_input":"2025-12-05T13:04:37.826380Z","iopub.status.idle":"2025-12-05T13:04:39.334882Z","shell.execute_reply.started":"2025-12-05T13:04:37.826357Z","shell.execute_reply":"2025-12-05T13:04:39.333811Z"}},"outputs":[{"name":"stdout","text":"Loaded test file. Shape: (176038, 76)\nModel expects 75 features.\nPrepared X_df shape: (176038, 75)\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost.get_float_feature\u001b[0;34m()\u001b[0m\n","\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._FloatOrNan\u001b[0;34m()\u001b[0m\n","\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._FloatOrNanFromString\u001b[0;34m()\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Cannot convert '192.168.25.13-10.0.232.160-6-61153-443' to float","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mCatBoostError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/609929768.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0my_pred_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, prediction_type, ntree_start, ntree_end, thread_count, verbose, task_type)\u001b[0m\n\u001b[1;32m   5306\u001b[0m         \"\"\"\n\u001b[0;32m-> 5307\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntree_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntree_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthread_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'predict'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36m_predict\u001b[0;34m(self, data, prediction_type, ntree_start, ntree_end, thread_count, verbose, parent_method_name, task_type)\u001b[0m\n\u001b[1;32m   2619\u001b[0m             \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2620\u001b[0;31m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_is_single_object\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_predict_input_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent_method_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthread_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2621\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_prediction_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36m_process_predict_input_data\u001b[0;34m(self, data, parent_method_name, thread_count, label)\u001b[0m\n\u001b[1;32m   2599\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2600\u001b[0;31m             data = Pool(\n\u001b[0m\u001b[1;32m   2601\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_single_object\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, label, cat_features, text_features, embedding_features, embedding_features_data, column_description, pairs, graph, delimiter, has_header, ignore_csv_quoting, weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, timestamp, feature_names, feature_tags, thread_count, log_cout, log_cerr, data_can_be_none)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m                     self._init(data, label, cat_features, text_features, embedding_features, embedding_features_data, pairs, graph, weight,\n\u001b[0m\u001b[1;32m    856\u001b[0m                                group_id, group_weight, subgroup_id, pairs_weight, baseline, timestamp, feature_names, feature_tags, thread_count)\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36m_init\u001b[0;34m(self, data, label, cat_features, text_features, embedding_features, embedding_features_data, pairs, graph, weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, timestamp, feature_names, feature_tags, thread_count)\u001b[0m\n\u001b[1;32m   1490\u001b[0m             \u001b[0mfeature_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_transform_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_tags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1491\u001b[0;31m         self._init_pool(data, label, cat_features, text_features, embedding_features, embedding_features_data, pairs, graph, weight,\n\u001b[0m\u001b[1;32m   1492\u001b[0m                         group_id, group_weight, subgroup_id, pairs_weight, baseline, timestamp, feature_names, feature_tags, thread_count)\n","\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._PoolBase._init_pool\u001b[0;34m()\u001b[0m\n","\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._PoolBase._init_pool\u001b[0;34m()\u001b[0m\n","\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._PoolBase._init_features_order_layout_pool\u001b[0;34m()\u001b[0m\n","\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._set_features_order_data_pd_data_frame\u001b[0;34m()\u001b[0m\n","\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost.create_num_factor_data\u001b[0;34m()\u001b[0m\n","\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost.get_float_feature\u001b[0;34m()\u001b[0m\n","\u001b[0;31mCatBoostError\u001b[0m: Bad value for num_feature[non_default_doc_idx=0,feature_idx=71]=\"192.168.25.13-10.0.232.160-6-61153-443\": Cannot convert '192.168.25.13-10.0.232.160-6-61153-443' to float","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost.get_float_feature\u001b[0;34m()\u001b[0m\n","\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._FloatOrNan\u001b[0;34m()\u001b[0m\n","\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._FloatOrNanFromString\u001b[0;34m()\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Cannot convert '192.168.25.13-10.0.232.160-6-61153-443' to float","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mCatBoostError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/609929768.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mcatboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0my_pred_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, label, cat_features, text_features, embedding_features, embedding_features_data, column_description, pairs, graph, delimiter, has_header, ignore_csv_quoting, weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, timestamp, feature_names, feature_tags, thread_count, log_cout, log_cerr, data_can_be_none)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m                     self._init(data, label, cat_features, text_features, embedding_features, embedding_features_data, pairs, graph, weight,\n\u001b[0m\u001b[1;32m    856\u001b[0m                                group_id, group_weight, subgroup_id, pairs_weight, baseline, timestamp, feature_names, feature_tags, thread_count)\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36m_init\u001b[0;34m(self, data, label, cat_features, text_features, embedding_features, embedding_features_data, pairs, graph, weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, timestamp, feature_names, feature_tags, thread_count)\u001b[0m\n\u001b[1;32m   1490\u001b[0m             \u001b[0mfeature_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_transform_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_tags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1491\u001b[0;31m         self._init_pool(data, label, cat_features, text_features, embedding_features, embedding_features_data, pairs, graph, weight,\n\u001b[0m\u001b[1;32m   1492\u001b[0m                         group_id, group_weight, subgroup_id, pairs_weight, baseline, timestamp, feature_names, feature_tags, thread_count)\n","\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._PoolBase._init_pool\u001b[0;34m()\u001b[0m\n","\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._PoolBase._init_pool\u001b[0;34m()\u001b[0m\n","\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._PoolBase._init_features_order_layout_pool\u001b[0;34m()\u001b[0m\n","\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._set_features_order_data_pd_data_frame\u001b[0;34m()\u001b[0m\n","\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost.create_num_factor_data\u001b[0;34m()\u001b[0m\n","\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost.get_float_feature\u001b[0;34m()\u001b[0m\n","\u001b[0;31mCatBoostError\u001b[0m: Bad value for num_feature[non_default_doc_idx=0,feature_idx=71]=\"192.168.25.13-10.0.232.160-6-61153-443\": Cannot convert '192.168.25.13-10.0.232.160-6-61153-443' to float","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/609929768.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0my_pred_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Prediction failed: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Prediction failed: Bad value for num_feature[non_default_doc_idx=0,feature_idx=71]=\"192.168.25.13-10.0.232.160-6-61153-443\": Cannot convert '192.168.25.13-10.0.232.160-6-61153-443' to float"],"ename":"RuntimeError","evalue":"Prediction failed: Bad value for num_feature[non_default_doc_idx=0,feature_idx=71]=\"192.168.25.13-10.0.232.160-6-61153-443\": Cannot convert '192.168.25.13-10.0.232.160-6-61153-443' to float","output_type":"error"}],"execution_count":84},{"cell_type":"code","source":"rr=pd.read_csv(\"/kaggle/working/2019_cleaned_combined_binary.csv\")\nrr.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T13:08:29.738114Z","iopub.execute_input":"2025-12-05T13:08:29.738679Z","iopub.status.idle":"2025-12-05T13:08:30.002329Z","shell.execute_reply.started":"2025-12-05T13:08:29.738658Z","shell.execute_reply":"2025-12-05T13:08:30.001504Z"}},"outputs":[{"execution_count":88,"output_type":"execute_result","data":{"text/plain":"Index(['Dst Port', 'Timestamp', 'Flow Duration', 'Fwd Pkt Len Min',\n       'Fwd Pkt Len Mean', 'Bwd Pkt Len Max', 'Bwd Pkt Len Min',\n       'Bwd Pkt Len Std', 'Flow Pkts/s', 'Fwd IAT Std', 'Fwd Header Len',\n       'Fwd Pkts/s', 'Pkt Len Mean', 'Pkt Len Var', 'SYN Flag Cnt',\n       'CWE Flag Count', 'ECE Flag Cnt', 'Down/Up Ratio', 'Subflow Bwd Byts',\n       'Init Bwd Win Byts', 'Fwd Seg Size Min', 'Idle Mean', 'Idle Std',\n       'Idle Min', 'Src IP', 'Dst IP', 'Label'],\n      dtype='object')"},"metadata":{}}],"execution_count":88},{"cell_type":"code","source":"rr.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}