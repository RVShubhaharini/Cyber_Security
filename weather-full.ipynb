{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1733506,"sourceType":"datasetVersion","datasetId":6012,"isSourceIdPinned":false}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import kagglehub\n\npath = kagglehub.dataset_download(\"jsphyg/weather-dataset-rattle-package\")\nprint(\"Path to dataset files:\", path)\ndf = pd.read_csv(\"/kaggle/input/weather-dataset-rattle-package/weatherAUS.csv\")\nprint(df.columns)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv(\"/kaggle/input/weather-dataset-rattle-package/weatherAUS.csv\")\nprint(df.columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T08:31:13.022338Z","iopub.execute_input":"2025-12-14T08:31:13.022608Z","iopub.status.idle":"2025-12-14T08:31:15.978495Z","shell.execute_reply.started":"2025-12-14T08:31:13.022584Z","shell.execute_reply":"2025-12-14T08:31:15.977272Z"}},"outputs":[{"name":"stdout","text":"Index(['Date', 'Location', 'MinTemp', 'MaxTemp', 'Rainfall', 'Evaporation',\n       'Sunshine', 'WindGustDir', 'WindGustSpeed', 'WindDir9am', 'WindDir3pm',\n       'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am', 'Humidity3pm',\n       'Pressure9am', 'Pressure3pm', 'Cloud9am', 'Cloud3pm', 'Temp9am',\n       'Temp3pm', 'RainToday', 'RainTomorrow'],\n      dtype='object')\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T08:31:20.763331Z","iopub.execute_input":"2025-12-14T08:31:20.764117Z","iopub.status.idle":"2025-12-14T08:31:20.772667Z","shell.execute_reply.started":"2025-12-14T08:31:20.764080Z","shell.execute_reply":"2025-12-14T08:31:20.770998Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"(145460, 23)"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"# This gives a summary of columns, counts, and data types\ndf.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T08:31:48.574121Z","iopub.execute_input":"2025-12-14T08:31:48.574719Z","iopub.status.idle":"2025-12-14T08:31:48.662843Z","shell.execute_reply.started":"2025-12-14T08:31:48.574688Z","shell.execute_reply":"2025-12-14T08:31:48.661865Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 145460 entries, 0 to 145459\nData columns (total 23 columns):\n #   Column         Non-Null Count   Dtype  \n---  ------         --------------   -----  \n 0   Date           145460 non-null  object \n 1   Location       145460 non-null  object \n 2   MinTemp        143975 non-null  float64\n 3   MaxTemp        144199 non-null  float64\n 4   Rainfall       142199 non-null  float64\n 5   Evaporation    82670 non-null   float64\n 6   Sunshine       75625 non-null   float64\n 7   WindGustDir    135134 non-null  object \n 8   WindGustSpeed  135197 non-null  float64\n 9   WindDir9am     134894 non-null  object \n 10  WindDir3pm     141232 non-null  object \n 11  WindSpeed9am   143693 non-null  float64\n 12  WindSpeed3pm   142398 non-null  float64\n 13  Humidity9am    142806 non-null  float64\n 14  Humidity3pm    140953 non-null  float64\n 15  Pressure9am    130395 non-null  float64\n 16  Pressure3pm    130432 non-null  float64\n 17  Cloud9am       89572 non-null   float64\n 18  Cloud3pm       86102 non-null   float64\n 19  Temp9am        143693 non-null  float64\n 20  Temp3pm        141851 non-null  float64\n 21  RainToday      142199 non-null  object \n 22  RainTomorrow   142193 non-null  object \ndtypes: float64(16), object(7)\nmemory usage: 25.5+ MB\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# This lists every unique label and how many times it appears\nprint(df['RainTomorrow'].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T08:32:23.954728Z","iopub.execute_input":"2025-12-14T08:32:23.955559Z","iopub.status.idle":"2025-12-14T08:32:23.972032Z","shell.execute_reply.started":"2025-12-14T08:32:23.955522Z","shell.execute_reply":"2025-12-14T08:32:23.971095Z"}},"outputs":[{"name":"stdout","text":"RainTomorrow\nNo     110316\nYes     31877\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import pandas as pd\nimport os\nimport kagglehub\n\n# Download dataset\npath = kagglehub.dataset_download(\"jsphyg/weather-dataset-rattle-package\")\nprint(\"Path to dataset files:\", path)\n\n# Find CSV file\nfiles = [f for f in os.listdir(path) if f.endswith(\".csv\")]\nif not files:\n    raise FileNotFoundError(\"No CSV file found in downloaded dataset.\")\ncsv_path = os.path.join(path, files[0])\n\ndf = pd.read_csv(csv_path)\nprint(\"Loaded shape:\", df.shape)\n\n# Ensure column exists\nif \"RainTomorrow\" not in df.columns:\n    raise ValueError(\"RainTomorrow column not found. Columns:\\n\" + str(df.columns))\n\n# Remove rows with missing target values\ndf = df.dropna(subset=[\"RainTomorrow\"]).reset_index(drop=True)\n\n# Count classes\nclass_counts = df[\"RainTomorrow\"].value_counts()\nprint(\"\\nClass distribution:\")\nprint(class_counts)\n\n# Determine minority count\nminority_count = class_counts.min()\n\n# Downsample both Yes and No to smallest count\ndf_yes = df[df[\"RainTomorrow\"] == \"Yes\"].sample(minority_count, random_state=42)\ndf_no  = df[df[\"RainTomorrow\"] == \"No\"].sample(minority_count, random_state=42)\n\n# Create balanced dataset\ndf_balanced = pd.concat([df_yes, df_no], ignore_index=True).sample(frac=1, random_state=42)\n\nprint(\"\\nBalanced dataset shape:\", df_balanced.shape)\nprint(df_balanced[\"RainTomorrow\"].value_counts())\n\n# Save new balanced dataset\noutput_path = \"/kaggle/working/weather_balanced.csv\"\ndf_balanced.to_csv(output_path, index=False)\n\nprint(\"\\n‚úÖ Balanced dataset saved to:\", output_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T10:12:24.687911Z","iopub.execute_input":"2025-12-09T10:12:24.688556Z","iopub.status.idle":"2025-12-09T10:12:25.811796Z","shell.execute_reply.started":"2025-12-09T10:12:24.688530Z","shell.execute_reply":"2025-12-09T10:12:25.811183Z"}},"outputs":[{"name":"stdout","text":"Path to dataset files: /kaggle/input/weather-dataset-rattle-package\nLoaded shape: (145460, 23)\n\nClass distribution:\nRainTomorrow\nNo     110316\nYes     31877\nName: count, dtype: int64\n\nBalanced dataset shape: (63754, 23)\nRainTomorrow\nYes    31877\nNo     31877\nName: count, dtype: int64\n\n‚úÖ Balanced dataset saved to: /kaggle/working/weather_balanced.csv\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import pandas as pd\nimport os\nimport kagglehub\n\n\nprint(\"Loaded shape:\", df.shape)\n\n# Ensure column exists\nif \"RainTomorrow\" not in df.columns:\n    raise ValueError(\"RainTomorrow column not found. Columns:\\n\" + str(df.columns))\n\n# Remove rows with missing target values\ndf = df.dropna(subset=[\"RainTomorrow\"]).reset_index(drop=True)\n\n# Count classes\nclass_counts = df[\"RainTomorrow\"].value_counts()\nprint(\"\\nClass distribution:\")\nprint(class_counts)\n\n# Determine minority count\nminority_count = class_counts.min()\n\n# Downsample both Yes and No to smallest count\ndf_yes = df[df[\"RainTomorrow\"] == \"Yes\"].sample(minority_count, random_state=42)\ndf_no  = df[df[\"RainTomorrow\"] == \"No\"].sample(minority_count, random_state=42)\n\n# Create balanced dataset\ndf_balanced = pd.concat([df_yes, df_no], ignore_index=True).sample(frac=1, random_state=42)\n\nprint(\"\\nBalanced dataset shape:\", df_balanced.shape)\nprint(df_balanced[\"RainTomorrow\"].value_counts())\n\n# Save new balanced dataset\noutput_path = \"/kaggle/working/weather_balanced.csv\"\ndf_balanced.to_csv(output_path, index=False)\n\nprint(\"\\n‚úÖ Balanced dataset saved to:\", output_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T14:08:04.786143Z","iopub.execute_input":"2025-12-10T14:08:04.786449Z","iopub.status.idle":"2025-12-10T14:08:06.214696Z","shell.execute_reply.started":"2025-12-10T14:08:04.786427Z","shell.execute_reply":"2025-12-10T14:08:06.213641Z"}},"outputs":[{"name":"stdout","text":"Loaded shape: (145460, 23)\n\nClass distribution:\nRainTomorrow\nNo     110316\nYes     31877\nName: count, dtype: int64\n\nBalanced dataset shape: (63754, 23)\nRainTomorrow\nYes    31877\nNo     31877\nName: count, dtype: int64\n\n‚úÖ Balanced dataset saved to: /kaggle/working/weather_balanced.csv\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T14:36:55.045574Z","iopub.execute_input":"2025-12-10T14:36:55.045918Z","iopub.status.idle":"2025-12-10T14:36:55.054277Z","shell.execute_reply.started":"2025-12-10T14:36:55.045891Z","shell.execute_reply":"2025-12-10T14:36:55.053094Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"Index(['Date', 'Location', 'MinTemp', 'MaxTemp', 'Rainfall', 'Evaporation',\n       'Sunshine', 'WindGustDir', 'WindGustSpeed', 'WindDir9am', 'WindDir3pm',\n       'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am', 'Humidity3pm',\n       'Pressure9am', 'Pressure3pm', 'Cloud9am', 'Cloud3pm', 'Temp9am',\n       'Temp3pm', 'RainToday', 'RainTomorrow'],\n      dtype='object')"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"df[\"RainTomorrow\"].unique()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T14:37:32.745300Z","iopub.execute_input":"2025-12-10T14:37:32.746014Z","iopub.status.idle":"2025-12-10T14:37:32.754317Z","shell.execute_reply.started":"2025-12-10T14:37:32.745983Z","shell.execute_reply":"2025-12-10T14:37:32.753259Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"array([1, 0])"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\n\n# ================================================================\n# 1. LOAD BALANCED WEATHER DATASET\n# ================================================================\ninput_path = \"/kaggle/working/weather_balanced.csv\"  # <-- change if needed\ndf = pd.read_csv(input_path, low_memory=False)\n\nprint(\"Initial shape:\", df.shape)\nprint(\"Initial dtypes (sample):\\n\", df.dtypes.head(20))\n\n\n# ================================================================\n# 2. BASIC CLEANING\n# ================================================================\ndf = df.dropna(axis=1, how=\"all\")                      # remove fully-empty columns\ndf = df.loc[:, (df != 0).any(axis=0)]                 # remove all-zero columns\ndf = df.drop_duplicates().reset_index(drop=True)      # remove exact duplicates\n\n\n# ================================================================\n# 3. ROBUST NUMERIC COLUMN DETECTION\n# ================================================================\nnumeric_candidates = []\nconversion_stats = {}\n\nfor col in df.columns:\n    coerced = pd.to_numeric(df[col], errors=\"coerce\")\n    non_na_ratio = coerced.notna().sum() / len(coerced)\n    conversion_stats[col] = non_na_ratio\n\n    if non_na_ratio >= 0.80:  # at least 80% numeric-like\n        numeric_candidates.append(col)\n\nprint(f\"Detected {len(numeric_candidates)} numeric-like columns.\")\n\n\n# Convert numeric candidates to numeric dtype\nfor col in numeric_candidates:\n    df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n\n\n# ================================================================\n# 4. HANDLE INF + EXTREME VALUES\n# ================================================================\ninf_cols = [c for c in numeric_candidates if np.isinf(df[c].to_numpy()).any()]\nprint(\"Columns with ¬±inf:\", inf_cols)\n\nif inf_cols:\n    df[numeric_candidates] = df[numeric_candidates].replace([np.inf, -np.inf], np.nan)\n\nhuge_cols = []\nfor col in numeric_candidates:\n    try:\n        max_abs = np.nanmax(np.abs(df[col].to_numpy()))\n        if np.isfinite(max_abs) and max_abs > 1e300:\n            huge_cols.append((col, max_abs))\n    except:\n        pass\n\nprint(\"Columns with extremely large values (>1e300):\", huge_cols)\n\nCLIP_LIMIT = 1e300\ndf[numeric_candidates] = df[numeric_candidates].apply(\n    lambda s: s.clip(lower=-CLIP_LIMIT, upper=CLIP_LIMIT)\n)\n\n\n# ================================================================\n# 5. RECOMPUTE NUMERIC + CATEGORICAL COLUMNS\n# ================================================================\nnum_cols = df.select_dtypes(include=[\"float64\", \"int64\"]).columns.tolist()\ncat_cols = df.select_dtypes(include=[\"object\"]).columns.tolist()\n\nprint(\"Final numeric columns:\", len(num_cols))\nprint(\"Final categorical columns:\", len(cat_cols))\n\n\n# ================================================================\n# 6. HANDLE MISSING VALUES\n# ================================================================\nif len(num_cols) > 0:\n    df[num_cols] = df[num_cols].fillna(df[num_cols].median())\n\nfor col in cat_cols:\n    if df[col].isna().any():\n        mode_val = df[col].mode(dropna=True)\n        df[col] = df[col].fillna(mode_val.iloc[0] if len(mode_val) else \"\")\n\n\n# ================================================================\n# 7. LABEL-ENCODE CATEGORICAL COLUMNS\n# ================================================================\nle = LabelEncoder()\nfor col in cat_cols:\n    df[col] = le.fit_transform(df[col].astype(str))\n\n\n# ================================================================\n# 8. FINAL CHECK BEFORE SCALING\n# ================================================================\ndf[num_cols] = df[num_cols].replace([np.inf, -np.inf], np.nan)\ndf[num_cols] = df[num_cols].fillna(df[num_cols].median())\n\nfinite_check = {c: np.isfinite(df[c].to_numpy()).all() for c in num_cols}\nbad_cols = [c for c, ok in finite_check.items() if not ok]\nprint(\"Non-finite numeric columns (should be empty):\", bad_cols)\n\n\n# ================================================================\n# 9. MIN-MAX SCALING\n# ================================================================\nscaler = MinMaxScaler()\nif len(num_cols) > 0:\n    df[num_cols] = scaler.fit_transform(df[num_cols])\n\n\n# ================================================================\n# 10. SAVE CLEANED DATASET\n# ================================================================\noutput_path = \"/kaggle/working/weather_balanced_cleaned.csv\"\ndf.to_csv(output_path, index=False)\n\nprint(\"\\n‚úÖ PREPROCESSING COMPLETE!\")\nprint(\"üìÅ Saved cleaned dataset as:\", output_path)\nprint(\"Final shape:\", df.shape)\n\n# Display label distribution (Yes/No)\nif \"RainTomorrow\" in df.columns:\n    print(\"\\nLabel distribution after cleaning:\")\n    print(df[\"RainTomorrow\"].value_counts())\nelse:\n    print(\"‚ö†Ô∏è No 'RainTomorrow' column found after preprocessing.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T14:08:20.391345Z","iopub.execute_input":"2025-12-10T14:08:20.391692Z","iopub.status.idle":"2025-12-10T14:08:23.663353Z","shell.execute_reply.started":"2025-12-10T14:08:20.391667Z","shell.execute_reply":"2025-12-10T14:08:23.662423Z"}},"outputs":[{"name":"stdout","text":"Initial shape: (63754, 23)\nInitial dtypes (sample):\n Date              object\nLocation          object\nMinTemp          float64\nMaxTemp          float64\nRainfall         float64\nEvaporation      float64\nSunshine         float64\nWindGustDir       object\nWindGustSpeed    float64\nWindDir9am        object\nWindDir3pm        object\nWindSpeed9am     float64\nWindSpeed3pm     float64\nHumidity9am      float64\nHumidity3pm      float64\nPressure9am      float64\nPressure3pm      float64\nCloud9am         float64\nCloud3pm         float64\nTemp9am          float64\ndtype: object\nDetected 12 numeric-like columns.\nColumns with ¬±inf: []\nColumns with extremely large values (>1e300): []\nFinal numeric columns: 16\nFinal categorical columns: 7\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less_equal\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less_equal\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less_equal\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less_equal\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less_equal\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less_equal\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less_equal\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less_equal\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less_equal\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less_equal\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less_equal\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less_equal\n  return op(a, b)\n","output_type":"stream"},{"name":"stdout","text":"Non-finite numeric columns (should be empty): []\n\n‚úÖ PREPROCESSING COMPLETE!\nüìÅ Saved cleaned dataset as: /kaggle/working/weather_balanced_cleaned.csv\nFinal shape: (63754, 23)\n\nLabel distribution after cleaning:\nRainTomorrow\n1    31877\n0    31877\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import pandas as pd\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n# -----------------------------\n# 1Ô∏è‚É£ Load your dataset\n# -----------------------------\ndf = pd.read_csv(\"/kaggle/working/weather_balanced_cleaned.csv\")  # change file if needed\n\nTARGET = \"RainTomorrow\"   # <-- change if your target column is different\n\nX = df.drop(TARGET, axis=1)\ny = df[TARGET].astype(int)\n\nprint(\"Original dimension:\", X.shape[1])\n\n# -----------------------------\n# 2Ô∏è‚É£ Train-test split\n# -----------------------------\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.20, stratify=y, random_state=42\n)\n\n# -----------------------------\n# 3Ô∏è‚É£ XGBoost Model (500 Iterations)\n# -----------------------------\nmodel = XGBClassifier(\n    n_estimators=500,          # <-- 500 boosting rounds\n    learning_rate=0.05,\n    max_depth=6,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    objective=\"binary:logistic\",\n    eval_metric=\"logloss\",\n    random_state=42,\n    n_jobs=-1\n)\n\nmodel.fit(X_train, y_train)\n\n# -----------------------------\n# 4Ô∏è‚É£ Predictions\n# -----------------------------\ny_pred = model.predict(X_test)\n\ntest_acc = accuracy_score(y_test, y_pred)\ntest_prec = precision_score(y_test, y_pred, zero_division=0)\ntest_rec = recall_score(y_test, y_pred, zero_division=0)\ntest_f1 = f1_score(y_test, y_pred, zero_division=0)\n\n# -----------------------------\n# 5Ô∏è‚É£ Results\n# -----------------------------\nprint(\"\\n===== XGBoost Results (500 Iterations) =====\")\nprint(f\"Test Accuracy : {test_acc:.8f}\")\nprint(f\"Precision     : {test_prec:.8f}\")\nprint(f\"Recall        : {test_rec:.8f}\")\nprint(f\"F1 Score      : {test_f1:.8f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T14:10:13.089393Z","iopub.execute_input":"2025-12-10T14:10:13.089760Z","iopub.status.idle":"2025-12-10T14:10:16.531604Z","shell.execute_reply.started":"2025-12-10T14:10:13.089734Z","shell.execute_reply":"2025-12-10T14:10:16.530615Z"}},"outputs":[{"name":"stdout","text":"Original dimension: 22\n\n===== XGBoost Results (500 Iterations) =====\nTest Accuracy : 0.81295585\nPrecision     : 0.81516588\nRecall        : 0.80941176\nF1 Score      : 0.81227863\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"weather xgboost full","metadata":{}},{"cell_type":"code","source":"# intersection_hlo_with_hillclimb_fast.py\n# Pipeline (reduced budget + hill-climb) with UNION, INTERSECTION, and VOTING candidate flows:\n#  PSO + GA + GWO (CatBoost fitness, lighter during opt) -> derive UNION / INTERSECTION / VOTING\n#  For each candidate set: HLO (on candidates) -> Greedy hill-climb (restricted) -> Final CatBoost eval (5-fold CV)\n#  Additionally: train a CatBoost model on 80% of the data and evaluate on the held-out 20% test set\n#  Train & save a CatBoost model for each flow (union / intersection / voting) using the 80/20 split.\n# Prints logs, mean ¬± std for metrics, stage timings, saves results and models.\n\nimport time\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport warnings\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, make_scorer\nfrom sklearn.base import clone\nfrom xgboost import XGBClassifier\n\n\nwarnings.filterwarnings(\"ignore\")\nnp.random.seed(42)\n\n# -------------------- USER / EXPERIMENT SETTINGS --------------------\n# If you prefer to load CSV instead, uncomment and change:\ndf = pd.read_csv(\"/kaggle/working/weather_balanced_cleaned.csv\")\nTARGET_COL = \"RainTomorrow\"   # target column\nMODEL_VERBOSE = 0            # CatBoost verbosity: 0 = silent\nRANDOM_STATE = 42\n\n# ---------- Reduced budgets for faster runs (you can tune these) ----------\nPSO_SWARM = 15   # reduced swarm\nPSO_ITERS = 10   # reduced iterations\n\nGA_POP = 30      # reduced population\nGA_GENS = 10     # reduced generations\n\nGWO_WOLVES = 10\nGWO_ITERS = 10\n\nHLO_POP = 15\nHLO_ITERS = 10\nHLO_TEACHER_FACTOR = 0.75\nHLO_MUTATION = 0.12\n\n# Greedy hill-climb after HLO\nHILLCLIMB_MAX_STEPS = 100   # stop if no improvement or step limit\nHILLCLIMB_EVAL_CAP = 500    # safety cap on evaluations (prevent runaway)\n\n# CV folds\nCV_OPT = 2    # cheaper CV during optimization + HLO (speed)\nCV_FINAL = 5  # final evaluation (A1 requested)\n\n# CatBoost iterations\nCB_ITER_OPT = 100    # iterations during optimization (smaller)\nCB_ITER_HLO = 200\nCB_ITER_FINAL = 500  # final evaluation iterations (bigger)\n\n# Train/test split for final saved models\nFINAL_TEST_SIZE = 0.2\n\nSAVE_PREFIX = \"hybrid_hlo_models\"\n# ------------------------------------------------------------------------\n\n# Ensure df exists\ntry:\n    df\nexcept NameError:\n    raise RuntimeError(\"DataFrame `df` not found. Assign your dataset to variable `df` or load at top.\")\n\n# Prepare data\nX = df.drop(TARGET_COL, axis=1)\n\ny = df[TARGET_COL].astype(int)\nFEATURE_NAMES = X.columns.tolist()\nN_FEATURES = X.shape[1]\n\n# -------------------- Model factory (CatBoost) --------------------\n\ndef get_xgb_model(iterations=100):\n    return XGBClassifier(\n        n_estimators=iterations,\n        learning_rate=0.05,\n        max_depth=6,\n        subsample=1.0,\n        colsample_bytree=1.0,\n        random_state=RANDOM_STATE,\n        n_jobs=-1,\n        eval_metric=\"logloss\",\n       \n    )\n\n\n# -------------------- Fitness cache --------------------\n# key: tuple(selected original indices) -> float score\nfitness_cache = {}\n\ndef key_from_mask(mask_bool):\n    return tuple(sorted(np.where(np.array(mask_bool).astype(bool))[0].tolist()))\n\ndef evaluate_mask_global(mask_bool, cv=CV_OPT, cb_iter=CB_ITER_OPT):\n    \"\"\"\n    Evaluate mask using CatBoost with CV and return average of acc,prec,rec,f1.\n    Caches results to avoid re-evaluating identical subsets.\n    \"\"\"\n    key = key_from_mask(mask_bool)\n    if key in fitness_cache:\n        return fitness_cache[key]\n    if len(key) == 0:\n        fitness_cache[key] = 0.0\n        return 0.0\n\n    X_sel = X.iloc[:, list(key)]\n    \n    model = get_xgb_model(iterations=cb_iter)\n\n    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=RANDOM_STATE)\n\n    accs = cross_val_score(clone(model), X_sel, y, cv=skf, scoring=\"accuracy\", n_jobs=-1)\n    precs = cross_val_score(clone(model), X_sel, y, cv=skf, scoring=make_scorer(precision_score, zero_division=0), n_jobs=-1)\n    recs = cross_val_score(clone(model), X_sel, y, cv=skf, scoring=make_scorer(recall_score, zero_division=0), n_jobs=-1)\n    f1s = cross_val_score(clone(model), X_sel, y, cv=skf, scoring=make_scorer(f1_score, zero_division=0), n_jobs=-1)\n\n    score = float((np.mean(accs) + np.mean(precs) + np.mean(recs) + np.mean(f1s)) / 4.0)\n    fitness_cache[key] = score\n    return score\n\n# -------------------- Helpers --------------------\ndef mask_to_features(mask):\n    idxs = np.where(np.array(mask).astype(bool))[0].tolist()\n    return [FEATURE_NAMES[i] for i in idxs]\n\ndef log(msg):\n    print(f\"[{time.strftime('%H:%M:%S')}] {msg}\", flush=True)\n\n# -------------------- PSO (binary) --------------------\ndef run_pso(swarm_size=PSO_SWARM, iters=PSO_ITERS, cv=CV_OPT):\n    log(f\"PSO START (swarm={swarm_size}, iters={iters}, cv={cv})\")\n    t0 = time.time()\n    dim = N_FEATURES\n    pos = np.random.randint(0,2,(swarm_size,dim)).astype(int)\n    vel = np.random.uniform(-1,1,(swarm_size,dim))\n\n    pbest = pos.copy()\n    pbest_scores = np.array([evaluate_mask_global(p.astype(bool), cv=cv, cb_iter=CB_ITER_OPT) for p in pos])\n\n    gbest_idx = int(np.argmax(pbest_scores))\n    gbest = pbest[gbest_idx].copy()\n    gbest_score = pbest_scores[gbest_idx]\n\n    w = 0.6; c1 = c2 = 1.5\n    for t in range(iters):\n        log(f\" PSO iter {t+1}/{iters} best_global={gbest_score:.8}\")\n        for i in range(swarm_size):\n            r1 = np.random.rand(dim); r2 = np.random.rand(dim)\n            vel[i] = w*vel[i] + c1*r1*(pbest[i] - pos[i]) + c2*r2*(gbest - pos[i])\n            s = 1.0 / (1.0 + np.exp(-vel[i]))\n            pos[i] = (np.random.rand(dim) < s).astype(int)\n\n            sc = evaluate_mask_global(pos[i].astype(bool), cv=cv, cb_iter=CB_ITER_OPT)\n            if sc > pbest_scores[i]:\n                pbest[i] = pos[i].copy()\n                pbest_scores[i] = sc\n            if sc > gbest_score:\n                gbest = pos[i].copy()\n                gbest_score = sc\n        w = max(0.2, w*0.97)\n\n    best_idx = int(np.argmax(pbest_scores))\n    best_mask = pbest[best_idx].copy()\n    best_score = pbest_scores[best_idx]\n    t1 = time.time()\n    log(f\"PSO DONE in {int(t1-t0)}s best_score={best_score:.8f} selected={int(np.sum(best_mask))}\")\n    log(f\"PSO SELECTED FEATURES: {mask_to_features(best_mask)}\")\n\n    return best_mask, best_score, int(t1-t0)\n\n# -------------------- GA (binary) --------------------\ndef run_ga(pop_size=GA_POP, gens=GA_GENS, cv=CV_OPT):\n    log(f\"GA START (pop={pop_size}, gens={gens}, cv={cv})\")\n    t0 = time.time()\n    dim = N_FEATURES\n    pop = np.random.randint(0,2,(pop_size, dim)).astype(int)\n    fitness_scores = np.array([evaluate_mask_global(ind.astype(bool), cv=cv, cb_iter=CB_ITER_OPT) for ind in pop])\n\n    def tournament_select(k=3):\n        idxs = np.random.randint(0, pop_size, k)\n        return idxs[np.argmax(fitness_scores[idxs])]\n\n    for g in range(gens):\n        log(f\" GA gen {g+1}/{gens} current_best={np.max(fitness_scores):.8f}\")\n        new_pop = []\n        # elitism\n        elite_idxs = np.argsort(fitness_scores)[-2:]\n        new_pop.extend(pop[elite_idxs].tolist())\n\n        while len(new_pop) < pop_size:\n            i1 = tournament_select(); i2 = tournament_select()\n            p1 = pop[i1].copy(); p2 = pop[i2].copy()\n            # crossover\n            if np.random.rand() < 0.7:\n                pt = np.random.randint(1, dim)\n                c1 = np.concatenate([p1[:pt], p2[pt:]])\n                c2 = np.concatenate([p2[:pt], p1[pt:]])\n            else:\n                c1, c2 = p1, p2\n            # mutation\n            for child in (c1, c2):\n                for d in range(dim):\n                    if np.random.rand() < 0.1:\n                        child[d] = 1 - child[d]\n                new_pop.append(child)\n                if len(new_pop) >= pop_size:\n                    break\n        pop = np.array(new_pop[:pop_size])\n        fitness_scores = np.array([evaluate_mask_global(ind.astype(bool), cv=cv, cb_iter=CB_ITER_OPT) for ind in pop])\n\n    best_idx = int(np.argmax(fitness_scores))\n    best_mask = pop[best_idx].copy()\n    best_score = fitness_scores[best_idx]\n    t1 = time.time()\n    log(f\"GA DONE in {int(t1-t0)}s best_score={best_score:.8f} selected={int(np.sum(best_mask))}\")\n    log(f\"GA SELECTED FEATURES: {mask_to_features(best_mask)}\")\n\n    return best_mask, best_score, int(t1-t0)\n\n# -------------------- GWO (binary) --------------------\ndef run_gwo(wolves=GWO_WOLVES, iters=GWO_ITERS, cv=CV_OPT):\n    log(f\"GWO START (wolves={wolves}, iters={iters}, cv={cv})\")\n    t0 = time.time()\n    dim = N_FEATURES\n    pop = np.random.randint(0,2,(wolves, dim)).astype(int)\n    fitness_scores = np.array([evaluate_mask_global(ind.astype(bool), cv=cv, cb_iter=CB_ITER_OPT) for ind in pop])\n\n    Alpha = Beta = Delta = None\n    Alpha_score = Beta_score = Delta_score = -1.0\n\n    for itr in range(iters):\n        log(f\" GWO iter {itr+1}/{iters} best_alpha={Alpha_score:.8f}\")\n        for i in range(wolves):\n            sc = fitness_scores[i]\n            if sc > Alpha_score:\n                Delta_score, Beta_score, Alpha_score = Beta_score, Alpha_score, sc\n                Delta, Beta, Alpha = Beta, Alpha, pop[i].copy()\n            elif sc > Beta_score:\n                Delta_score, Beta_score = Beta_score, sc\n                Delta, Beta = Beta, pop[i].copy()\n            elif sc > Delta_score:\n                Delta_score = sc\n                Delta = pop[i].copy()\n\n        a = 2 - itr * (2.0 / iters)\n        for i in range(wolves):\n            for d in range(dim):\n                if Alpha is None:\n                    continue\n                r1, r2 = np.random.rand(), np.random.rand()\n                A1 = 2 * a * r1 - a; C1 = 2 * r2\n                D_alpha = abs(C1 * Alpha[d] - pop[i][d])\n                X1 = Alpha[d] - A1 * D_alpha\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                A2 = 2 * a * r1 - a; C2 = 2 * r2\n                D_beta = abs(C2 * Beta[d] - pop[i][d])\n                X2 = Beta[d] - A2 * D_beta\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                A3 = 2 * a * r1 - a; C3 = 2 * r2\n                D_delta = abs(C3 * Delta[d] - pop[i][d])\n                X3 = Delta[d] - A3 * D_delta\n\n                new_pos = (X1 + X2 + X3) / 3.0\n                s = 1.0 / (1.0 + np.exp(-new_pos))\n                pop[i][d] = 1 if np.random.rand() < s else 0\n\n        fitness_scores = np.array([evaluate_mask_global(ind.astype(bool), cv=cv, cb_iter=CB_ITER_OPT) for ind in pop])\n\n    best_idx = int(np.argmax(fitness_scores))\n    best_mask = pop[best_idx].copy()\n    best_score = fitness_scores[best_idx]\n    t1 = time.time()\n    log(f\"GWO DONE in {int(t1-t0)}s best_score={best_score:.8f} selected={int(np.sum(best_mask))}\")\n    log(f\"GWO SELECTED FEATURES: {mask_to_features(best_mask)}\")\n\n    return best_mask, best_score, int(t1-t0)\n\n# -------------------- INTERSECTION / UNION / VOTING --------------------\ndef get_intersection_mask(*masks):\n    \"\"\"Return mask that contains only features present in ALL provided masks.\"\"\"\n    if len(masks) == 0:\n        return np.zeros(N_FEATURES, dtype=int)\n    inter_idx = set(np.where(np.array(masks[0]).astype(bool))[0].tolist())\n    for m in masks[1:]:\n        idxs = set(np.where(np.array(m).astype(bool))[0].tolist())\n        inter_idx = inter_idx.intersection(idxs)\n    mask = np.zeros(N_FEATURES, dtype=int)\n    for i in inter_idx:\n        mask[i] = 1\n    return mask\n\n\ndef get_union_mask(*masks):\n    union_idx = set()\n    for m in masks:\n        idxs = np.where(np.array(m).astype(bool))[0].tolist()\n        union_idx.update(idxs)\n    mask = np.zeros(N_FEATURES, dtype=int)\n    for i in union_idx:\n        mask[i] = 1\n    return mask\n\n\ndef get_voting_mask(*masks, threshold=2):\n    \"\"\"Return mask of features selected by at least `threshold` methods (default majority of 3 => 2).\"\"\"\n    if len(masks) == 0:\n        return np.zeros(N_FEATURES, dtype=int)\n    counts = np.zeros(N_FEATURES, dtype=int)\n    for m in masks:\n        counts += np.array(m).astype(int)\n    mask = (counts >= threshold).astype(int)\n    return mask\n\n# -------------------- HLO on candidates --------------------\ndef hlo_on_candidates(candidate_mask, pop_size=HLO_POP, iters=HLO_ITERS, cv=CV_OPT):\n    candidate_indices = np.where(np.array(candidate_mask).astype(bool))[0].tolist()\n    k = len(candidate_indices)\n    if k == 0:\n        raise ValueError(\"Candidate set is empty.\")\n\n    log(f\"HLO START on {k} candidate features (pop={pop_size}, iters={iters})\")\n    t0 = time.time()\n\n    pop = np.random.randint(0,2,(pop_size, k)).astype(int)\n\n    def fitness_candidate(bitmask):\n        full_mask = np.zeros(N_FEATURES, dtype=int)\n        for j,bit in enumerate(bitmask):\n            if bit == 1:\n                full_mask[candidate_indices[j]] = 1\n        return evaluate_mask_global(full_mask.astype(bool), cv=cv, cb_iter=CB_ITER_HLO)\n\n    fitness_scores = np.array([fitness_candidate(ind) for ind in pop])\n    best_idx = int(np.argmax(fitness_scores))\n    best_solution = pop[best_idx].copy()\n    best_score = fitness_scores[best_idx]\n\n    for it in range(iters):\n        log(f\" HLO iter {it+1}/{iters} current_best={best_score:.8f}\")\n        teacher = pop[int(np.argmax(fitness_scores))].copy()\n        new_pop = []\n        for i in range(pop_size):\n            learner = pop[i].copy()\n            # teaching phase\n            for d in range(k):\n                if np.random.rand() < HLO_TEACHER_FACTOR:\n                    learner[d] = teacher[d]\n            # peer learning\n            partner = pop[np.random.randint(pop_size)].copy()\n            for d in range(k):\n                if learner[d] != partner[d] and np.random.rand() < 0.5:\n                    learner[d] = partner[d]\n            # mutation\n            for d in range(k):\n                if np.random.rand() < HLO_MUTATION:\n                    learner[d] = 1 - learner[d]\n            new_pop.append(learner)\n        pop = np.array(new_pop)\n        fitness_scores = np.array([fitness_candidate(ind) for ind in pop])\n        gen_best_idx = int(np.argmax(fitness_scores))\n        gen_best_score = fitness_scores[gen_best_idx]\n        gen_best_sol = pop[gen_best_idx].copy()\n        if gen_best_score > best_score:\n            best_score = gen_best_score\n            best_solution = gen_best_sol.copy()\n\n    # map back to full mask\n    final_full_mask = np.zeros(N_FEATURES, dtype=int)\n    for j,bit in enumerate(best_solution):\n        if bit == 1:\n            final_full_mask[candidate_indices[j]] = 1\n\n    t1 = time.time()\n    log(f\"HLO DONE in {int(t1-t0)}s best_score={best_score:.8f} final_selected={int(np.sum(final_full_mask))}\")\n    return final_full_mask, best_score, int(t1-t0)\n\n# -------------------- Greedy Hill-Climb (local search) --------------------\ndef hill_climb_on_candidates(initial_mask, candidate_mask, max_steps=HILLCLIMB_MAX_STEPS, eval_cap=HILLCLIMB_EVAL_CAP, cv=CV_OPT):\n    \"\"\"\n    Greedy single-bit flip hill-climb restricted to candidate indices.\n    Starts from initial_mask (full-length). Tries flipping each candidate feature's bit:\n    - If flip improves fitness, accept and restart scanning.\n    - Stops when no improving flip found or max_steps/eval_cap reached.\n    \"\"\"\n    candidate_indices = np.where(np.array(candidate_mask).astype(bool))[0].tolist()\n    if len(candidate_indices) == 0:\n        log(\"Hill-climb: candidate set empty, skipping.\")\n        return initial_mask, 0.0, 0\n\n    log(f\"Hill-climb START over {len(candidate_indices)} candidates (max_steps={max_steps}, eval_cap={eval_cap})\")\n    t0 = time.time()\n    current_mask = initial_mask.copy()\n    current_score = evaluate_mask_global(current_mask.astype(bool), cv=cv, cb_iter=CB_ITER_HLO)\n    evals = 0\n    steps = 0\n    improved = True\n\n    while improved and steps < max_steps and evals < eval_cap:\n        improved = False\n        for idx in np.random.permutation(candidate_indices):\n            trial_mask = current_mask.copy()\n            trial_mask[idx] = 1 - trial_mask[idx]  # flip\n            trial_score = evaluate_mask_global(trial_mask.astype(bool), cv=cv, cb_iter=CB_ITER_HLO)\n            evals += 1\n            if trial_score > current_score + 1e-8:\n                current_mask = trial_mask\n                current_score = trial_score\n                improved = True\n                steps += 1\n                log(f\" Hill-climb step {steps}: flipped {FEATURE_NAMES[idx]} -> new_score={current_score:.4f} (evals={evals})\")\n                break\n            if evals >= eval_cap or steps >= max_steps:\n                break\n    t1 = time.time()\n    log(f\"Hill-climb DONE in {int(t1-t0)}s steps={steps} evals={evals} final_score={current_score:.8f} selected={int(np.sum(current_mask))}\")\n    return current_mask, current_score, int(t1-t0)\n\n# -------------------- Final evaluation (5-fold CV) --------------------\ndef final_evaluation(mask_bool, cv=CV_FINAL, cb_iter=CB_ITER_FINAL):\n    idxs = np.where(np.array(mask_bool).astype(bool))[0].tolist()\n    if len(idxs) == 0:\n        raise ValueError(\"Final mask selects zero features.\")\n    X_sel = X.iloc[:, idxs]\n    model = get_xgb_model(iterations=cb_iter)\n\n    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=RANDOM_STATE)\n    accs = []; precs = []; recs = []; f1s = []\n    t0 = time.time()\n    for tr,te in skf.split(X_sel, y):\n        m = clone(model); m.fit(X_sel.iloc[tr], y.iloc[tr])\n        pred = m.predict(X_sel.iloc[te])\n        accs.append(accuracy_score(y.iloc[te], pred))\n        precs.append(precision_score(y.iloc[te], pred, zero_division=0))\n        recs.append(recall_score(y.iloc[te], pred, zero_division=0))\n        f1s.append(f1_score(y.iloc[te], pred, zero_division=0))\n    t1 = time.time()\n    results = {\n        \"n_features\": len(idxs),\n        \"features\": [FEATURE_NAMES[i] for i in idxs],\n        \"acc_mean\": float(np.mean(accs)), \"acc_std\": float(np.std(accs)),\n        \"prec_mean\": float(np.mean(precs)), \"prec_std\": float(np.std(precs)),\n        \"rec_mean\": float(np.mean(recs)), \"rec_std\": float(np.std(recs)),\n        \"f1_mean\": float(np.mean(f1s)), \"f1_std\": float(np.std(f1s)),\n        \"eval_time_s\": int(t1 - t0)\n    }\n    return results\n\n# -------------------- MAIN PIPELINE --------------------\nif __name__ == \"__main__\":\n    total_t0 = time.time()\n    log(\"===== HYBRID (reduced budget) + HLO + HILL-CLIMB (UNION/INTERSECTION/VOTING) START =====\")\n\n    # PSO\n    pso_mask, pso_score, pso_time = run_pso(swarm_size=PSO_SWARM, iters=PSO_ITERS, cv=CV_OPT)\n\n    # GA\n    ga_mask, ga_score, ga_time = run_ga(pop_size=GA_POP, gens=GA_GENS, cv=CV_OPT)\n\n    # GWO\n    gwo_mask, gwo_score, gwo_time = run_gwo(wolves=GWO_WOLVES, iters=GWO_ITERS, cv=CV_OPT)\n\n    # Derive candidate masks\n    union_mask = get_union_mask(pso_mask, ga_mask, gwo_mask)\n    inter_mask = get_intersection_mask(pso_mask, ga_mask, gwo_mask)\n    vote_mask = get_voting_mask(pso_mask, ga_mask, gwo_mask, threshold=2)\n\n    candidate_sets = {\n        'union': union_mask,\n        'intersection': inter_mask,\n        'voting': vote_mask\n    }\n\n    results_all = {}\n\n    # run HLO -> hill-climb -> final evaluation -> train & save model for each candidate set\n    for name, cand_mask in candidate_sets.items():\n        log(f\"===== PROCESSING {name.upper()} CANDIDATES =====\")\n        n_cand = int(np.sum(cand_mask))\n        log(f\"{name.upper()} candidate features: {n_cand}\")\n        if n_cand == 0:\n            log(f\"{name.upper()} empty ‚Äî skipping HLO/hill-climb and model training.\")\n            results_all[name] = {'skipped': True, 'n_candidates': 0}\n            continue\n\n        # HLO on this candidate set\n        hlo_mask, hlo_score, hlo_time = hlo_on_candidates(cand_mask, pop_size=HLO_POP, iters=HLO_ITERS, cv=CV_OPT)\n\n        # hill-climb restricted to candidate set\n        hc_mask, hc_score, hc_time = hill_climb_on_candidates(hlo_mask, cand_mask, max_steps=HILLCLIMB_MAX_STEPS, eval_cap=HILLCLIMB_EVAL_CAP, cv=CV_OPT)\n\n        # final CV evaluation\n        final_res = final_evaluation(hc_mask, cv=CV_FINAL, cb_iter=CB_ITER_FINAL)\n\n        # Train final CatBoost model on 80% train and evaluate on 20% test (stratified)\n        sel_idxs = np.where(np.array(hc_mask).astype(bool))[0].tolist()\n        sel_features = [FEATURE_NAMES[i] for i in sel_idxs]\n\n        if len(sel_features) == 0:\n            log(f\"No features selected after hill-climb for {name}, skipping model train.\")\n            results_all[name] = {'skipped': True, 'n_candidates': n_cand}\n            continue\n\n        X_sel = X[sel_features]\n        X_train, X_test, y_train, y_test = train_test_split(X_sel, y, test_size=FINAL_TEST_SIZE, stratify=y, random_state=RANDOM_STATE)\n\n        \n        model = get_xgb_model(iterations=CB_ITER_FINAL)\n        model.fit(X_train, y_train)\n\n        # evaluate on held-out test set (20%)\n        y_pred = model.predict(X_test)\n        test_acc = accuracy_score(y_test, y_pred)\n        test_prec = precision_score(y_test, y_pred, zero_division=0)\n        test_rec = recall_score(y_test, y_pred, zero_division=0)\n        test_f1 = f1_score(y_test, y_pred, zero_division=0)\n\n        test_metrics = {\n            'acc': float(test_acc), 'prec': float(test_prec), 'rec': float(test_rec), 'f1': float(test_f1),\n            'n_test': int(X_test.shape[0])\n        }\n\n        # Save model to file (pickle)\n        model_filename = f\"{SAVE_PREFIX}_{name}_model.pkl\"\n        with open(model_filename, 'wb') as mf:\n            pickle.dump(model, mf)\n\n        # store results\n        results_all[name] = {\n            'n_candidates': n_cand,\n            'hlo_score': float(hlo_score), 'hlo_time': int(hlo_time),\n            'hc_score': float(hc_score), 'hc_time': int(hc_time),\n            'final_eval': final_res,\n            'selected_features': sel_features,\n            'model_file': model_filename,\n            'test_metrics': test_metrics\n        }\n\n        log(f\"Saved trained XGboost model for {name} -> {model_filename} (test_f1={test_f1:.8f})\")\n\n    total_t1 = time.time()\n    elapsed_total = int(total_t1 - total_t0)\n\n    # Summary / save aggregated results\n    print(\"==================== AGGREGATE SUMMARY ====================\")\n    print(f\"PSO  -> opt_score={pso_score:.8f} selected={int(np.sum(pso_mask))} time={pso_time}s\")\n    print(f\"GA   -> opt_score={ga_score:.8f} selected={int(np.sum(ga_mask))} time={ga_time}s\")\n    print(f\"GWO  -> opt_score={gwo_score:.8f} selected={int(np.sum(gwo_mask))} time={gwo_time}s\")\n    print(f\"Union candidates    : {int(np.sum(union_mask))}\")\n    print(f\"Intersection candidates: {int(np.sum(inter_mask))}\")\n    print(f\"Voting candidates   : {int(np.sum(vote_mask))}\")\n    print(\"-------------------------------------------------\")\n\n    for name, info in results_all.items():\n        print(f\"-- {name.upper()} SUMMARY --\")\n        if info.get('skipped'):\n            print(\" skipped (no candidates)\")\n            continue\n        fe = info['final_eval']\n        tm = info['test_metrics']\n        print(f\" Selected ({fe['n_features']}): {fe['features']}\")\n        print(f\" CV F1   : {fe['f1_mean']:.8f} ¬± {fe['f1_std']:.8f}\")\n        print(f\" Test F1 : {tm['f1']:.8f} (n_test={tm['n_test']})\")\n        print(f\" Accuracy : {fe['acc_mean']:.8f} ¬± {fe['acc_std']:.8f}\")\n        print(f\" Precision: {fe['prec_mean']:.8f} ¬± {fe['prec_std']:.8f}\")\n        print(f\" Recall   : {fe['rec_mean']:.8f} ¬± {fe['rec_std']:.8f}\")\n        print(f\" Model file: {info['model_file']}\")\n\n\n\n    # Save aggregated pipeline outputs\n    out = {\n        \"pso_mask\": pso_mask, \"pso_score\": pso_score, \"pso_time\": pso_time,\n        \"ga_mask\": ga_mask, \"ga_score\": ga_score, \"ga_time\": ga_time,\n        \"gwo_mask\": gwo_mask, \"gwo_score\": gwo_score, \"gwo_time\": gwo_time,\n        \"union_mask\": union_mask, \"intersection_mask\": inter_mask, \"voting_mask\": vote_mask,\n        \"results_all\": results_all,\n        \"fitness_cache_len\": len(fitness_cache)\n    }\n    with open(f\"{SAVE_PREFIX}_results.pkl\", \"wb\") as f:\n        pickle.dump(out, f)\n\n    log(f\"Saved results to {SAVE_PREFIX}_results.pkl\")\n    log(\"===== PIPELINE COMPLETE =====\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T10:14:37.337339Z","iopub.execute_input":"2025-12-09T10:14:37.337630Z","iopub.status.idle":"2025-12-09T11:15:29.707152Z","shell.execute_reply.started":"2025-12-09T10:14:37.337608Z","shell.execute_reply":"2025-12-09T11:15:29.706464Z"}},"outputs":[{"name":"stdout","text":"[10:14:37] ===== HYBRID (reduced budget) + HLO + HILL-CLIMB (UNION/INTERSECTION/VOTING) START =====\n[10:14:37] PSO START (swarm=15, iters=10, cv=2)\n[10:15:17]  PSO iter 1/10 best_global=0.7894176\n[10:15:56]  PSO iter 2/10 best_global=0.7894176\n[10:16:37]  PSO iter 3/10 best_global=0.78994821\n[10:17:18]  PSO iter 4/10 best_global=0.78994821\n[10:17:59]  PSO iter 5/10 best_global=0.78994821\n[10:18:41]  PSO iter 6/10 best_global=0.78994821\n[10:19:22]  PSO iter 7/10 best_global=0.78994821\n[10:20:04]  PSO iter 8/10 best_global=0.78994821\n[10:20:46]  PSO iter 9/10 best_global=0.78994821\n[10:21:27]  PSO iter 10/10 best_global=0.79005626\n[10:22:07] PSO DONE in 449s best_score=0.79005626 selected=17\n[10:22:07] PSO SELECTED FEATURES: ['Date', 'Location', 'MaxTemp', 'Rainfall', 'Evaporation', 'Sunshine', 'WindGustSpeed', 'WindDir9am', 'WindDir3pm', 'WindSpeed9am', 'Humidity3pm', 'Pressure9am', 'Pressure3pm', 'Cloud9am', 'Cloud3pm', 'Temp9am', 'RainToday']\n[10:22:07] GA START (pop=30, gens=10, cv=2)\n[10:23:24]  GA gen 1/10 current_best=0.78446705\n[10:24:36]  GA gen 2/10 current_best=0.78724034\n[10:25:50]  GA gen 3/10 current_best=0.78847185\n[10:26:54]  GA gen 4/10 current_best=0.78927638\n[10:28:05]  GA gen 5/10 current_best=0.78946360\n[10:29:19]  GA gen 6/10 current_best=0.78991018\n[10:30:30]  GA gen 7/10 current_best=0.78991018\n[10:31:47]  GA gen 8/10 current_best=0.79099591\n[10:33:00]  GA gen 9/10 current_best=0.79104068\n[10:34:19]  GA gen 10/10 current_best=0.79104068\n[10:35:36] GA DONE in 808s best_score=0.79104068 selected=17\n[10:35:36] GA SELECTED FEATURES: ['Location', 'MinTemp', 'MaxTemp', 'Rainfall', 'Sunshine', 'WindGustDir', 'WindGustSpeed', 'WindDir3pm', 'WindSpeed9am', 'WindSpeed3pm', 'Humidity3pm', 'Pressure9am', 'Pressure3pm', 'Cloud9am', 'Cloud3pm', 'Temp3pm', 'RainToday']\n[10:35:36] GWO START (wolves=10, iters=10, cv=2)\n[10:36:02]  GWO iter 1/10 best_alpha=-1.00000000\n[10:36:31]  GWO iter 2/10 best_alpha=0.78526251\n[10:37:00]  GWO iter 3/10 best_alpha=0.79025733\n[10:37:30]  GWO iter 4/10 best_alpha=0.79025733\n[10:37:59]  GWO iter 5/10 best_alpha=0.79062443\n[10:38:28]  GWO iter 6/10 best_alpha=0.79062443\n[10:38:56]  GWO iter 7/10 best_alpha=0.79062443\n[10:39:25]  GWO iter 8/10 best_alpha=0.79062443\n[10:39:53]  GWO iter 9/10 best_alpha=0.79098702\n[10:40:20]  GWO iter 10/10 best_alpha=0.79098702\n[10:40:50] GWO DONE in 313s best_score=0.78747893 selected=17\n[10:40:50] GWO SELECTED FEATURES: ['Date', 'Location', 'MinTemp', 'MaxTemp', 'Rainfall', 'Evaporation', 'Sunshine', 'WindGustDir', 'WindGustSpeed', 'WindSpeed9am', 'Humidity9am', 'Humidity3pm', 'Pressure3pm', 'Cloud9am', 'Cloud3pm', 'Temp9am', 'Temp3pm']\n[10:40:50] ===== PROCESSING UNION CANDIDATES =====\n[10:40:50] UNION candidate features: 22\n[10:40:50] HLO START on 22 candidate features (pop=15, iters=10)\n[10:41:50]  HLO iter 1/10 current_best=0.79200242\n[10:42:52]  HLO iter 2/10 current_best=0.79200242\n[10:43:53]  HLO iter 3/10 current_best=0.79200242\n[10:44:54]  HLO iter 4/10 current_best=0.79200242\n[10:45:52]  HLO iter 5/10 current_best=0.79200242\n[10:46:52]  HLO iter 6/10 current_best=0.79200242\n[10:47:52]  HLO iter 7/10 current_best=0.79200242\n[10:48:53]  HLO iter 8/10 current_best=0.79262378\n[10:49:48]  HLO iter 9/10 current_best=0.79455024\n[10:50:46]  HLO iter 10/10 current_best=0.79467445\n[10:51:46] HLO DONE in 655s best_score=0.79540345 final_selected=12\n[10:51:46] Hill-climb START over 22 candidates (max_steps=100, eval_cap=500)\n[10:51:50]  Hill-climb step 1: flipped Cloud3pm -> new_score=0.7957 (evals=1)\n[10:52:17]  Hill-climb step 2: flipped Temp9am -> new_score=0.7959 (evals=8)\n[10:52:26]  Hill-climb step 3: flipped Evaporation -> new_score=0.7960 (evals=11)\n[10:53:31]  Hill-climb step 4: flipped WindSpeed3pm -> new_score=0.7964 (evals=27)\n[10:55:04] Hill-climb DONE in 198s steps=4 evals=49 final_score=0.79643575 selected=16\n[10:55:13] Saved trained XGboost model for union -> hybrid_hlo_models_union_model.pkl (test_f1=0.80490119)\n[10:55:13] ===== PROCESSING INTERSECTION CANDIDATES =====\n[10:55:13] INTERSECTION candidate features: 10\n[10:55:13] HLO START on 10 candidate features (pop=15, iters=10)\n[10:55:56]  HLO iter 1/10 current_best=0.78674330\n[10:56:42]  HLO iter 2/10 current_best=0.78700893\n[10:57:30]  HLO iter 3/10 current_best=0.78809254\n[10:58:09]  HLO iter 4/10 current_best=0.78875997\n[10:58:42]  HLO iter 5/10 current_best=0.78875997\n[10:59:15]  HLO iter 6/10 current_best=0.78975120\n[10:59:34]  HLO iter 7/10 current_best=0.78975120\n[11:00:03]  HLO iter 8/10 current_best=0.78975120\n[11:00:35]  HLO iter 9/10 current_best=0.78975120\n[11:00:54]  HLO iter 10/10 current_best=0.78975120\n[11:01:23] HLO DONE in 370s best_score=0.78975120 final_selected=10\n[11:01:23] Hill-climb START over 10 candidates (max_steps=100, eval_cap=500)\n[11:01:37] Hill-climb DONE in 14s steps=0 evals=10 final_score=0.78975120 selected=10\n[11:01:44] Saved trained XGboost model for intersection -> hybrid_hlo_models_intersection_model.pkl (test_f1=0.79946355)\n[11:01:44] ===== PROCESSING VOTING CANDIDATES =====\n[11:01:44] VOTING candidate features: 19\n[11:01:44] HLO START on 19 candidate features (pop=15, iters=10)\n[11:02:38]  HLO iter 1/10 current_best=0.78431886\n[11:03:34]  HLO iter 2/10 current_best=0.79213018\n[11:04:31]  HLO iter 3/10 current_best=0.79213018\n[11:05:30]  HLO iter 4/10 current_best=0.79220129\n[11:06:23]  HLO iter 5/10 current_best=0.79334868\n[11:07:21]  HLO iter 6/10 current_best=0.79372841\n[11:08:18]  HLO iter 7/10 current_best=0.79380360\n[11:09:15]  HLO iter 8/10 current_best=0.79422602\n[11:10:15]  HLO iter 9/10 current_best=0.79457463\n[11:11:15]  HLO iter 10/10 current_best=0.79457463\n[11:12:15] HLO DONE in 630s best_score=0.79505741 final_selected=14\n[11:12:15] Hill-climb START over 19 candidates (max_steps=100, eval_cap=500)\n[11:13:01]  Hill-climb step 1: flipped WindSpeed9am -> new_score=0.7951 (evals=11)\n[11:13:46]  Hill-climb step 2: flipped Pressure9am -> new_score=0.7954 (evals=23)\n[11:14:03]  Hill-climb step 3: flipped Cloud3pm -> new_score=0.7955 (evals=27)\n[11:15:20] Hill-climb DONE in 185s steps=3 evals=46 final_score=0.79553278 selected=13\n[11:15:29] Saved trained XGboost model for voting -> hybrid_hlo_models_voting_model.pkl (test_f1=0.80662199)\n==================== AGGREGATE SUMMARY ====================\nPSO  -> opt_score=0.79005626 selected=17 time=449s\nGA   -> opt_score=0.79104068 selected=17 time=808s\nGWO  -> opt_score=0.78747893 selected=17 time=313s\nUnion candidates    : 22\nIntersection candidates: 10\nVoting candidates   : 19\n-------------------------------------------------\n-- UNION SUMMARY --\n Selected (16): ['Location', 'Rainfall', 'Evaporation', 'Sunshine', 'WindGustDir', 'WindGustSpeed', 'WindDir3pm', 'WindSpeed3pm', 'Humidity3pm', 'Pressure9am', 'Pressure3pm', 'Cloud9am', 'Cloud3pm', 'Temp9am', 'Temp3pm', 'RainToday']\n CV F1   : 0.80302064 ¬± 0.00215500\n Test F1 : 0.80490119 (n_test=12751)\n Accuracy : 0.80418481 ¬± 0.00194394\n Precision: 0.80781659 ¬± 0.00172719\n Recall   : 0.79828716 ¬± 0.00330140\n Model file: hybrid_hlo_models_union_model.pkl\n-- INTERSECTION SUMMARY --\n Selected (10): ['Location', 'MaxTemp', 'Rainfall', 'Sunshine', 'WindGustSpeed', 'WindSpeed9am', 'Humidity3pm', 'Pressure3pm', 'Cloud9am', 'Cloud3pm']\n CV F1   : 0.79444655 ¬± 0.00284020\n Test F1 : 0.79946355 (n_test=12751)\n Accuracy : 0.79574608 ¬± 0.00304655\n Precision: 0.79955153 ¬± 0.00378291\n Recall   : 0.78940928 ¬± 0.00209003\n Model file: hybrid_hlo_models_intersection_model.pkl\n-- VOTING SUMMARY --\n Selected (13): ['Date', 'Location', 'MinTemp', 'Rainfall', 'Sunshine', 'WindGustDir', 'WindGustSpeed', 'WindDir3pm', 'Humidity3pm', 'Pressure9am', 'Pressure3pm', 'Cloud9am', 'Temp9am']\n CV F1   : 0.80547566 ¬± 0.00158233\n Test F1 : 0.80662199 (n_test=12751)\n Accuracy : 0.80617688 ¬± 0.00129133\n Precision: 0.80839430 ¬± 0.00110065\n Recall   : 0.80258504 ¬± 0.00306114\n Model file: hybrid_hlo_models_voting_model.pkl\n[11:15:29] Saved results to hybrid_hlo_models_results.pkl\n[11:15:29] ===== PIPELINE COMPLETE =====\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T11:56:41.477453Z","iopub.execute_input":"2025-12-09T11:56:41.477978Z","iopub.status.idle":"2025-12-09T11:56:41.483187Z","shell.execute_reply.started":"2025-12-09T11:56:41.477953Z","shell.execute_reply":"2025-12-09T11:56:41.482583Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"(63754, 23)"},"metadata":{}}],"execution_count":15},{"cell_type":"markdown","source":"weather pca","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nimport numpy as np\nimport pandas as pd\nfrom xgboost import XGBClassifier   # üî• NEW\n\ndf = pd.read_csv(\"/kaggle/working/weather_balanced_cleaned.csv\")\n\nX = df.drop(\"RainTomorrow\",axis=1)\ny = df[\"RainTomorrow\"].astype(int)\n\n# 1) Scale\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# 2) PCA to retain 95% variance\npca = PCA(n_components=0.95)  \nX_pca = pca.fit_transform(X_scaled)\n\nprint(\"Original dimension:\", X.shape[1])\nprint(\"PCA dimension:\", X_pca.shape[1])\n\n# 3) Train/Test split\nX_train, X_test, y_train, y_test = train_test_split(\n    X_pca, y, test_size=0.20, stratify=y, random_state=42\n)\n\n# ---------------------------------------------------\n# 4) ‚ùó Replace CatBoost with XGBoost\n# ---------------------------------------------------\nmodel = XGBClassifier(\n    n_estimators=500,\n    learning_rate=0.05,\n    max_depth=6,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    objective=\"binary:logistic\",\n    eval_metric=\"logloss\",\n    tree_method=\"hist\"        # Fast training\n)\n\n# Fit\nmodel.fit(X_train, y_train)\n\n# Test prediction\ny_pred = model.predict(X_test)\n\n# Metrics\ntest_acc = accuracy_score(y_test, y_pred)\ntest_prec = precision_score(y_test, y_pred, zero_division=0)\ntest_rec = recall_score(y_test, y_pred, zero_division=0)\ntest_f1 = f1_score(y_test, y_pred, zero_division=0)\n\nprint(\"\\n=== PCA + XGBOOST MODEL RESULTS ===\")\nprint(\"Test Accuracy :\", test_acc)\nprint(\"Precision     :\", test_prec)\nprint(\"Recall        :\", test_rec)\nprint(\"F1 Score      :\", test_f1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T11:55:26.789555Z","iopub.execute_input":"2025-12-09T11:55:26.789825Z","iopub.status.idle":"2025-12-09T11:55:29.353980Z","shell.execute_reply.started":"2025-12-09T11:55:26.789805Z","shell.execute_reply":"2025-12-09T11:55:29.353257Z"}},"outputs":[{"name":"stdout","text":"Original dimension: 22\nPCA dimension: 16\n\n=== PCA + XGBOOST MODEL RESULTS ===\nTest Accuracy : 0.7918594620029802\nPrecision     : 0.7902948977999688\nRecall        : 0.7945098039215687\nF1 Score      : 0.7923967459324156\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"chi sqaure","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\nfrom xgboost import XGBClassifier   # ‚Üê changed\n\n# -------------------------\n# 1. Prepare data\n# -------------------------\nTARGET_COL = \"RainTomorrow\"\n\nX = df.drop(\"RainTomorrow\",axis=1)\ny = df[\"RainTomorrow\"].astype(int)\n\nprint(\"Original dimension:\", X.shape[1])\n\n# Train-test split (same style as your hybrid pipeline)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.20,\n    stratify=y,\n    random_state=42\n)\n\n# -------------------------\n# 2. Scale to non-negative for chi-square\n# -------------------------\nscaler = MinMaxScaler()   # maps features to [0, 1]\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# -------------------------\n# 3. Try different numbers of selected features\n# -------------------------\nk_values = [5, 10, 15, 20, 16,13]\n\nresults_chi2 = {}\n\nfor k in k_values:\n    print(\"\\n\" + \"=\"*60)\n    print(f\"CHI-SQUARE + XGBoost with top-{k} features\")\n    print(\"=\"*60)\n\n    # 3.1 Chi-Square feature selection\n    selector = SelectKBest(score_func=chi2, k=k)\n    X_train_k = selector.fit_transform(X_train_scaled, y_train)\n    X_test_k = selector.transform(X_test_scaled)\n\n    # Get selected feature names (from original X)\n    selected_mask = selector.get_support()\n    selected_features = X.columns[selected_mask].tolist()\n    print(f\"Selected {k} features:\")\n    print(selected_features)\n\n    # 3.2 Train XGBoost on selected features\n    model = XGBClassifier(\n        n_estimators=500,\n        learning_rate=0.05,\n        max_depth=6,\n        subsample=0.9,\n        colsample_bytree=0.9,\n        objective=\"binary:logistic\",\n        use_label_encoder=False,   # avoid label encoder warning\n        eval_metric=\"logloss\",\n        random_state=42,\n        n_jobs=-1\n    )\n\n    model.fit(X_train_k, y_train)\n\n    # 3.3 Evaluate on test set\n    y_pred = model.predict(X_test_k)\n\n    acc = accuracy_score(y_test, y_pred)\n    prec = precision_score(y_test, y_pred, zero_division=0)\n    rec = recall_score(y_test, y_pred, zero_division=0)\n    f1 = f1_score(y_test, y_pred, zero_division=0)\n\n    print(f\"Test Accuracy : {acc:.6f}\")\n    print(f\"Precision     : {prec:.6f}\")\n    print(f\"Recall        : {rec:.6f}\")\n    print(f\"F1 Score      : {f1:.6f}\")\n\n    # store results for later comparison\n    results_chi2[k] = {\n        \"features\": selected_features,\n        \"acc\": acc,\n        \"prec\": prec,\n        \"rec\": rec,\n        \"f1\": f1\n    }\n\nprint(\"\\n=========== SUMMARY: Chi-Square + XGBoost ===========\")\nfor k, info in results_chi2.items():\n    print(f\"Top-{k} features -> F1={info['f1']:.6f}, Acc={info['acc']:.6f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T12:27:56.610425Z","iopub.execute_input":"2025-12-09T12:27:56.611070Z","iopub.status.idle":"2025-12-09T12:28:04.761896Z","shell.execute_reply.started":"2025-12-09T12:27:56.611034Z","shell.execute_reply":"2025-12-09T12:28:04.761352Z"}},"outputs":[{"name":"stdout","text":"Original dimension: 22\n\n============================================================\nCHI-SQUARE + XGBoost with top-5 features\n============================================================\nSelected 5 features:\n['Sunshine', 'Humidity3pm', 'Cloud9am', 'Cloud3pm', 'RainToday']\nTest Accuracy : 0.751627\nPrecision     : 0.761238\nRecall        : 0.733176\nF1 Score      : 0.746944\n\n============================================================\nCHI-SQUARE + XGBoost with top-10 features\n============================================================\nSelected 10 features:\n['Rainfall', 'Sunshine', 'WindGustSpeed', 'Humidity9am', 'Humidity3pm', 'Pressure9am', 'Cloud9am', 'Cloud3pm', 'Temp3pm', 'RainToday']\nTest Accuracy : 0.789820\nPrecision     : 0.793674\nRecall        : 0.783216\nF1 Score      : 0.788410\n\n============================================================\nCHI-SQUARE + XGBoost with top-15 features\n============================================================\nSelected 15 features:\n['MaxTemp', 'Rainfall', 'Sunshine', 'WindGustDir', 'WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am', 'Humidity3pm', 'Pressure9am', 'Pressure3pm', 'Cloud9am', 'Cloud3pm', 'Temp3pm', 'RainToday']\nTest Accuracy : 0.798682\nPrecision     : 0.801839\nRecall        : 0.793412\nF1 Score      : 0.797603\n\n============================================================\nCHI-SQUARE + XGBoost with top-20 features\n============================================================\nSelected 20 features:\n['MinTemp', 'MaxTemp', 'Rainfall', 'Evaporation', 'Sunshine', 'WindGustDir', 'WindGustSpeed', 'WindDir9am', 'WindDir3pm', 'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am', 'Humidity3pm', 'Pressure9am', 'Pressure3pm', 'Cloud9am', 'Cloud3pm', 'Temp9am', 'Temp3pm', 'RainToday']\nTest Accuracy : 0.804956\nPrecision     : 0.808376\nRecall        : 0.799373\nF1 Score      : 0.803849\n\n============================================================\nCHI-SQUARE + XGBoost with top-16 features\n============================================================\nSelected 16 features:\n['MinTemp', 'MaxTemp', 'Rainfall', 'Sunshine', 'WindGustDir', 'WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am', 'Humidity3pm', 'Pressure9am', 'Pressure3pm', 'Cloud9am', 'Cloud3pm', 'Temp3pm', 'RainToday']\nTest Accuracy : 0.800173\nPrecision     : 0.804261\nRecall        : 0.793412\nF1 Score      : 0.798800\n\n============================================================\nCHI-SQUARE + XGBoost with top-13 features\n============================================================\nSelected 13 features:\n['MaxTemp', 'Rainfall', 'Sunshine', 'WindGustDir', 'WindGustSpeed', 'Humidity9am', 'Humidity3pm', 'Pressure9am', 'Pressure3pm', 'Cloud9am', 'Cloud3pm', 'Temp3pm', 'RainToday']\nTest Accuracy : 0.798682\nPrecision     : 0.801552\nRecall        : 0.793882\nF1 Score      : 0.797699\n\n=========== SUMMARY: Chi-Square + XGBoost ===========\nTop-5 features -> F1=0.746944, Acc=0.751627\nTop-10 features -> F1=0.788410, Acc=0.789820\nTop-15 features -> F1=0.797603, Acc=0.798682\nTop-20 features -> F1=0.803849, Acc=0.804956\nTop-16 features -> F1=0.798800, Acc=0.800173\nTop-13 features -> F1=0.797699, Acc=0.798682\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\nfrom xgboost import XGBClassifier   # ‚Üê changed\n\n# -------------------------\n# 1. Prepare data\n# -------------------------\nTARGET_COL = \"RainTomorrow\"\n\nX = df.drop(\"RainTomorrow\",axis=1)\ny = df[\"RainTomorrow\"].astype(int)\n\nprint(\"Original dimension:\", X.shape[1])\n\n# Train-test split (same style as your hybrid pipeline)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.20,\n    stratify=y,\n    random_state=42\n)\n\n# -------------------------\n# 2. Scale to non-negative for chi-square\n# -------------------------\nscaler = MinMaxScaler()   # maps features to [0, 1]\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# -------------------------\n# 3. Try different numbers of selected features\n# -------------------------\nk_values = [5,10,15,20,16,13]\n\nresults_chi2 = {}\n\nfor k in k_values:\n    print(\"\\n\" + \"=\"*60)\n    print(f\"CHI-SQUARE + XGBoost with top-{k} features\")\n    print(\"=\"*60)\n\n    # 3.1 Chi-Square feature selection\n    selector = SelectKBest(score_func=chi2, k=k)\n    X_train_k = selector.fit_transform(X_train_scaled, y_train)\n    X_test_k = selector.transform(X_test_scaled)\n\n    # Get selected feature names (from original X)\n    selected_mask = selector.get_support()\n    selected_features = X.columns[selected_mask].tolist()\n    print(f\"Selected {k} features:\")\n    print(selected_features)\n\n    # 3.2 Train XGBoost on selected features\n    model = XGBClassifier(\n        n_estimators=500,\n        learning_rate=0.05,\n        max_depth=6,\n        subsample=0.9,\n        colsample_bytree=0.9,\n        objective=\"binary:logistic\",\n        use_label_encoder=False,   # avoid label encoder warning\n        eval_metric=\"logloss\",\n        random_state=42,\n        n_jobs=-1\n    )\n\n    model.fit(X_train_k, y_train)\n\n    # 3.3 Evaluate on test set\n    y_pred = model.predict(X_test_k)\n\n    acc = accuracy_score(y_test, y_pred)\n    prec = precision_score(y_test, y_pred, zero_division=0)\n    rec = recall_score(y_test, y_pred, zero_division=0)\n    f1 = f1_score(y_test, y_pred, zero_division=0)\n\n    print(f\"Test Accuracy : {acc:.6f}\")\n    print(f\"Precision     : {prec:.6f}\")\n    print(f\"Recall        : {rec:.6f}\")\n    print(f\"F1 Score      : {f1:.6f}\")\n\n    # store results for later comparison\n    results_chi2[k] = {\n        \"features\": selected_features,\n        \"acc\": acc,\n        \"prec\": prec,\n        \"rec\": rec,\n        \"f1\": f1\n    }\n\nprint(\"\\n=========== SUMMARY: Chi-Square + XGBoost ===========\")\nfor k, info in results_chi2.items():\n    print(f\"Top-{k} features -> F1={info['f1']:.6f}, Acc={info['acc']:.6f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T12:27:33.587681Z","iopub.execute_input":"2025-12-09T12:27:33.588279Z","iopub.status.idle":"2025-12-09T12:27:41.691614Z","shell.execute_reply.started":"2025-12-09T12:27:33.588257Z","shell.execute_reply":"2025-12-09T12:27:41.691005Z"}},"outputs":[{"name":"stdout","text":"Original dimension: 22\n\n============================================================\nCHI-SQUARE + XGBoost with top-5 features\n============================================================\nSelected 5 features:\n['Sunshine', 'Humidity3pm', 'Cloud9am', 'Cloud3pm', 'RainToday']\nTest Accuracy : 0.751627\nPrecision     : 0.761238\nRecall        : 0.733176\nF1 Score      : 0.746944\n\n============================================================\nCHI-SQUARE + XGBoost with top-10 features\n============================================================\nSelected 10 features:\n['Rainfall', 'Sunshine', 'WindGustSpeed', 'Humidity9am', 'Humidity3pm', 'Pressure9am', 'Cloud9am', 'Cloud3pm', 'Temp3pm', 'RainToday']\nTest Accuracy : 0.789820\nPrecision     : 0.793674\nRecall        : 0.783216\nF1 Score      : 0.788410\n\n============================================================\nCHI-SQUARE + XGBoost with top-15 features\n============================================================\nSelected 15 features:\n['MaxTemp', 'Rainfall', 'Sunshine', 'WindGustDir', 'WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am', 'Humidity3pm', 'Pressure9am', 'Pressure3pm', 'Cloud9am', 'Cloud3pm', 'Temp3pm', 'RainToday']\nTest Accuracy : 0.798682\nPrecision     : 0.801839\nRecall        : 0.793412\nF1 Score      : 0.797603\n\n============================================================\nCHI-SQUARE + XGBoost with top-20 features\n============================================================\nSelected 20 features:\n['MinTemp', 'MaxTemp', 'Rainfall', 'Evaporation', 'Sunshine', 'WindGustDir', 'WindGustSpeed', 'WindDir9am', 'WindDir3pm', 'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am', 'Humidity3pm', 'Pressure9am', 'Pressure3pm', 'Cloud9am', 'Cloud3pm', 'Temp9am', 'Temp3pm', 'RainToday']\nTest Accuracy : 0.804956\nPrecision     : 0.808376\nRecall        : 0.799373\nF1 Score      : 0.803849\n\n============================================================\nCHI-SQUARE + XGBoost with top-16 features\n============================================================\nSelected 16 features:\n['MinTemp', 'MaxTemp', 'Rainfall', 'Sunshine', 'WindGustDir', 'WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am', 'Humidity3pm', 'Pressure9am', 'Pressure3pm', 'Cloud9am', 'Cloud3pm', 'Temp3pm', 'RainToday']\nTest Accuracy : 0.800173\nPrecision     : 0.804261\nRecall        : 0.793412\nF1 Score      : 0.798800\n\n============================================================\nCHI-SQUARE + XGBoost with top-13 features\n============================================================\nSelected 13 features:\n['MaxTemp', 'Rainfall', 'Sunshine', 'WindGustDir', 'WindGustSpeed', 'Humidity9am', 'Humidity3pm', 'Pressure9am', 'Pressure3pm', 'Cloud9am', 'Cloud3pm', 'Temp3pm', 'RainToday']\nTest Accuracy : 0.798682\nPrecision     : 0.801552\nRecall        : 0.793882\nF1 Score      : 0.797699\n\n=========== SUMMARY: Chi-Square + XGBoost ===========\nTop-5 features -> F1=0.746944, Acc=0.751627\nTop-10 features -> F1=0.788410, Acc=0.789820\nTop-15 features -> F1=0.797603, Acc=0.798682\nTop-20 features -> F1=0.803849, Acc=0.804956\nTop-16 features -> F1=0.798800, Acc=0.800173\nTop-13 features -> F1=0.797699, Acc=0.798682\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"mutual information","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import SelectKBest, mutual_info_classif\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\nfrom xgboost import XGBClassifier   # <-- changed\n\n# -------------------------\n# 1. Prepare data\n# -------------------------\n\nprint(\"Original dimension:\", X.shape[1])\n\n# Train-test split (keep same style as other experiments)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.20,\n    stratify=y,\n    random_state=42\n)\n\n# -------------------------\n# 2. Try different numbers of selected features\n# -------------------------\nk_values = [5,10,15,20,16,13]\n\nresults_mi = {}\n\nfor k in k_values:\n    print(\"\\n\" + \"=\"*60)\n    print(f\"MUTUAL INFORMATION + XGBoost with top-{k} features\")\n    print(\"=\"*60)\n\n    # 2.1 Mutual Information feature selection\n    selector = SelectKBest(score_func=mutual_info_classif, k=k)\n    X_train_k = selector.fit_transform(X_train, y_train)\n    X_test_k = selector.transform(X_test)\n\n    # Get selected feature names (from original X)\n    selected_mask = selector.get_support()\n    selected_features = X.columns[selected_mask].tolist()\n    print(f\"Selected {k} features:\")\n    print(selected_features)\n\n    # 2.2 Train XGBoost on selected features\n    model = XGBClassifier(\n        n_estimators=500,\n        learning_rate=0.05,\n        max_depth=6,\n        subsample=0.9,\n        colsample_bytree=0.9,\n        objective=\"binary:logistic\",\n        use_label_encoder=False,   # avoid label-encoder warning in older xgboost\n        eval_metric=\"logloss\",\n        random_state=42,\n        n_jobs=-1\n    )\n\n    model.fit(X_train_k, y_train)\n\n    # 2.3 Evaluate on test set\n    y_pred = model.predict(X_test_k)\n\n    acc = accuracy_score(y_test, y_pred)\n    prec = precision_score(y_test, y_pred, zero_division=0)\n    rec = recall_score(y_test, y_pred, zero_division=0)\n    f1 = f1_score(y_test, y_pred, zero_division=0)\n\n    print(f\"Test Accuracy : {acc:.6f}\")\n    print(f\"Precision     : {prec:.6f}\")\n    print(f\"Recall        : {rec:.6f}\")\n    print(f\"F1 Score      : {f1:.6f}\")\n\n    # store results for later comparison\n    results_mi[k] = {\n        \"features\": selected_features,\n        \"acc\": acc,\n        \"prec\": prec,\n        \"rec\": rec,\n        \"f1\": f1\n    }\n\nprint(\"\\n=========== SUMMARY: Mutual Information + XGBoost ===========\")\nfor k, info in results_mi.items():\n    print(f\"Top-{k} features -> F1={info['f1']:.6f}, Acc={info['acc']:.6f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T12:26:31.466848Z","iopub.execute_input":"2025-12-09T12:26:31.467517Z","iopub.status.idle":"2025-12-09T12:27:08.969888Z","shell.execute_reply.started":"2025-12-09T12:26:31.467490Z","shell.execute_reply":"2025-12-09T12:27:08.968831Z"}},"outputs":[{"name":"stdout","text":"Original dimension: 22\n\n============================================================\nMUTUAL INFORMATION + XGBoost with top-5 features\n============================================================\nSelected 5 features:\n['Date', 'Rainfall', 'Sunshine', 'Humidity3pm', 'Cloud3pm']\nTest Accuracy : 0.755313\nPrecision     : 0.762288\nRecall        : 0.741961\nF1 Score      : 0.751987\n\n============================================================\nMUTUAL INFORMATION + XGBoost with top-10 features\n============================================================\nSelected 10 features:\n['Date', 'Rainfall', 'Sunshine', 'Humidity9am', 'Humidity3pm', 'Pressure9am', 'Pressure3pm', 'Cloud9am', 'Cloud3pm', 'RainToday']\nTest Accuracy : 0.786134\nPrecision     : 0.788243\nRecall        : 0.782431\nF1 Score      : 0.785326\n\n============================================================\nMUTUAL INFORMATION + XGBoost with top-15 features\n============================================================\nSelected 15 features:\n['Date', 'Location', 'MaxTemp', 'Rainfall', 'Sunshine', 'WindGustSpeed', 'WindDir9am', 'Humidity9am', 'Humidity3pm', 'Pressure9am', 'Pressure3pm', 'Cloud9am', 'Cloud3pm', 'Temp3pm', 'RainToday']\nTest Accuracy : 0.807152\nPrecision     : 0.810399\nRecall        : 0.801882\nF1 Score      : 0.806118\n\n============================================================\nMUTUAL INFORMATION + XGBoost with top-20 features\n============================================================\nSelected 20 features:\n['Date', 'Location', 'MinTemp', 'MaxTemp', 'Rainfall', 'Evaporation', 'Sunshine', 'WindGustDir', 'WindGustSpeed', 'WindDir9am', 'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am', 'Humidity3pm', 'Pressure9am', 'Pressure3pm', 'Cloud9am', 'Cloud3pm', 'Temp3pm', 'RainToday']\nTest Accuracy : 0.809740\nPrecision     : 0.811681\nRecall        : 0.806588\nF1 Score      : 0.809127\n\n============================================================\nMUTUAL INFORMATION + XGBoost with top-16 features\n============================================================\nSelected 16 features:\n['Date', 'Location', 'MaxTemp', 'Rainfall', 'Evaporation', 'Sunshine', 'WindGustSpeed', 'WindDir9am', 'Humidity9am', 'Humidity3pm', 'Pressure9am', 'Pressure3pm', 'Cloud9am', 'Cloud3pm', 'Temp3pm', 'RainToday']\nTest Accuracy : 0.807858\nPrecision     : 0.811755\nRecall        : 0.801569\nF1 Score      : 0.806630\n\n============================================================\nMUTUAL INFORMATION + XGBoost with top-13 features\n============================================================\nSelected 13 features:\n['Date', 'Location', 'Rainfall', 'Sunshine', 'WindGustSpeed', 'Humidity9am', 'Humidity3pm', 'Pressure9am', 'Pressure3pm', 'Cloud9am', 'Cloud3pm', 'Temp3pm', 'RainToday']\nTest Accuracy : 0.803153\nPrecision     : 0.807380\nRecall        : 0.796235\nF1 Score      : 0.801769\n\n=========== SUMMARY: Mutual Information + XGBoost ===========\nTop-5 features -> F1=0.751987, Acc=0.755313\nTop-10 features -> F1=0.785326, Acc=0.786134\nTop-15 features -> F1=0.806118, Acc=0.807152\nTop-20 features -> F1=0.809127, Acc=0.809740\nTop-16 features -> F1=0.806630, Acc=0.807858\nTop-13 features -> F1=0.801769, Acc=0.803153\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"recursive feature elimination","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import RFE\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\nfrom xgboost import XGBClassifier   # <-- changed\n\n# -------------------------\n# 1. Prepare data\n# -------------------------\n\nprint(\"Original dimension:\", X.shape[1])\n\n# Train-test split (same style as other baselines)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.20,\n    stratify=y,\n    random_state=42\n)\n\n# -------------------------\n# 2. Try different numbers of selected features with RFE\n# -------------------------\nk_values = [5,10,15,20,16,13]\n\nresults_rfe = {}\n\nfor k in k_values:\n    print(\"\\n\" + \"=\"*60)\n    print(f\"RFE (XGBoost) with top-{k} features\")\n    print(\"=\"*60)\n\n    # Base estimator for RFE (reduced iterations to save time)\n    base_model = XGBClassifier(\n        n_estimators=200,            # reduced for speed during RFE\n        learning_rate=0.05,\n        max_depth=6,\n        objective=\"binary:logistic\",\n        use_label_encoder=False,     # suppress older xgboost warning\n        eval_metric=\"logloss\",\n        random_state=42,\n        n_jobs=-1\n    )\n\n    # 2.1 RFE setup\n    selector = RFE(\n        estimator=base_model,\n        n_features_to_select=k,\n        step=1\n    )\n\n    # Fit RFE on training data\n    selector.fit(X_train, y_train)\n\n    # Transform train and test sets\n    X_train_k = selector.transform(X_train)\n    X_test_k = selector.transform(X_test)\n\n    # Get selected feature names\n    selected_mask = selector.get_support()\n    selected_features = X.columns[selected_mask].tolist()\n    print(f\"Selected {k} features:\")\n    print(selected_features)\n\n    # 2.2 Train a fresh XGBoost on the selected features (for fair comparison)\n    model = XGBClassifier(\n        n_estimators=500,\n        learning_rate=0.05,\n        max_depth=6,\n        subsample=0.9,\n        colsample_bytree=0.9,\n        objective=\"binary:logistic\",\n        use_label_encoder=False,\n        eval_metric=\"logloss\",\n        random_state=42,\n        n_jobs=-1\n    )\n\n    model.fit(X_train_k, y_train)\n\n    # 2.3 Evaluate on test set\n    y_pred = model.predict(X_test_k)\n\n    acc = accuracy_score(y_test, y_pred)\n    prec = precision_score(y_test, y_pred, zero_division=0)\n    rec = recall_score(y_test, y_pred, zero_division=0)\n    f1 = f1_score(y_test, y_pred, zero_division=0)\n\n    print(f\"Test Accuracy : {acc:.6f}\")\n    print(f\"Precision     : {prec:.6f}\")\n    print(f\"Recall        : {rec:.6f}\")\n    print(f\"F1 Score      : {f1:.6f}\")\n\n    # store results for later comparison\n    results_rfe[k] = {\n        \"features\": selected_features,\n        \"acc\": acc,\n        \"prec\": prec,\n        \"rec\": rec,\n        \"f1\": f1\n    }\n\nprint(\"\\n=========== SUMMARY: RFE (XGBoost) ===========\")\nfor k, info in results_rfe.items():\n    print(f\"Top-{k} features -> F1={info['f1']:.6f}, Acc={info['acc']:.6f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T12:25:24.794439Z","iopub.execute_input":"2025-12-09T12:25:24.794939Z","iopub.status.idle":"2025-12-09T12:26:11.253439Z","shell.execute_reply.started":"2025-12-09T12:25:24.794916Z","shell.execute_reply":"2025-12-09T12:26:11.252796Z"}},"outputs":[{"name":"stdout","text":"Original dimension: 22\n\n============================================================\nRFE (XGBoost) with top-5 features\n============================================================\nSelected 5 features:\n['Sunshine', 'WindGustSpeed', 'Humidity3pm', 'Pressure3pm', 'Cloud3pm']\nTest Accuracy : 0.778527\nPrecision     : 0.773955\nRecall        : 0.786824\nF1 Score      : 0.780336\n\n============================================================\nRFE (XGBoost) with top-10 features\n============================================================\nSelected 10 features:\n['Location', 'Rainfall', 'Sunshine', 'WindGustDir', 'WindGustSpeed', 'WindDir9am', 'WindDir3pm', 'Humidity3pm', 'Pressure3pm', 'Cloud3pm']\nTest Accuracy : 0.806996\nPrecision     : 0.810143\nRecall        : 0.801882\nF1 Score      : 0.805991\n\n============================================================\nRFE (XGBoost) with top-15 features\n============================================================\nSelected 15 features:\n['Location', 'MaxTemp', 'Rainfall', 'Sunshine', 'WindGustDir', 'WindGustSpeed', 'WindDir9am', 'WindDir3pm', 'Humidity9am', 'Humidity3pm', 'Pressure3pm', 'Cloud3pm', 'Temp9am', 'Temp3pm', 'RainToday']\nTest Accuracy : 0.808878\nPrecision     : 0.811452\nRecall        : 0.804706\nF1 Score      : 0.808065\n\n============================================================\nRFE (XGBoost) with top-20 features\n============================================================\nSelected 20 features:\n['Location', 'MinTemp', 'MaxTemp', 'Rainfall', 'Sunshine', 'WindGustDir', 'WindGustSpeed', 'WindDir9am', 'WindDir3pm', 'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am', 'Humidity3pm', 'Pressure9am', 'Pressure3pm', 'Cloud9am', 'Cloud3pm', 'Temp9am', 'Temp3pm', 'RainToday']\nTest Accuracy : 0.809819\nPrecision     : 0.812500\nRecall        : 0.805490\nF1 Score      : 0.808980\n\n============================================================\nRFE (XGBoost) with top-16 features\n============================================================\nSelected 16 features:\n['Location', 'MaxTemp', 'Rainfall', 'Sunshine', 'WindGustDir', 'WindGustSpeed', 'WindDir9am', 'WindDir3pm', 'Humidity9am', 'Humidity3pm', 'Pressure9am', 'Pressure3pm', 'Cloud3pm', 'Temp9am', 'Temp3pm', 'RainToday']\nTest Accuracy : 0.808721\nPrecision     : 0.812381\nRecall        : 0.802824\nF1 Score      : 0.807574\n\n============================================================\nRFE (XGBoost) with top-13 features\n============================================================\nSelected 13 features:\n['Location', 'MaxTemp', 'Rainfall', 'Sunshine', 'WindGustDir', 'WindGustSpeed', 'WindDir9am', 'WindDir3pm', 'Humidity3pm', 'Pressure3pm', 'Cloud3pm', 'Temp9am', 'RainToday']\nTest Accuracy : 0.809348\nPrecision     : 0.809965\nRecall        : 0.808314\nF1 Score      : 0.809139\n\n=========== SUMMARY: RFE (XGBoost) ===========\nTop-5 features -> F1=0.780336, Acc=0.778527\nTop-10 features -> F1=0.805991, Acc=0.806996\nTop-15 features -> F1=0.808065, Acc=0.808878\nTop-20 features -> F1=0.808980, Acc=0.809819\nTop-16 features -> F1=0.807574, Acc=0.808721\nTop-13 features -> F1=0.809139, Acc=0.809348\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"l1 lasso feature selection + xgboost","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom xgboost import XGBClassifier   # <-- changed\nimport numpy as np\nimport pandas as pd\n\ndf = pd.read_csv(\"/kaggle/working/weather_balanced_cleaned.csv\")\n\nX = df.drop(\"RainTomorrow\",axis=1)\ny = df[\"RainTomorrow\"].astype(int)\n\nfeature_names = X.columns.tolist()\n\nprint(\"Original dimension:\", X.shape[1])\n\n# Train Logistic regression with L1 penalty to select features\nlog_reg = LogisticRegression(penalty='l1', solver='liblinear', C=0.3, max_iter=2000)\nlog_reg.fit(X, y)\n\n# Get non-zero coefficients\ncoef = log_reg.coef_[0]\nselected_idx = np.where(coef != 0)[0]\nselected_features = [feature_names[i] for i in selected_idx]\n\nprint(\"\\nSelected features using L1 (Lasso):\")\nprint(selected_features)\nprint(\"Total selected:\", len(selected_features))\n\n# ----- XGBoost on selected features -----\n\nX_sel = X[selected_features]\nX_train, X_test, y_train, y_test = train_test_split(\n    X_sel, y, test_size=0.20, stratify=y, random_state=42\n)\n\nmodel = XGBClassifier(\n    n_estimators=500,\n    learning_rate=0.05,\n    max_depth=6,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    objective=\"binary:logistic\",\n    use_label_encoder=False,   # suppress older xgboost label-encoder warning\n    eval_metric=\"logloss\",\n    random_state=42,\n    n_jobs=-1\n)\n\nmodel.fit(X_train, y_train)\n\n# Predictions\ny_pred = model.predict(X_test)\n\ntest_acc = accuracy_score(y_test, y_pred)\ntest_prec = precision_score(y_test, y_pred, zero_division=0)\ntest_rec = recall_score(y_test, y_pred, zero_division=0)\ntest_f1 = f1_score(y_test, y_pred, zero_division=0)\n\nprint(\"\\n===== L1 (Lasso) + XGBoost Results =====\")\nprint(f\"Test Accuracy : {test_acc:.8f}\")\nprint(f\"Precision     : {test_prec:.8f}\")\nprint(f\"Recall        : {test_rec:.8f}\")\nprint(f\"F1 Score      : {test_f1:.8f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T12:32:55.793454Z","iopub.execute_input":"2025-12-09T12:32:55.794119Z","iopub.status.idle":"2025-12-09T12:33:09.713266Z","shell.execute_reply.started":"2025-12-09T12:32:55.794095Z","shell.execute_reply":"2025-12-09T12:33:09.712673Z"}},"outputs":[{"name":"stdout","text":"Original dimension: 22\n\nSelected features using L1 (Lasso):\n['Date', 'Location', 'MinTemp', 'MaxTemp', 'Rainfall', 'Sunshine', 'WindGustDir', 'WindGustSpeed', 'WindDir9am', 'WindDir3pm', 'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am', 'Humidity3pm', 'Pressure9am', 'Pressure3pm', 'Cloud3pm', 'Temp9am', 'Temp3pm', 'RainToday']\nTotal selected: 20\n\n===== L1 (Lasso) + XGBoost Results =====\nTest Accuracy : 0.81279900\nPrecision     : 0.81660845\nRecall        : 0.80674510\nF1 Score      : 0.81164681\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom xgboost import XGBClassifier   # <-- changed\nimport numpy as np\nimport pandas as pd\n\ndf = pd.read_csv(\"/kaggle/working/weather_balanced_cleaned.csv\")\n\nX = df.drop(\"RainTomorrow\",axis=1)\ny = df[\"RainTomorrow\"].astype(int)\n\nfeature_names = X.columns.tolist()\n\nprint(\"Original dimension:\", X.shape[1])\n\n# Train Logistic regression with L1 penalty to select features\nlog_reg = LogisticRegression(penalty='l1', solver='liblinear', C=0.3, max_iter=2000)\nlog_reg.fit(X, y)\n\n# Get non-zero coefficients\ncoef = log_reg.coef_[0]\nselected_idx = np.where(coef != 0)[0]\nselected_features = [feature_names[i] for i in selected_idx]\n\nprint(\"\\nSelected features using L1 (Lasso):\")\nprint(selected_features)\nprint(\"Total selected:\", len(selected_features))\n\n# ----- XGBoost on selected features -----\n\nX_sel = X[selected_features]\nX_train, X_test, y_train, y_test = train_test_split(\n    X_sel, y, test_size=0.20, stratify=y, random_state=42\n)\n\nmodel = XGBClassifier(\n    n_estimators=500,\n    learning_rate=0.05,\n    max_depth=6,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    objective=\"binary:logistic\",\n    use_label_encoder=False,   # suppress older xgboost label-encoder warning\n    eval_metric=\"logloss\",\n    random_state=42,\n    n_jobs=-1\n)\n\nmodel.fit(X_train, y_train)\n\n# Predictions\ny_pred = model.predict(X_test)\n\ntest_acc = accuracy_score(y_test, y_pred)\ntest_prec = precision_score(y_test, y_pred, zero_division=0)\ntest_rec = recall_score(y_test, y_pred, zero_division=0)\ntest_f1 = f1_score(y_test, y_pred, zero_division=0)\n\nprint(\"\\n===== L1 (Lasso) + XGBoost Results =====\")\nprint(f\"Test Accuracy : {test_acc:.8f}\")\nprint(f\"Precision     : {test_prec:.8f}\")\nprint(f\"Recall        : {test_rec:.8f}\")\nprint(f\"F1 Score      : {test_f1:.8f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T14:12:02.008145Z","iopub.execute_input":"2025-12-10T14:12:02.008499Z","iopub.status.idle":"2025-12-10T14:12:20.411570Z","shell.execute_reply.started":"2025-12-10T14:12:02.008473Z","shell.execute_reply":"2025-12-10T14:12:20.410330Z"}},"outputs":[{"name":"stdout","text":"Original dimension: 22\n\nSelected features using L1 (Lasso):\n['Date', 'Location', 'MinTemp', 'MaxTemp', 'Rainfall', 'Sunshine', 'WindGustDir', 'WindGustSpeed', 'WindDir9am', 'WindDir3pm', 'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am', 'Humidity3pm', 'Pressure9am', 'Pressure3pm', 'Cloud3pm', 'Temp9am', 'Temp3pm', 'RainToday']\nTotal selected: 20\n\n===== L1 (Lasso) + XGBoost Results =====\nTest Accuracy : 0.81279900\nPrecision     : 0.81660845\nRecall        : 0.80674510\nF1 Score      : 0.81164681\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\nfrom xgboost import XGBClassifier\n\n# -------------------------\n# 1. Load data\n# -------------------------\n\nfeature_names = X.columns.tolist()\nprint(\"Original dimension:\", X.shape[1])\n\n# -------------------------\n# 2. Train XGBoost on ALL features to get importance\n# -------------------------\nbase_model = XGBClassifier(\n    n_estimators=500,\n    learning_rate=0.05,\n    max_depth=6,\n    objective=\"binary:logistic\",\n    use_label_encoder=False,\n    eval_metric=\"logloss\",\n    random_state=42,\n    n_jobs=-1\n)\n\nbase_model.fit(X, y)\n\n# Get feature importances from XGBoost\nimportances = base_model.feature_importances_\n# Sort indices by importance (descending)\nsorted_idx = np.argsort(importances)[::-1]\nsorted_features = [feature_names[i] for i in sorted_idx]\n\nprint(\"\\nTop 20 features by XGBoost importance:\")\nfor i in range(min(20, len(sorted_features))):\n    print(f\"{i+1:2d}. {sorted_features[i]}  (importance={importances[sorted_idx[i]]:.6f})\")\n\n# -------------------------\n# 3. Evaluate XGBoost using top-k important features\n# -------------------------\nK_values = [5, 10, 15, 20,16,13]\nresults_xgb_imp = {}\n\nprint(\"\\n============================================================\")\nprint(\"XGBOOST FEATURE IMPORTANCE + XGBOOST BASELINE\")\nprint(\"============================================================\")\n\nfor k in K_values:\n    top_k_features = sorted_features[:k]\n    X_sel = X[top_k_features]\n\n    # Same train-test strategy as other baselines\n    X_train, X_test, y_train, y_test = train_test_split(\n        X_sel, y,\n        test_size=0.20,\n        stratify=y,\n        random_state=42\n    )\n\n    model = XGBClassifier(\n        n_estimators=500,\n        learning_rate=0.05,\n        max_depth=6,\n        subsample=0.9,\n        colsample_bytree=0.9,\n        objective=\"binary:logistic\",\n        use_label_encoder=False,\n        eval_metric=\"logloss\",\n        random_state=42,\n        n_jobs=-1\n    )\n\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n\n    acc = accuracy_score(y_test, y_pred)\n    prec = precision_score(y_test, y_pred, zero_division=0)\n    rec = recall_score(y_test, y_pred, zero_division=0)\n    f1 = f1_score(y_test, y_pred, zero_division=0)\n\n    print(f\"\\n============================================================\")\n    print(f\"XGBoost-Importance + XGBoost with top-{k} features\")\n    print(\"============================================================\")\n    print(f\"Selected {k} features:\")\n    print(top_k_features)\n    print(f\"Test Accuracy : {acc:.8f}\")\n    print(f\"Precision     : {prec:.8f}\")\n    print(f\"Recall        : {rec:.8f}\")\n    print(f\"F1 Score      : {f1:.8f}\")\n\n    results_xgb_imp[k] = {\n        \"features\": top_k_features,\n        \"acc\": acc,\n        \"prec\": prec,\n        \"rec\": rec,\n        \"f1\": f1\n    }\n\nprint(\"\\n=========== SUMMARY: XGBoost Importance + XGBoost ===========\")\nfor k, info in results_xgb_imp.items():\n    print(f\"Top-{k} features -> F1={info['f1']:.8f}, Acc={info['acc']:.8f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T12:35:48.854860Z","iopub.execute_input":"2025-12-09T12:35:48.855166Z","iopub.status.idle":"2025-12-09T12:35:58.996607Z","shell.execute_reply.started":"2025-12-09T12:35:48.855143Z","shell.execute_reply":"2025-12-09T12:35:58.995578Z"}},"outputs":[{"name":"stdout","text":"Original dimension: 22\n\nTop 20 features by XGBoost importance:\n 1. Humidity3pm  (importance=0.312722)\n 2. RainToday  (importance=0.097032)\n 3. WindGustSpeed  (importance=0.077055)\n 4. Sunshine  (importance=0.062624)\n 5. Cloud3pm  (importance=0.057892)\n 6. Pressure3pm  (importance=0.053490)\n 7. Rainfall  (importance=0.047490)\n 8. WindDir3pm  (importance=0.028441)\n 9. Location  (importance=0.025580)\n10. WindGustDir  (importance=0.021798)\n11. WindDir9am  (importance=0.021311)\n12. MaxTemp  (importance=0.020267)\n13. Temp9am  (importance=0.019595)\n14. Humidity9am  (importance=0.019303)\n15. Pressure9am  (importance=0.019285)\n16. MinTemp  (importance=0.019138)\n17. Temp3pm  (importance=0.018964)\n18. Evaporation  (importance=0.016564)\n19. WindSpeed9am  (importance=0.015570)\n20. Cloud9am  (importance=0.015445)\n\n============================================================\nXGBOOST FEATURE IMPORTANCE + XGBOOST BASELINE\n============================================================\n\n============================================================\nXGBoost-Importance + XGBoost with top-5 features\n============================================================\nSelected 5 features:\n['Humidity3pm', 'RainToday', 'WindGustSpeed', 'Sunshine', 'Cloud3pm']\nTest Accuracy : 0.76950827\nPrecision     : 0.77700742\nRecall        : 0.75592157\nF1 Score      : 0.76631947\n\n============================================================\nXGBoost-Importance + XGBoost with top-10 features\n============================================================\nSelected 10 features:\n['Humidity3pm', 'RainToday', 'WindGustSpeed', 'Sunshine', 'Cloud3pm', 'Pressure3pm', 'Rainfall', 'WindDir3pm', 'Location', 'WindGustDir']\nTest Accuracy : 0.80417222\nPrecision     : 0.80826709\nRecall        : 0.79749020\nF1 Score      : 0.80284248\n\n============================================================\nXGBoost-Importance + XGBoost with top-15 features\n============================================================\nSelected 15 features:\n['Humidity3pm', 'RainToday', 'WindGustSpeed', 'Sunshine', 'Cloud3pm', 'Pressure3pm', 'Rainfall', 'WindDir3pm', 'Location', 'WindGustDir', 'WindDir9am', 'MaxTemp', 'Temp9am', 'Humidity9am', 'Pressure9am']\nTest Accuracy : 0.80942671\nPrecision     : 0.81107081\nRecall        : 0.80674510\nF1 Score      : 0.80890217\n\n============================================================\nXGBoost-Importance + XGBoost with top-20 features\n============================================================\nSelected 20 features:\n['Humidity3pm', 'RainToday', 'WindGustSpeed', 'Sunshine', 'Cloud3pm', 'Pressure3pm', 'Rainfall', 'WindDir3pm', 'Location', 'WindGustDir', 'WindDir9am', 'MaxTemp', 'Temp9am', 'Humidity9am', 'Pressure9am', 'MinTemp', 'Temp3pm', 'Evaporation', 'WindSpeed9am', 'Cloud9am']\nTest Accuracy : 0.81115207\nPrecision     : 0.81389460\nRecall        : 0.80674510\nF1 Score      : 0.81030408\n\n============================================================\nXGBoost-Importance + XGBoost with top-16 features\n============================================================\nSelected 16 features:\n['Humidity3pm', 'RainToday', 'WindGustSpeed', 'Sunshine', 'Cloud3pm', 'Pressure3pm', 'Rainfall', 'WindDir3pm', 'Location', 'WindGustDir', 'WindDir9am', 'MaxTemp', 'Temp9am', 'Humidity9am', 'Pressure9am', 'MinTemp']\nTest Accuracy : 0.80926986\nPrecision     : 0.81199557\nRecall        : 0.80486275\nF1 Score      : 0.80841342\n\n============================================================\nXGBoost-Importance + XGBoost with top-13 features\n============================================================\nSelected 13 features:\n['Humidity3pm', 'RainToday', 'WindGustSpeed', 'Sunshine', 'Cloud3pm', 'Pressure3pm', 'Rainfall', 'WindDir3pm', 'Location', 'WindGustDir', 'WindDir9am', 'MaxTemp', 'Temp9am']\nTest Accuracy : 0.80754451\nPrecision     : 0.81025479\nRecall        : 0.80313725\nF1 Score      : 0.80668032\n\n=========== SUMMARY: XGBoost Importance + XGBoost ===========\nTop-5 features -> F1=0.76631947, Acc=0.76950827\nTop-10 features -> F1=0.80284248, Acc=0.80417222\nTop-15 features -> F1=0.80890217, Acc=0.80942671\nTop-20 features -> F1=0.81030408, Acc=0.81115207\nTop-16 features -> F1=0.80841342, Acc=0.80926986\nTop-13 features -> F1=0.80668032, Acc=0.80754451\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"from xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nimport pandas as pd\n\n# Load dataset\ndf = pd.read_csv(\"/kaggle/working/weather_balanced_cleaned.csv\")\n\n# Split features & target\nX = df.drop(\"RainTomorrow\", axis=1)\ny = df[\"RainTomorrow\"].astype(int)\n\nprint(\"Original dimension:\", X.shape[1])\n\n# -----------------------------\n# Train-test split\n# -----------------------------\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.20, stratify=y, random_state=42\n)\n\n# -----------------------------\n# XGBoost Model\n# -----------------------------\nmodel = XGBClassifier(\n    n_estimators=500,\n    learning_rate=0.05,\n    max_depth=6,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    objective=\"binary:logistic\",\n    eval_metric=\"logloss\",\n    random_state=42,\n    n_jobs=-1\n)\n\nmodel.fit(X_train, y_train)\n\n# -----------------------------\n# Predictions\n# -----------------------------\ny_pred = model.predict(X_test)\n\ntest_acc = accuracy_score(y_test, y_pred)\ntest_prec = precision_score(y_test, y_pred, zero_division=0)\ntest_rec = recall_score(y_test, y_pred, zero_division=0)\ntest_f1 = f1_score(y_test, y_pred, zero_division=0)\n\n# -----------------------------\n# Final Results\n# -----------------------------\nprint(\"\\n===== XGBoost Results =====\")\nprint(f\"Test Accuracy : {test_acc:.8f}\")\nprint(f\"Precision     : {test_prec:.8f}\")\nprint(f\"Recall        : {test_rec:.8f}\")\nprint(f\"F1 Score      : {test_f1:.8f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}