{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4759005,"sourceType":"datasetVersion","datasetId":2754352,"isSourceIdPinned":false},{"sourceId":14010819,"sourceType":"datasetVersion","datasetId":8925407}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"hi\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T12:16:41.836196Z","iopub.execute_input":"2025-12-08T12:16:41.836381Z","iopub.status.idle":"2025-12-08T12:16:41.842893Z","shell.execute_reply.started":"2025-12-08T12:16:41.836364Z","shell.execute_reply":"2025-12-08T12:16:41.842207Z"}},"outputs":[{"name":"stdout","text":"hi\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# intersection_hlo_with_hillclimb_fast.py\n# Pipeline (reduced budget + hill-climb) with UNION, INTERSECTION, and VOTING candidate flows:\n#  PSO + GA + GWO (CatBoost fitness, lighter during opt) -> derive UNION / INTERSECTION / VOTING\n#  For each candidate set: HLO (on candidates) -> Greedy hill-climb (restricted) -> Final CatBoost eval (5-fold CV)\n#  Additionally: train a CatBoost model on 80% of the data and evaluate on the held-out 20% test set\n#  Train & save a CatBoost model for each flow (union / intersection / voting) using the 80/20 split.\n# Prints logs, mean ± std for metrics, stage timings, saves results and models.\n\nimport time\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport warnings\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, make_scorer\nfrom sklearn.base import clone\n\nwarnings.filterwarnings(\"ignore\")\nnp.random.seed(42)\n\n# -------------------- USER / EXPERIMENT SETTINGS --------------------\n# If you prefer to load CSV instead, uncomment and change:\ndf = pd.read_csv(\"/kaggle/input/ids-cleaned/ids2018_cleaned_combined_1.csv\")\n\nTARGET_COL = \"Label\"   # target column\nMODEL_VERBOSE = 0            # CatBoost verbosity: 0 = silent\nRANDOM_STATE = 42\n\n# ---------- Reduced budgets for faster runs (you can tune these) ----------\nPSO_SWARM = 15   # reduced swarm\nPSO_ITERS = 3   # reduced iterations\n\nGA_POP = 30      # reduced population\nGA_GENS = 3     # reduced generations\n\nGWO_WOLVES = 10\nGWO_ITERS = 3\n\nHLO_POP = 15\nHLO_ITERS = 5\nHLO_TEACHER_FACTOR = 0.75\nHLO_MUTATION = 0.12\n\n# Greedy hill-climb after HLO\nHILLCLIMB_MAX_STEPS = 100   # stop if no improvement or step limit\nHILLCLIMB_EVAL_CAP = 500    # safety cap on evaluations (prevent runaway)\n\n# CV folds\nCV_OPT = 2    # cheaper CV during optimization + HLO (speed)\nCV_FINAL = 5  # final evaluation (A1 requested)\n\n# CatBoost iterations\nCB_ITER_OPT = 100    # iterations during optimization (smaller)\nCB_ITER_HLO = 200\nCB_ITER_FINAL = 500  # final evaluation iterations (bigger)\n\n# Train/test split for final saved models\nFINAL_TEST_SIZE = 0.2\n\nSAVE_PREFIX = \"hybrid_hlo_models\"\n# ------------------------------------------------------------------------\n\n# Ensure df exists\ntry:\n    df\nexcept NameError:\n    raise RuntimeError(\"DataFrame `df` not found. Assign your dataset to variable `df` or load at top.\")\n\n# Prepare data\nX = df.drop(TARGET_COL, axis=1)\n\ny = df[TARGET_COL].astype(int)\nFEATURE_NAMES = X.columns.tolist()\nN_FEATURES = X.shape[1]\n\n# -------------------- Model factory (CatBoost) --------------------\ndef get_catboost_model(iterations=100):\n    try:\n        from catboost import CatBoostClassifier\n    except Exception as e:\n        raise ImportError(\"catboost not installed. Install with: pip install catboost\") from e\n    return CatBoostClassifier(iterations=iterations, learning_rate=0.05, depth=6,\n                              verbose=MODEL_VERBOSE, random_seed=RANDOM_STATE, thread_count=-1)\n\n# -------------------- Fitness cache --------------------\n# key: tuple(selected original indices) -> float score\nfitness_cache = {}\n\ndef key_from_mask(mask_bool):\n    return tuple(sorted(np.where(np.array(mask_bool).astype(bool))[0].tolist()))\n\ndef evaluate_mask_global(mask_bool, cv=CV_OPT, cb_iter=CB_ITER_OPT):\n    \"\"\"\n    Evaluate mask using CatBoost with CV and return average of acc,prec,rec,f1.\n    Caches results to avoid re-evaluating identical subsets.\n    \"\"\"\n    key = key_from_mask(mask_bool)\n    if key in fitness_cache:\n        return fitness_cache[key]\n    if len(key) == 0:\n        fitness_cache[key] = 0.0\n        return 0.0\n\n    X_sel = X.iloc[:, list(key)]\n    model = get_catboost_model(iterations=cb_iter)\n    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=RANDOM_STATE)\n\n    accs = cross_val_score(clone(model), X_sel, y, cv=skf, scoring=\"accuracy\", n_jobs=-1)\n    precs = cross_val_score(clone(model), X_sel, y, cv=skf, scoring=make_scorer(precision_score, zero_division=0), n_jobs=-1)\n    recs = cross_val_score(clone(model), X_sel, y, cv=skf, scoring=make_scorer(recall_score, zero_division=0), n_jobs=-1)\n    f1s = cross_val_score(clone(model), X_sel, y, cv=skf, scoring=make_scorer(f1_score, zero_division=0), n_jobs=-1)\n\n    score = float((np.mean(accs) + np.mean(precs) + np.mean(recs) + np.mean(f1s)) / 4.0)\n    fitness_cache[key] = score\n    return score\n\n# -------------------- Helpers --------------------\ndef mask_to_features(mask):\n    idxs = np.where(np.array(mask).astype(bool))[0].tolist()\n    return [FEATURE_NAMES[i] for i in idxs]\n\ndef log(msg):\n    print(f\"[{time.strftime('%H:%M:%S')}] {msg}\", flush=True)\n\n# -------------------- PSO (binary) --------------------\ndef run_pso(swarm_size=PSO_SWARM, iters=PSO_ITERS, cv=CV_OPT):\n    log(f\"PSO START (swarm={swarm_size}, iters={iters}, cv={cv})\")\n    t0 = time.time()\n    dim = N_FEATURES\n    pos = np.random.randint(0,2,(swarm_size,dim)).astype(int)\n    vel = np.random.uniform(-1,1,(swarm_size,dim))\n\n    pbest = pos.copy()\n    pbest_scores = np.array([evaluate_mask_global(p.astype(bool), cv=cv, cb_iter=CB_ITER_OPT) for p in pos])\n\n    gbest_idx = int(np.argmax(pbest_scores))\n    gbest = pbest[gbest_idx].copy()\n    gbest_score = pbest_scores[gbest_idx]\n\n    w = 0.6; c1 = c2 = 1.5\n    for t in range(iters):\n        log(f\" PSO iter {t+1}/{iters} best_global={gbest_score:.4f}\")\n        for i in range(swarm_size):\n            r1 = np.random.rand(dim); r2 = np.random.rand(dim)\n            vel[i] = w*vel[i] + c1*r1*(pbest[i] - pos[i]) + c2*r2*(gbest - pos[i])\n            s = 1.0 / (1.0 + np.exp(-vel[i]))\n            pos[i] = (np.random.rand(dim) < s).astype(int)\n\n            sc = evaluate_mask_global(pos[i].astype(bool), cv=cv, cb_iter=CB_ITER_OPT)\n            if sc > pbest_scores[i]:\n                pbest[i] = pos[i].copy()\n                pbest_scores[i] = sc\n            if sc > gbest_score:\n                gbest = pos[i].copy()\n                gbest_score = sc\n        w = max(0.2, w*0.97)\n\n    best_idx = int(np.argmax(pbest_scores))\n    best_mask = pbest[best_idx].copy()\n    best_score = pbest_scores[best_idx]\n    t1 = time.time()\n    log(f\"PSO DONE in {int(t1-t0)}s best_score={best_score:.4f} selected={int(np.sum(best_mask))}\")\n    log(f\"PSO SELECTED FEATURES: {mask_to_features(best_mask)}\")\n\n    return best_mask, best_score, int(t1-t0)\n\n# -------------------- GA (binary) --------------------\ndef run_ga(pop_size=GA_POP, gens=GA_GENS, cv=CV_OPT):\n    log(f\"GA START (pop={pop_size}, gens={gens}, cv={cv})\")\n    t0 = time.time()\n    dim = N_FEATURES\n    pop = np.random.randint(0,2,(pop_size, dim)).astype(int)\n    fitness_scores = np.array([evaluate_mask_global(ind.astype(bool), cv=cv, cb_iter=CB_ITER_OPT) for ind in pop])\n\n    def tournament_select(k=3):\n        idxs = np.random.randint(0, pop_size, k)\n        return idxs[np.argmax(fitness_scores[idxs])]\n\n    for g in range(gens):\n        log(f\" GA gen {g+1}/{gens} current_best={np.max(fitness_scores):.4f}\")\n        new_pop = []\n        # elitism\n        elite_idxs = np.argsort(fitness_scores)[-2:]\n        new_pop.extend(pop[elite_idxs].tolist())\n\n        while len(new_pop) < pop_size:\n            i1 = tournament_select(); i2 = tournament_select()\n            p1 = pop[i1].copy(); p2 = pop[i2].copy()\n            # crossover\n            if np.random.rand() < 0.7:\n                pt = np.random.randint(1, dim)\n                c1 = np.concatenate([p1[:pt], p2[pt:]])\n                c2 = np.concatenate([p2[:pt], p1[pt:]])\n            else:\n                c1, c2 = p1, p2\n            # mutation\n            for child in (c1, c2):\n                for d in range(dim):\n                    if np.random.rand() < 0.1:\n                        child[d] = 1 - child[d]\n                new_pop.append(child)\n                if len(new_pop) >= pop_size:\n                    break\n        pop = np.array(new_pop[:pop_size])\n        fitness_scores = np.array([evaluate_mask_global(ind.astype(bool), cv=cv, cb_iter=CB_ITER_OPT) for ind in pop])\n\n    best_idx = int(np.argmax(fitness_scores))\n    best_mask = pop[best_idx].copy()\n    best_score = fitness_scores[best_idx]\n    t1 = time.time()\n    log(f\"GA DONE in {int(t1-t0)}s best_score={best_score:.4f} selected={int(np.sum(best_mask))}\")\n    log(f\"GA SELECTED FEATURES: {mask_to_features(best_mask)}\")\n\n    return best_mask, best_score, int(t1-t0)\n\n# -------------------- GWO (binary) --------------------\ndef run_gwo(wolves=GWO_WOLVES, iters=GWO_ITERS, cv=CV_OPT):\n    log(f\"GWO START (wolves={wolves}, iters={iters}, cv={cv})\")\n    t0 = time.time()\n    dim = N_FEATURES\n    pop = np.random.randint(0,2,(wolves, dim)).astype(int)\n    fitness_scores = np.array([evaluate_mask_global(ind.astype(bool), cv=cv, cb_iter=CB_ITER_OPT) for ind in pop])\n\n    Alpha = Beta = Delta = None\n    Alpha_score = Beta_score = Delta_score = -1.0\n\n    for itr in range(iters):\n        log(f\" GWO iter {itr+1}/{iters} best_alpha={Alpha_score:.4f}\")\n        for i in range(wolves):\n            sc = fitness_scores[i]\n            if sc > Alpha_score:\n                Delta_score, Beta_score, Alpha_score = Beta_score, Alpha_score, sc\n                Delta, Beta, Alpha = Beta, Alpha, pop[i].copy()\n            elif sc > Beta_score:\n                Delta_score, Beta_score = Beta_score, sc\n                Delta, Beta = Beta, pop[i].copy()\n            elif sc > Delta_score:\n                Delta_score = sc\n                Delta = pop[i].copy()\n\n        a = 2 - itr * (2.0 / iters)\n        for i in range(wolves):\n            for d in range(dim):\n                if Alpha is None:\n                    continue\n                r1, r2 = np.random.rand(), np.random.rand()\n                A1 = 2 * a * r1 - a; C1 = 2 * r2\n                D_alpha = abs(C1 * Alpha[d] - pop[i][d])\n                X1 = Alpha[d] - A1 * D_alpha\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                A2 = 2 * a * r1 - a; C2 = 2 * r2\n                D_beta = abs(C2 * Beta[d] - pop[i][d])\n                X2 = Beta[d] - A2 * D_beta\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                A3 = 2 * a * r1 - a; C3 = 2 * r2\n                D_delta = abs(C3 * Delta[d] - pop[i][d])\n                X3 = Delta[d] - A3 * D_delta\n\n                new_pos = (X1 + X2 + X3) / 3.0\n                s = 1.0 / (1.0 + np.exp(-new_pos))\n                pop[i][d] = 1 if np.random.rand() < s else 0\n\n        fitness_scores = np.array([evaluate_mask_global(ind.astype(bool), cv=cv, cb_iter=CB_ITER_OPT) for ind in pop])\n\n    best_idx = int(np.argmax(fitness_scores))\n    best_mask = pop[best_idx].copy()\n    best_score = fitness_scores[best_idx]\n    t1 = time.time()\n    log(f\"GWO DONE in {int(t1-t0)}s best_score={best_score:.4f} selected={int(np.sum(best_mask))}\")\n    log(f\"GWO SELECTED FEATURES: {mask_to_features(best_mask)}\")\n\n    return best_mask, best_score, int(t1-t0)\n\n# -------------------- INTERSECTION / UNION / VOTING --------------------\ndef get_intersection_mask(*masks):\n    \"\"\"Return mask that contains only features present in ALL provided masks.\"\"\"\n    if len(masks) == 0:\n        return np.zeros(N_FEATURES, dtype=int)\n    inter_idx = set(np.where(np.array(masks[0]).astype(bool))[0].tolist())\n    for m in masks[1:]:\n        idxs = set(np.where(np.array(m).astype(bool))[0].tolist())\n        inter_idx = inter_idx.intersection(idxs)\n    mask = np.zeros(N_FEATURES, dtype=int)\n    for i in inter_idx:\n        mask[i] = 1\n    return mask\n\n\ndef get_union_mask(*masks):\n    union_idx = set()\n    for m in masks:\n        idxs = np.where(np.array(m).astype(bool))[0].tolist()\n        union_idx.update(idxs)\n    mask = np.zeros(N_FEATURES, dtype=int)\n    for i in union_idx:\n        mask[i] = 1\n    return mask\n\n\ndef get_voting_mask(*masks, threshold=2):\n    \"\"\"Return mask of features selected by at least `threshold` methods (default majority of 3 => 2).\"\"\"\n    if len(masks) == 0:\n        return np.zeros(N_FEATURES, dtype=int)\n    counts = np.zeros(N_FEATURES, dtype=int)\n    for m in masks:\n        counts += np.array(m).astype(int)\n    mask = (counts >= threshold).astype(int)\n    return mask\n\n# -------------------- HLO on candidates --------------------\ndef hlo_on_candidates(candidate_mask, pop_size=HLO_POP, iters=HLO_ITERS, cv=CV_OPT):\n    candidate_indices = np.where(np.array(candidate_mask).astype(bool))[0].tolist()\n    k = len(candidate_indices)\n    if k == 0:\n        raise ValueError(\"Candidate set is empty.\")\n\n    log(f\"HLO START on {k} candidate features (pop={pop_size}, iters={iters})\")\n    t0 = time.time()\n\n    pop = np.random.randint(0,2,(pop_size, k)).astype(int)\n\n    def fitness_candidate(bitmask):\n        full_mask = np.zeros(N_FEATURES, dtype=int)\n        for j,bit in enumerate(bitmask):\n            if bit == 1:\n                full_mask[candidate_indices[j]] = 1\n        return evaluate_mask_global(full_mask.astype(bool), cv=cv, cb_iter=CB_ITER_HLO)\n\n    fitness_scores = np.array([fitness_candidate(ind) for ind in pop])\n    best_idx = int(np.argmax(fitness_scores))\n    best_solution = pop[best_idx].copy()\n    best_score = fitness_scores[best_idx]\n\n    for it in range(iters):\n        log(f\" HLO iter {it+1}/{iters} current_best={best_score:.4f}\")\n        teacher = pop[int(np.argmax(fitness_scores))].copy()\n        new_pop = []\n        for i in range(pop_size):\n            learner = pop[i].copy()\n            # teaching phase\n            for d in range(k):\n                if np.random.rand() < HLO_TEACHER_FACTOR:\n                    learner[d] = teacher[d]\n            # peer learning\n            partner = pop[np.random.randint(pop_size)].copy()\n            for d in range(k):\n                if learner[d] != partner[d] and np.random.rand() < 0.5:\n                    learner[d] = partner[d]\n            # mutation\n            for d in range(k):\n                if np.random.rand() < HLO_MUTATION:\n                    learner[d] = 1 - learner[d]\n            new_pop.append(learner)\n        pop = np.array(new_pop)\n        fitness_scores = np.array([fitness_candidate(ind) for ind in pop])\n        gen_best_idx = int(np.argmax(fitness_scores))\n        gen_best_score = fitness_scores[gen_best_idx]\n        gen_best_sol = pop[gen_best_idx].copy()\n        if gen_best_score > best_score:\n            best_score = gen_best_score\n            best_solution = gen_best_sol.copy()\n\n    # map back to full mask\n    final_full_mask = np.zeros(N_FEATURES, dtype=int)\n    for j,bit in enumerate(best_solution):\n        if bit == 1:\n            final_full_mask[candidate_indices[j]] = 1\n\n    t1 = time.time()\n    log(f\"HLO DONE in {int(t1-t0)}s best_score={best_score:.4f} final_selected={int(np.sum(final_full_mask))}\")\n    return final_full_mask, best_score, int(t1-t0)\n\n# -------------------- Greedy Hill-Climb (local search) --------------------\ndef hill_climb_on_candidates(initial_mask, candidate_mask, max_steps=HILLCLIMB_MAX_STEPS, eval_cap=HILLCLIMB_EVAL_CAP, cv=CV_OPT):\n    \"\"\"\n    Greedy single-bit flip hill-climb restricted to candidate indices.\n    Starts from initial_mask (full-length). Tries flipping each candidate feature's bit:\n    - If flip improves fitness, accept and restart scanning.\n    - Stops when no improving flip found or max_steps/eval_cap reached.\n    \"\"\"\n    candidate_indices = np.where(np.array(candidate_mask).astype(bool))[0].tolist()\n    if len(candidate_indices) == 0:\n        log(\"Hill-climb: candidate set empty, skipping.\")\n        return initial_mask, 0.0, 0\n\n    log(f\"Hill-climb START over {len(candidate_indices)} candidates (max_steps={max_steps}, eval_cap={eval_cap})\")\n    t0 = time.time()\n    current_mask = initial_mask.copy()\n    current_score = evaluate_mask_global(current_mask.astype(bool), cv=cv, cb_iter=CB_ITER_HLO)\n    evals = 0\n    steps = 0\n    improved = True\n\n    while improved and steps < max_steps and evals < eval_cap:\n        improved = False\n        for idx in np.random.permutation(candidate_indices):\n            trial_mask = current_mask.copy()\n            trial_mask[idx] = 1 - trial_mask[idx]  # flip\n            trial_score = evaluate_mask_global(trial_mask.astype(bool), cv=cv, cb_iter=CB_ITER_HLO)\n            evals += 1\n            if trial_score > current_score + 1e-8:\n                current_mask = trial_mask\n                current_score = trial_score\n                improved = True\n                steps += 1\n                log(f\" Hill-climb step {steps}: flipped {FEATURE_NAMES[idx]} -> new_score={current_score:.4f} (evals={evals})\")\n                break\n            if evals >= eval_cap or steps >= max_steps:\n                break\n    t1 = time.time()\n    log(f\"Hill-climb DONE in {int(t1-t0)}s steps={steps} evals={evals} final_score={current_score:.4f} selected={int(np.sum(current_mask))}\")\n    return current_mask, current_score, int(t1-t0)\n\n# -------------------- Final evaluation (5-fold CV) --------------------\ndef final_evaluation(mask_bool, cv=CV_FINAL, cb_iter=CB_ITER_FINAL):\n    idxs = np.where(np.array(mask_bool).astype(bool))[0].tolist()\n    if len(idxs) == 0:\n        raise ValueError(\"Final mask selects zero features.\")\n    X_sel = X.iloc[:, idxs]\n    model = get_catboost_model(iterations=cb_iter)\n    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=RANDOM_STATE)\n    accs = []; precs = []; recs = []; f1s = []\n    t0 = time.time()\n    for tr,te in skf.split(X_sel, y):\n        m = clone(model); m.fit(X_sel.iloc[tr], y.iloc[tr])\n        pred = m.predict(X_sel.iloc[te])\n        accs.append(accuracy_score(y.iloc[te], pred))\n        precs.append(precision_score(y.iloc[te], pred, zero_division=0))\n        recs.append(recall_score(y.iloc[te], pred, zero_division=0))\n        f1s.append(f1_score(y.iloc[te], pred, zero_division=0))\n    t1 = time.time()\n    results = {\n        \"n_features\": len(idxs),\n        \"features\": [FEATURE_NAMES[i] for i in idxs],\n        \"acc_mean\": float(np.mean(accs)), \"acc_std\": float(np.std(accs)),\n        \"prec_mean\": float(np.mean(precs)), \"prec_std\": float(np.std(precs)),\n        \"rec_mean\": float(np.mean(recs)), \"rec_std\": float(np.std(recs)),\n        \"f1_mean\": float(np.mean(f1s)), \"f1_std\": float(np.std(f1s)),\n        \"eval_time_s\": int(t1 - t0)\n    }\n    return results\n\n# -------------------- MAIN PIPELINE --------------------\nif __name__ == \"__main__\":\n    total_t0 = time.time()\n    log(\"===== HYBRID (reduced budget) + HLO + HILL-CLIMB (UNION/INTERSECTION/VOTING) START =====\")\n\n    # PSO\n    pso_mask, pso_score, pso_time = run_pso(swarm_size=PSO_SWARM, iters=PSO_ITERS, cv=CV_OPT)\n\n    # GA\n    ga_mask, ga_score, ga_time = run_ga(pop_size=GA_POP, gens=GA_GENS, cv=CV_OPT)\n\n    # GWO\n    gwo_mask, gwo_score, gwo_time = run_gwo(wolves=GWO_WOLVES, iters=GWO_ITERS, cv=CV_OPT)\n\n    # Derive candidate masks\n    union_mask = get_union_mask(pso_mask, ga_mask, gwo_mask)\n    inter_mask = get_intersection_mask(pso_mask, ga_mask, gwo_mask)\n    vote_mask = get_voting_mask(pso_mask, ga_mask, gwo_mask, threshold=2)\n\n    candidate_sets = {\n        'union': union_mask,\n        'intersection': inter_mask,\n        'voting': vote_mask\n    }\n\n    results_all = {}\n\n    # run HLO -> hill-climb -> final evaluation -> train & save model for each candidate set\n    for name, cand_mask in candidate_sets.items():\n        log(f\"===== PROCESSING {name.upper()} CANDIDATES =====\")\n        n_cand = int(np.sum(cand_mask))\n        log(f\"{name.upper()} candidate features: {n_cand}\")\n        if n_cand == 0:\n            log(f\"{name.upper()} empty — skipping HLO/hill-climb and model training.\")\n            results_all[name] = {'skipped': True, 'n_candidates': 0}\n            continue\n\n        # HLO on this candidate set\n        hlo_mask, hlo_score, hlo_time = hlo_on_candidates(cand_mask, pop_size=HLO_POP, iters=HLO_ITERS, cv=CV_OPT)\n\n        # hill-climb restricted to candidate set\n        hc_mask, hc_score, hc_time = hill_climb_on_candidates(hlo_mask, cand_mask, max_steps=HILLCLIMB_MAX_STEPS, eval_cap=HILLCLIMB_EVAL_CAP, cv=CV_OPT)\n\n        # final CV evaluation\n        final_res = final_evaluation(hc_mask, cv=CV_FINAL, cb_iter=CB_ITER_FINAL)\n\n        # Train final CatBoost model on 80% train and evaluate on 20% test (stratified)\n        sel_idxs = np.where(np.array(hc_mask).astype(bool))[0].tolist()\n        sel_features = [FEATURE_NAMES[i] for i in sel_idxs]\n\n        if len(sel_features) == 0:\n            log(f\"No features selected after hill-climb for {name}, skipping model train.\")\n            results_all[name] = {'skipped': True, 'n_candidates': n_cand}\n            continue\n\n        X_sel = X[sel_features]\n        X_train, X_test, y_train, y_test = train_test_split(X_sel, y, test_size=FINAL_TEST_SIZE, stratify=y, random_state=RANDOM_STATE)\n\n        model = get_catboost_model(iterations=CB_ITER_FINAL)\n        model.fit(X_train, y_train)\n\n        # evaluate on held-out test set (20%)\n        y_pred = model.predict(X_test)\n        test_acc = accuracy_score(y_test, y_pred)\n        test_prec = precision_score(y_test, y_pred, zero_division=0)\n        test_rec = recall_score(y_test, y_pred, zero_division=0)\n        test_f1 = f1_score(y_test, y_pred, zero_division=0)\n\n        test_metrics = {\n            'acc': float(test_acc), 'prec': float(test_prec), 'rec': float(test_rec), 'f1': float(test_f1),\n            'n_test': int(X_test.shape[0])\n        }\n\n        # Save model to file (pickle)\n        model_filename = f\"{SAVE_PREFIX}_{name}_model.pkl\"\n        with open(model_filename, 'wb') as mf:\n            pickle.dump(model, mf)\n\n        # store results\n        results_all[name] = {\n            'n_candidates': n_cand,\n            'hlo_score': float(hlo_score), 'hlo_time': int(hlo_time),\n            'hc_score': float(hc_score), 'hc_time': int(hc_time),\n            'final_eval': final_res,\n            'selected_features': sel_features,\n            'model_file': model_filename,\n            'test_metrics': test_metrics\n        }\n\n        log(f\"Saved trained CatBoost model for {name} -> {model_filename} (test_f1={test_f1:.4f})\")\n\n    total_t1 = time.time()\n    elapsed_total = int(total_t1 - total_t0)\n\n    # Summary / save aggregated results\n    print(\"==================== AGGREGATE SUMMARY ====================\")\n    print(f\"PSO  -> opt_score={pso_score:.4f} selected={int(np.sum(pso_mask))} time={pso_time}s\")\n    print(f\"GA   -> opt_score={ga_score:.4f} selected={int(np.sum(ga_mask))} time={ga_time}s\")\n    print(f\"GWO  -> opt_score={gwo_score:.4f} selected={int(np.sum(gwo_mask))} time={gwo_time}s\")\n    print(f\"Union candidates    : {int(np.sum(union_mask))}\")\n    print(f\"Intersection candidates: {int(np.sum(inter_mask))}\")\n    print(f\"Voting candidates   : {int(np.sum(vote_mask))}\")\n    print(\"-------------------------------------------------\")\n\n    for name, info in results_all.items():\n        print(f\"-- {name.upper()} SUMMARY --\")\n        if info.get('skipped'):\n            print(\" skipped (no candidates)\")\n            continue\n        fe = info['final_eval']\n        tm = info['test_metrics']\n        print(f\" Selected ({fe['n_features']}): {fe['features']}\")\n        print(f\" CV F1   : {fe['f1_mean']:.4f} ± {fe['f1_std']:.4f}\")\n        print(f\" Test F1 : {tm['f1']:.4f} (n_test={tm['n_test']})\")\n        print(f\" Accuracy : {fe['acc_mean']:.4f} ± {fe['acc_std']:.4f}\")\n        print(f\" Precision: {fe['prec_mean']:.4f} ± {fe['prec_std']:.4f}\")\n        print(f\" Recall   : {fe['rec_mean']:.4f} ± {fe['rec_std']:.4f}\")\n        print(f\" Model file: {info['model_file']}\")\n\n\n\n    # Save aggregated pipeline outputs\n    out = {\n        \"pso_mask\": pso_mask, \"pso_score\": pso_score, \"pso_time\": pso_time,\n        \"ga_mask\": ga_mask, \"ga_score\": ga_score, \"ga_time\": ga_time,\n        \"gwo_mask\": gwo_mask, \"gwo_score\": gwo_score, \"gwo_time\": gwo_time,\n        \"union_mask\": union_mask, \"intersection_mask\": inter_mask, \"voting_mask\": vote_mask,\n        \"results_all\": results_all,\n        \"fitness_cache_len\": len(fitness_cache)\n    }\n    with open(f\"{SAVE_PREFIX}_results.pkl\", \"wb\") as f:\n        pickle.dump(out, f)\n\n    log(f\"Saved results to {SAVE_PREFIX}_results.pkl\")\n    log(\"===== PIPELINE COMPLETE =====\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T12:05:27.666564Z","iopub.execute_input":"2025-12-05T12:05:27.666883Z","iopub.status.idle":"2025-12-05T15:01:09.243433Z","shell.execute_reply.started":"2025-12-05T12:05:27.666858Z","shell.execute_reply":"2025-12-05T15:01:09.242810Z"}},"outputs":[{"name":"stdout","text":"[12:05:29] ===== HYBRID (reduced budget) + HLO + HILL-CLIMB (UNION/INTERSECTION/VOTING) START =====\n[12:05:29] PSO START (swarm=15, iters=3, cv=2)\n[12:08:35]  PSO iter 1/3 best_global=0.9992\n[12:11:35]  PSO iter 2/3 best_global=0.9993\n[12:14:35]  PSO iter 3/3 best_global=0.9993\n[12:17:30] PSO DONE in 721s best_score=0.9993 selected=34\n[12:17:30] PSO SELECTED FEATURES: ['Dst Port', 'Timestamp', 'Tot Fwd Pkts', 'TotLen Fwd Pkts', 'Fwd Pkt Len Mean', 'Bwd Pkt Len Mean', 'Bwd Pkt Len Std', 'Flow Pkts/s', 'Fwd IAT Mean', 'Fwd IAT Max', 'Bwd IAT Tot', 'Bwd IAT Min', 'Fwd PSH Flags', 'Fwd Header Len', 'Bwd Pkts/s', 'Pkt Len Min', 'Pkt Len Mean', 'FIN Flag Cnt', 'ACK Flag Cnt', 'URG Flag Cnt', 'CWE Flag Count', 'ECE Flag Cnt', 'Fwd Seg Size Avg', 'Subflow Fwd Pkts', 'Subflow Fwd Byts', 'Subflow Bwd Pkts', 'Subflow Bwd Byts', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean', 'Src IP', 'Src Port']\n[12:17:30] GA START (pop=30, gens=3, cv=2)\n[12:23:29]  GA gen 1/3 current_best=0.9993\n[12:29:06]  GA gen 2/3 current_best=0.9993\n[12:34:44]  GA gen 3/3 current_best=0.9993\n[12:40:26] GA DONE in 1376s best_score=0.9993 selected=37\n[12:40:26] GA SELECTED FEATURES: ['Dst Port', 'Timestamp', 'Tot Fwd Pkts', 'TotLen Fwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Min', 'Bwd Pkt Len Max', 'Bwd Pkt Len Std', 'Flow IAT Mean', 'Flow IAT Max', 'Fwd IAT Std', 'Fwd IAT Min', 'Bwd IAT Mean', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Fwd Header Len', 'Bwd Header Len', 'Bwd Pkts/s', 'Pkt Len Var', 'SYN Flag Cnt', 'CWE Flag Count', 'ECE Flag Cnt', 'Down/Up Ratio', 'Pkt Size Avg', 'Fwd Seg Size Avg', 'Subflow Fwd Pkts', 'Subflow Fwd Byts', 'Subflow Bwd Pkts', 'Subflow Bwd Byts', 'Active Std', 'Idle Std', 'Idle Max', 'Idle Min', 'Flow ID', 'Src IP', 'Dst IP']\n[12:40:26] GWO START (wolves=10, iters=3, cv=2)\n[12:42:28]  GWO iter 1/3 best_alpha=-1.0000\n[12:44:39]  GWO iter 2/3 best_alpha=0.9992\n[12:46:51]  GWO iter 3/3 best_alpha=0.9993\n[12:49:07] GWO DONE in 520s best_score=0.9993 selected=44\n[12:49:07] GWO SELECTED FEATURES: ['Dst Port', 'Protocol', 'Timestamp', 'Flow Duration', 'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Bwd Pkt Len Min', 'Bwd Pkt Len Mean', 'Bwd Pkt Len Std', 'Flow IAT Mean', 'Flow IAT Std', 'Fwd IAT Std', 'Fwd IAT Min', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd Header Len', 'Bwd Header Len', 'Fwd Pkts/s', 'Pkt Len Min', 'Pkt Len Mean', 'Pkt Len Std', 'SYN Flag Cnt', 'RST Flag Cnt', 'ACK Flag Cnt', 'URG Flag Cnt', 'CWE Flag Count', 'Fwd Seg Size Avg', 'Bwd Seg Size Avg', 'Subflow Fwd Pkts', 'Subflow Bwd Pkts', 'Init Bwd Win Byts', 'Fwd Act Data Pkts', 'Active Mean', 'Active Std', 'Active Max', 'Idle Max', 'Flow ID', 'Src IP', 'Src Port', 'Dst IP']\n[12:49:07] ===== PROCESSING UNION CANDIDATES =====\n[12:49:07] UNION candidate features: 65\n[12:49:07] HLO START on 65 candidate features (pop=15, iters=5)\n[12:54:19]  HLO iter 1/5 current_best=0.9993\n[12:59:12]  HLO iter 2/5 current_best=0.9993\n[13:04:08]  HLO iter 3/5 current_best=0.9993\n[13:09:21]  HLO iter 4/5 current_best=0.9993\n[13:14:28]  HLO iter 5/5 current_best=0.9993\n[13:19:39] HLO DONE in 1832s best_score=0.9993 final_selected=33\n[13:19:39] Hill-climb START over 65 candidates (max_steps=100, eval_cap=500)\n[13:32:56]  Hill-climb step 1: flipped Active Mean -> new_score=0.9994 (evals=40)\n[13:54:25] Hill-climb DONE in 2085s steps=1 evals=105 final_score=0.9994 selected=34\n[13:54:59] Saved trained CatBoost model for union -> hybrid_hlo_models_union_model.pkl (test_f1=0.9993)\n[13:54:59] ===== PROCESSING INTERSECTION CANDIDATES =====\n[13:54:59] INTERSECTION candidate features: 12\n[13:54:59] HLO START on 12 candidate features (pop=15, iters=5)\n[13:58:32]  HLO iter 1/5 current_best=0.9991\n[14:02:05]  HLO iter 2/5 current_best=0.9991\n[14:05:27]  HLO iter 3/5 current_best=0.9991\n[14:08:18]  HLO iter 4/5 current_best=0.9991\n[14:11:13]  HLO iter 5/5 current_best=0.9991\n[14:14:42] HLO DONE in 1182s best_score=0.9991 final_selected=7\n[14:14:42] Hill-climb START over 12 candidates (max_steps=100, eval_cap=500)\n[14:16:17]  Hill-climb step 1: flipped Bwd IAT Min -> new_score=0.9991 (evals=7)\n[14:18:52] Hill-climb DONE in 250s steps=1 evals=19 final_score=0.9991 selected=8\n[14:19:15] Saved trained CatBoost model for intersection -> hybrid_hlo_models_intersection_model.pkl (test_f1=0.9992)\n[14:19:15] ===== PROCESSING VOTING CANDIDATES =====\n[14:19:15] VOTING candidate features: 38\n[14:19:15] HLO START on 38 candidate features (pop=15, iters=5)\n[14:23:43]  HLO iter 1/5 current_best=0.9992\n[14:28:12]  HLO iter 2/5 current_best=0.9993\n[14:32:51]  HLO iter 3/5 current_best=0.9993\n[14:37:16]  HLO iter 4/5 current_best=0.9993\n[14:41:40]  HLO iter 5/5 current_best=0.9993\n[14:46:04] HLO DONE in 1608s best_score=0.9993 final_selected=22\n[14:46:04] Hill-climb START over 38 candidates (max_steps=100, eval_cap=500)\n[14:46:57]  Hill-climb step 1: flipped Active Std -> new_score=0.9993 (evals=3)\n[14:49:02]  Hill-climb step 2: flipped Fwd IAT Std -> new_score=0.9994 (evals=10)\n[15:00:37] Hill-climb DONE in 872s steps=2 evals=48 final_score=0.9994 selected=24\n[15:01:09] Saved trained CatBoost model for voting -> hybrid_hlo_models_voting_model.pkl (test_f1=0.9992)\n==================== AGGREGATE SUMMARY ====================\nPSO  -> opt_score=0.9993 selected=34 time=721s\nGA   -> opt_score=0.9993 selected=37 time=1376s\nGWO  -> opt_score=0.9993 selected=44 time=520s\nUnion candidates    : 65\nIntersection candidates: 12\nVoting candidates   : 38\n-------------------------------------------------\n-- UNION SUMMARY --\n Selected (34): ['Dst Port', 'Protocol', 'Timestamp', 'Tot Fwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Bwd Pkt Len Max', 'Bwd Pkt Len Min', 'Fwd IAT Std', 'Bwd IAT Tot', 'Bwd IAT Mean', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd Header Len', 'Fwd Pkts/s', 'Bwd Pkts/s', 'Pkt Len Mean', 'Pkt Len Std', 'SYN Flag Cnt', 'RST Flag Cnt', 'CWE Flag Count', 'Down/Up Ratio', 'Fwd Seg Size Avg', 'Bwd Seg Size Avg', 'Subflow Fwd Pkts', 'Subflow Bwd Pkts', 'Init Bwd Win Byts', 'Fwd Act Data Pkts', 'Active Mean', 'Active Max', 'Idle Min', 'Src IP', 'Dst IP']\n CV F1   : 0.9993 ± 0.0001\n Test F1 : 0.9993 (n_test=19561)\n Accuracy : 0.9994 ± 0.0001\n Precision: 0.9996 ± 0.0002\n Recall   : 0.9991 ± 0.0002\n Model file: hybrid_hlo_models_union_model.pkl\n-- INTERSECTION SUMMARY --\n Selected (8): ['Dst Port', 'Timestamp', 'Bwd IAT Min', 'Fwd Header Len', 'Fwd Seg Size Avg', 'Subflow Fwd Pkts', 'Active Std', 'Src IP']\n CV F1   : 0.9991 ± 0.0002\n Test F1 : 0.9992 (n_test=19561)\n Accuracy : 0.9992 ± 0.0002\n Precision: 0.9994 ± 0.0002\n Recall   : 0.9988 ± 0.0003\n Model file: hybrid_hlo_models_intersection_model.pkl\n-- VOTING SUMMARY --\n Selected (24): ['Dst Port', 'Timestamp', 'Fwd Pkt Len Mean', 'Bwd Pkt Len Std', 'Flow IAT Mean', 'Fwd IAT Std', 'Fwd IAT Min', 'Bwd IAT Mean', 'Fwd Header Len', 'Bwd Pkts/s', 'Pkt Len Mean', 'SYN Flag Cnt', 'URG Flag Cnt', 'CWE Flag Count', 'ECE Flag Cnt', 'Fwd Seg Size Avg', 'Subflow Fwd Pkts', 'Subflow Fwd Byts', 'Subflow Bwd Pkts', 'Subflow Bwd Byts', 'Active Mean', 'Active Std', 'Flow ID', 'Src Port']\n CV F1   : 0.9993 ± 0.0001\n Test F1 : 0.9992 (n_test=19561)\n Accuracy : 0.9993 ± 0.0001\n Precision: 0.9995 ± 0.0003\n Recall   : 0.9992 ± 0.0003\n Model file: hybrid_hlo_models_voting_model.pkl\n[15:01:09] Saved results to hybrid_hlo_models_results.pkl\n[15:01:09] ===== PIPELINE COMPLETE =====\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"pca","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom catboost import CatBoostClassifier\nimport numpy as np\nimport pandas as pd\ndf = pd.read_csv(\"/kaggle/input/ids-cleaned/ids2018_cleaned_combined_1.csv\")\n\nX = df.drop(\"Label\",axis=1)\ny = df[\"Label\"].astype(int)\n\n# 1) Scale\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# 2) PCA to retain 95% variance\npca = PCA(n_components=0.95)  \nX_pca = pca.fit_transform(X_scaled)\n\nprint(\"Original dimension:\", X.shape[1])\nprint(\"PCA dimension:\", X_pca.shape[1])\n\n# 3) Train/Test split\nX_train, X_test, y_train, y_test = train_test_split(\n    X_pca, y, test_size=0.20, stratify=y, random_state=42\n)\n\n# 4) CatBoost\nmodel = CatBoostClassifier(iterations=500,\n                           learning_rate=0.05,\n                           depth=6,\n                           verbose=0)\n\n# Fit\nmodel.fit(X_train, y_train)\n\n# Test prediction\ny_pred = model.predict(X_test)\n\n# Metrics\ntest_acc = accuracy_score(y_test, y_pred)\ntest_prec = precision_score(y_test, y_pred, zero_division=0)\ntest_rec = recall_score(y_test, y_pred, zero_division=0)\ntest_f1 = f1_score(y_test, y_pred, zero_division=0)\n\nprint(\"\\n=== PCA MODEL RESULTS ===\")\nprint(\"Test Accuracy :\", test_acc)\nprint(\"Precision      :\", test_prec)\nprint(\"Recall         :\", test_rec)\nprint(\"F1 Score       :\", test_f1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T12:31:31.104357Z","iopub.execute_input":"2025-12-08T12:31:31.104894Z","iopub.status.idle":"2025-12-08T12:31:41.247136Z","shell.execute_reply.started":"2025-12-08T12:31:31.104872Z","shell.execute_reply":"2025-12-08T12:31:41.246342Z"}},"outputs":[{"name":"stdout","text":"Original dimension: 75\nPCA dimension: 24\n\n=== PCA MODEL RESULTS ===\nTest Accuracy : 0.9941209549614027\nPrecision      : 0.9963223704949039\nRecall         : 0.9916335494666387\nF1 Score       : 0.9939724304208817\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"Chi sqaure ","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\nfrom catboost import CatBoostClassifier\n\n# -------------------------\n# 1. Prepare data\n# -------------------------\nTARGET_COL = \"Label\"\n\nX = df.drop(TARGET_COL, axis=1)\ny = df[TARGET_COL].astype(int)\n\nprint(\"Original dimension:\", X.shape[1])\n\n# Train-test split (same style as your hybrid pipeline)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.20,\n    stratify=y,\n    random_state=42\n)\n\n# -------------------------\n# 2. Scale to non-negative for chi-square\n# -------------------------\nscaler = MinMaxScaler()   # maps features to [0, 1]\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# -------------------------\n# 3. Try different numbers of selected features\n# -------------------------\nk_values = [5, 10, 15, 20, 30]\n\nresults_chi2 = {}\n\nfor k in k_values:\n    print(\"\\n\" + \"=\"*60)\n    print(f\"CHI-SQUARE + CatBoost with top-{k} features\")\n    print(\"=\"*60)\n\n    # 3.1 Chi-Square feature selection\n    selector = SelectKBest(score_func=chi2, k=k)\n    X_train_k = selector.fit_transform(X_train_scaled, y_train)\n    X_test_k = selector.transform(X_test_scaled)\n\n    # Get selected feature names (from original X)\n    selected_mask = selector.get_support()\n    selected_features = X.columns[selected_mask].tolist()\n    print(f\"Selected {k} features:\")\n    print(selected_features)\n\n    # 3.2 Train CatBoost on selected features\n    model = CatBoostClassifier(\n        iterations=500,\n        learning_rate=0.05,\n        depth=6,\n        verbose=0,\n        random_seed=42\n    )\n\n    model.fit(X_train_k, y_train)\n\n    # 3.3 Evaluate on test set\n    y_pred = model.predict(X_test_k)\n\n    acc = accuracy_score(y_test, y_pred)\n    prec = precision_score(y_test, y_pred, zero_division=0)\n    rec = recall_score(y_test, y_pred, zero_division=0)\n    f1 = f1_score(y_test, y_pred, zero_division=0)\n\n    print(f\"Test Accuracy : {acc:.6f}\")\n    print(f\"Precision     : {prec:.6f}\")\n    print(f\"Recall        : {rec:.6f}\")\n    print(f\"F1 Score      : {f1:.6f}\")\n\n    # store results for later comparison\n    results_chi2[k] = {\n        \"features\": selected_features,\n        \"acc\": acc,\n        \"prec\": prec,\n        \"rec\": rec,\n        \"f1\": f1\n    }\n\nprint(\"\\n=========== SUMMARY: Chi-Square + CatBoost ===========\")\nfor k, info in results_chi2.items():\n    print(f\"Top-{k} features -> F1={info['f1']:.6f}, Acc={info['acc']:.6f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T12:37:02.764186Z","iopub.execute_input":"2025-12-08T12:37:02.764838Z","iopub.status.idle":"2025-12-08T12:37:28.691252Z","shell.execute_reply.started":"2025-12-08T12:37:02.764811Z","shell.execute_reply":"2025-12-08T12:37:28.690497Z"}},"outputs":[{"name":"stdout","text":"Original dimension: 75\n\n============================================================\nCHI-SQUARE + CatBoost with top-5 features\n============================================================\nSelected 5 features:\n['Dst Port', 'Timestamp', 'Bwd Pkts/s', 'Init Bwd Win Byts', 'Fwd Seg Size Min']\nTest Accuracy : 0.999131\nPrecision     : 0.999686\nRecall        : 0.998536\nF1 Score      : 0.999111\n\n============================================================\nCHI-SQUARE + CatBoost with top-10 features\n============================================================\nSelected 10 features:\n['Dst Port', 'Timestamp', 'Bwd Pkt Len Min', 'Flow Pkts/s', 'Bwd IAT Min', 'Bwd Pkts/s', 'ACK Flag Cnt', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Seg Size Min']\nTest Accuracy : 0.999080\nPrecision     : 0.999581\nRecall        : 0.998536\nF1 Score      : 0.999058\n\n============================================================\nCHI-SQUARE + CatBoost with top-15 features\n============================================================\nSelected 15 features:\n['Dst Port', 'Timestamp', 'Fwd Pkt Len Max', 'Bwd Pkt Len Max', 'Bwd Pkt Len Min', 'Bwd Pkt Len Mean', 'Flow Pkts/s', 'Bwd IAT Mean', 'Bwd IAT Min', 'Bwd Pkts/s', 'ACK Flag Cnt', 'Bwd Seg Size Avg', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Seg Size Min']\nTest Accuracy : 0.999233\nPrecision     : 0.999791\nRecall        : 0.998640\nF1 Score      : 0.999215\n\n============================================================\nCHI-SQUARE + CatBoost with top-20 features\n============================================================\nSelected 20 features:\n['Dst Port', 'Protocol', 'Timestamp', 'Fwd Pkt Len Max', 'Bwd Pkt Len Max', 'Bwd Pkt Len Min', 'Bwd Pkt Len Mean', 'Flow Pkts/s', 'Flow IAT Min', 'Bwd IAT Mean', 'Bwd IAT Min', 'Fwd Pkts/s', 'Bwd Pkts/s', 'Pkt Len Max', 'ACK Flag Cnt', 'Bwd Seg Size Avg', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Act Data Pkts', 'Fwd Seg Size Min']\nTest Accuracy : 0.999387\nPrecision     : 0.999895\nRecall        : 0.998850\nF1 Score      : 0.999372\n\n============================================================\nCHI-SQUARE + CatBoost with top-30 features\n============================================================\nSelected 30 features:\n['Dst Port', 'Protocol', 'Timestamp', 'Tot Fwd Pkts', 'TotLen Fwd Pkts', 'Fwd Pkt Len Max', 'Bwd Pkt Len Max', 'Bwd Pkt Len Min', 'Bwd Pkt Len Mean', 'Flow Pkts/s', 'Flow IAT Min', 'Bwd IAT Mean', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd Header Len', 'Fwd Pkts/s', 'Bwd Pkts/s', 'Pkt Len Max', 'Pkt Len Mean', 'Pkt Len Std', 'ACK Flag Cnt', 'Pkt Size Avg', 'Bwd Seg Size Avg', 'Subflow Fwd Pkts', 'Subflow Fwd Byts', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Idle Std']\nTest Accuracy : 0.999233\nPrecision     : 0.999791\nRecall        : 0.998640\nF1 Score      : 0.999215\n\n=========== SUMMARY: Chi-Square + CatBoost ===========\nTop-5 features -> F1=0.999111, Acc=0.999131\nTop-10 features -> F1=0.999058, Acc=0.999080\nTop-15 features -> F1=0.999215, Acc=0.999233\nTop-20 features -> F1=0.999372, Acc=0.999387\nTop-30 features -> F1=0.999215, Acc=0.999233\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"Mutual information","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import SelectKBest, mutual_info_classif\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\nfrom catboost import CatBoostClassifier\n\n# -------------------------\n# 1. Prepare data\n# -------------------------\nTARGET_COL = \"Label\"\n\nX = df.drop(TARGET_COL, axis=1)\ny = df[TARGET_COL].astype(int)\n\nprint(\"Original dimension:\", X.shape[1])\n\n# Train-test split (keep same style as other experiments)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.20,\n    stratify=y,\n    random_state=42\n)\n\n# -------------------------\n# 2. Try different numbers of selected features\n# -------------------------\nk_values = [5, 10, 15, 20, 30]\n\nresults_mi = {}\n\nfor k in k_values:\n    print(\"\\n\" + \"=\"*60)\n    print(f\"MUTUAL INFORMATION + CatBoost with top-{k} features\")\n    print(\"=\"*60)\n\n    # 2.1 Mutual Information feature selection\n    selector = SelectKBest(score_func=mutual_info_classif, k=k)\n    X_train_k = selector.fit_transform(X_train, y_train)\n    X_test_k = selector.transform(X_test)\n\n    # Get selected feature names (from original X)\n    selected_mask = selector.get_support()\n    selected_features = X.columns[selected_mask].tolist()\n    print(f\"Selected {k} features:\")\n    print(selected_features)\n\n    # 2.2 Train CatBoost on selected features\n    model = CatBoostClassifier(\n        iterations=500,\n        learning_rate=0.05,\n        depth=6,\n        verbose=0,\n        random_seed=42\n    )\n\n    model.fit(X_train_k, y_train)\n\n    # 2.3 Evaluate on test set\n    y_pred = model.predict(X_test_k)\n\n    acc = accuracy_score(y_test, y_pred)\n    prec = precision_score(y_test, y_pred, zero_division=0)\n    rec = recall_score(y_test, y_pred, zero_division=0)\n    f1 = f1_score(y_test, y_pred, zero_division=0)\n\n    print(f\"Test Accuracy : {acc:.6f}\")\n    print(f\"Precision     : {prec:.6f}\")\n    print(f\"Recall        : {rec:.6f}\")\n    print(f\"F1 Score      : {f1:.6f}\")\n\n    # store results for later comparison\n    results_mi[k] = {\n        \"features\": selected_features,\n        \"acc\": acc,\n        \"prec\": prec,\n        \"rec\": rec,\n        \"f1\": f1\n    }\n\nprint(\"\\n=========== SUMMARY: Mutual Information + CatBoost ===========\")\nfor k, info in results_mi.items():\n    print(f\"Top-{k} features -> F1={info['f1']:.6f}, Acc={info['acc']:.6f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T12:40:37.309663Z","iopub.execute_input":"2025-12-08T12:40:37.309999Z","iopub.status.idle":"2025-12-08T12:43:23.585166Z","shell.execute_reply.started":"2025-12-08T12:40:37.309976Z","shell.execute_reply":"2025-12-08T12:43:23.584332Z"}},"outputs":[{"name":"stdout","text":"Original dimension: 75\n\n============================================================\nMUTUAL INFORMATION + CatBoost with top-5 features\n============================================================\nSelected 5 features:\n['Dst Port', 'Timestamp', 'Fwd Header Len', 'Pkt Len Max', 'Init Fwd Win Byts']\nTest Accuracy : 0.999284\nPrecision     : 0.999895\nRecall        : 0.998640\nF1 Score      : 0.999267\n\n============================================================\nMUTUAL INFORMATION + CatBoost with top-10 features\n============================================================\nSelected 10 features:\n['Dst Port', 'Timestamp', 'TotLen Fwd Pkts', 'Fwd Header Len', 'Pkt Len Max', 'Pkt Len Mean', 'Pkt Len Std', 'Pkt Len Var', 'Subflow Fwd Byts', 'Init Fwd Win Byts']\nTest Accuracy : 0.999131\nPrecision     : 0.999895\nRecall        : 0.998327\nF1 Score      : 0.999110\n\n============================================================\nMUTUAL INFORMATION + CatBoost with top-15 features\n============================================================\nSelected 15 features:\n['Dst Port', 'Timestamp', 'TotLen Fwd Pkts', 'Fwd Pkt Len Max', 'Flow IAT Max', 'Fwd Header Len', 'Pkt Len Max', 'Pkt Len Mean', 'Pkt Len Std', 'Pkt Len Var', 'Pkt Size Avg', 'Fwd Seg Size Avg', 'Subflow Fwd Byts', 'Subflow Bwd Byts', 'Init Fwd Win Byts']\nTest Accuracy : 0.999029\nPrecision     : 0.999686\nRecall        : 0.998327\nF1 Score      : 0.999006\n\n============================================================\nMUTUAL INFORMATION + CatBoost with top-20 features\n============================================================\nSelected 20 features:\n['Dst Port', 'Timestamp', 'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Mean', 'Bwd Pkt Len Max', 'Bwd Pkt Len Mean', 'Flow IAT Max', 'Fwd Header Len', 'Fwd Pkts/s', 'Pkt Len Max', 'Pkt Len Mean', 'Pkt Len Std', 'Pkt Len Var', 'Pkt Size Avg', 'Fwd Seg Size Avg', 'Subflow Fwd Byts', 'Subflow Bwd Byts', 'Init Fwd Win Byts']\nTest Accuracy : 0.999182\nPrecision     : 0.999895\nRecall        : 0.998431\nF1 Score      : 0.999163\n\n============================================================\nMUTUAL INFORMATION + CatBoost with top-30 features\n============================================================\nSelected 30 features:\n['Dst Port', 'Timestamp', 'Flow Duration', 'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Mean', 'Bwd Pkt Len Max', 'Bwd Pkt Len Mean', 'Flow Pkts/s', 'Flow IAT Mean', 'Flow IAT Max', 'Fwd IAT Mean', 'Fwd IAT Max', 'Fwd Header Len', 'Bwd Header Len', 'Fwd Pkts/s', 'Bwd Pkts/s', 'Pkt Len Max', 'Pkt Len Mean', 'Pkt Len Std', 'Pkt Len Var', 'Pkt Size Avg', 'Fwd Seg Size Avg', 'Bwd Seg Size Avg', 'Subflow Fwd Byts', 'Subflow Bwd Byts', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Seg Size Min']\nTest Accuracy : 0.999182\nPrecision     : 0.999791\nRecall        : 0.998536\nF1 Score      : 0.999163\n\n=========== SUMMARY: Mutual Information + CatBoost ===========\nTop-5 features -> F1=0.999267, Acc=0.999284\nTop-10 features -> F1=0.999110, Acc=0.999131\nTop-15 features -> F1=0.999006, Acc=0.999029\nTop-20 features -> F1=0.999163, Acc=0.999182\nTop-30 features -> F1=0.999163, Acc=0.999182\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"Recursive feature elimination","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import RFE\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\nfrom catboost import CatBoostClassifier\n\n# -------------------------\n# 1. Prepare data\n# -------------------------\nTARGET_COL = \"Label\"\n\nX = df.drop(TARGET_COL, axis=1)\ny = df[TARGET_COL].astype(int)\n\nprint(\"Original dimension:\", X.shape[1])\n\n# Train-test split (same style as other baselines)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.20,\n    stratify=y,\n    random_state=42\n)\n\n# -------------------------\n# 2. Try different numbers of selected features with RFE\n# -------------------------\nk_values = [5, 10, 15, 20, 30]\n\nresults_rfe = {}\n\nfor k in k_values:\n    print(\"\\n\" + \"=\"*60)\n    print(f\"RFE (CatBoost) with top-{k} features\")\n    print(\"=\"*60)\n\n    # Base estimator for RFE (reduced iterations to save time)\n    base_model = CatBoostClassifier(\n        iterations=200,\n        learning_rate=0.05,\n        depth=6,\n        verbose=0,\n        random_seed=42\n    )\n\n    # 2.1 RFE setup\n    selector = RFE(\n        estimator=base_model,\n        n_features_to_select=k,\n        step=1\n    )\n\n    # Fit RFE on training data\n    selector.fit(X_train, y_train)\n\n    # Transform train and test sets\n    X_train_k = selector.transform(X_train)\n    X_test_k = selector.transform(X_test)\n\n    # Get selected feature names\n    selected_mask = selector.get_support()\n    selected_features = X.columns[selected_mask].tolist()\n    print(f\"Selected {k} features:\")\n    print(selected_features)\n\n    # 2.2 Train a fresh CatBoost on the selected features (for fair comparison)\n    model = CatBoostClassifier(\n        iterations=500,\n        learning_rate=0.05,\n        depth=6,\n        verbose=0,\n        random_seed=42\n    )\n\n    model.fit(X_train_k, y_train)\n\n    # 2.3 Evaluate on test set\n    y_pred = model.predict(X_test_k)\n\n    acc = accuracy_score(y_test, y_pred)\n    prec = precision_score(y_test, y_pred, zero_division=0)\n    rec = recall_score(y_test, y_pred, zero_division=0)\n    f1 = f1_score(y_test, y_pred, zero_division=0)\n\n    print(f\"Test Accuracy : {acc:.6f}\")\n    print(f\"Precision     : {prec:.6f}\")\n    print(f\"Recall        : {rec:.6f}\")\n    print(f\"F1 Score      : {f1:.6f}\")\n\n    # store results for later comparison\n    results_rfe[k] = {\n        \"features\": selected_features,\n        \"acc\": acc,\n        \"prec\": prec,\n        \"rec\": rec,\n        \"f1\": f1\n    }\n\nprint(\"\\n=========== SUMMARY: RFE (CatBoost) ===========\")\nfor k, info in results_rfe.items():\n    print(f\"Top-{k} features -> F1={info['f1']:.6f}, Acc={info['acc']:.6f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T12:46:39.564016Z","iopub.execute_input":"2025-12-08T12:46:39.564363Z","iopub.status.idle":"2025-12-08T13:03:15.497596Z","shell.execute_reply.started":"2025-12-08T12:46:39.564340Z","shell.execute_reply":"2025-12-08T13:03:15.496920Z"}},"outputs":[{"name":"stdout","text":"Original dimension: 75\n\n============================================================\nRFE (CatBoost) with top-5 features\n============================================================\nSelected 5 features:\n['Dst Port', 'Timestamp', 'Flow IAT Mean', 'Fwd IAT Mean', 'Init Bwd Win Byts']\nTest Accuracy : 0.999335\nPrecision     : 0.999686\nRecall        : 0.998954\nF1 Score      : 0.999320\n\n============================================================\nRFE (CatBoost) with top-10 features\n============================================================\nSelected 10 features:\n['Dst Port', 'Timestamp', 'Bwd Pkt Len Min', 'Flow Pkts/s', 'Flow IAT Mean', 'Fwd IAT Mean', 'Bwd Seg Size Avg', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Seg Size Min']\nTest Accuracy : 0.999029\nPrecision     : 0.999477\nRecall        : 0.998536\nF1 Score      : 0.999006\n\n============================================================\nRFE (CatBoost) with top-15 features\n============================================================\nSelected 15 features:\n['Dst Port', 'Timestamp', 'Flow Duration', 'Bwd Pkt Len Min', 'Flow Pkts/s', 'Flow IAT Mean', 'Flow IAT Min', 'Fwd IAT Tot', 'Fwd IAT Mean', 'Fwd Pkts/s', 'ECE Flag Cnt', 'Bwd Seg Size Avg', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Seg Size Min']\nTest Accuracy : 0.999131\nPrecision     : 0.999477\nRecall        : 0.998745\nF1 Score      : 0.999111\n\n============================================================\nRFE (CatBoost) with top-20 features\n============================================================\nSelected 20 features:\n['Dst Port', 'Timestamp', 'Flow Duration', 'Fwd Pkt Len Mean', 'Bwd Pkt Len Min', 'Flow Pkts/s', 'Flow IAT Mean', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Tot', 'Fwd IAT Mean', 'Fwd IAT Min', 'Fwd Pkts/s', 'RST Flag Cnt', 'ECE Flag Cnt', 'Pkt Size Avg', 'Bwd Seg Size Avg', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Seg Size Min']\nTest Accuracy : 0.999233\nPrecision     : 0.999791\nRecall        : 0.998640\nF1 Score      : 0.999215\n\n============================================================\nRFE (CatBoost) with top-30 features\n============================================================\nSelected 30 features:\n['Dst Port', 'Timestamp', 'Flow Duration', 'Tot Fwd Pkts', 'Fwd Pkt Len Mean', 'Bwd Pkt Len Min', 'Bwd Pkt Len Mean', 'Flow Byts/s', 'Flow Pkts/s', 'Flow IAT Mean', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Tot', 'Fwd IAT Mean', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Min', 'Fwd Pkts/s', 'Bwd Pkts/s', 'Pkt Len Std', 'RST Flag Cnt', 'ECE Flag Cnt', 'Pkt Size Avg', 'Fwd Seg Size Avg', 'Bwd Seg Size Avg', 'Subflow Fwd Pkts', 'Subflow Fwd Byts', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Seg Size Min']\nTest Accuracy : 0.999233\nPrecision     : 0.999791\nRecall        : 0.998640\nF1 Score      : 0.999215\n\n=========== SUMMARY: RFE (CatBoost) ===========\nTop-5 features -> F1=0.999320, Acc=0.999335\nTop-10 features -> F1=0.999006, Acc=0.999029\nTop-15 features -> F1=0.999111, Acc=0.999131\nTop-20 features -> F1=0.999215, Acc=0.999233\nTop-30 features -> F1=0.999215, Acc=0.999233\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom catboost import CatBoostClassifier\nimport numpy as np\nimport pandas as pd\n\n# Load your dataset\ndf = pd.read_csv(\"/kaggle/input/ids-cleaned/ids2018_cleaned_combined_1.csv\")\n\nX = df.drop(\"Label\",axis=1)\ny = df[\"Label\"].astype(int)\n\nfeature_names = X.columns.tolist()\nnum_features = X.shape[1]\n\n# Train RF to get feature importance (100 trees is enough)\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\nrf.fit(X, y)\n\n# Importance list (sort descending)\nimportances = rf.feature_importances_\nrank_idx = np.argsort(importances)[::-1]  # descending order\nsorted_features = [feature_names[i] for i in rank_idx]\n\nprint(\"Original dimension:\", num_features)\n\n# Evaluate CatBoost using varying top-k selected features\nK_values = [5, 10, 15, 20, 30]\nresults = {}\n\nprint(\"\\n============================================================\")\nprint(\"RANDOM FOREST IMPORTANCE + CATBOOST BASELINE\")\nprint(\"============================================================\")\n\nfor k in K_values:\n    top_k_features = sorted_features[:k]\n    X_sel = X[top_k_features]\n\n    X_train, X_test, y_train, y_test = train_test_split(\n        X_sel, y, test_size=0.20, stratify=y, random_state=42\n    )\n\n    model = CatBoostClassifier(iterations=500, learning_rate=0.05, depth=6, verbose=0)\n    model.fit(X_train, y_train)\n\n    y_pred = model.predict(X_test)\n\n    test_acc = accuracy_score(y_test, y_pred)\n    test_prec = precision_score(y_test, y_pred, zero_division=0)\n    test_rec = recall_score(y_test, y_pred, zero_division=0)\n    test_f1 = f1_score(y_test, y_pred, zero_division=0)\n\n    print(f\"\\n============================================================\")\n    print(f\"RF Importance + CatBoost with top-{k} features\")\n    print(\"============================================================\")\n    print(\"Selected\", k, \"features:\")\n    print(top_k_features)\n    print(f\"Test Accuracy : {test_acc:.8f}\")\n    print(f\"Precision     : {test_prec:.8f}\")\n    print(f\"Recall        : {test_rec:.8f}\")\n    print(f\"F1 Score      : {test_f1:.8f}\")\n\n    results[k] = test_f1\n\nprint(\"\\n=========== SUMMARY: Random Forest Importance + CatBoost ===========\")\nfor k in K_values:\n    print(f\"Top-{k} features -> F1={results[k]:.8f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T13:10:43.427857Z","iopub.execute_input":"2025-12-08T13:10:43.428439Z","iopub.status.idle":"2025-12-08T13:11:23.181824Z","shell.execute_reply.started":"2025-12-08T13:10:43.428413Z","shell.execute_reply":"2025-12-08T13:11:23.181205Z"}},"outputs":[{"name":"stdout","text":"Original dimension: 75\n\n============================================================\nRANDOM FOREST IMPORTANCE + CATBOOST BASELINE\n============================================================\n\n============================================================\nRF Importance + CatBoost with top-5 features\n============================================================\nSelected 5 features:\n['Timestamp', 'Fwd Seg Size Min', 'Dst Port', 'Init Fwd Win Byts', 'Init Bwd Win Byts']\nTest Accuracy : 0.99913092\nPrecision     : 0.99958128\nRecall        : 0.99864045\nF1 Score      : 0.99911065\n\n============================================================\nRF Importance + CatBoost with top-10 features\n============================================================\nSelected 10 features:\n['Timestamp', 'Fwd Seg Size Min', 'Dst Port', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Pkt Len Max', 'TotLen Fwd Pkts', 'Fwd Header Len', 'Bwd Pkt Len Mean', 'Fwd Pkt Len Mean']\nTest Accuracy : 0.99918205\nPrecision     : 0.99989527\nRecall        : 0.99843129\nF1 Score      : 0.99916274\n\n============================================================\nRF Importance + CatBoost with top-15 features\n============================================================\nSelected 15 features:\n['Timestamp', 'Fwd Seg Size Min', 'Dst Port', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Pkt Len Max', 'TotLen Fwd Pkts', 'Fwd Header Len', 'Bwd Pkt Len Mean', 'Fwd Pkt Len Mean', 'Fwd Pkts/s', 'Pkt Len Max', 'Bwd Seg Size Avg', 'Flow IAT Min', 'Flow Pkts/s']\nTest Accuracy : 0.99913092\nPrecision     : 0.99947671\nRecall        : 0.99874503\nF1 Score      : 0.99911074\n\n============================================================\nRF Importance + CatBoost with top-20 features\n============================================================\nSelected 20 features:\n['Timestamp', 'Fwd Seg Size Min', 'Dst Port', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Pkt Len Max', 'TotLen Fwd Pkts', 'Fwd Header Len', 'Bwd Pkt Len Mean', 'Fwd Pkt Len Mean', 'Fwd Pkts/s', 'Pkt Len Max', 'Bwd Seg Size Avg', 'Flow IAT Min', 'Flow Pkts/s', 'Bwd Pkt Len Max', 'Fwd IAT Mean', 'Flow IAT Mean', 'Fwd IAT Tot', 'Flow Duration']\nTest Accuracy : 0.99928429\nPrecision     : 0.99968600\nRecall        : 0.99884961\nF1 Score      : 0.99926763\n\n============================================================\nRF Importance + CatBoost with top-30 features\n============================================================\nSelected 30 features:\n['Timestamp', 'Fwd Seg Size Min', 'Dst Port', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Pkt Len Max', 'TotLen Fwd Pkts', 'Fwd Header Len', 'Bwd Pkt Len Mean', 'Fwd Pkt Len Mean', 'Fwd Pkts/s', 'Pkt Len Max', 'Bwd Seg Size Avg', 'Flow IAT Min', 'Flow Pkts/s', 'Bwd Pkt Len Max', 'Fwd IAT Mean', 'Flow IAT Mean', 'Fwd IAT Tot', 'Flow Duration', 'Pkt Len Std', 'Subflow Fwd Byts', 'RST Flag Cnt', 'Fwd IAT Max', 'TotLen Bwd Pkts', 'Pkt Len Mean', 'Pkt Len Var', 'Fwd IAT Min', 'Bwd Pkts/s', 'Flow IAT Max']\nTest Accuracy : 0.99928429\nPrecision     : 0.99979062\nRecall        : 0.99874503\nF1 Score      : 0.99926755\n\n=========== SUMMARY: Random Forest Importance + CatBoost ===========\nTop-5 features -> F1=0.99911065\nTop-10 features -> F1=0.99916274\nTop-15 features -> F1=0.99911074\nTop-20 features -> F1=0.99926763\nTop-30 features -> F1=0.99926755\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"L1 Lasso Feature Selection + CatBoost","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom catboost import CatBoostClassifier\nimport numpy as np\nimport pandas as pd\n\n# Load dataset\ndf = pd.read_csv(\"/kaggle/input/ids-cleaned/ids2018_cleaned_combined_1.csv\")\n\nX = df.drop(\"Label\", axis=1)\ny = df[\"Label\"].astype(int)\nfeature_names = X.columns.tolist()\n\nprint(\"Original dimension:\", X.shape[1])\n\n# Train Logistic regression with L1 penalty to select features\nlog_reg = LogisticRegression(penalty='l1', solver='liblinear', C=0.3, max_iter=2000)\nlog_reg.fit(X, y)\n\n# Get non-zero coefficients\ncoef = log_reg.coef_[0]\nselected_idx = np.where(coef != 0)[0]\nselected_features = [feature_names[i] for i in selected_idx]\n\nprint(\"\\nSelected features using L1 (Lasso):\")\nprint(selected_features)\nprint(\"Total selected:\", len(selected_features))\n\n# ----- CatBoost on selected features -----\n\nX_sel = X[selected_features]\nX_train, X_test, y_train, y_test = train_test_split(\n    X_sel, y, test_size=0.20, stratify=y, random_state=42\n)\n\nmodel = CatBoostClassifier(iterations=500, learning_rate=0.05, depth=6, verbose=0)\nmodel.fit(X_train, y_train)\n\n# Predictions\ny_pred = model.predict(X_test)\n\ntest_acc = accuracy_score(y_test, y_pred)\ntest_prec = precision_score(y_test, y_pred, zero_division=0)\ntest_rec = recall_score(y_test, y_pred, zero_division=0)\ntest_f1 = f1_score(y_test, y_pred, zero_division=0)\n\nprint(\"\\n===== L1 (Lasso) + CatBoost Results =====\")\nprint(f\"Test Accuracy : {test_acc:.8f}\")\nprint(f\"Precision     : {test_prec:.8f}\")\nprint(f\"Recall        : {test_rec:.8f}\")\nprint(f\"F1 Score      : {test_f1:.8f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T13:17:35.755144Z","iopub.execute_input":"2025-12-08T13:17:35.755825Z","iopub.status.idle":"2025-12-08T13:18:13.922277Z","shell.execute_reply.started":"2025-12-08T13:17:35.755791Z","shell.execute_reply":"2025-12-08T13:18:13.921484Z"}},"outputs":[{"name":"stdout","text":"Original dimension: 75\n\nSelected features using L1 (Lasso):\n['Dst Port', 'Protocol', 'Timestamp', 'Flow Duration', 'TotLen Fwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std', 'Bwd Pkt Len Max', 'Bwd Pkt Len Min', 'Bwd Pkt Len Mean', 'Flow Byts/s', 'Flow IAT Mean', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Tot', 'Bwd IAT Tot', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Fwd Pkts/s', 'Bwd Pkts/s', 'Pkt Len Min', 'Pkt Len Std', 'Pkt Len Var', 'FIN Flag Cnt', 'SYN Flag Cnt', 'RST Flag Cnt', 'PSH Flag Cnt', 'ACK Flag Cnt', 'URG Flag Cnt', 'ECE Flag Cnt', 'Pkt Size Avg', 'Fwd Seg Size Avg', 'Bwd Seg Size Avg', 'Subflow Fwd Byts', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Seg Size Min', 'Active Max', 'Idle Max', 'Flow ID', 'Src IP', 'Src Port']\nTotal selected: 44\n\n===== L1 (Lasso) + CatBoost Results =====\nTest Accuracy : 0.99928429\nPrecision     : 0.99989529\nRecall        : 0.99864045\nF1 Score      : 0.99926748\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\nfrom catboost import CatBoostClassifier\n\n# -------------------------\n# 1. Load data\n# -------------------------\ndf = pd.read_csv(\"/kaggle/input/ids-cleaned/ids2018_cleaned_combined_1.csv\")\n\nTARGET_COL = \"Label\"\n\nX = df.drop(TARGET_COL, axis=1)\ny = df[TARGET_COL].astype(int)\n\nfeature_names = X.columns.tolist()\nprint(\"Original dimension:\", X.shape[1])\n\n# -------------------------\n# 2. Train CatBoost on ALL features to get importance\n# -------------------------\nbase_model = CatBoostClassifier(\n    iterations=500,\n    learning_rate=0.05,\n    depth=6,\n    verbose=0,\n    random_seed=42\n)\n\nbase_model.fit(X, y)\n\n# Get feature importances from CatBoost\nimportances = base_model.get_feature_importance()\n# Sort indices by importance (descending)\nsorted_idx = np.argsort(importances)[::-1]\nsorted_features = [feature_names[i] for i in sorted_idx]\n\nprint(\"\\nTop 20 features by CatBoost importance:\")\nfor i in range(20):\n    print(f\"{i+1:2d}. {sorted_features[i]}  (importance={importances[sorted_idx[i]]:.6f})\")\n\n# -------------------------\n# 3. Evaluate CatBoost using top-k important features\n# -------------------------\nK_values = [5, 10, 15, 20, 30]\nresults_cb_imp = {}\n\nprint(\"\\n============================================================\")\nprint(\"CATBOOST FEATURE IMPORTANCE + CATBOOST BASELINE\")\nprint(\"============================================================\")\n\nfor k in K_values:\n    top_k_features = sorted_features[:k]\n    X_sel = X[top_k_features]\n\n    # Same train-test strategy as other baselines\n    X_train, X_test, y_train, y_test = train_test_split(\n        X_sel, y,\n        test_size=0.20,\n        stratify=y,\n        random_state=42\n    )\n\n    model = CatBoostClassifier(\n        iterations=500,\n        learning_rate=0.05,\n        depth=6,\n        verbose=0,\n        random_seed=42\n    )\n\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n\n    acc = accuracy_score(y_test, y_pred)\n    prec = precision_score(y_test, y_pred, zero_division=0)\n    rec = recall_score(y_test, y_pred, zero_division=0)\n    f1 = f1_score(y_test, y_pred, zero_division=0)\n\n    print(f\"\\n============================================================\")\n    print(f\"CatBoost-Importance + CatBoost with top-{k} features\")\n    print(\"============================================================\")\n    print(f\"Selected {k} features:\")\n    print(top_k_features)\n    print(f\"Test Accuracy : {acc:.8f}\")\n    print(f\"Precision     : {prec:.8f}\")\n    print(f\"Recall        : {rec:.8f}\")\n    print(f\"F1 Score      : {f1:.8f}\")\n\n    results_cb_imp[k] = {\n        \"features\": top_k_features,\n        \"acc\": acc,\n        \"prec\": prec,\n        \"rec\": rec,\n        \"f1\": f1\n    }\n\nprint(\"\\n=========== SUMMARY: CatBoost Importance + CatBoost ===========\")\nfor k, info in results_cb_imp.items():\n    print(f\"Top-{k} features -> F1={info['f1']:.8f}, Acc={info['acc']:.8f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T13:20:56.826735Z","iopub.execute_input":"2025-12-08T13:20:56.827620Z","iopub.status.idle":"2025-12-08T13:21:32.771887Z","shell.execute_reply.started":"2025-12-08T13:20:56.827591Z","shell.execute_reply":"2025-12-08T13:21:32.771211Z"}},"outputs":[{"name":"stdout","text":"Original dimension: 75\n\nTop 20 features by CatBoost importance:\n 1. Timestamp  (importance=71.969810)\n 2. Dst Port  (importance=16.404052)\n 3. Flow IAT Min  (importance=1.128789)\n 4. Init Fwd Win Byts  (importance=0.973730)\n 5. Bwd Pkt Len Min  (importance=0.850694)\n 6. Fwd Seg Size Min  (importance=0.802357)\n 7. Fwd Pkts/s  (importance=0.709243)\n 8. Fwd IAT Min  (importance=0.548268)\n 9. RST Flag Cnt  (importance=0.462437)\n10. Flow Pkts/s  (importance=0.458003)\n11. Flow IAT Mean  (importance=0.453412)\n12. Init Bwd Win Byts  (importance=0.393335)\n13. Fwd IAT Mean  (importance=0.331477)\n14. Fwd IAT Max  (importance=0.300318)\n15. Bwd Pkt Len Mean  (importance=0.293593)\n16. Bwd Pkt Len Max  (importance=0.284212)\n17. Bwd Seg Size Avg  (importance=0.273176)\n18. Fwd IAT Tot  (importance=0.258784)\n19. ECE Flag Cnt  (importance=0.247212)\n20. Flow IAT Max  (importance=0.243708)\n\n============================================================\nCATBOOST FEATURE IMPORTANCE + CATBOOST BASELINE\n============================================================\n\n============================================================\nCatBoost-Importance + CatBoost with top-5 features\n============================================================\nSelected 5 features:\n['Timestamp', 'Dst Port', 'Flow IAT Min', 'Init Fwd Win Byts', 'Bwd Pkt Len Min']\nTest Accuracy : 0.99918205\nPrecision     : 0.99947677\nRecall        : 0.99884961\nF1 Score      : 0.99916309\n\n============================================================\nCatBoost-Importance + CatBoost with top-10 features\n============================================================\nSelected 10 features:\n['Timestamp', 'Dst Port', 'Flow IAT Min', 'Init Fwd Win Byts', 'Bwd Pkt Len Min', 'Fwd Seg Size Min', 'Fwd Pkts/s', 'Fwd IAT Min', 'RST Flag Cnt', 'Flow Pkts/s']\nTest Accuracy : 0.99918205\nPrecision     : 0.99947677\nRecall        : 0.99884961\nF1 Score      : 0.99916309\n\n============================================================\nCatBoost-Importance + CatBoost with top-15 features\n============================================================\nSelected 15 features:\n['Timestamp', 'Dst Port', 'Flow IAT Min', 'Init Fwd Win Byts', 'Bwd Pkt Len Min', 'Fwd Seg Size Min', 'Fwd Pkts/s', 'Fwd IAT Min', 'RST Flag Cnt', 'Flow Pkts/s', 'Flow IAT Mean', 'Init Bwd Win Byts', 'Fwd IAT Mean', 'Fwd IAT Max', 'Bwd Pkt Len Mean']\nTest Accuracy : 0.99913092\nPrecision     : 0.99947671\nRecall        : 0.99874503\nF1 Score      : 0.99911074\n\n============================================================\nCatBoost-Importance + CatBoost with top-20 features\n============================================================\nSelected 20 features:\n['Timestamp', 'Dst Port', 'Flow IAT Min', 'Init Fwd Win Byts', 'Bwd Pkt Len Min', 'Fwd Seg Size Min', 'Fwd Pkts/s', 'Fwd IAT Min', 'RST Flag Cnt', 'Flow Pkts/s', 'Flow IAT Mean', 'Init Bwd Win Byts', 'Fwd IAT Mean', 'Fwd IAT Max', 'Bwd Pkt Len Mean', 'Bwd Pkt Len Max', 'Bwd Seg Size Avg', 'Fwd IAT Tot', 'ECE Flag Cnt', 'Flow IAT Max']\nTest Accuracy : 0.99907980\nPrecision     : 0.99937212\nRecall        : 0.99874503\nF1 Score      : 0.99905848\n\n============================================================\nCatBoost-Importance + CatBoost with top-30 features\n============================================================\nSelected 30 features:\n['Timestamp', 'Dst Port', 'Flow IAT Min', 'Init Fwd Win Byts', 'Bwd Pkt Len Min', 'Fwd Seg Size Min', 'Fwd Pkts/s', 'Fwd IAT Min', 'RST Flag Cnt', 'Flow Pkts/s', 'Flow IAT Mean', 'Init Bwd Win Byts', 'Fwd IAT Mean', 'Fwd IAT Max', 'Bwd Pkt Len Mean', 'Bwd Pkt Len Max', 'Bwd Seg Size Avg', 'Fwd IAT Tot', 'ECE Flag Cnt', 'Flow IAT Max', 'Flow Byts/s', 'Fwd Header Len', 'Bwd Pkts/s', 'Flow Duration', 'Pkt Size Avg', 'TotLen Bwd Pkts', 'Bwd IAT Min', 'PSH Flag Cnt', 'Pkt Len Std', 'ACK Flag Cnt']\nTest Accuracy : 0.99928429\nPrecision     : 0.99979062\nRecall        : 0.99874503\nF1 Score      : 0.99926755\n\n=========== SUMMARY: CatBoost Importance + CatBoost ===========\nTop-5 features -> F1=0.99916309, Acc=0.99918205\nTop-10 features -> F1=0.99916309, Acc=0.99918205\nTop-15 features -> F1=0.99911074, Acc=0.99913092\nTop-20 features -> F1=0.99905848, Acc=0.99907980\nTop-30 features -> F1=0.99926755, Acc=0.99928429\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"#VOTING FOR 20 ITERATIONS\n\n\n# hybrid_voting_hlo_ddos_pipeline.py\n# Single-file: PSO + GA + GWO -> VOTING -> HLO -> Hill-climb -> Final CatBoost\n# Option A: optimization subset = 3000 rows (1500 benign + 1500 attack)\n\nimport kagglehub\nimport glob, os, time, pickle, warnings\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\nfrom sklearn.model_selection import StratifiedKFold, train_test_split, cross_val_score\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, make_scorer\nfrom sklearn.base import clone\nfrom catboost import CatBoostClassifier\n\nwarnings.filterwarnings(\"ignore\")\nnp.random.seed(42)\n\n# -----------------------\n# USER CONFIG\n# -----------------------\nDATA_PATH = kagglehub.dataset_download(\"sizlingdhairya1/cicddos2019\")  # your loader\nOPT_SUBSET_PER_CLASS = 1500     # 1500 benign + 1500 attack => 3000 rows\nPSO_SWARM = 8\nPSO_ITERS = 20\nGA_POP = 12\nGA_GENS = 20\nGWO_WOLVES = 8\nGWO_ITERS = 20\nHLO_POP = 8\nHLO_ITERS = 10\nFIT_CB_ITERS_OPT = 80    # CatBoost iterations used inside fitness (fast)\nCV_OPT = 2               # cheap CV inside optimizer\nFINAL_CB_ITERS = 1000    # final model iterations (early stopping used)\nFINAL_EARLY_STOP = 50\nSAVE_PREFIX = \"ddos_hybrid_voting_hlo\"\nLEAKAGE_SINGLE_FEATURE_THRESHOLD = 0.99999  # single-feature accuracy threshold to treat as leakage\n\nprint(\"DATA_PATH:\", DATA_PATH)\n\n# -----------------------\n# 1) load all CSVs from dataset path\n# -----------------------\ncsv_files = glob.glob(os.path.join(DATA_PATH, \"*.csv\"))\nif len(csv_files) == 0:\n    raise RuntimeError(\"No CSV files found in DATA_PATH: \" + DATA_PATH)\n\nprint(f\"Found {len(csv_files)} CSV files. Loading & merging (may take a bit)...\")\ndfs = []\nfor f in csv_files:\n    print(\" ->\", os.path.basename(f))\n    dfs.append(pd.read_csv(f, low_memory=False))\ndf = pd.concat(dfs, ignore_index=True)\nprint(\"Merged dataset shape:\", df.shape)\n\n# -----------------------\n# 2) normalize columns and find label\n# -----------------------\ndf.columns = df.columns.str.strip()\nTARGET_CANDIDATES = [\"Label\", \"label\", \" Attack\", \"attack_cat\", \"Label \"]\nfound_label = None\nfor c in [\"Label\", \"label\", \"Attack\", \"attack\", \"attack_cat\"]:\n    if c in df.columns:\n        found_label = c\n        break\nif found_label is None:\n    # try case-insensitive lookup\n    for c in df.columns:\n        if c.strip().lower() == \"label\" or c.strip().lower() == \"attack\":\n            found_label = c\n            break\nif found_label is None:\n    raise RuntimeError(\"Cannot find label column. Columns available: \" + \", \".join(df.columns[:30]))\n\n# normalize label column name\ndf.rename(columns={found_label: \"Label\"}, inplace=True)\nprint(\"Using target column 'Label' (original: {})\".format(found_label))\n\n# -----------------------\n# 3) keep only rows with non-null label and make binary target\n# -----------------------\ndf = df[df[\"Label\"].notna()].copy()\ndf[\"Label\"] = df[\"Label\"].astype(str).str.strip().str.lower()\n# convert benign -> 0 else -> 1 (attack)\ndf[\"Label\"] = df[\"Label\"].apply(lambda x: 0 if x == \"benign\" else 1)\nprint(\"Label counts (full):\\n\", df[\"Label\"].value_counts())\n\n# -----------------------\n# 4) drop obviously leaking columns if present (IDs, IPs, timestamps)\n# -----------------------\npossible_leak_cols = [c for c in df.columns if c.strip().lower() in (\n    \"id\", \"flow id\", \"flowid\", \"timestamp\", \"ts\", \"source ip\", \"destination ip\",\n    \"src ip\", \"dst ip\", \"sourceip\", \"destinationip\", \"srcip\", \"dstip\")]\nif possible_leak_cols:\n    print(\"Dropping likely-leakage columns (ids/timestamps/ips):\", possible_leak_cols)\n    df.drop(columns=[c for c in possible_leak_cols if c in df.columns], inplace=True)\n\n# -----------------------\n# 5) basic cleaning: drop all-empty columns, replace inf, drop rows with NaN\n# -----------------------\ndf.replace([np.inf, -np.inf], np.nan, inplace=True)\ndf.dropna(axis=1, how=\"all\", inplace=True)\ndf.dropna(axis=0, how=\"any\", inplace=True)   # safe because we'll use full dataset for final, but optimizer needs no missing\nprint(\"After basic cleaning:\", df.shape)\n\n# -----------------------\n# 6) create balanced small subset for optimization: OPT_SUBSET_PER_CLASS * 2 rows\n# -----------------------\ncounts = df[\"Label\"].value_counts().to_dict()\nn_attack = counts.get(1, 0)\nn_benign = counts.get(0, 0)\ntake_attack = min(OPT_SUBSET_PER_CLASS, n_attack)\ntake_benign = min(OPT_SUBSET_PER_CLASS, n_benign)\nif take_attack < 10 or take_benign < 10:\n    raise RuntimeError(\"Not enough rows in one class to form the optimization subset. counts=\" + str(counts))\n\ndf_attack = df[df[\"Label\"] == 1].sample(take_attack, random_state=42)\ndf_benign = df[df[\"Label\"] == 0].sample(take_benign, random_state=42)\ndf_sub = pd.concat([df_attack, df_benign], ignore_index=True).sample(frac=1.0, random_state=42).reset_index(drop=True)\nprint(\"Optimization subset shape:\", df_sub.shape, \"Label counts:\", df_sub[\"Label\"].value_counts().to_dict())\n\n# -----------------------\n# 7) preprocess subset: encode categorical & scale numeric\n# -----------------------\nTARGET_COL = \"Label\"\nX_sub = df_sub.drop(columns=[TARGET_COL]).copy()\ny_sub = df_sub[TARGET_COL].astype(int).copy()\n\n# encode object columns\nobj_cols = X_sub.select_dtypes(include=[\"object\"]).columns.tolist()\nfor c in obj_cols:\n    X_sub[c] = LabelEncoder().fit_transform(X_sub[c].astype(str))\n# numeric scaling\nnum_cols_sub = X_sub.select_dtypes(include=[np.number]).columns.tolist()\nif len(num_cols_sub) > 0:\n    X_sub[num_cols_sub] = MinMaxScaler().fit_transform(X_sub[num_cols_sub])\n\nFEATURE_NAMES = X_sub.columns.tolist()\nN_FEATURES = len(FEATURE_NAMES)\nprint(\"Subset features:\", N_FEATURES)\n\n# -----------------------\n# 8) CatBoost factory & fitness function with caching\n# -----------------------\ndef get_catboost_model(iterations=FIT_CB_ITERS_OPT):\n    return CatBoostClassifier(iterations=iterations, learning_rate=0.05, depth=6,\n                              verbose=0, random_seed=42)\n\nfitness_cache = {}\ndef evaluate_mask(mask_bool, cv=CV_OPT, cb_iter=FIT_CB_ITERS_OPT):\n    key = tuple(int(x) for x in mask_bool)\n    if key in fitness_cache:\n        return fitness_cache[key]\n    idxs = [i for i,b in enumerate(key) if b==1]\n    if len(idxs) == 0:\n        fitness_cache[key] = 0.0\n        return 0.0\n    Xsel = X_sub.iloc[:, idxs]\n    model = get_catboost_model(iterations=cb_iter)\n    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=42)\n    try:\n        scores = cross_val_score(clone(model), Xsel, y_sub, cv=skf, scoring=make_scorer(f1_score), n_jobs=-1)\n    except Exception as e:\n        # if CatBoost fails (e.g. unexpected types), return 0\n        fitness_cache[key] = 0.0\n        return 0.0\n    val = float(np.mean(scores))\n    fitness_cache[key] = val\n    return val\n\n# -----------------------\n# 9) PSO (binary) - reduced\n# -----------------------\ndef run_pso(swarm_size=PSO_SWARM, iters=PSO_ITERS):\n    print(\"[PSO] start: swarm\", swarm_size, \"iters\", iters)\n    dim = N_FEATURES\n    pos = np.random.randint(0,2,(swarm_size,dim))\n    vel = np.random.uniform(-1,1,(swarm_size,dim))\n    pbest = pos.copy()\n    pbest_scores = np.array([evaluate_mask(p) for p in pbest])\n    gbest_idx = int(np.argmax(pbest_scores))\n    gbest = pbest[gbest_idx].copy()\n    gbest_score = pbest_scores[gbest_idx]\n    w = 0.6; c1 = c2 = 1.5\n    for t in range(iters):\n        print(\" PSO iter\", t+1, \"/\", iters, \"best\", gbest_score)\n        for i in range(swarm_size):\n            r1 = np.random.rand(dim); r2 = np.random.rand(dim)\n            vel[i] = w*vel[i] + c1*r1*(pbest[i] - pos[i]) + c2*r2*(gbest - pos[i])\n            s = 1.0 / (1.0 + np.exp(-vel[i]))\n            pos[i] = (np.random.rand(dim) < s).astype(int)\n            sc = evaluate_mask(pos[i])\n            if sc > pbest_scores[i]:\n                pbest[i] = pos[i].copy(); pbest_scores[i] = sc\n            if sc > gbest_score:\n                gbest = pos[i].copy(); gbest_score = sc\n        w = max(0.2, w*0.97)\n    print(\"[PSO] done best score\", gbest_score, \"selected\", int(np.sum(gbest)))\n    return gbest\n\n# -----------------------\n# 10) GA (binary) - reduced\n# -----------------------\ndef run_ga(pop_size=GA_POP, gens=GA_GENS):\n    print(\"[GA] start: pop\", pop_size, \"gens\", gens)\n    dim = N_FEATURES\n    pop = np.random.randint(0,2,(pop_size, dim))\n    fitnesses = np.array([evaluate_mask(ind) for ind in pop])\n    for g in range(gens):\n        print(\" GA gen\", g+1, \"/\", gens, \"best\", fitnesses.max())\n        elite_idxs = np.argsort(fitnesses)[-2:]\n        new_pop = [pop[elite_idxs[0]].copy(), pop[elite_idxs[1]].copy()]\n        while len(new_pop) < pop_size:\n            p1 = pop[np.random.randint(pop_size)].copy()\n            p2 = pop[np.random.randint(pop_size)].copy()\n            if np.random.rand() < 0.7:\n                pt = np.random.randint(1, dim)\n                child = np.concatenate([p1[:pt], p2[pt:]])\n            else:\n                child = p1\n            # mutation\n            for d in range(dim):\n                if np.random.rand() < 0.05:\n                    child[d] = 1-child[d]\n            new_pop.append(child)\n        pop = np.array(new_pop[:pop_size])\n        fitnesses = np.array([evaluate_mask(ind) for ind in pop])\n    best = pop[np.argmax(fitnesses)]\n    print(\"[GA] done best score\", fitnesses.max(), \"selected\", int(np.sum(best)))\n    return best\n\n# -----------------------\n# 11) GWO (binary) - reduced\n# -----------------------\ndef run_gwo(wolves=GWO_WOLVES, iters=GWO_ITERS):\n    print(\"[GWO] start: wolves\", wolves, \"iters\", iters)\n    dim = N_FEATURES\n    pack = np.random.randint(0,2,(wolves, dim))\n    fitnesses = np.array([evaluate_mask(ind) for ind in pack])\n    Alpha = Beta = Delta = None\n    Alpha_score = Beta_score = Delta_score = -1.0\n    for itr in range(iters):\n        print(\" GWO iter\", itr+1, \"/\", iters, \"best\", Alpha_score)\n        # update alpha/beta/delta\n        for i in range(wolves):\n            sc = fitnesses[i]\n            if sc > Alpha_score:\n                Delta_score, Beta_score, Alpha_score = Beta_score, Alpha_score, sc\n                Delta, Beta, Alpha = Beta, Alpha, pack[i].copy()\n            elif sc > Beta_score:\n                Delta_score, Beta_score = Beta_score, sc\n                Delta, Beta = Beta, pack[i].copy()\n            elif sc > Delta_score:\n                Delta_score = sc; Delta = pack[i].copy()\n        a = 2 - itr*(2.0/iters)\n        for i in range(wolves):\n            if Alpha is None:\n                continue\n            for d in range(dim):\n                r1, r2 = np.random.rand(), np.random.rand()\n                A1 = 2*a*r1 - a; C1 = 2*r2\n                D_alpha = abs(C1*Alpha[d] - pack[i][d])\n                X1 = Alpha[d] - A1*D_alpha\n                # use X1 approx only (keeps it simple + fast)\n                s = 1.0/(1.0+np.exp(-X1))\n                pack[i][d] = 1 if np.random.rand() < s else 0\n        fitnesses = np.array([evaluate_mask(ind) for ind in pack])\n    best = pack[np.argmax(fitnesses)]\n    print(\"[GWO] done best score\", fitnesses.max(), \"selected\", int(np.sum(best)))\n    return best\n\n# -----------------------\n# 12) RUN OPTIMIZERS (PSO, GA, GWO)\n# -----------------------\nt0 = time.time()\nmask_pso = run_pso()\nmask_ga = run_ga()\nmask_gwo = run_gwo()\nt1 = time.time()\nprint(\"Optimizers finished in\", int(t1-t0), \"s\")\n\n# Save raw masks\nos.makedirs(\"outputs\", exist_ok=True)\npickle.dump({\"mask_pso\": mask_pso.tolist(), \"mask_ga\": mask_ga.tolist(), \"mask_gwo\": mask_gwo.tolist()}, open(os.path.join(\"outputs\", SAVE_PREFIX + \"_raw_masks.pkl\"), \"wb\"))\n\n\n# -----------------------\n# 13) INTERSECTION of PSO, GA, GWO (strict intersection — NO fallback)\n# -----------------------\nmask_pso_arr = np.array(mask_pso).astype(int)\nmask_ga_arr  = np.array(mask_ga).astype(int)\nmask_gwo_arr = np.array(mask_gwo).astype(int)\n\n# strict intersection: feature must be chosen by ALL three optimizers\nintersection_mask = (mask_pso_arr & mask_ga_arr & mask_gwo_arr).astype(int)\nselected_indices = list(np.where(intersection_mask == 1)[0])\nselected_features_intersection = [FEATURE_NAMES[i] for i in selected_indices]\n\nprint(\"Intersection selected features count:\", len(selected_indices))\nprint(\"Intersection selected features:\", selected_features_intersection)\n\n# Save intersection mask\npickle.dump(\n    {\"intersection_mask\": intersection_mask.tolist(), \"selected_features_intersection\": selected_features_intersection},\n    open(os.path.join(\"outputs\", SAVE_PREFIX + \"_intersection.pkl\"), \"wb\")\n)\n\n# -----------------------\n# 14) HLO on candidate set (candidates = intersection selected)\n# -----------------------\ndef hlo_on_candidates(candidate_mask, pop_size=HLO_POP, iters=HLO_ITERS):\n    cand_idxs = np.where(np.array(candidate_mask).astype(bool))[0].tolist()\n    k = len(cand_idxs)\n    if k == 0:\n        raise RuntimeError(\"No candidates for HLO (intersection is empty)\")\n    print(\"[HLO] start on\", k, \"candidates\")\n    pop = np.random.randint(0,2,(pop_size, k))\n    def fitness_local(bitmask):\n        full = np.zeros(N_FEATURES, dtype=int)\n        for j,b in enumerate(bitmask):\n            if int(b)==1:\n                full[cand_idxs[j]] = 1\n        return evaluate_mask(full)\n    fitness_scores = np.array([fitness_local(ind) for ind in pop])\n    best_idx = int(np.argmax(fitness_scores))\n    best_solution = pop[best_idx].copy()\n    best_score = fitness_scores[best_idx]\n    for it in range(iters):\n        print(\" HLO iter\", it+1, \"/\", iters, \"best\", best_score)\n        teacher = pop[int(np.argmax([fitness_local(x) for x in pop]))].copy()\n        new_pop = []\n        for i in range(pop_size):\n            learner = pop[i].copy()\n            # teaching\n            for d in range(k):\n                if np.random.rand() < 0.75:\n                    learner[d] = teacher[d]\n            # peer learning\n            partner = pop[np.random.randint(pop_size)].copy()\n            for d in range(k):\n                if learner[d] != partner[d] and np.random.rand() < 0.5:\n                    learner[d] = partner[d]\n            # mutation\n            for d in range(k):\n                if np.random.rand() < 0.12:\n                    learner[d] = 1 - learner[d]\n            new_pop.append(learner)\n        pop = np.array(new_pop)\n        fitness_scores = np.array([fitness_local(ind) for ind in pop])\n        gen_best_idx = int(np.argmax(fitness_scores))\n        gen_best_score = fitness_scores[gen_best_idx]\n        gen_best_sol = pop[gen_best_idx].copy()\n        if gen_best_score > best_score:\n            best_score = gen_best_score\n            best_solution = gen_best_sol.copy()\n    final_full = np.zeros(N_FEATURES, dtype=int)\n    for j,b in enumerate(best_solution):\n        if int(b)==1:\n            final_full[cand_idxs[j]] = 1\n    print(\"[HLO] done best local score\", best_score, \"selected\", int(final_full.sum()))\n    return final_full, best_score\n\n# call HLO using the strict intersection mask\nhlo_mask, hlo_score = hlo_on_candidates(intersection_mask)\npickle.dump({\"hlo_mask\": hlo_mask.tolist(), \"hlo_score\": hlo_score},\n            open(os.path.join(\"outputs\", SAVE_PREFIX + \"_hlo.pkl\"), \"wb\"))\n\n# -----------------------\n# 15) Greedy hill-climb restricted to candidate indices (use intersection_mask)\n# -----------------------\ndef hill_climb(initial_mask, candidate_mask, max_steps=100, eval_cap=500):\n    cand_idxs = np.where(np.array(candidate_mask).astype(bool))[0].tolist()\n    cur = initial_mask.copy()\n    cur_score = evaluate_mask(cur)\n    steps = 0\n    evals = 0\n    improved = True\n    print(\"[HC] start: candidates\", len(cand_idxs))\n    while improved and steps < max_steps and evals < eval_cap:\n        improved = False\n        for idx in np.random.permutation(cand_idxs):\n            trial = cur.copy()\n            trial[idx] = 1 - trial[idx]\n            sc = evaluate_mask(trial)\n            evals += 1\n            if sc > cur_score + 1e-8:\n                cur = trial\n                cur_score = sc\n                improved = True\n                steps += 1\n                print(f\" HC step {steps}: flipped {FEATURE_NAMES[idx]} -> new_score {cur_score:.4f} (evals={evals})\")\n                break\n    print(\"[HC] done steps\", steps, \"evals\", evals, \"final_score\", cur_score, \"selected\", int(cur.sum()))\n    return cur, cur_score\n\nhc_mask, hc_score = hill_climb(hlo_mask, intersection_mask)\npickle.dump({\"hc_mask\": hc_mask.tolist(), \"hc_score\": hc_score},\n            open(os.path.join(\"outputs\", SAVE_PREFIX + \"_hc.pkl\"), \"wb\"))\n\n\n\n# -----------------------\n# 16) Selected features after hill-climb (final_mask)\n# -----------------------\nfinal_mask = hc_mask\nfinal_selected_indices = np.where(np.array(final_mask).astype(bool))[0].tolist()\nfinal_selected = [FEATURE_NAMES[i] for i in final_selected_indices]\nprint(\"Final selected features:\", final_selected, \"count:\", len(final_selected))\n\n# -----------------------\n# 17) Leakage check: drop single-feature perfect predictors\n# -----------------------\ndef single_feature_predictive_accuracy(feature_series, labels):\n    # map each feature value to most common label for that value, compute accuracy\n    mapping = feature_series.groupby(feature_series).apply(lambda s: labels[s.index].mode().iloc[0])\n    preds = feature_series.map(mapping)\n    return (preds.values == labels.values).mean()\n\n# check each final feature; if single-feature accuracy >= threshold, drop it\nto_drop = []\nfor f in final_selected:\n    acc = single_feature_predictive_accuracy(X_sub[f], y_sub)\n    if acc >= LEAKAGE_SINGLE_FEATURE_THRESHOLD or acc == 1.0:\n        print(f\"Leakage-suspect feature '{f}' single-feature accuracy={acc:.6f} -> will drop\")\n        to_drop.append(f)\n\nif to_drop:\n    final_selected = [f for f in final_selected if f not in to_drop]\n    final_selected_indices = [FEATURE_NAMES.index(f) for f in final_selected]\n    print(\"After dropping leakage suspects, final features:\", final_selected)\n\nif len(final_selected) == 0:\n    raise RuntimeError(\"No safe features remain after leakage check. Consider lowering threshold or manual check.\")\n\n# Save final selected features\npickle.dump({\"final_selected\": final_selected, \"final_mask\": final_mask.tolist()},\n            open(os.path.join(\"outputs\", SAVE_PREFIX + \"_final_selected.pkl\"), \"wb\"))\n\n# -----------------------\n# 18) Prepare FULL dataset with same preprocessing for final training\n# -----------------------\n# Reuse df (full merged) earlier but ensure the same preprocessing as subset\ndf_full = df.copy()\n# Already dropped leak columns earlier and trimmed nulls; ensure same features exist\nmissing_in_full = [f for f in final_selected if f not in df_full.columns]\nif missing_in_full:\n    raise RuntimeError(\"Selected features missing from full dataset: \" + str(missing_in_full))\n\n# Keep only final selected + label\ndf_full = df_full[final_selected + [\"Label\"]].copy()\n\n# Convert object columns to numeric (LabelEncode) and fill NaN\nfor c in df_full.columns:\n    if c != \"Label\" and df_full[c].dtype == \"object\":\n        df_full[c] = LabelEncoder().fit_transform(df_full[c].astype(str))\ndf_full.replace([np.inf, -np.inf], np.nan, inplace=True)\ndf_full.fillna(0, inplace=True)\n\n# Scale numeric columns (MinMax) using full data\nnum_cols = [c for c in final_selected if pd.api.types.is_numeric_dtype(df_full[c])]\nif len(num_cols) > 0:\n    df_full[num_cols] = MinMaxScaler().fit_transform(df_full[num_cols])\n\nX_full = df_full.drop(columns=[\"Label\"])\ny_full = df_full[\"Label\"].astype(int)\nprint(\"Full final training shape:\", X_full.shape, \"Label dist:\", y_full.value_counts().to_dict())\n\n# -----------------------\n# 19) Final train/test split (80/20 stratified) and final CatBoost training with regularization + early stopping\n# -----------------------\nminclass = y_full.value_counts().min()\nif minclass < 10:\n    print(\"Warning: small class size after selecting features:\", minclass)\n\nX_train, X_test, y_train, y_test = train_test_split(X_full, y_full, test_size=0.2, stratify=y_full, random_state=42)\nX_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.15, stratify=y_train, random_state=42)\n\nfinal_params = {\n    \"iterations\": FINAL_CB_ITERS,\n    \"learning_rate\": 0.03,\n    \"depth\": 6,\n    \"l2_leaf_reg\": 7.0,\n    \"bootstrap_type\": \"Bernoulli\",\n    \"subsample\": 0.8,\n    \"random_strength\": 1.0,\n    \"verbose\": 50,\n    \"random_seed\": 42\n}\nfinal_model = CatBoostClassifier(**final_params)\n\nprint(\"Training final model on full data with early stopping...\")\nfinal_model.fit(X_tr, y_tr, eval_set=(X_val, y_val), early_stopping_rounds=FINAL_EARLY_STOP, use_best_model=True)\n\n# Evaluate on hold-out test\ny_pred = final_model.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred, zero_division=0)\nrec = recall_score(y_test, y_pred, zero_division=0)\nf1 = f1_score(y_test, y_pred, zero_division=0)\nprint(\"\\n=== FINAL HOLDOUT METRICS ===\")\nprint(\"Accuracy:\", acc)\nprint(\"Precision:\", prec)\nprint(\"Recall:\", rec)\nprint(\"F1:\", f1)\nprint(\"\\nClassification report:\\n\", classification_report(y_test, y_pred))\n\n# Quick 5-fold CV estimate (fast: reduced iters)\ncv_model = CatBoostClassifier(iterations=200, learning_rate=0.03, depth=6, l2_leaf_reg=7.0,\n                              bootstrap_type=\"Bernoulli\", subsample=0.8, random_seed=42, verbose=0)\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\naccs = cross_val_score(cv_model, X_full, y_full, cv=skf, scoring=\"accuracy\", n_jobs=-1)\nf1s = cross_val_score(cv_model, X_full, y_full, cv=skf, scoring=make_scorer(f1_score), n_jobs=-1)\nprint(\"\\n5-fold CV (quick estimate) -> Accuracy: %.4f ± %.4f ; F1: %.4f ± %.4f\" % (accs.mean(), accs.std(), f1s.mean(), f1s.std()))\n\n# -----------------------\n# 20) Save final model & selected features\n# -----------------------\npickle.dump({\"model\": final_model, \"features\": final_selected, \"mask\": final_mask.tolist()},\n            open(os.path.join(\"outputs\", SAVE_PREFIX + \"_final_model.pkl\"), \"wb\"))\nprint(\"Saved final model + features -> outputs/{}_final_model.pkl\".format(SAVE_PREFIX))\n\nprint(\"PIPELINE COMPLETE\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T15:19:48.579511Z","iopub.execute_input":"2025-12-05T15:19:48.579809Z","iopub.status.idle":"2025-12-05T15:25:50.199648Z","shell.execute_reply.started":"2025-12-05T15:19:48.579776Z","shell.execute_reply":"2025-12-05T15:25:50.199023Z"}},"outputs":[{"name":"stdout","text":"DATA_PATH: /kaggle/input/cicddos2019\nFound 1 CSV files. Loading & merging (may take a bit)...\n -> Random_combine_final.csv\nMerged dataset shape: (300000, 88)\nUsing target column 'Label' (original: Label)\nLabel counts (full):\n Label\n1    299513\n0       487\nName: count, dtype: int64\nDropping likely-leakage columns (ids/timestamps/ips): ['Flow ID', 'Source IP', 'Destination IP', 'Timestamp']\nAfter basic cleaning: (290753, 84)\nOptimization subset shape: (1980, 84) Label counts: {1: 1500, 0: 480}\nSubset features: 83\n[PSO] start: swarm 8 iters 20\n PSO iter 1 / 20 best 0.9993342210386151\n PSO iter 2 / 20 best 0.9993342210386151\n PSO iter 3 / 20 best 0.9993342210386151\n PSO iter 4 / 20 best 0.9993342210386151\n PSO iter 5 / 20 best 0.9993342210386151\n PSO iter 6 / 20 best 0.9993342210386151\n PSO iter 7 / 20 best 1.0\n PSO iter 8 / 20 best 1.0\n PSO iter 9 / 20 best 1.0\n PSO iter 10 / 20 best 1.0\n PSO iter 11 / 20 best 1.0\n PSO iter 12 / 20 best 1.0\n PSO iter 13 / 20 best 1.0\n PSO iter 14 / 20 best 1.0\n PSO iter 15 / 20 best 1.0\n PSO iter 16 / 20 best 1.0\n PSO iter 17 / 20 best 1.0\n PSO iter 18 / 20 best 1.0\n PSO iter 19 / 20 best 1.0\n PSO iter 20 / 20 best 1.0\n[PSO] done best score 1.0 selected 40\n[GA] start: pop 12 gens 20\n GA gen 1 / 20 best 0.9996668887408395\n GA gen 2 / 20 best 0.9996668887408395\n GA gen 3 / 20 best 1.0\n GA gen 4 / 20 best 1.0\n GA gen 5 / 20 best 1.0\n GA gen 6 / 20 best 1.0\n GA gen 7 / 20 best 1.0\n GA gen 8 / 20 best 1.0\n GA gen 9 / 20 best 1.0\n GA gen 10 / 20 best 1.0\n GA gen 11 / 20 best 1.0\n GA gen 12 / 20 best 1.0\n GA gen 13 / 20 best 1.0\n GA gen 14 / 20 best 1.0\n GA gen 15 / 20 best 1.0\n GA gen 16 / 20 best 1.0\n GA gen 17 / 20 best 1.0\n GA gen 18 / 20 best 1.0\n GA gen 19 / 20 best 1.0\n GA gen 20 / 20 best 1.0\n[GA] done best score 1.0 selected 38\n[GWO] start: wolves 8 iters 20\n GWO iter 1 / 20 best -1.0\n GWO iter 2 / 20 best 0.9993342210386151\n GWO iter 3 / 20 best 0.9996668887408395\n GWO iter 4 / 20 best 1.0\n GWO iter 5 / 20 best 1.0\n GWO iter 6 / 20 best 1.0\n GWO iter 7 / 20 best 1.0\n GWO iter 8 / 20 best 1.0\n GWO iter 9 / 20 best 1.0\n GWO iter 10 / 20 best 1.0\n GWO iter 11 / 20 best 1.0\n GWO iter 12 / 20 best 1.0\n GWO iter 13 / 20 best 1.0\n GWO iter 14 / 20 best 1.0\n GWO iter 15 / 20 best 1.0\n GWO iter 16 / 20 best 1.0\n GWO iter 17 / 20 best 1.0\n GWO iter 18 / 20 best 1.0\n GWO iter 19 / 20 best 1.0\n GWO iter 20 / 20 best 1.0\n[GWO] done best score 0.9996668887408395 selected 52\nOptimizers finished in 267 s\nIntersection selected features count: 16\nIntersection selected features: ['Source Port', 'Destination Port', 'Total Backward Packets', 'Bwd Packet Length Min', 'Flow IAT Std', 'ECE Flag Count', 'Avg Bwd Segment Size', 'Fwd Avg Packets/Bulk', 'Bwd Avg Bulk Rate', 'Subflow Fwd Bytes', 'Subflow Bwd Bytes', 'Init_Win_bytes_forward', 'Init_Win_bytes_backward', 'Active Max', 'Idle Std', 'Idle Min']\n[HLO] start on 16 candidates\n HLO iter 1 / 10 best 0.9979942034935219\n HLO iter 2 / 10 best 0.9993333333333334\n HLO iter 3 / 10 best 0.9993333333333334\n HLO iter 4 / 10 best 0.9993333333333334\n HLO iter 5 / 10 best 0.9993333333333334\n HLO iter 6 / 10 best 0.9993333333333334\n HLO iter 7 / 10 best 0.9993333333333334\n HLO iter 8 / 10 best 0.9993333333333334\n HLO iter 9 / 10 best 0.9993333333333334\n HLO iter 10 / 10 best 0.9993333333333334\n[HLO] done best local score 0.9993333333333334 selected 7\n[HC] start: candidates 16\n HC step 1: flipped Init_Win_bytes_backward -> new_score 0.9997 (evals=3)\n[HC] done steps 1 evals 19 final_score 0.9996664442961974 selected 6\nFinal selected features: ['Source Port', 'Bwd Packet Length Min', 'Avg Bwd Segment Size', 'Subflow Bwd Bytes', 'Init_Win_bytes_forward', 'Idle Min'] count: 6\nFull final training shape: (290753, 6) Label dist: {1: 290273, 0: 480}\nTraining final model on full data with early stopping...\n0:\tlearn: 0.5926101\ttest: 0.5927743\tbest: 0.5927743 (0)\ttotal: 20.6ms\tremaining: 20.6s\n50:\tlearn: 0.0021281\ttest: 0.0021991\tbest: 0.0021991 (50)\ttotal: 838ms\tremaining: 15.6s\n100:\tlearn: 0.0010678\ttest: 0.0011647\tbest: 0.0011647 (100)\ttotal: 1.59s\tremaining: 14.1s\n150:\tlearn: 0.0009341\ttest: 0.0010305\tbest: 0.0010305 (150)\ttotal: 2.28s\tremaining: 12.8s\n200:\tlearn: 0.0008929\ttest: 0.0009874\tbest: 0.0009874 (200)\ttotal: 2.98s\tremaining: 11.9s\n250:\tlearn: 0.0008642\ttest: 0.0009575\tbest: 0.0009575 (250)\ttotal: 3.71s\tremaining: 11.1s\n300:\tlearn: 0.0008476\ttest: 0.0009418\tbest: 0.0009418 (300)\ttotal: 4.39s\tremaining: 10.2s\n350:\tlearn: 0.0008409\ttest: 0.0009356\tbest: 0.0009356 (350)\ttotal: 5.08s\tremaining: 9.39s\n400:\tlearn: 0.0008113\ttest: 0.0009056\tbest: 0.0009056 (400)\ttotal: 5.76s\tremaining: 8.61s\n450:\tlearn: 0.0008085\ttest: 0.0009025\tbest: 0.0009025 (450)\ttotal: 6.44s\tremaining: 7.84s\n500:\tlearn: 0.0007960\ttest: 0.0008897\tbest: 0.0008897 (500)\ttotal: 7.13s\tremaining: 7.1s\n550:\tlearn: 0.0007868\ttest: 0.0008797\tbest: 0.0008797 (550)\ttotal: 7.82s\tremaining: 6.37s\n600:\tlearn: 0.0007770\ttest: 0.0008742\tbest: 0.0008742 (600)\ttotal: 8.51s\tremaining: 5.65s\n650:\tlearn: 0.0007552\ttest: 0.0008528\tbest: 0.0008528 (649)\ttotal: 9.2s\tremaining: 4.93s\n700:\tlearn: 0.0007478\ttest: 0.0008451\tbest: 0.0008451 (697)\ttotal: 9.88s\tremaining: 4.21s\n750:\tlearn: 0.0007217\ttest: 0.0008196\tbest: 0.0008196 (750)\ttotal: 10.6s\tremaining: 3.51s\n800:\tlearn: 0.0007072\ttest: 0.0008064\tbest: 0.0008064 (800)\ttotal: 11.3s\tremaining: 2.8s\n850:\tlearn: 0.0007004\ttest: 0.0008002\tbest: 0.0008002 (848)\ttotal: 12s\tremaining: 2.1s\n900:\tlearn: 0.0006982\ttest: 0.0007983\tbest: 0.0007983 (900)\ttotal: 12.7s\tremaining: 1.39s\n950:\tlearn: 0.0006916\ttest: 0.0007919\tbest: 0.0007919 (950)\ttotal: 13.5s\tremaining: 695ms\n999:\tlearn: 0.0006884\ttest: 0.0007882\tbest: 0.0007882 (999)\ttotal: 14.1s\tremaining: 0us\n\nbestTest = 0.0007882004144\nbestIteration = 999\n\n\n=== FINAL HOLDOUT METRICS ===\nAccuracy: 0.999742050867569\nPrecision: 0.9999138673557278\nRecall: 0.9998277495478426\nF1: 0.9998708065974764\n\nClassification report:\n               precision    recall  f1-score   support\n\n           0       0.90      0.95      0.92        96\n           1       1.00      1.00      1.00     58055\n\n    accuracy                           1.00     58151\n   macro avg       0.95      0.97      0.96     58151\nweighted avg       1.00      1.00      1.00     58151\n\n\n5-fold CV (quick estimate) -> Accuracy: 0.9997 ± 0.0001 ; F1: 0.9999 ± 0.0000\nSaved final model + features -> outputs/ddos_hybrid_voting_hlo_final_model.pkl\nPIPELINE COMPLETE\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"ALL MODELS RUN CHECK","metadata":{}},{"cell_type":"markdown","source":"1. PSO + XGBOOST","metadata":{}},{"cell_type":"code","source":"import time\nimport warnings\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, make_scorer\n\nfrom xgboost import XGBClassifier\n\nwarnings.filterwarnings(\"ignore\")\nnp.random.seed(42)\n\n# ================== USER SETTINGS ==================\nDATA_PATH = \"/kaggle/input/ids-cleaned/ids2018_cleaned_combined_1.csv\"\nTARGET_COL = \"Label\"\n\nPSO_SWARM = 20       # swarm size\nPSO_ITERS = 10       # *** you asked for 10 iterations ***\nCV_FOLDS = 3         # CV folds for fitness (can increase if you want)\nTEST_SIZE = 0.2      # final 80/20 split for evaluation\nRANDOM_STATE = 42\n# ===================================================\n\n# ---------- Load data ----------\ndf = pd.read_csv(DATA_PATH)\nX = df.drop(TARGET_COL, axis=1)\ny = df[TARGET_COL].astype(int)\n\nFEATURE_NAMES = X.columns.tolist()\nN_FEATURES = X.shape[1]\n\nprint(f\"Loaded data: {df.shape}, features={N_FEATURES}\")\n\n# ---------- XGBoost model factory ----------\ndef get_xgb_model():\n    return XGBClassifier(\n        n_estimators=300,\n        learning_rate=0.05,\n        max_depth=6,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        objective=\"binary:logistic\",\n        eval_metric=\"logloss\",\n        random_state=RANDOM_STATE,\n        n_jobs=-1\n    )\n\n# ---------- Fitness cache ----------\nfitness_cache = {}\n\ndef key_from_mask(mask_bool):\n    return tuple(sorted(np.where(np.array(mask_bool).astype(bool))[0].tolist()))\n\ndef evaluate_mask(mask_bool):\n    \"\"\"\n    Evaluate a feature mask using XGBoost with CV.\n    Fitness = mean of (accuracy, precision, recall, f1).\n    \"\"\"\n    key = key_from_mask(mask_bool)\n    if key in fitness_cache:\n        return fitness_cache[key]\n\n    idxs = np.where(np.array(mask_bool).astype(bool))[0].tolist()\n    if len(idxs) == 0:\n        fitness_cache[key] = 0.0\n        return 0.0\n\n    X_sel = X.iloc[:, idxs]\n\n    model = get_xgb_model()\n    skf = StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n\n    accs = cross_val_score(model, X_sel, y, cv=skf, scoring=\"accuracy\", n_jobs=-1)\n    precs = cross_val_score(model, X_sel, y, cv=skf,\n                            scoring=make_scorer(precision_score, zero_division=0), n_jobs=-1)\n    recs = cross_val_score(model, X_sel, y, cv=skf,\n                           scoring=make_scorer(recall_score, zero_division=0), n_jobs=-1)\n    f1s = cross_val_score(model, X_sel, y, cv=skf,\n                          scoring=make_scorer(f1_score, zero_division=0), n_jobs=-1)\n\n    score = float((np.mean(accs) + np.mean(precs) + np.mean(recs) + np.mean(f1s)) / 4.0)\n    fitness_cache[key] = score\n    return score\n\ndef mask_to_features(mask):\n    idxs = np.where(np.array(mask).astype(bool))[0].tolist()\n    return [FEATURE_NAMES[i] for i in idxs]\n\ndef log(msg):\n    print(f\"[{time.strftime('%H:%M:%S')}] {msg}\", flush=True)\n\n# =============== PSO (binary) for feature selection ===============\ndef run_pso(swarm_size=PSO_SWARM, iters=PSO_ITERS):\n    log(f\"PSO START (swarm={swarm_size}, iters={iters}, cv={CV_FOLDS})\")\n    t0 = time.time()\n    dim = N_FEATURES\n\n    # Initialize positions (0/1) and velocities\n    pos = np.random.randint(0, 2, (swarm_size, dim)).astype(int)\n    vel = np.random.uniform(-1, 1, (swarm_size, dim))\n\n    # Personal bests\n    pbest = pos.copy()\n    pbest_scores = np.array([evaluate_mask(p.astype(bool)) for p in pos])\n\n    # Global best\n    gbest_idx = int(np.argmax(pbest_scores))\n    gbest = pbest[gbest_idx].copy()\n    gbest_score = pbest_scores[gbest_idx]\n\n    w = 0.6\n    c1 = 1.5\n    c2 = 1.5\n\n    for t in range(iters):\n        log(f\" PSO iter {t+1}/{iters} best_global={gbest_score:.8f}\")\n        for i in range(swarm_size):\n            r1 = np.random.rand(dim)\n            r2 = np.random.rand(dim)\n\n            vel[i] = (\n                w * vel[i]\n                + c1 * r1 * (pbest[i] - pos[i])\n                + c2 * r2 * (gbest - pos[i])\n            )\n            # Sigmoid + sampling to get binary position\n            s = 1.0 / (1.0 + np.exp(-vel[i]))\n            pos[i] = (np.random.rand(dim) < s).astype(int)\n\n            sc = evaluate_mask(pos[i].astype(bool))\n            if sc > pbest_scores[i]:\n                pbest[i] = pos[i].copy()\n                pbest_scores[i] = sc\n            if sc > gbest_score:\n                gbest = pos[i].copy()\n                gbest_score = sc\n\n        # inertia decay\n        w = max(0.2, w * 0.97)\n\n    best_idx = int(np.argmax(pbest_scores))\n    best_mask = pbest[best_idx].copy()\n    best_score = pbest_scores[best_idx]\n    t1 = time.time()\n\n    log(f\"PSO DONE in {int(t1 - t0)}s best_score={best_score:.8f} selected={int(np.sum(best_mask))}\")\n    log(f\"PSO SELECTED FEATURES ({int(np.sum(best_mask))}): {mask_to_features(best_mask)}\")\n    return best_mask, best_score, int(t1 - t0)\n\n# =============== MAIN: PSO + XGBoost Final Model ===============\nif __name__ == \"__main__\":\n    total_t0 = time.time()\n\n    # 1) Run PSO for 10 iterations to select features\n    best_mask, best_score, pso_time = run_pso(swarm_size=PSO_SWARM, iters=PSO_ITERS)\n\n    selected_indices = np.where(best_mask.astype(bool))[0].tolist()\n    selected_features = [FEATURE_NAMES[i] for i in selected_indices]\n\n    print(\"\\n============= FINAL PSO SELECTION =============\")\n    print(f\"Number of selected features: {len(selected_features)}\")\n    print(\"Selected features:\")\n    print(selected_features)\n    print(f\"Best PSO fitness score (CV-based): {best_score:.8f}\")\n\n    # 2) Train/Test split on selected features\n    X_sel = X[selected_features]\n    X_train, X_test, y_train, y_test = train_test_split(\n        X_sel, y,\n        test_size=TEST_SIZE,\n        stratify=y,\n        random_state=RANDOM_STATE\n    )\n\n    # 3) Final XGBoost on selected features\n    final_model = get_xgb_model()\n    final_model.fit(X_train, y_train)\n\n    y_pred = final_model.predict(X_test)\n\n    test_acc = accuracy_score(y_test, y_pred)\n    test_prec = precision_score(y_test, y_pred, zero_division=0)\n    test_rec = recall_score(y_test, y_pred, zero_division=0)\n    test_f1 = f1_score(y_test, y_pred, zero_division=0)\n\n    print(\"\\n============= FINAL XGBOOST RESULTS (PSO-Selected Features) =============\")\n    print(f\"Test Accuracy : {test_acc:.8f}\")\n    print(f\"Precision     : {test_prec:.8f}\")\n    print(f\"Recall        : {test_rec:.8f}\")\n    print(f\"F1 Score      : {test_f1:.8f}\")\n\n    total_t1 = time.time()\n    print(f\"\\nTotal pipeline time: {int(total_t1 - total_t0)} seconds\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T13:29:55.466859Z","iopub.execute_input":"2025-12-08T13:29:55.467158Z","iopub.status.idle":"2025-12-08T14:30:41.033701Z","shell.execute_reply.started":"2025-12-08T13:29:55.467137Z","shell.execute_reply":"2025-12-08T14:30:41.033072Z"}},"outputs":[{"name":"stdout","text":"Loaded data: (97802, 76), features=75\n[13:29:56] PSO START (swarm=20, iters=10, cv=3)\n[13:35:35]  PSO iter 1/10 best_global=0.99929282\n[13:41:14]  PSO iter 2/10 best_global=0.99934475\n[13:46:47]  PSO iter 3/10 best_global=0.99934477\n[13:52:22]  PSO iter 4/10 best_global=0.99936559\n[13:57:41]  PSO iter 5/10 best_global=0.99936559\n[14:03:00]  PSO iter 6/10 best_global=0.99936559\n[14:08:31]  PSO iter 7/10 best_global=0.99937597\n[14:14:06]  PSO iter 8/10 best_global=0.99937597\n[14:19:32]  PSO iter 9/10 best_global=0.99937597\n[14:25:01]  PSO iter 10/10 best_global=0.99937597\n[14:30:39] PSO DONE in 3642s best_score=0.99937597 selected=39\n[14:30:39] PSO SELECTED FEATURES (39): ['Dst Port', 'Protocol', 'Timestamp', 'Tot Fwd Pkts', 'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Bwd Pkt Len Max', 'Bwd Pkt Len Min', 'Flow Pkts/s', 'Fwd IAT Tot', 'Fwd IAT Std', 'Bwd IAT Tot', 'Bwd IAT Mean', 'Fwd PSH Flags', 'Fwd Header Len', 'Fwd Pkts/s', 'Pkt Len Mean', 'Pkt Len Std', 'FIN Flag Cnt', 'SYN Flag Cnt', 'RST Flag Cnt', 'ACK Flag Cnt', 'URG Flag Cnt', 'ECE Flag Cnt', 'Down/Up Ratio', 'Pkt Size Avg', 'Bwd Seg Size Avg', 'Subflow Bwd Pkts', 'Init Bwd Win Byts', 'Fwd Act Data Pkts', 'Active Mean', 'Active Std', 'Idle Mean', 'Idle Std', 'Idle Min', 'Flow ID', 'Src IP', 'Src Port', 'Dst IP']\n\n============= FINAL PSO SELECTION =============\nNumber of selected features: 39\nSelected features:\n['Dst Port', 'Protocol', 'Timestamp', 'Tot Fwd Pkts', 'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Bwd Pkt Len Max', 'Bwd Pkt Len Min', 'Flow Pkts/s', 'Fwd IAT Tot', 'Fwd IAT Std', 'Bwd IAT Tot', 'Bwd IAT Mean', 'Fwd PSH Flags', 'Fwd Header Len', 'Fwd Pkts/s', 'Pkt Len Mean', 'Pkt Len Std', 'FIN Flag Cnt', 'SYN Flag Cnt', 'RST Flag Cnt', 'ACK Flag Cnt', 'URG Flag Cnt', 'ECE Flag Cnt', 'Down/Up Ratio', 'Pkt Size Avg', 'Bwd Seg Size Avg', 'Subflow Bwd Pkts', 'Init Bwd Win Byts', 'Fwd Act Data Pkts', 'Active Mean', 'Active Std', 'Idle Mean', 'Idle Std', 'Idle Min', 'Flow ID', 'Src IP', 'Src Port', 'Dst IP']\nBest PSO fitness score (CV-based): 0.99937597\n\n============= FINAL XGBOOST RESULTS (PSO-Selected Features) =============\nTest Accuracy : 0.99928429\nPrecision     : 0.99968600\nRecall        : 0.99884961\nF1 Score      : 0.99926763\n\nTotal pipeline time: 3644 seconds\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import time\nimport warnings\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, make_scorer\n\nfrom catboost import CatBoostClassifier\n\nwarnings.filterwarnings(\"ignore\")\nnp.random.seed(42)\n\n# ================== USER SETTINGS ==================\nDATA_PATH = \"/kaggle/input/ids-cleaned/ids2018_cleaned_combined_1.csv\"\nTARGET_COL = \"Label\"\n\nPSO_SWARM = 20       # swarm size\nPSO_ITERS = 10       # PSO iterations\nCV_FOLDS = 3         # CV folds for fitness\nTEST_SIZE = 0.2      # final 80/20 split\nRANDOM_STATE = 42\n# ===================================================\n\n# ---------- Load data ----------\ndf = pd.read_csv(DATA_PATH)\nX = df.drop(TARGET_COL, axis=1)\ny = df[TARGET_COL].astype(int)\n\nFEATURE_NAMES = X.columns.tolist()\nN_FEATURES = X.shape[1]\n\nprint(f\"Loaded data: {df.shape}, features={N_FEATURES}\")\n\n# ---------- CatBoost model factory ----------\ndef get_cat_model(iterations=300):\n    return CatBoostClassifier(\n        iterations=iterations,\n        learning_rate=0.05,\n        depth=6,\n        verbose=0,\n        random_seed=RANDOM_STATE,\n        thread_count=-1\n    )\n\n# ---------- Fitness cache ----------\nfitness_cache = {}\n\ndef key_from_mask(mask_bool):\n    return tuple(sorted(np.where(np.array(mask_bool).astype(bool))[0].tolist()))\n\ndef evaluate_mask(mask_bool):\n    \"\"\"\n    Evaluate a feature mask using CatBoost with CV.\n    Fitness = mean of (accuracy, precision, recall, f1).\n    \"\"\"\n    key = key_from_mask(mask_bool)\n    if key in fitness_cache:\n        return fitness_cache[key]\n\n    idxs = np.where(np.array(mask_bool).astype(bool))[0].tolist()\n    if len(idxs) == 0:\n        fitness_cache[key] = 0.0\n        return 0.0\n\n    X_sel = X.iloc[:, idxs]\n\n    model = get_cat_model(iterations=200)  # slightly smaller for CV\n    skf = StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n\n    accs = cross_val_score(model, X_sel, y, cv=skf, scoring=\"accuracy\", n_jobs=-1)\n    precs = cross_val_score(model, X_sel, y, cv=skf,\n                            scoring=make_scorer(precision_score, zero_division=0), n_jobs=-1)\n    recs = cross_val_score(model, X_sel, y, cv=skf,\n                           scoring=make_scorer(recall_score, zero_division=0), n_jobs=-1)\n    f1s = cross_val_score(model, X_sel, y, cv=skf,\n                          scoring=make_scorer(f1_score, zero_division=0), n_jobs=-1)\n\n    score = float((np.mean(accs) + np.mean(precs) + np.mean(recs) + np.mean(f1s)) / 4.0)\n    fitness_cache[key] = score\n    return score\n\ndef mask_to_features(mask):\n    idxs = np.where(np.array(mask).astype(bool))[0].tolist()\n    return [FEATURE_NAMES[i] for i in idxs]\n\ndef log(msg):\n    print(f\"[{time.strftime('%H:%M:%S')}] {msg}\", flush=True)\n\n# =============== PSO (binary) for feature selection ===============\ndef run_pso(swarm_size=PSO_SWARM, iters=PSO_ITERS):\n    log(f\"PSO START (swarm={swarm_size}, iters={iters}, cv={CV_FOLDS})\")\n    t0 = time.time()\n    dim = N_FEATURES\n\n    # Initialize positions (0/1) and velocities\n    pos = np.random.randint(0, 2, (swarm_size, dim)).astype(int)\n    vel = np.random.uniform(-1, 1, (swarm_size, dim))\n\n    # Personal bests\n    pbest = pos.copy()\n    pbest_scores = np.array([evaluate_mask(p.astype(bool)) for p in pos])\n\n    # Global best\n    gbest_idx = int(np.argmax(pbest_scores))\n    gbest = pbest[gbest_idx].copy()\n    gbest_score = pbest_scores[gbest_idx]\n\n    w = 0.6\n    c1 = 1.5\n    c2 = 1.5\n\n    for t in range(iters):\n        log(f\" PSO iter {t+1}/{iters} best_global={gbest_score:.8f}\")\n        for i in range(swarm_size):\n            r1 = np.random.rand(dim)\n            r2 = np.random.rand(dim)\n\n            vel[i] = (\n                w * vel[i]\n                + c1 * r1 * (pbest[i] - pos[i])\n                + c2 * r2 * (gbest - pos[i])\n            )\n            # Sigmoid + sampling to get binary position\n            s = 1.0 / (1.0 + np.exp(-vel[i]))\n            pos[i] = (np.random.rand(dim) < s).astype(int)\n\n            sc = evaluate_mask(pos[i].astype(bool))\n            if sc > pbest_scores[i]:\n                pbest[i] = pos[i].copy()\n                pbest_scores[i] = sc\n            if sc > gbest_score:\n                gbest = pos[i].copy()\n                gbest_score = sc\n\n        # inertia decay\n        w = max(0.2, w * 0.97)\n\n    best_idx = int(np.argmax(pbest_scores))\n    best_mask = pbest[best_idx].copy()\n    best_score = pbest_scores[best_idx]\n    t1 = time.time()\n\n    log(f\"PSO DONE in {int(t1 - t0)}s best_score={best_score:.8f} selected={int(np.sum(best_mask))}\")\n    log(f\"PSO SELECTED FEATURES ({int(np.sum(best_mask))}): {mask_to_features(best_mask)}\")\n    return best_mask, best_score, int(t1 - t0)\n\n# =============== MAIN: PSO + CatBoost Final Model ===============\nif __name__ == \"__main__\":\n    total_t0 = time.time()\n\n    # 1) Run PSO for 10 iterations to select features\n    best_mask, best_score, pso_time = run_pso(swarm_size=PSO_SWARM, iters=PSO_ITERS)\n\n    selected_indices = np.where(best_mask.astype(bool))[0].tolist()\n    selected_features = [FEATURE_NAMES[i] for i in selected_indices]\n\n    print(\"\\n============= FINAL PSO SELECTION =============\")\n    print(f\"Number of selected features: {len(selected_features)}\")\n    print(\"Selected features:\")\n    print(selected_features)\n    print(f\"Best PSO fitness score (CV-based): {best_score:.8f}\")\n\n    # 2) Train/Test split on selected features\n    X_sel = X[selected_features]\n    X_train, X_test, y_train, y_test = train_test_split(\n        X_sel, y,\n        test_size=TEST_SIZE,\n        stratify=y,\n        random_state=RANDOM_STATE\n    )\n\n    # 3) Final CatBoost on selected features\n    final_model = get_cat_model(iterations=500)\n    final_model.fit(X_train, y_train)\n\n    y_pred = final_model.predict(X_test)\n\n    test_acc = accuracy_score(y_test, y_pred)\n    test_prec = precision_score(y_test, y_pred, zero_division=0)\n    test_rec = recall_score(y_test, y_pred, zero_division=0)\n    test_f1 = f1_score(y_test, y_pred, zero_division=0)\n\n    print(\"\\n============= FINAL CATBOOST RESULTS (PSO-Selected Features) =============\")\n    print(f\"Test Accuracy : {test_acc:.8f}\")\n    print(f\"Precision     : {test_prec:.8f}\")\n    print(f\"Recall        : {test_rec:.8f}\")\n    print(f\"F1 Score      : {test_f1:.8f}\")\n\n    total_t1 = time.time()\n    print(f\"\\nTotal pipeline time: {int(total_t1 - total_t0)} seconds\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T14:33:35.632641Z","iopub.execute_input":"2025-12-08T14:33:35.632918Z","iopub.status.idle":"2025-12-08T17:14:08.210028Z","shell.execute_reply.started":"2025-12-08T14:33:35.632898Z","shell.execute_reply":"2025-12-08T17:14:08.209181Z"}},"outputs":[{"name":"stdout","text":"Loaded data: (97802, 76), features=75\n[14:33:36] PSO START (swarm=20, iters=10, cv=3)\n[14:48:35]  PSO iter 1/10 best_global=0.99938638\n[15:03:31]  PSO iter 2/10 best_global=0.99938638\n[15:18:26]  PSO iter 3/10 best_global=0.99939676\n[15:33:16]  PSO iter 4/10 best_global=0.99944877\n[15:47:20]  PSO iter 5/10 best_global=0.99944877\n[16:01:31]  PSO iter 6/10 best_global=0.99944877\n[16:16:01]  PSO iter 7/10 best_global=0.99944877\n[16:30:34]  PSO iter 8/10 best_global=0.99944877\n[16:45:01]  PSO iter 9/10 best_global=0.99944877\n[16:59:26]  PSO iter 10/10 best_global=0.99944877\n[17:14:02] PSO DONE in 9625s best_score=0.99944877 selected=33\n[17:14:02] PSO SELECTED FEATURES (33): ['Dst Port', 'Protocol', 'Timestamp', 'Tot Bwd Pkts', 'TotLen Fwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std', 'Bwd Pkt Len Min', 'Bwd Pkt Len Mean', 'Flow Pkts/s', 'Flow IAT Std', 'Flow IAT Max', 'Fwd IAT Tot', 'Fwd IAT Max', 'Bwd IAT Tot', 'Bwd IAT Min', 'Fwd PSH Flags', 'Pkt Len Min', 'Pkt Len Mean', 'Pkt Len Std', 'FIN Flag Cnt', 'SYN Flag Cnt', 'RST Flag Cnt', 'PSH Flag Cnt', 'ACK Flag Cnt', 'Fwd Seg Size Avg', 'Subflow Bwd Byts', 'Active Mean', 'Active Max', 'Idle Max', 'Src IP']\n\n============= FINAL PSO SELECTION =============\nNumber of selected features: 33\nSelected features:\n['Dst Port', 'Protocol', 'Timestamp', 'Tot Bwd Pkts', 'TotLen Fwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std', 'Bwd Pkt Len Min', 'Bwd Pkt Len Mean', 'Flow Pkts/s', 'Flow IAT Std', 'Flow IAT Max', 'Fwd IAT Tot', 'Fwd IAT Max', 'Bwd IAT Tot', 'Bwd IAT Min', 'Fwd PSH Flags', 'Pkt Len Min', 'Pkt Len Mean', 'Pkt Len Std', 'FIN Flag Cnt', 'SYN Flag Cnt', 'RST Flag Cnt', 'PSH Flag Cnt', 'ACK Flag Cnt', 'Fwd Seg Size Avg', 'Subflow Bwd Byts', 'Active Mean', 'Active Max', 'Idle Max', 'Src IP']\nBest PSO fitness score (CV-based): 0.99944877\n\n============= FINAL CATBOOST RESULTS (PSO-Selected Features) =============\nTest Accuracy : 0.99923317\nPrecision     : 0.99958137\nRecall        : 0.99884961\nF1 Score      : 0.99921536\n\nTotal pipeline time: 9631 seconds\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# intersection_hlo_with_hillclimb_fast.py\n# Pipeline (reduced budget + hill-climb) with UNION, INTERSECTION, and VOTING candidate flows:\n#  PSO + GA + GWO (CatBoost fitness, lighter during opt) -> derive UNION / INTERSECTION / VOTING\n#  For each candidate set: HLO (on candidates) -> Greedy hill-climb (restricted) -> Final CatBoost eval (5-fold CV)\n#  Additionally: train a CatBoost model on 80% of the data and evaluate on the held-out 20% test set\n#  Train & save a CatBoost model for each flow (union / intersection / voting) using the 80/20 split.\n# Prints logs, mean ± std for metrics, stage timings, saves results and models.\n\nimport time\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport warnings\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, make_scorer\nfrom sklearn.base import clone\nfrom xgboost import XGBClassifier\n\n\nwarnings.filterwarnings(\"ignore\")\nnp.random.seed(42)\n\n# -------------------- USER / EXPERIMENT SETTINGS --------------------\n# If you prefer to load CSV instead, uncomment and change:\ndf = pd.read_csv(\"/kaggle/input/ids-cleaned/ids2018_cleaned_combined_1.csv\")\n\nTARGET_COL = \"Label\"   # target column\nMODEL_VERBOSE = 0            # CatBoost verbosity: 0 = silent\nRANDOM_STATE = 42\n\n# ---------- Reduced budgets for faster runs (you can tune these) ----------\nPSO_SWARM = 15   # reduced swarm\nPSO_ITERS = 10   # reduced iterations\n\nGA_POP = 30      # reduced population\nGA_GENS = 10     # reduced generations\n\nGWO_WOLVES = 10\nGWO_ITERS = 10\n\nHLO_POP = 15\nHLO_ITERS = 10\nHLO_TEACHER_FACTOR = 0.75\nHLO_MUTATION = 0.12\n\n# Greedy hill-climb after HLO\nHILLCLIMB_MAX_STEPS = 100   # stop if no improvement or step limit\nHILLCLIMB_EVAL_CAP = 500    # safety cap on evaluations (prevent runaway)\n\n# CV folds\nCV_OPT = 2    # cheaper CV during optimization + HLO (speed)\nCV_FINAL = 5  # final evaluation (A1 requested)\n\n# CatBoost iterations\nCB_ITER_OPT = 100    # iterations during optimization (smaller)\nCB_ITER_HLO = 200\nCB_ITER_FINAL = 500  # final evaluation iterations (bigger)\n\n# Train/test split for final saved models\nFINAL_TEST_SIZE = 0.2\n\nSAVE_PREFIX = \"hybrid_hlo_models\"\n# ------------------------------------------------------------------------\n\n# Ensure df exists\ntry:\n    df\nexcept NameError:\n    raise RuntimeError(\"DataFrame `df` not found. Assign your dataset to variable `df` or load at top.\")\n\n# Prepare data\nX = df.drop(TARGET_COL, axis=1)\n\ny = df[TARGET_COL].astype(int)\nFEATURE_NAMES = X.columns.tolist()\nN_FEATURES = X.shape[1]\n\n# -------------------- Model factory (CatBoost) --------------------\n\ndef get_xgb_model(iterations=100):\n    return XGBClassifier(\n        n_estimators=iterations,\n        learning_rate=0.05,\n        max_depth=6,\n        subsample=1.0,\n        colsample_bytree=1.0,\n        random_state=RANDOM_STATE,\n        n_jobs=-1,\n        eval_metric=\"logloss\",\n       \n    )\n\n\n# -------------------- Fitness cache --------------------\n# key: tuple(selected original indices) -> float score\nfitness_cache = {}\n\ndef key_from_mask(mask_bool):\n    return tuple(sorted(np.where(np.array(mask_bool).astype(bool))[0].tolist()))\n\ndef evaluate_mask_global(mask_bool, cv=CV_OPT, cb_iter=CB_ITER_OPT):\n    \"\"\"\n    Evaluate mask using CatBoost with CV and return average of acc,prec,rec,f1.\n    Caches results to avoid re-evaluating identical subsets.\n    \"\"\"\n    key = key_from_mask(mask_bool)\n    if key in fitness_cache:\n        return fitness_cache[key]\n    if len(key) == 0:\n        fitness_cache[key] = 0.0\n        return 0.0\n\n    X_sel = X.iloc[:, list(key)]\n    \n    model = get_xgb_model(iterations=cb_iter)\n\n    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=RANDOM_STATE)\n\n    accs = cross_val_score(clone(model), X_sel, y, cv=skf, scoring=\"accuracy\", n_jobs=-1)\n    precs = cross_val_score(clone(model), X_sel, y, cv=skf, scoring=make_scorer(precision_score, zero_division=0), n_jobs=-1)\n    recs = cross_val_score(clone(model), X_sel, y, cv=skf, scoring=make_scorer(recall_score, zero_division=0), n_jobs=-1)\n    f1s = cross_val_score(clone(model), X_sel, y, cv=skf, scoring=make_scorer(f1_score, zero_division=0), n_jobs=-1)\n\n    score = float((np.mean(accs) + np.mean(precs) + np.mean(recs) + np.mean(f1s)) / 4.0)\n    fitness_cache[key] = score\n    return score\n\n# -------------------- Helpers --------------------\ndef mask_to_features(mask):\n    idxs = np.where(np.array(mask).astype(bool))[0].tolist()\n    return [FEATURE_NAMES[i] for i in idxs]\n\ndef log(msg):\n    print(f\"[{time.strftime('%H:%M:%S')}] {msg}\", flush=True)\n\n# -------------------- PSO (binary) --------------------\ndef run_pso(swarm_size=PSO_SWARM, iters=PSO_ITERS, cv=CV_OPT):\n    log(f\"PSO START (swarm={swarm_size}, iters={iters}, cv={cv})\")\n    t0 = time.time()\n    dim = N_FEATURES\n    pos = np.random.randint(0,2,(swarm_size,dim)).astype(int)\n    vel = np.random.uniform(-1,1,(swarm_size,dim))\n\n    pbest = pos.copy()\n    pbest_scores = np.array([evaluate_mask_global(p.astype(bool), cv=cv, cb_iter=CB_ITER_OPT) for p in pos])\n\n    gbest_idx = int(np.argmax(pbest_scores))\n    gbest = pbest[gbest_idx].copy()\n    gbest_score = pbest_scores[gbest_idx]\n\n    w = 0.6; c1 = c2 = 1.5\n    for t in range(iters):\n        log(f\" PSO iter {t+1}/{iters} best_global={gbest_score:.8}\")\n        for i in range(swarm_size):\n            r1 = np.random.rand(dim); r2 = np.random.rand(dim)\n            vel[i] = w*vel[i] + c1*r1*(pbest[i] - pos[i]) + c2*r2*(gbest - pos[i])\n            s = 1.0 / (1.0 + np.exp(-vel[i]))\n            pos[i] = (np.random.rand(dim) < s).astype(int)\n\n            sc = evaluate_mask_global(pos[i].astype(bool), cv=cv, cb_iter=CB_ITER_OPT)\n            if sc > pbest_scores[i]:\n                pbest[i] = pos[i].copy()\n                pbest_scores[i] = sc\n            if sc > gbest_score:\n                gbest = pos[i].copy()\n                gbest_score = sc\n        w = max(0.2, w*0.97)\n\n    best_idx = int(np.argmax(pbest_scores))\n    best_mask = pbest[best_idx].copy()\n    best_score = pbest_scores[best_idx]\n    t1 = time.time()\n    log(f\"PSO DONE in {int(t1-t0)}s best_score={best_score:.8f} selected={int(np.sum(best_mask))}\")\n    log(f\"PSO SELECTED FEATURES: {mask_to_features(best_mask)}\")\n\n    return best_mask, best_score, int(t1-t0)\n\n# -------------------- GA (binary) --------------------\ndef run_ga(pop_size=GA_POP, gens=GA_GENS, cv=CV_OPT):\n    log(f\"GA START (pop={pop_size}, gens={gens}, cv={cv})\")\n    t0 = time.time()\n    dim = N_FEATURES\n    pop = np.random.randint(0,2,(pop_size, dim)).astype(int)\n    fitness_scores = np.array([evaluate_mask_global(ind.astype(bool), cv=cv, cb_iter=CB_ITER_OPT) for ind in pop])\n\n    def tournament_select(k=3):\n        idxs = np.random.randint(0, pop_size, k)\n        return idxs[np.argmax(fitness_scores[idxs])]\n\n    for g in range(gens):\n        log(f\" GA gen {g+1}/{gens} current_best={np.max(fitness_scores):.8f}\")\n        new_pop = []\n        # elitism\n        elite_idxs = np.argsort(fitness_scores)[-2:]\n        new_pop.extend(pop[elite_idxs].tolist())\n\n        while len(new_pop) < pop_size:\n            i1 = tournament_select(); i2 = tournament_select()\n            p1 = pop[i1].copy(); p2 = pop[i2].copy()\n            # crossover\n            if np.random.rand() < 0.7:\n                pt = np.random.randint(1, dim)\n                c1 = np.concatenate([p1[:pt], p2[pt:]])\n                c2 = np.concatenate([p2[:pt], p1[pt:]])\n            else:\n                c1, c2 = p1, p2\n            # mutation\n            for child in (c1, c2):\n                for d in range(dim):\n                    if np.random.rand() < 0.1:\n                        child[d] = 1 - child[d]\n                new_pop.append(child)\n                if len(new_pop) >= pop_size:\n                    break\n        pop = np.array(new_pop[:pop_size])\n        fitness_scores = np.array([evaluate_mask_global(ind.astype(bool), cv=cv, cb_iter=CB_ITER_OPT) for ind in pop])\n\n    best_idx = int(np.argmax(fitness_scores))\n    best_mask = pop[best_idx].copy()\n    best_score = fitness_scores[best_idx]\n    t1 = time.time()\n    log(f\"GA DONE in {int(t1-t0)}s best_score={best_score:.8f} selected={int(np.sum(best_mask))}\")\n    log(f\"GA SELECTED FEATURES: {mask_to_features(best_mask)}\")\n\n    return best_mask, best_score, int(t1-t0)\n\n# -------------------- GWO (binary) --------------------\ndef run_gwo(wolves=GWO_WOLVES, iters=GWO_ITERS, cv=CV_OPT):\n    log(f\"GWO START (wolves={wolves}, iters={iters}, cv={cv})\")\n    t0 = time.time()\n    dim = N_FEATURES\n    pop = np.random.randint(0,2,(wolves, dim)).astype(int)\n    fitness_scores = np.array([evaluate_mask_global(ind.astype(bool), cv=cv, cb_iter=CB_ITER_OPT) for ind in pop])\n\n    Alpha = Beta = Delta = None\n    Alpha_score = Beta_score = Delta_score = -1.0\n\n    for itr in range(iters):\n        log(f\" GWO iter {itr+1}/{iters} best_alpha={Alpha_score:.8f}\")\n        for i in range(wolves):\n            sc = fitness_scores[i]\n            if sc > Alpha_score:\n                Delta_score, Beta_score, Alpha_score = Beta_score, Alpha_score, sc\n                Delta, Beta, Alpha = Beta, Alpha, pop[i].copy()\n            elif sc > Beta_score:\n                Delta_score, Beta_score = Beta_score, sc\n                Delta, Beta = Beta, pop[i].copy()\n            elif sc > Delta_score:\n                Delta_score = sc\n                Delta = pop[i].copy()\n\n        a = 2 - itr * (2.0 / iters)\n        for i in range(wolves):\n            for d in range(dim):\n                if Alpha is None:\n                    continue\n                r1, r2 = np.random.rand(), np.random.rand()\n                A1 = 2 * a * r1 - a; C1 = 2 * r2\n                D_alpha = abs(C1 * Alpha[d] - pop[i][d])\n                X1 = Alpha[d] - A1 * D_alpha\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                A2 = 2 * a * r1 - a; C2 = 2 * r2\n                D_beta = abs(C2 * Beta[d] - pop[i][d])\n                X2 = Beta[d] - A2 * D_beta\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                A3 = 2 * a * r1 - a; C3 = 2 * r2\n                D_delta = abs(C3 * Delta[d] - pop[i][d])\n                X3 = Delta[d] - A3 * D_delta\n\n                new_pos = (X1 + X2 + X3) / 3.0\n                s = 1.0 / (1.0 + np.exp(-new_pos))\n                pop[i][d] = 1 if np.random.rand() < s else 0\n\n        fitness_scores = np.array([evaluate_mask_global(ind.astype(bool), cv=cv, cb_iter=CB_ITER_OPT) for ind in pop])\n\n    best_idx = int(np.argmax(fitness_scores))\n    best_mask = pop[best_idx].copy()\n    best_score = fitness_scores[best_idx]\n    t1 = time.time()\n    log(f\"GWO DONE in {int(t1-t0)}s best_score={best_score:.8f} selected={int(np.sum(best_mask))}\")\n    log(f\"GWO SELECTED FEATURES: {mask_to_features(best_mask)}\")\n\n    return best_mask, best_score, int(t1-t0)\n\n# -------------------- INTERSECTION / UNION / VOTING --------------------\ndef get_intersection_mask(*masks):\n    \"\"\"Return mask that contains only features present in ALL provided masks.\"\"\"\n    if len(masks) == 0:\n        return np.zeros(N_FEATURES, dtype=int)\n    inter_idx = set(np.where(np.array(masks[0]).astype(bool))[0].tolist())\n    for m in masks[1:]:\n        idxs = set(np.where(np.array(m).astype(bool))[0].tolist())\n        inter_idx = inter_idx.intersection(idxs)\n    mask = np.zeros(N_FEATURES, dtype=int)\n    for i in inter_idx:\n        mask[i] = 1\n    return mask\n\n\ndef get_union_mask(*masks):\n    union_idx = set()\n    for m in masks:\n        idxs = np.where(np.array(m).astype(bool))[0].tolist()\n        union_idx.update(idxs)\n    mask = np.zeros(N_FEATURES, dtype=int)\n    for i in union_idx:\n        mask[i] = 1\n    return mask\n\n\ndef get_voting_mask(*masks, threshold=2):\n    \"\"\"Return mask of features selected by at least `threshold` methods (default majority of 3 => 2).\"\"\"\n    if len(masks) == 0:\n        return np.zeros(N_FEATURES, dtype=int)\n    counts = np.zeros(N_FEATURES, dtype=int)\n    for m in masks:\n        counts += np.array(m).astype(int)\n    mask = (counts >= threshold).astype(int)\n    return mask\n\n# -------------------- HLO on candidates --------------------\ndef hlo_on_candidates(candidate_mask, pop_size=HLO_POP, iters=HLO_ITERS, cv=CV_OPT):\n    candidate_indices = np.where(np.array(candidate_mask).astype(bool))[0].tolist()\n    k = len(candidate_indices)\n    if k == 0:\n        raise ValueError(\"Candidate set is empty.\")\n\n    log(f\"HLO START on {k} candidate features (pop={pop_size}, iters={iters})\")\n    t0 = time.time()\n\n    pop = np.random.randint(0,2,(pop_size, k)).astype(int)\n\n    def fitness_candidate(bitmask):\n        full_mask = np.zeros(N_FEATURES, dtype=int)\n        for j,bit in enumerate(bitmask):\n            if bit == 1:\n                full_mask[candidate_indices[j]] = 1\n        return evaluate_mask_global(full_mask.astype(bool), cv=cv, cb_iter=CB_ITER_HLO)\n\n    fitness_scores = np.array([fitness_candidate(ind) for ind in pop])\n    best_idx = int(np.argmax(fitness_scores))\n    best_solution = pop[best_idx].copy()\n    best_score = fitness_scores[best_idx]\n\n    for it in range(iters):\n        log(f\" HLO iter {it+1}/{iters} current_best={best_score:.8f}\")\n        teacher = pop[int(np.argmax(fitness_scores))].copy()\n        new_pop = []\n        for i in range(pop_size):\n            learner = pop[i].copy()\n            # teaching phase\n            for d in range(k):\n                if np.random.rand() < HLO_TEACHER_FACTOR:\n                    learner[d] = teacher[d]\n            # peer learning\n            partner = pop[np.random.randint(pop_size)].copy()\n            for d in range(k):\n                if learner[d] != partner[d] and np.random.rand() < 0.5:\n                    learner[d] = partner[d]\n            # mutation\n            for d in range(k):\n                if np.random.rand() < HLO_MUTATION:\n                    learner[d] = 1 - learner[d]\n            new_pop.append(learner)\n        pop = np.array(new_pop)\n        fitness_scores = np.array([fitness_candidate(ind) for ind in pop])\n        gen_best_idx = int(np.argmax(fitness_scores))\n        gen_best_score = fitness_scores[gen_best_idx]\n        gen_best_sol = pop[gen_best_idx].copy()\n        if gen_best_score > best_score:\n            best_score = gen_best_score\n            best_solution = gen_best_sol.copy()\n\n    # map back to full mask\n    final_full_mask = np.zeros(N_FEATURES, dtype=int)\n    for j,bit in enumerate(best_solution):\n        if bit == 1:\n            final_full_mask[candidate_indices[j]] = 1\n\n    t1 = time.time()\n    log(f\"HLO DONE in {int(t1-t0)}s best_score={best_score:.8f} final_selected={int(np.sum(final_full_mask))}\")\n    return final_full_mask, best_score, int(t1-t0)\n\n# -------------------- Greedy Hill-Climb (local search) --------------------\ndef hill_climb_on_candidates(initial_mask, candidate_mask, max_steps=HILLCLIMB_MAX_STEPS, eval_cap=HILLCLIMB_EVAL_CAP, cv=CV_OPT):\n    \"\"\"\n    Greedy single-bit flip hill-climb restricted to candidate indices.\n    Starts from initial_mask (full-length). Tries flipping each candidate feature's bit:\n    - If flip improves fitness, accept and restart scanning.\n    - Stops when no improving flip found or max_steps/eval_cap reached.\n    \"\"\"\n    candidate_indices = np.where(np.array(candidate_mask).astype(bool))[0].tolist()\n    if len(candidate_indices) == 0:\n        log(\"Hill-climb: candidate set empty, skipping.\")\n        return initial_mask, 0.0, 0\n\n    log(f\"Hill-climb START over {len(candidate_indices)} candidates (max_steps={max_steps}, eval_cap={eval_cap})\")\n    t0 = time.time()\n    current_mask = initial_mask.copy()\n    current_score = evaluate_mask_global(current_mask.astype(bool), cv=cv, cb_iter=CB_ITER_HLO)\n    evals = 0\n    steps = 0\n    improved = True\n\n    while improved and steps < max_steps and evals < eval_cap:\n        improved = False\n        for idx in np.random.permutation(candidate_indices):\n            trial_mask = current_mask.copy()\n            trial_mask[idx] = 1 - trial_mask[idx]  # flip\n            trial_score = evaluate_mask_global(trial_mask.astype(bool), cv=cv, cb_iter=CB_ITER_HLO)\n            evals += 1\n            if trial_score > current_score + 1e-8:\n                current_mask = trial_mask\n                current_score = trial_score\n                improved = True\n                steps += 1\n                log(f\" Hill-climb step {steps}: flipped {FEATURE_NAMES[idx]} -> new_score={current_score:.4f} (evals={evals})\")\n                break\n            if evals >= eval_cap or steps >= max_steps:\n                break\n    t1 = time.time()\n    log(f\"Hill-climb DONE in {int(t1-t0)}s steps={steps} evals={evals} final_score={current_score:.8f} selected={int(np.sum(current_mask))}\")\n    return current_mask, current_score, int(t1-t0)\n\n# -------------------- Final evaluation (5-fold CV) --------------------\ndef final_evaluation(mask_bool, cv=CV_FINAL, cb_iter=CB_ITER_FINAL):\n    idxs = np.where(np.array(mask_bool).astype(bool))[0].tolist()\n    if len(idxs) == 0:\n        raise ValueError(\"Final mask selects zero features.\")\n    X_sel = X.iloc[:, idxs]\n    model = get_xgb_model(iterations=cb_iter)\n\n    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=RANDOM_STATE)\n    accs = []; precs = []; recs = []; f1s = []\n    t0 = time.time()\n    for tr,te in skf.split(X_sel, y):\n        m = clone(model); m.fit(X_sel.iloc[tr], y.iloc[tr])\n        pred = m.predict(X_sel.iloc[te])\n        accs.append(accuracy_score(y.iloc[te], pred))\n        precs.append(precision_score(y.iloc[te], pred, zero_division=0))\n        recs.append(recall_score(y.iloc[te], pred, zero_division=0))\n        f1s.append(f1_score(y.iloc[te], pred, zero_division=0))\n    t1 = time.time()\n    results = {\n        \"n_features\": len(idxs),\n        \"features\": [FEATURE_NAMES[i] for i in idxs],\n        \"acc_mean\": float(np.mean(accs)), \"acc_std\": float(np.std(accs)),\n        \"prec_mean\": float(np.mean(precs)), \"prec_std\": float(np.std(precs)),\n        \"rec_mean\": float(np.mean(recs)), \"rec_std\": float(np.std(recs)),\n        \"f1_mean\": float(np.mean(f1s)), \"f1_std\": float(np.std(f1s)),\n        \"eval_time_s\": int(t1 - t0)\n    }\n    return results\n\n# -------------------- MAIN PIPELINE --------------------\nif __name__ == \"__main__\":\n    total_t0 = time.time()\n    log(\"===== HYBRID (reduced budget) + HLO + HILL-CLIMB (UNION/INTERSECTION/VOTING) START =====\")\n\n    # PSO\n    pso_mask, pso_score, pso_time = run_pso(swarm_size=PSO_SWARM, iters=PSO_ITERS, cv=CV_OPT)\n\n    # GA\n    ga_mask, ga_score, ga_time = run_ga(pop_size=GA_POP, gens=GA_GENS, cv=CV_OPT)\n\n    # GWO\n    gwo_mask, gwo_score, gwo_time = run_gwo(wolves=GWO_WOLVES, iters=GWO_ITERS, cv=CV_OPT)\n\n    # Derive candidate masks\n    union_mask = get_union_mask(pso_mask, ga_mask, gwo_mask)\n    inter_mask = get_intersection_mask(pso_mask, ga_mask, gwo_mask)\n    vote_mask = get_voting_mask(pso_mask, ga_mask, gwo_mask, threshold=2)\n\n    candidate_sets = {\n        'union': union_mask,\n        'intersection': inter_mask,\n        'voting': vote_mask\n    }\n\n    results_all = {}\n\n    # run HLO -> hill-climb -> final evaluation -> train & save model for each candidate set\n    for name, cand_mask in candidate_sets.items():\n        log(f\"===== PROCESSING {name.upper()} CANDIDATES =====\")\n        n_cand = int(np.sum(cand_mask))\n        log(f\"{name.upper()} candidate features: {n_cand}\")\n        if n_cand == 0:\n            log(f\"{name.upper()} empty — skipping HLO/hill-climb and model training.\")\n            results_all[name] = {'skipped': True, 'n_candidates': 0}\n            continue\n\n        # HLO on this candidate set\n        hlo_mask, hlo_score, hlo_time = hlo_on_candidates(cand_mask, pop_size=HLO_POP, iters=HLO_ITERS, cv=CV_OPT)\n\n        # hill-climb restricted to candidate set\n        hc_mask, hc_score, hc_time = hill_climb_on_candidates(hlo_mask, cand_mask, max_steps=HILLCLIMB_MAX_STEPS, eval_cap=HILLCLIMB_EVAL_CAP, cv=CV_OPT)\n\n        # final CV evaluation\n        final_res = final_evaluation(hc_mask, cv=CV_FINAL, cb_iter=CB_ITER_FINAL)\n\n        # Train final CatBoost model on 80% train and evaluate on 20% test (stratified)\n        sel_idxs = np.where(np.array(hc_mask).astype(bool))[0].tolist()\n        sel_features = [FEATURE_NAMES[i] for i in sel_idxs]\n\n        if len(sel_features) == 0:\n            log(f\"No features selected after hill-climb for {name}, skipping model train.\")\n            results_all[name] = {'skipped': True, 'n_candidates': n_cand}\n            continue\n\n        X_sel = X[sel_features]\n        X_train, X_test, y_train, y_test = train_test_split(X_sel, y, test_size=FINAL_TEST_SIZE, stratify=y, random_state=RANDOM_STATE)\n\n        \n        model = get_xgb_model(iterations=CB_ITER_FINAL)\n        model.fit(X_train, y_train)\n\n        # evaluate on held-out test set (20%)\n        y_pred = model.predict(X_test)\n        test_acc = accuracy_score(y_test, y_pred)\n        test_prec = precision_score(y_test, y_pred, zero_division=0)\n        test_rec = recall_score(y_test, y_pred, zero_division=0)\n        test_f1 = f1_score(y_test, y_pred, zero_division=0)\n\n        test_metrics = {\n            'acc': float(test_acc), 'prec': float(test_prec), 'rec': float(test_rec), 'f1': float(test_f1),\n            'n_test': int(X_test.shape[0])\n        }\n\n        # Save model to file (pickle)\n        model_filename = f\"{SAVE_PREFIX}_{name}_model.pkl\"\n        with open(model_filename, 'wb') as mf:\n            pickle.dump(model, mf)\n\n        # store results\n        results_all[name] = {\n            'n_candidates': n_cand,\n            'hlo_score': float(hlo_score), 'hlo_time': int(hlo_time),\n            'hc_score': float(hc_score), 'hc_time': int(hc_time),\n            'final_eval': final_res,\n            'selected_features': sel_features,\n            'model_file': model_filename,\n            'test_metrics': test_metrics\n        }\n\n        log(f\"Saved trained XGboost model for {name} -> {model_filename} (test_f1={test_f1:.8f})\")\n\n    total_t1 = time.time()\n    elapsed_total = int(total_t1 - total_t0)\n\n    # Summary / save aggregated results\n    print(\"==================== AGGREGATE SUMMARY ====================\")\n    print(f\"PSO  -> opt_score={pso_score:.8f} selected={int(np.sum(pso_mask))} time={pso_time}s\")\n    print(f\"GA   -> opt_score={ga_score:.8f} selected={int(np.sum(ga_mask))} time={ga_time}s\")\n    print(f\"GWO  -> opt_score={gwo_score:.8f} selected={int(np.sum(gwo_mask))} time={gwo_time}s\")\n    print(f\"Union candidates    : {int(np.sum(union_mask))}\")\n    print(f\"Intersection candidates: {int(np.sum(inter_mask))}\")\n    print(f\"Voting candidates   : {int(np.sum(vote_mask))}\")\n    print(\"-------------------------------------------------\")\n\n    for name, info in results_all.items():\n        print(f\"-- {name.upper()} SUMMARY --\")\n        if info.get('skipped'):\n            print(\" skipped (no candidates)\")\n            continue\n        fe = info['final_eval']\n        tm = info['test_metrics']\n        print(f\" Selected ({fe['n_features']}): {fe['features']}\")\n        print(f\" CV F1   : {fe['f1_mean']:.8f} ± {fe['f1_std']:.8f}\")\n        print(f\" Test F1 : {tm['f1']:.8f} (n_test={tm['n_test']})\")\n        print(f\" Accuracy : {fe['acc_mean']:.8f} ± {fe['acc_std']:.8f}\")\n        print(f\" Precision: {fe['prec_mean']:.8f} ± {fe['prec_std']:.8f}\")\n        print(f\" Recall   : {fe['rec_mean']:.8f} ± {fe['rec_std']:.8f}\")\n        print(f\" Model file: {info['model_file']}\")\n\n\n\n    # Save aggregated pipeline outputs\n    out = {\n        \"pso_mask\": pso_mask, \"pso_score\": pso_score, \"pso_time\": pso_time,\n        \"ga_mask\": ga_mask, \"ga_score\": ga_score, \"ga_time\": ga_time,\n        \"gwo_mask\": gwo_mask, \"gwo_score\": gwo_score, \"gwo_time\": gwo_time,\n        \"union_mask\": union_mask, \"intersection_mask\": inter_mask, \"voting_mask\": vote_mask,\n        \"results_all\": results_all,\n        \"fitness_cache_len\": len(fitness_cache)\n    }\n    with open(f\"{SAVE_PREFIX}_results.pkl\", \"wb\") as f:\n        pickle.dump(out, f)\n\n    log(f\"Saved results to {SAVE_PREFIX}_results.pkl\")\n    log(\"===== PIPELINE COMPLETE =====\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T09:26:44.519937Z","iopub.execute_input":"2025-12-09T09:26:44.520585Z","iopub.status.idle":"2025-12-09T11:06:27.803238Z","shell.execute_reply.started":"2025-12-09T09:26:44.520556Z","shell.execute_reply":"2025-12-09T11:06:27.802654Z"}},"outputs":[{"name":"stdout","text":"[09:26:45] ===== HYBRID (reduced budget) + HLO + HILL-CLIMB (UNION/INTERSECTION/VOTING) START =====\n[09:26:45] PSO START (swarm=15, iters=10, cv=2)\n[09:27:57]  PSO iter 1/10 best_global=0.9992303\n[09:29:07]  PSO iter 2/10 best_global=0.99924071\n[09:30:18]  PSO iter 3/10 best_global=0.99924071\n[09:31:23]  PSO iter 4/10 best_global=0.99924071\n[09:32:32]  PSO iter 5/10 best_global=0.99924071\n[09:33:43]  PSO iter 6/10 best_global=0.99927191\n[09:34:56]  PSO iter 7/10 best_global=0.99927191\n[09:36:06]  PSO iter 8/10 best_global=0.99927191\n[09:37:12]  PSO iter 9/10 best_global=0.9993031\n[09:38:18]  PSO iter 10/10 best_global=0.9993031\n[09:39:28] PSO DONE in 762s best_score=0.99930310 selected=34\n[09:39:28] PSO SELECTED FEATURES: ['Dst Port', 'Protocol', 'Timestamp', 'Tot Fwd Pkts', 'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std', 'Bwd Pkt Len Max', 'Bwd Pkt Len Min', 'Bwd Pkt Len Mean', 'Flow Pkts/s', 'Flow IAT Std', 'Flow IAT Min', 'Fwd IAT Tot', 'Fwd IAT Mean', 'Fwd IAT Max', 'Fwd IAT Min', 'Fwd URG Flags', 'Pkt Len Max', 'Pkt Len Std', 'FIN Flag Cnt', 'SYN Flag Cnt', 'RST Flag Cnt', 'URG Flag Cnt', 'CWE Flag Count', 'ECE Flag Cnt', 'Subflow Fwd Pkts', 'Subflow Fwd Byts', 'Fwd Seg Size Min', 'Active Mean', 'Active Min', 'Idle Std', 'Idle Max', 'Dst IP']\n[09:39:28] GA START (pop=30, gens=10, cv=2)\n[09:41:44]  GA gen 1/10 current_best=0.99918874\n[09:43:48]  GA gen 2/10 current_best=0.99918874\n[09:45:52]  GA gen 3/10 current_best=0.99923035\n[09:47:53]  GA gen 4/10 current_best=0.99926150\n[09:49:53]  GA gen 5/10 current_best=0.99929269\n[09:51:59]  GA gen 6/10 current_best=0.99930308\n[09:54:02]  GA gen 7/10 current_best=0.99930309\n[09:56:03]  GA gen 8/10 current_best=0.99930309\n[09:58:04]  GA gen 9/10 current_best=0.99930309\n[10:00:09]  GA gen 10/10 current_best=0.99932391\n[10:02:12] GA DONE in 1364s best_score=0.99932391 selected=35\n[10:02:12] GA SELECTED FEATURES: ['Dst Port', 'Protocol', 'Timestamp', 'Flow Duration', 'Tot Bwd Pkts', 'Fwd Pkt Len Mean', 'Bwd Pkt Len Std', 'Flow IAT Min', 'Fwd IAT Tot', 'Fwd IAT Std', 'Bwd IAT Tot', 'Bwd IAT Mean', 'Bwd IAT Max', 'Fwd PSH Flags', 'Fwd Header Len', 'Fwd Pkts/s', 'Bwd Pkts/s', 'Pkt Len Min', 'Pkt Len Max', 'Pkt Len Var', 'SYN Flag Cnt', 'RST Flag Cnt', 'Fwd Seg Size Avg', 'Bwd Seg Size Avg', 'Subflow Fwd Byts', 'Subflow Bwd Pkts', 'Init Bwd Win Byts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Active Max', 'Idle Min', 'Flow ID', 'Src Port', 'Dst IP']\n[10:02:12] GWO START (wolves=10, iters=10, cv=2)\n[10:02:57]  GWO iter 1/10 best_alpha=-1.00000000\n[10:03:47]  GWO iter 2/10 best_alpha=0.99916792\n[10:04:39]  GWO iter 3/10 best_alpha=0.99916792\n[10:05:33]  GWO iter 4/10 best_alpha=0.99926148\n[10:06:30]  GWO iter 5/10 best_alpha=0.99926148\n[10:07:28]  GWO iter 6/10 best_alpha=0.99926148\n[10:08:20]  GWO iter 7/10 best_alpha=0.99926148\n[10:09:12]  GWO iter 8/10 best_alpha=0.99926148\n[10:10:04]  GWO iter 9/10 best_alpha=0.99926148\n[10:10:59]  GWO iter 10/10 best_alpha=0.99926148\n[10:11:51] GWO DONE in 579s best_score=0.99925110 selected=51\n[10:11:51] GWO SELECTED FEATURES: ['Dst Port', 'Timestamp', 'Tot Fwd Pkts', 'Tot Bwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Min', 'Fwd Pkt Len Std', 'Bwd Pkt Len Max', 'Bwd Pkt Len Mean', 'Bwd Pkt Len Std', 'Flow Pkts/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Min', 'Fwd IAT Tot', 'Fwd IAT Mean', 'Bwd IAT Mean', 'Bwd IAT Std', 'Fwd URG Flags', 'Fwd Header Len', 'Bwd Header Len', 'Fwd Pkts/s', 'Bwd Pkts/s', 'Pkt Len Max', 'Pkt Len Mean', 'Pkt Len Std', 'FIN Flag Cnt', 'SYN Flag Cnt', 'RST Flag Cnt', 'PSH Flag Cnt', 'ACK Flag Cnt', 'CWE Flag Count', 'ECE Flag Cnt', 'Down/Up Ratio', 'Bwd Seg Size Avg', 'Subflow Fwd Pkts', 'Subflow Fwd Byts', 'Subflow Bwd Pkts', 'Subflow Bwd Byts', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Mean', 'Active Min', 'Idle Std', 'Idle Max', 'Flow ID', 'Src Port', 'Dst IP']\n[10:11:51] ===== PROCESSING UNION CANDIDATES =====\n[10:11:51] UNION candidate features: 68\n[10:11:51] HLO START on 68 candidate features (pop=15, iters=10)\n[10:13:34]  HLO iter 1/10 current_best=0.99921995\n[10:15:22]  HLO iter 2/10 current_best=0.99921995\n[10:17:00]  HLO iter 3/10 current_best=0.99924076\n[10:18:40]  HLO iter 4/10 current_best=0.99924076\n[10:20:16]  HLO iter 5/10 current_best=0.99926157\n[10:21:55]  HLO iter 6/10 current_best=0.99926157\n[10:23:37]  HLO iter 7/10 current_best=0.99928235\n[10:25:17]  HLO iter 8/10 current_best=0.99930315\n[10:27:00]  HLO iter 9/10 current_best=0.99930315\n[10:28:41]  HLO iter 10/10 current_best=0.99931356\n[10:30:22] HLO DONE in 1111s best_score=0.99934475 final_selected=32\n[10:30:22] Hill-climb START over 68 candidates (max_steps=100, eval_cap=500)\n[10:32:19]  Hill-climb step 1: flipped Tot Fwd Pkts -> new_score=0.9994 (evals=19)\n[10:39:14] Hill-climb DONE in 531s steps=1 evals=87 final_score=0.99935514 selected=33\n[10:39:29] Saved trained XGboost model for union -> hybrid_hlo_models_union_model.pkl (test_f1=0.99937225)\n[10:39:29] ===== PROCESSING INTERSECTION CANDIDATES =====\n[10:39:29] INTERSECTION candidate features: 11\n[10:39:29] HLO START on 11 candidate features (pop=15, iters=10)\n[10:40:16]  HLO iter 1/10 current_best=0.99927197\n[10:41:01]  HLO iter 2/10 current_best=0.99927197\n[10:41:45]  HLO iter 3/10 current_best=0.99927197\n[10:42:11]  HLO iter 4/10 current_best=0.99927197\n[10:42:43]  HLO iter 5/10 current_best=0.99927197\n[10:43:27]  HLO iter 6/10 current_best=0.99927197\n[10:43:57]  HLO iter 7/10 current_best=0.99927197\n[10:44:21]  HLO iter 8/10 current_best=0.99927197\n[10:44:47]  HLO iter 9/10 current_best=0.99927197\n[10:45:16]  HLO iter 10/10 current_best=0.99927197\n[10:45:47] HLO DONE in 377s best_score=0.99927197 final_selected=7\n[10:45:47] Hill-climb START over 11 candidates (max_steps=100, eval_cap=500)\n[10:45:56] Hill-climb DONE in 8s steps=0 evals=11 final_score=0.99927197 selected=7\n[10:46:03] Saved trained XGboost model for intersection -> hybrid_hlo_models_intersection_model.pkl (test_f1=0.99937232)\n[10:46:03] ===== PROCESSING VOTING CANDIDATES =====\n[10:46:03] VOTING candidate features: 41\n[10:46:03] HLO START on 41 candidate features (pop=15, iters=10)\n[10:47:16]  HLO iter 1/10 current_best=0.99928236\n[10:48:27]  HLO iter 2/10 current_best=0.99928236\n[10:49:39]  HLO iter 3/10 current_best=0.99929270\n[10:50:56]  HLO iter 4/10 current_best=0.99929270\n[10:52:14]  HLO iter 5/10 current_best=0.99931349\n[10:53:27]  HLO iter 6/10 current_best=0.99931349\n[10:54:38]  HLO iter 7/10 current_best=0.99931358\n[10:55:48]  HLO iter 8/10 current_best=0.99933433\n[10:56:56]  HLO iter 9/10 current_best=0.99933433\n[10:58:05]  HLO iter 10/10 current_best=0.99933433\n[10:59:12] HLO DONE in 789s best_score=0.99933433 final_selected=19\n[10:59:12] Hill-climb START over 41 candidates (max_steps=100, eval_cap=500)\n[10:59:24]  Hill-climb step 1: flipped Bwd IAT Mean -> new_score=0.9993 (evals=3)\n[11:00:15]  Hill-climb step 2: flipped Fwd Pkt Len Std -> new_score=0.9994 (evals=16)\n[11:01:24]  Hill-climb step 3: flipped Bwd IAT Mean -> new_score=0.9994 (evals=32)\n[11:03:23]  Hill-climb step 4: flipped Bwd Pkt Len Max -> new_score=0.9994 (evals=62)\n[11:06:16] Hill-climb DONE in 424s steps=4 evals=103 final_score=0.99938633 selected=21\n[11:06:27] Saved trained XGboost model for voting -> hybrid_hlo_models_voting_model.pkl (test_f1=0.99937225)\n==================== AGGREGATE SUMMARY ====================\nPSO  -> opt_score=0.99930310 selected=34 time=762s\nGA   -> opt_score=0.99932391 selected=35 time=1364s\nGWO  -> opt_score=0.99925110 selected=51 time=579s\nUnion candidates    : 68\nIntersection candidates: 11\nVoting candidates   : 41\n-------------------------------------------------\n-- UNION SUMMARY --\n Selected (33): ['Dst Port', 'Timestamp', 'Tot Fwd Pkts', 'Tot Bwd Pkts', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std', 'Bwd Pkt Len Max', 'Flow Pkts/s', 'Flow IAT Mean', 'Flow IAT Min', 'Fwd IAT Tot', 'Fwd IAT Mean', 'Fwd IAT Std', 'Bwd IAT Tot', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Fwd Header Len', 'Bwd Header Len', 'Bwd Pkts/s', 'Pkt Len Mean', 'Pkt Len Var', 'ACK Flag Cnt', 'URG Flag Cnt', 'ECE Flag Cnt', 'Fwd Seg Size Avg', 'Subflow Fwd Byts', 'Subflow Bwd Pkts', 'Init Fwd Win Byts', 'Active Mean', 'Active Max', 'Idle Min', 'Src Port']\n CV F1   : 0.99933058 ± 0.00010134\n Test F1 : 0.99937225 (n_test=19561)\n Accuracy : 0.99934562 ± 0.00009913\n Precision: 0.99947702 ± 0.00025603\n Recall   : 0.99918425 ± 0.00012198\n Model file: hybrid_hlo_models_union_model.pkl\n-- INTERSECTION SUMMARY --\n Selected (7): ['Dst Port', 'Timestamp', 'Fwd IAT Tot', 'RST Flag Cnt', 'Subflow Fwd Byts', 'Fwd Seg Size Min', 'Dst IP']\n CV F1   : 0.99932020 ± 0.00011926\n Test F1 : 0.99937232 (n_test=19561)\n Accuracy : 0.99933539 ± 0.00011658\n Precision: 0.99933076 ± 0.00026084\n Recall   : 0.99930975 ± 0.00025274\n Model file: hybrid_hlo_models_intersection_model.pkl\n-- VOTING SUMMARY --\n Selected (21): ['Dst Port', 'Timestamp', 'Tot Fwd Pkts', 'Tot Bwd Pkts', 'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std', 'Bwd Pkt Len Max', 'Flow IAT Min', 'Fwd IAT Mean', 'Fwd Header Len', 'SYN Flag Cnt', 'RST Flag Cnt', 'CWE Flag Count', 'ECE Flag Cnt', 'Bwd Seg Size Avg', 'Subflow Fwd Pkts', 'Fwd Seg Size Min', 'Active Mean', 'Idle Max', 'Dst IP']\n CV F1   : 0.99941429 ± 0.00006092\n Test F1 : 0.99937225 (n_test=19561)\n Accuracy : 0.99942742 ± 0.00005961\n Precision: 0.99949799 ± 0.00020262\n Recall   : 0.99933067 ± 0.00014187\n Model file: hybrid_hlo_models_voting_model.pkl\n[11:06:27] Saved results to hybrid_hlo_models_results.pkl\n[11:06:27] ===== PIPELINE COMPLETE =====\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"print(\"hi\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T16:54:08.788783Z","iopub.execute_input":"2026-01-10T16:54:08.789089Z","iopub.status.idle":"2026-01-10T16:54:08.800285Z","shell.execute_reply.started":"2026-01-10T16:54:08.789059Z","shell.execute_reply":"2026-01-10T16:54:08.799321Z"}},"outputs":[{"name":"stdout","text":"hi\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ==========================================================\n# ABLATION-1 : OPTIMIZER CONTRIBUTION STUDY\n# PSO vs GA vs GWO vs Hybrid (Voting)\n# ==========================================================\n\nimport numpy as np\nimport pandas as pd\nimport time\nimport pickle\nimport warnings\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, make_scorer\nfrom sklearn.base import clone\nfrom xgboost import XGBClassifier\n\nwarnings.filterwarnings(\"ignore\")\nnp.random.seed(42)\n\n# ------------------ DATA ------------------\ndf = pd.read_csv(\"/kaggle/input/ids-cleaned/ids2018_cleaned_combined_1.csv\")\nTARGET_COL = \"Label\"\n\nX = df.drop(TARGET_COL, axis=1)\ny = df[TARGET_COL].astype(int)\nFEATURE_NAMES = X.columns.tolist()\nN_FEATURES = X.shape[1]\n\n# ------------------ MODEL ------------------\ndef get_xgb_model(iterations=200):\n    return XGBClassifier(\n        n_estimators=iterations,\n        learning_rate=0.05,\n        max_depth=6,\n        subsample=1.0,\n        colsample_bytree=1.0,\n        random_state=42,\n        n_jobs=-1,\n        eval_metric=\"logloss\"\n    )\n\n# ------------------ FITNESS ------------------\nfitness_cache = {}\n\ndef key_from_mask(mask):\n    return tuple(sorted(np.where(mask)[0].tolist()))\n\ndef evaluate_mask(mask, cv=3, iters=100):\n    key = key_from_mask(mask)\n    if key in fitness_cache:\n        return fitness_cache[key]\n    if len(key) == 0:\n        return 0.0\n\n    X_sel = X.iloc[:, list(key)]\n    model = get_xgb_model(iters)\n    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=42)\n\n    acc = cross_val_score(model, X_sel, y, cv=skf, scoring=\"accuracy\").mean()\n    f1  = cross_val_score(model, X_sel, y, cv=skf, scoring=make_scorer(f1_score)).mean()\n\n    score = (acc + f1) / 2\n    fitness_cache[key] = score\n    return score\n\n# ------------------ PSO ------------------\ndef run_pso(swarm=15, iters=10):\n    dim = N_FEATURES\n    pos = np.random.randint(0,2,(swarm,dim))\n    vel = np.random.uniform(-1,1,(swarm,dim))\n\n    pbest = pos.copy()\n    pbest_score = np.array([evaluate_mask(p.astype(bool)) for p in pos])\n\n    gbest = pbest[np.argmax(pbest_score)]\n    gbest_score = np.max(pbest_score)\n\n    for _ in range(iters):\n        for i in range(swarm):\n            vel[i] = 0.5*vel[i] + 1.5*np.random.rand(dim)*(pbest[i]-pos[i]) + 1.5*np.random.rand(dim)*(gbest-pos[i])\n            s = 1/(1+np.exp(-vel[i]))\n            pos[i] = (np.random.rand(dim)<s).astype(int)\n            sc = evaluate_mask(pos[i].astype(bool))\n            if sc > pbest_score[i]:\n                pbest[i] = pos[i].copy()\n                pbest_score[i] = sc\n        gbest = pbest[np.argmax(pbest_score)]\n        gbest_score = np.max(pbest_score)\n\n    return gbest.astype(bool)\n\n# ------------------ GA ------------------\ndef run_ga(pop=30, gens=10):\n    dim = N_FEATURES\n    P = np.random.randint(0,2,(pop,dim))\n    fitness = np.array([evaluate_mask(p.astype(bool)) for p in P])\n\n    for _ in range(gens):\n        newP = []\n        elite = P[np.argmax(fitness)]\n        newP.append(elite)\n        while len(newP)<pop:\n            i,j = np.random.randint(0,pop,2)\n            p1,p2 = P[i],P[j]\n            pt = np.random.randint(dim)\n            c = np.concatenate([p1[:pt],p2[pt:]])\n            if np.random.rand()<0.1:\n                idx = np.random.randint(dim)\n                c[idx]=1-c[idx]\n            newP.append(c)\n        P = np.array(newP)\n        fitness = np.array([evaluate_mask(p.astype(bool)) for p in P])\n    return P[np.argmax(fitness)].astype(bool)\n\n# ------------------ GWO ------------------\ndef run_gwo(wolves=10, iters=10):\n    dim = N_FEATURES\n    W = np.random.randint(0,2,(wolves,dim))\n    fitness = np.array([evaluate_mask(w.astype(bool)) for w in W])\n\n    for t in range(iters):\n        idx = np.argsort(fitness)[::-1]\n        A,B,D = W[idx[0]],W[idx[1]],W[idx[2]]\n        a = 2 - 2*(t/iters)\n        for i in range(wolves):\n            for d in range(dim):\n                r1,r2 = np.random.rand(),np.random.rand()\n                X1 = A[d] - (2*a*r1-a)*abs(2*r2*A[d]-W[i][d])\n                r1,r2 = np.random.rand(),np.random.rand()\n                X2 = B[d] - (2*a*r1-a)*abs(2*r2*B[d]-W[i][d])\n                r1,r2 = np.random.rand(),np.random.rand()\n                X3 = D[d] - (2*a*r1-a)*abs(2*r2*D[d]-W[i][d])\n                s = 1/(1+np.exp(-(X1+X2+X3)/3))\n                W[i][d] = 1 if np.random.rand()<s else 0\n        fitness = np.array([evaluate_mask(w.astype(bool)) for w in W])\n    return W[np.argmax(fitness)].astype(bool)\n\n# ------------------ VOTING ------------------\ndef voting(pso,ga,gwo):\n    return ((pso.astype(int)+ga.astype(int)+gwo.astype(int))>=2)\n\n# ------------------ FINAL EVAL ------------------\ndef final_eval(mask):\n    idx = np.where(mask)[0]\n    Xs = X.iloc[:,idx]\n    model = get_xgb_model(500)\n    skf = StratifiedKFold(5,shuffle=True,random_state=42)\n    f1=[]\n    for tr,te in skf.split(Xs,y):\n        model.fit(Xs.iloc[tr],y.iloc[tr])\n        p=model.predict(Xs.iloc[te])\n        f1.append(f1_score(y.iloc[te],p))\n    return len(idx),np.mean(f1)\n\n# ================= RUN ABLATION =================\nprint(\"\\nRunning PSO...\")\npso = run_pso()\nprint(\"Running GA...\")\nga = run_ga()\nprint(\"Running GWO...\")\ngwo = run_gwo()\nhyb = voting(pso,ga,gwo)\n\nprint(\"\\nFinal 5-Fold CV\")\nprint(\"Method     | Features | F1\")\nprint(\"--------------------------------\")\nfor name,mask in zip([\"PSO\",\"GA\",\"GWO\",\"Hybrid\"],[pso,ga,gwo,hyb]):\n    n,f1 = final_eval(mask)\n    print(f\"{name:10s} | {n:8d} | {f1:.6f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T16:54:24.714098Z","iopub.execute_input":"2026-01-10T16:54:24.715158Z","iopub.status.idle":"2026-01-10T16:56:20.548322Z","shell.execute_reply.started":"2026-01-10T16:54:24.715116Z","shell.execute_reply":"2026-01-10T16:56:20.547081Z"}},"outputs":[{"name":"stdout","text":"\nRunning PSO...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/942788735.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;31m# ================= RUN ABLATION =================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nRunning PSO...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m \u001b[0mpso\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_pso\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Running GA...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0mga\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_ga\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_47/942788735.py\u001b[0m in \u001b[0;36mrun_pso\u001b[0;34m(swarm, iters)\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mvel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mpos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msc\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mpbest_score\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0mpbest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_47/942788735.py\u001b[0m in \u001b[0;36mevaluate_mask\u001b[0;34m(mask, cv, iters)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_sel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0mf1\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_sel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_scorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mf1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    513\u001b[0m     \u001b[0mscorer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m     cv_results = cross_validate(\n\u001b[0m\u001b[1;32m    516\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# independent, and that it is pickle-able.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0mparallel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     results = parallel(\n\u001b[0m\u001b[1;32m    267\u001b[0m         delayed(_fit_and_score)(\n\u001b[1;32m    268\u001b[0m             \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1984\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1985\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1986\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1988\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1912\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1913\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1914\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1915\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1517\u001b[0m             )\n\u001b[1;32m   1518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1519\u001b[0;31m             self._Booster = train(\n\u001b[0m\u001b[1;32m   1520\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1521\u001b[0m                 \u001b[0mtrain_dmatrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2049\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2050\u001b[0m             _check_call(\n\u001b[0;32m-> 2051\u001b[0;31m                 _LIB.XGBoosterUpdateOneIter(\n\u001b[0m\u001b[1;32m   2052\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2053\u001b[0m                 )\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":2},{"cell_type":"code","source":"# ==========================================================\n# ABLATION-1 : OPTIMIZER CONTRIBUTION STUDY\n# PSO vs GA vs GWO vs Hybrid (Voting)\n# ==========================================================\n\nimport numpy as np\nimport pandas as pd\nimport warnings\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\nfrom sklearn.metrics import f1_score, make_scorer\nfrom xgboost import XGBClassifier\n\nwarnings.filterwarnings(\"ignore\")\nnp.random.seed(42)\n\n# ---------------- DATA ----------------\ndf = pd.read_csv(\"/kaggle/input/ids-cleaned/ids2018_cleaned_combined_1.csv\")\nTARGET_COL = \"Label\"\n\nX = df.drop(TARGET_COL, axis=1)\ny = df[TARGET_COL].astype(int)\nN_FEATURES = X.shape[1]\n\n# ---------------- MODEL ----------------\ndef get_xgb_model(iters=150):\n    return XGBClassifier(\n        n_estimators=iters,\n        learning_rate=0.05,\n        max_depth=6,\n        subsample=1.0,\n        colsample_bytree=1.0,\n        random_state=42,\n        n_jobs=-1,\n        eval_metric=\"logloss\"\n    )\n\n# ---------------- FITNESS ----------------\nfitness_cache = {}\n\ndef key_from_mask(mask):\n    return tuple(sorted(np.where(mask)[0].tolist()))\n\ndef evaluate_mask(mask, cv=3):\n    key = key_from_mask(mask)\n    if key in fitness_cache:\n        return fitness_cache[key]\n    if len(key)==0:\n        return 0.0\n\n    Xs = X.iloc[:, list(key)]\n    model = get_xgb_model(100)\n    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=42)\n    f1 = cross_val_score(model, Xs, y, cv=skf, scoring=make_scorer(f1_score)).mean()\n    fitness_cache[key] = f1\n    return f1\n\n# ---------------- PSO ----------------\ndef run_pso(swarm=15, iters=5):\n    print(\"\\n[PSO] Start\")\n    dim = N_FEATURES\n    pos = np.random.randint(0,2,(swarm,dim))\n    vel = np.random.uniform(-1,1,(swarm,dim))\n    pbest = pos.copy()\n    pbest_score = np.array([evaluate_mask(p.astype(bool)) for p in pos])\n    gbest = pbest[np.argmax(pbest_score)]\n    gbest_score = np.max(pbest_score)\n\n    print(f\"[PSO] Initial best = {gbest_score:.6f}, features = {np.sum(gbest)}\")\n\n    for t in range(iters):\n        for i in range(swarm):\n            vel[i] = 0.5*vel[i] + 1.5*np.random.rand(dim)*(pbest[i]-pos[i]) + 1.5*np.random.rand(dim)*(gbest-pos[i])\n            s = 1/(1+np.exp(-vel[i]))\n            pos[i] = (np.random.rand(dim)<s).astype(int)\n            sc = evaluate_mask(pos[i].astype(bool))\n            if sc > pbest_score[i]:\n                pbest[i] = pos[i].copy()\n                pbest_score[i] = sc\n        gbest = pbest[np.argmax(pbest_score)]\n        gbest_score = np.max(pbest_score)\n        print(f\"[PSO] Iter {t+1}/{iters} best = {gbest_score:.6f}, features = {np.sum(gbest)}\")\n    return gbest.astype(bool)\n\n# ---------------- GA ----------------\ndef run_ga(pop=30, gens=5):\n    print(\"\\n[GA] Start\")\n    dim = N_FEATURES\n    P = np.random.randint(0,2,(pop,dim))\n    fitness = np.array([evaluate_mask(p.astype(bool)) for p in P])\n    print(f\"[GA] Initial best = {np.max(fitness):.6f}\")\n\n    for g in range(gens):\n        newP=[P[np.argmax(fitness)]]\n        while len(newP)<pop:\n            i,j = np.random.randint(0,pop,2)\n            p1,p2=P[i],P[j]\n            pt=np.random.randint(dim)\n            c=np.concatenate([p1[:pt],p2[pt:]])\n            if np.random.rand()<0.1:\n                idx=np.random.randint(dim); c[idx]=1-c[idx]\n            newP.append(c)\n        P=np.array(newP)\n        fitness=np.array([evaluate_mask(p.astype(bool)) for p in P])\n        best=P[np.argmax(fitness)]\n        print(f\"[GA] Gen {g+1}/{gens} best = {np.max(fitness):.6f}, features = {np.sum(best)}\")\n    return P[np.argmax(fitness)].astype(bool)\n\n# ---------------- GWO ----------------\ndef run_gwo(wolves=10, iters=5):\n    print(\"\\n[GWO] Start\")\n    dim = N_FEATURES\n    W = np.random.randint(0,2,(wolves,dim))\n    fitness = np.array([evaluate_mask(w.astype(bool)) for w in W])\n    print(f\"[GWO] Initial best = {np.max(fitness):.6f}\")\n\n    for t in range(iters):\n        idx=np.argsort(fitness)[::-1]\n        A,B,D=W[idx[0]],W[idx[1]],W[idx[2]]\n        a=2-2*(t/iters)\n        for i in range(wolves):\n            for d in range(dim):\n                r1,r2=np.random.rand(),np.random.rand()\n                X1=A[d]-(2*a*r1-a)*abs(2*r2*A[d]-W[i][d])\n                r1,r2=np.random.rand(),np.random.rand()\n                X2=B[d]-(2*a*r1-a)*abs(2*r2*B[d]-W[i][d])\n                r1,r2=np.random.rand(),np.random.rand()\n                X3=D[d]-(2*a*r1-a)*abs(2*r2*D[d]-W[i][d])\n                s=1/(1+np.exp(-(X1+X2+X3)/3))\n                W[i][d]=1 if np.random.rand()<s else 0\n        fitness=np.array([evaluate_mask(w.astype(bool)) for w in W])\n        best=W[np.argmax(fitness)]\n        print(f\"[GWO] Iter {t+1}/{iters} best = {np.max(fitness):.6f}, features = {np.sum(best)}\")\n    return W[np.argmax(fitness)].astype(bool)\n\n# ---------------- Voting ----------------\ndef voting(pso,ga,gwo):\n    return ((pso.astype(int)+ga.astype(int)+gwo.astype(int))>=2)\n\n# ---------------- Final Evaluation ----------------\ndef final_eval(mask):\n    idx=np.where(mask)[0]\n    Xs=X.iloc[:,idx]\n    model=get_xgb_model(400)\n    skf=StratifiedKFold(5,shuffle=True,random_state=42)\n    f1=[]\n    for tr,te in skf.split(Xs,y):\n        model.fit(Xs.iloc[tr],y.iloc[tr])\n        p=model.predict(Xs.iloc[te])\n        f1.append(f1_score(y.iloc[te],p))\n    return len(idx),np.mean(f1)\n\n# ================= RUN =================\npso=run_pso()\nga=run_ga()\ngwo=run_gwo()\nhyb=voting(pso,ga,gwo)\n\nprint(\"\\n=========== ABLATION RESULTS ===========\")\nprint(\"Method    | Features | F1\")\nprint(\"--------------------------------\")\nfor name,mask in zip([\"PSO\",\"GA\",\"GWO\",\"Hybrid\"],[pso,ga,gwo,hyb]):\n    n,f1=final_eval(mask)\n    print(f\"{name:9s} | {n:8d} | {f1:.6f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T16:59:07.745228Z","iopub.execute_input":"2026-01-10T16:59:07.746052Z","iopub.status.idle":"2026-01-10T17:14:54.000751Z","shell.execute_reply.started":"2026-01-10T16:59:07.746019Z","shell.execute_reply":"2026-01-10T17:14:53.999789Z"}},"outputs":[{"name":"stdout","text":"\n[PSO] Start\n[PSO] Initial best = 0.999310, features = 40\n[PSO] Iter 1/5 best = 0.999331, features = 36\n[PSO] Iter 2/5 best = 0.999331, features = 36\n[PSO] Iter 3/5 best = 0.999331, features = 36\n[PSO] Iter 4/5 best = 0.999331, features = 36\n[PSO] Iter 5/5 best = 0.999331, features = 39\n\n[GA] Start\n[GA] Initial best = 0.999310\n[GA] Gen 1/5 best = 0.999310, features = 42\n[GA] Gen 2/5 best = 0.999372, features = 39\n[GA] Gen 3/5 best = 0.999372, features = 39\n[GA] Gen 4/5 best = 0.999393, features = 33\n[GA] Gen 5/5 best = 0.999393, features = 33\n\n[GWO] Start\n[GWO] Initial best = 0.999289\n[GWO] Iter 1/5 best = 0.999310, features = 44\n[GWO] Iter 2/5 best = 0.999289, features = 55\n[GWO] Iter 3/5 best = 0.999310, features = 46\n[GWO] Iter 4/5 best = 0.999320, features = 49\n[GWO] Iter 5/5 best = 0.999299, features = 52\n\n=========== ABLATION RESULTS ===========\nMethod    | Features | F1\n--------------------------------\nPSO       |       39 | 0.999404\nGA        |       33 | 0.999425\nGWO       |       52 | 0.999299\nHybrid    |       43 | 0.999393\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# ==========================================================\n# ABLATION-3 : HLO CONTRIBUTION STUDY (WITH LOGS)\n# Voting + Hill-Climb  vs  Voting + HLO + Hill-Climb\n# ==========================================================\n\nimport numpy as np\nimport pandas as pd\nimport warnings\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import f1_score\nfrom xgboost import XGBClassifier\n\nwarnings.filterwarnings(\"ignore\")\nnp.random.seed(42)\n\n# ---------------- DATA ----------------\ndf = pd.read_csv(\"/kaggle/input/ids-cleaned/ids2018_cleaned_combined_1.csv\")\nTARGET_COL = \"Label\"\nX = df.drop(TARGET_COL, axis=1)\ny = df[TARGET_COL].astype(int)\nN_FEATURES = X.shape[1]\n\n# ---------------- MODEL ----------------\ndef get_model(iters=200):\n    return XGBClassifier(\n        n_estimators=iters,\n        learning_rate=0.05,\n        max_depth=6,\n        subsample=1.0,\n        colsample_bytree=1.0,\n        random_state=42,\n        n_jobs=-1,\n        eval_metric=\"logloss\"\n    )\n\n# ---------------- FITNESS ----------------\ndef fitness(mask):\n    idx = np.where(mask)[0]\n    if len(idx) == 0:\n        return 0.0\n    Xs = X.iloc[:, idx]\n    model = get_model(100)\n    skf = StratifiedKFold(3, shuffle=True, random_state=42)\n    return np.mean([\n        f1_score(y.iloc[te],\n                 model.fit(Xs.iloc[tr], y.iloc[tr]).predict(Xs.iloc[te]))\n        for tr, te in skf.split(Xs, y)\n    ])\n\n# ---------------- PSO ----------------\ndef run_pso():\n    pop = np.random.randint(0,2,(10,N_FEATURES))\n    print(\"\\nPSO started\")\n    for it in range(5):\n        scores = np.array([fitness(p.astype(bool)) for p in pop])\n        best = pop[np.argmax(scores)]\n        print(f\" PSO iter {it+1}/5 best_f1={scores.max():.6f}\")\n        for i in range(10):\n            flip = np.random.randint(N_FEATURES)\n            pop[i] = best.copy()\n            pop[i,flip] = 1 - pop[i,flip]\n    return best.astype(bool)\n\n# ---------------- GA ----------------\ndef run_ga():\n    pop = np.random.randint(0,2,(15,N_FEATURES))\n    print(\"\\nGA started\")\n    for g in range(5):\n        scores = np.array([fitness(p.astype(bool)) for p in pop])\n        print(f\" GA gen {g+1}/5 best_f1={scores.max():.6f}\")\n        elite = pop[np.argmax(scores)]\n        new = [elite]\n        while len(new) < 15:\n            i,j = np.random.randint(15,size=2)\n            pt = np.random.randint(N_FEATURES)\n            child = np.concatenate([pop[i][:pt], pop[j][pt:]])\n            if np.random.rand() < 0.1:\n                m = np.random.randint(N_FEATURES)\n                child[m] = 1-child[m]\n            new.append(child)\n        pop = np.array(new)\n    scores = np.array([fitness(p.astype(bool)) for p in pop])\n    return pop[np.argmax(scores)].astype(bool)\n\n# ---------------- Binary GWO ----------------\ndef run_gwo():\n    wolves = 10\n    pop = np.random.randint(0,2,(wolves,N_FEATURES))\n    print(\"\\nGWO started\")\n    for it in range(5):\n        scores = np.array([fitness(p.astype(bool)) for p in pop])\n        idx = np.argsort(scores)[::-1]\n        print(f\" GWO iter {it+1}/5 best_f1={scores.max():.6f}\")\n        alpha, beta, delta = pop[idx[0]], pop[idx[1]], pop[idx[2]]\n\n        for i in range(wolves):\n            for d in range(N_FEATURES):\n                s = (alpha[d] + beta[d] + delta[d]) / 3\n                pop[i,d] = 1 if np.random.rand() < s else 0\n    scores = np.array([fitness(p.astype(bool)) for p in pop])\n    return pop[np.argmax(scores)].astype(bool)\n\n# ---------------- Voting ----------------\ndef voting(a,b,c):\n    return ((a.astype(int)+b.astype(int)+c.astype(int))>=2)\n\n# ---------------- HLO ----------------\ndef HLO(mask):\n    idx = np.where(mask)[0]\n    if len(idx)==0:\n        return mask\n    pop = np.random.randint(0,2,(10,len(idx)))\n    best = pop[0]\n    print(\"\\nHLO started\")\n\n    for it in range(5):\n        scores=[]\n        for i in range(10):\n            if np.random.rand() < 0.7:\n                pop[i] = best.copy()\n            m = np.random.randint(len(idx))\n            pop[i,m] = 1-pop[i,m]\n\n            full = np.zeros(N_FEATURES)\n            full[idx[pop[i].astype(bool)]] = 1\n            scores.append(fitness(full.astype(bool)))\n\n        best = pop[np.argmax(scores)]\n        print(f\" HLO iter {it+1}/5 best_f1={max(scores):.6f}\")\n\n    final = np.zeros(N_FEATURES)\n    final[idx[best.astype(bool)]] = 1\n    return final.astype(bool)\n\n# ---------------- Hill Climb ----------------\ndef hill_climb(mask):\n    best = mask.copy()\n    best_score = fitness(best)\n    print(\"\\nHill-Climb started\")\n    for step in range(50):\n        i = np.random.randint(N_FEATURES)\n        trial = best.copy()\n        trial[i] = 1-trial[i]\n        s = fitness(trial)\n        if s > best_score:\n            best, best_score = trial, s\n            print(f\" HC step {step+1} improved -> f1={best_score:.6f}\")\n    return best\n\n# ---------------- Final Eval ----------------\ndef final_eval(mask):\n    idx = np.where(mask)[0]\n    Xs = X.iloc[:,idx]\n    model = get_model(400)\n    skf = StratifiedKFold(5, shuffle=True, random_state=42)\n    f1=[]\n    for tr,te in skf.split(Xs,y):\n        model.fit(Xs.iloc[tr],y.iloc[tr])\n        f1.append(f1_score(y.iloc[te],model.predict(Xs.iloc[te])))\n    return len(idx), np.mean(f1)\n\n# ================= RUN =================\nprint(\"\\nRunning PSO, GA, GWO...\")\npso = run_pso()\nga  = run_ga()\ngwo = run_gwo()\n\nvote = voting(pso,ga,gwo)\n\nprint(\"\\nRunning Voting + Hill-Climb (no HLO)\")\nno_hlo = hill_climb(vote)\n\nprint(\"\\nRunning Voting + HLO + Hill-Climb\")\nwith_hlo = hill_climb(HLO(vote))\n\n# ================= RESULTS =================\nprint(\"\\n=========== HLO ABLATION ===========\")\nprint(\"Variant                | Features | F1\")\nprint(\"------------------------------------------\")\n\nn1,f1 = final_eval(no_hlo)\nn2,f2 = final_eval(with_hlo)\n\nprint(f\"Voting + HillClimb     | {n1:8d} | {f1:.6f}\")\nprint(f\"Voting + HLO + HC      | {n2:8d} | {f2:.6f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T17:36:50.964690Z","iopub.execute_input":"2026-01-10T17:36:50.965487Z","iopub.status.idle":"2026-01-10T17:50:57.590642Z","shell.execute_reply.started":"2026-01-10T17:36:50.965451Z","shell.execute_reply":"2026-01-10T17:50:57.589685Z"}},"outputs":[{"name":"stdout","text":"\nRunning PSO, GA, GWO...\n\nPSO started\n PSO iter 1/5 best_f1=0.999257\n PSO iter 2/5 best_f1=0.999268\n PSO iter 3/5 best_f1=0.999331\n PSO iter 4/5 best_f1=0.999341\n PSO iter 5/5 best_f1=0.999341\n\nGA started\n GA gen 1/5 best_f1=0.999278\n GA gen 2/5 best_f1=0.999278\n GA gen 3/5 best_f1=0.999299\n GA gen 4/5 best_f1=0.999299\n GA gen 5/5 best_f1=0.999299\n\nGWO started\n GWO iter 1/5 best_f1=0.999268\n GWO iter 2/5 best_f1=0.999320\n GWO iter 3/5 best_f1=0.999320\n GWO iter 4/5 best_f1=0.999372\n GWO iter 5/5 best_f1=0.999372\n\nRunning Voting + Hill-Climb (no HLO)\n\nHill-Climb started\n HC step 2 improved -> f1=0.999320\n HC step 9 improved -> f1=0.999320\n HC step 12 improved -> f1=0.999320\n HC step 17 improved -> f1=0.999351\n HC step 18 improved -> f1=0.999362\n HC step 41 improved -> f1=0.999372\n HC step 48 improved -> f1=0.999383\n\nRunning Voting + HLO + Hill-Climb\n\nHLO started\n HLO iter 1/5 best_f1=0.998537\n HLO iter 2/5 best_f1=0.999257\n HLO iter 3/5 best_f1=0.999299\n HLO iter 4/5 best_f1=0.999352\n HLO iter 5/5 best_f1=0.999352\n\nHill-Climb started\n HC step 5 improved -> f1=0.999362\n\n=========== HLO ABLATION ===========\nVariant                | Features | F1\n------------------------------------------\nVoting + HillClimb     |       39 | 0.999341\nVoting + HLO + HC      |       25 | 0.999331\n","output_type":"stream"}],"execution_count":5}]}