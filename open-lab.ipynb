{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1733506,"sourceType":"datasetVersion","datasetId":6012,"isSourceIdPinned":false},{"sourceId":4059877,"sourceType":"datasetVersion","datasetId":2395943,"isSourceIdPinned":false},{"sourceId":4759005,"sourceType":"datasetVersion","datasetId":2754352,"isSourceIdPinned":false},{"sourceId":14094039,"sourceType":"datasetVersion","datasetId":8974906}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\n\n# ---- Step 1: Load the dataset ----\ndf = pd.read_csv(\"/kaggle/input/loan-final-normalized-csv/loan_final_normalized.csv\")\n\n# ---- Step 2: Print first few rows (optional) ----\nprint(\"Preview of data:\")\nprint(df.head(), \"\\n\")\n\n# ---- Step 3: Print dataset info ----\nprint(\"INFO:\")\nprint(df.info(), \"\\n\")\n\n# ---- Step 4: Print statistical summary ----\nprint(\"DESCRIBE:\")\nprint(df.describe(include='all'))  # include='all' to show categorical stats also","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-26T17:09:02.370943Z","iopub.execute_input":"2025-11-26T17:09:02.371237Z","iopub.status.idle":"2025-11-26T17:09:05.473436Z","shell.execute_reply.started":"2025-11-26T17:09:02.371218Z","shell.execute_reply":"2025-11-26T17:09:05.472619Z"}},"outputs":[{"name":"stdout","text":"Preview of data:\n   customer_id       age  occupation_status  years_employed  annual_income  \\\n0            0  0.423077                  0        0.431078       0.045017   \n1            1  0.288462                  0        0.182957       0.119519   \n2            2  0.461538                  2        0.027569       0.024851   \n3            3  0.673077                  2        0.012531       0.060200   \n4            4  0.269231                  0        0.313283       0.207051   \n\n   credit_score  credit_history_years  savings_assets  current_debt  \\\n0      0.685259              0.176667        0.002983      0.065897   \n1      0.555777              0.116667        0.000563      0.100990   \n2      0.679283              0.280000        0.000057      0.047721   \n3      0.685259              0.326667        0.004933      0.070693   \n4      0.561753              0.240000        0.000697      0.075721   \n\n   defaults_on_file  delinquencies_last_2yrs  derogatory_marks  product_type  \\\n0               0.0                 0.000000               0.0             0   \n1               0.0                 0.111111               0.0             2   \n2               0.0                 0.000000               0.0             0   \n3               0.0                 0.111111               0.0             0   \n4               0.0                 0.000000               0.0             2   \n\n   loan_intent  loan_amount  interest_rate  debt_to_income_ratio  \\\n0            0     0.001005       0.648235              0.527569   \n1            3     0.530653       0.476471              0.478697   \n2            1     0.016080       0.725294              0.469925   \n3            0     0.024121       0.749412              0.496241   \n4            2     0.995980       0.465882              0.241855   \n\n   loan_to_income_ratio  payment_to_income_ratio  loan_status  \n0              0.007526                 0.007530          1.0  \n1              0.616658                 0.615964          0.0  \n2              0.046663                 0.046687          1.0  \n3              0.045660                 0.045181          1.0  \n4              0.781234                 0.781627          1.0   \n\nINFO:\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 50000 entries, 0 to 49999\nData columns (total 20 columns):\n #   Column                   Non-Null Count  Dtype  \n---  ------                   --------------  -----  \n 0   customer_id              50000 non-null  int64  \n 1   age                      50000 non-null  float64\n 2   occupation_status        50000 non-null  int64  \n 3   years_employed           50000 non-null  float64\n 4   annual_income            50000 non-null  float64\n 5   credit_score             50000 non-null  float64\n 6   credit_history_years     50000 non-null  float64\n 7   savings_assets           50000 non-null  float64\n 8   current_debt             50000 non-null  float64\n 9   defaults_on_file         50000 non-null  float64\n 10  delinquencies_last_2yrs  50000 non-null  float64\n 11  derogatory_marks         50000 non-null  float64\n 12  product_type             50000 non-null  int64  \n 13  loan_intent              50000 non-null  int64  \n 14  loan_amount              50000 non-null  float64\n 15  interest_rate            50000 non-null  float64\n 16  debt_to_income_ratio     50000 non-null  float64\n 17  loan_to_income_ratio     50000 non-null  float64\n 18  payment_to_income_ratio  50000 non-null  float64\n 19  loan_status              50000 non-null  float64\ndtypes: float64(16), int64(4)\nmemory usage: 7.6 MB\nNone \n\nDESCRIBE:\n        customer_id           age  occupation_status  years_employed  \\\ncount  50000.000000  50000.000000       50000.000000    50000.000000   \nmean   24999.500000      0.326097           0.397580        0.186839   \nstd    14433.901067      0.213819           0.658421        0.190779   \nmin        0.000000      0.000000           0.000000        0.000000   \n25%    12499.750000      0.153846           0.000000        0.032581   \n50%    24999.500000      0.326923           0.000000        0.122807   \n75%    37499.250000      0.480769           1.000000        0.285714   \nmax    49999.000000      1.000000           2.000000        1.000000   \n\n       annual_income  credit_score  credit_history_years  savings_assets  \\\ncount   50000.000000  50000.000000          50000.000000    50000.000000   \nmean        0.149204      0.588874              0.272276        0.011985   \nstd         0.138853      0.128947              0.240252        0.044108   \nmin         0.000000      0.000000              0.000000        0.000000   \n25%         0.052257      0.501992              0.066667        0.000433   \n50%         0.113223      0.587649              0.203333        0.001893   \n75%         0.203078      0.675299              0.420000        0.007570   \nmax         1.000000      1.000000              1.000000        1.000000   \n\n       current_debt  defaults_on_file  delinquencies_last_2yrs  \\\ncount  50000.000000      50000.000000             50000.000000   \nmean       0.087151          0.053480                 0.061627   \nstd        0.081109          0.224991                 0.093894   \nmin        0.000000          0.000000                 0.000000   \n25%        0.033812          0.000000                 0.000000   \n50%        0.063233          0.000000                 0.000000   \n75%        0.112621          0.000000                 0.111111   \nmax        1.000000          1.000000                 1.000000   \n\n       derogatory_marks  product_type   loan_intent   loan_amount  \\\ncount      50000.000000  50000.000000  50000.000000  50000.000000   \nmean           0.036910      0.901360      2.801620      0.327054   \nstd            0.103249      0.888733      1.745694      0.262474   \nmin            0.000000      0.000000      0.000000      0.000000   \n25%            0.000000      0.000000      2.000000      0.118593   \n50%            0.000000      1.000000      3.000000      0.257286   \n75%            0.000000      2.000000      4.000000      0.482412   \nmax            1.000000      2.000000      5.000000      1.000000   \n\n       interest_rate  debt_to_income_ratio  loan_to_income_ratio  \\\ncount   50000.000000          50000.000000          50000.000000   \nmean        0.558741              0.355544              0.348218   \nstd         0.239291              0.200234              0.233712   \nmin         0.000000              0.000000              0.000000   \n25%         0.363529              0.199248              0.163071   \n50%         0.555294              0.329574              0.308078   \n75%         0.757059              0.484962              0.502885   \nmax         1.000000              1.000000              1.000000   \n\n       payment_to_income_ratio   loan_status  \ncount             50000.000000  50000.000000  \nmean                  0.347884      0.550460  \nstd                   0.233837      0.497452  \nmin                   0.000000      0.000000  \n25%                   0.162651      0.000000  \n50%                   0.307229      1.000000  \n75%                   0.503012      1.000000  \nmax                   1.000000      1.000000  \n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n# =====================================\n# DATA PREPARATION\n# =====================================\nX = df.drop(\"loan_status\", axis=1)\ny = df[\"loan_status\"].astype(int)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\n# =====================================\n# GA CONFIG\n# =====================================\npopulation_size = 20\nn_generations = 20\ndim = 5\n\nbounds = np.array([\n    [100, 500],\n    [1, 10],\n    [0.01, 0.3],\n    [1, 200],\n    [1, 200]\n])\n\nmutation_rate = 0.2\ncrossover_rate = 0.7\n\nhp_names = [\"n_estimators\", \"max_depth\", \"learning_rate\",\n            \"min_samples_split\", \"min_samples_leaf\"]\n\n# =====================================\n# DECODE INDIVIDUAL → MODEL PARAMS\n# =====================================\ndef decode(position):\n    return {\n        \"n_estimators\": int(position[0]),\n        \"max_depth\": int(position[1]),\n        \"learning_rate\": float(position[2]),\n        \"min_samples_split\": int(position[3]),\n        \"min_samples_leaf\": int(position[4]),\n        \"random_state\": 42\n    }\n\n# =====================================\n# FITNESS CACHE (SPEED BOOST)\n# =====================================\nfitness_cache = {}\n\ndef fitness_fn(position):\n    key = tuple(position.round(4))  # cache key\n\n    # If evaluated before, reuse score → huge speed boost\n    if key in fitness_cache:\n        return fitness_cache[key]\n\n    params = decode(position)\n    model = GradientBoostingClassifier(**params)\n    model.fit(X_train, y_train)\n\n    pred = model.predict(X_test)\n\n    acc  = accuracy_score(y_test, pred)\n    prec = precision_score(y_test, pred, zero_division=0)\n    rec  = recall_score(y_test, pred, zero_division=0)\n    f1   = f1_score(y_test, pred, zero_division=0)\n\n    score = (acc + prec + rec + f1) / 4.0\n    fitness_cache[key] = score  # cache the result\n    return score\n\n# =====================================\n# INIT POPULATION (vectorized)\n# =====================================\ndef init_population():\n    return bounds[:, 0] + (bounds[:, 1] - bounds[:, 0]) * np.random.rand(population_size, dim)\n\n# =====================================\n# SELECTION\n# =====================================\ndef select(pop, scores):\n    i, j = np.random.randint(0, population_size, 2)\n    return pop[i] if scores[i] > scores[j] else pop[j]\n\n# =====================================\n# CROSSOVER\n# =====================================\ndef crossover(p1, p2):\n    if np.random.rand() > crossover_rate:\n        return p1.copy(), p2.copy()\n\n    point = np.random.randint(1, dim)\n    return (\n        np.concatenate((p1[:point], p2[point:])),\n        np.concatenate((p2[:point], p1[point:]))\n    )\n\n# =====================================\n# MUTATION (vectorized)\n# =====================================\ndef mutate(child):\n    mask = np.random.rand(dim) < mutation_rate\n    random_values = np.random.uniform(bounds[:, 0], bounds[:, 1])\n    child[mask] = random_values[mask]\n    return child\n\n# =====================================\n# GA MAIN LOOP\n# =====================================\npopulation = init_population()\nfitness_scores = np.zeros(population_size)\n\nfor gen in range(n_generations):\n\n    # Compute fitness\n    for i in range(population_size):\n        fitness_scores[i] = fitness_fn(population[i])\n\n    best_idx = np.argmax(fitness_scores)\n    print(f\"Generation {gen+1}/{n_generations} → Best Fitness = {fitness_scores[best_idx]:.4f}\")\n\n    # GENERATE NEW POPULATION\n    new_pop = []\n\n    while len(new_pop) < population_size:\n        p1 = select(population, fitness_scores)\n        p2 = select(population, fitness_scores)\n\n        c1, c2 = crossover(p1, p2)\n\n        new_pop.append(mutate(c1))\n        new_pop.append(mutate(c2))\n\n    population = np.array(new_pop[:population_size])\n\n# =====================================\n# BEST PARAMETERS\n# =====================================\nbest_idx = np.argmax(fitness_scores)\nbest_params = decode(population[best_idx])\n\nprint(\"\\nBEST PARAMETERS FOUND BY GA:\")\nfor k, v in best_params.items():\n    print(f\" {k}: {v}\")\n\n# =====================================\n# FINAL MODEL\n# =====================================\nfinal_model = GradientBoostingClassifier(**best_params)\nfinal_model.fit(X_train, y_train)\npred = final_model.predict(X_test)\n\nacc = accuracy_score(y_test, pred)\nprec = precision_score(y_test, pred, zero_division=0)\nrec  = recall_score(y_test, pred, zero_division=0)\nf1   = f1_score(y_test, pred, zero_division=0)\n\nprint(\"\\nFINAL METRICS:\")\nprint(f\"Accuracy : {acc:.4f}\")\nprint(f\"Precision: {prec:.4f}\")\nprint(f\"Recall   : {rec:.4f}\")\nprint(f\"F1-score : {f1:.4f}\")\n\n# =====================================\n# FEATURE IMPORTANCE\n# =====================================\nimportances = final_model.feature_importances_\nfi_df = pd.DataFrame({\"Feature\": X.columns, \"Importance\": importances})\nfi_df = fi_df.sort_values(by=\"Importance\", ascending=False)\n\nthreshold = np.mean(importances)\nselected_features = fi_df[fi_df[\"Importance\"] > threshold][\"Feature\"].tolist()\n\nprint(\"\\nSELECTED FEATURES:\")\nfor f in selected_features:\n    print(\" -\", f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T17:09:42.873621Z","iopub.execute_input":"2025-11-26T17:09:42.874100Z"}},"outputs":[{"name":"stdout","text":"Generation 1/20 → Best Fitness = 0.9338\nGeneration 2/20 → Best Fitness = 0.9342\nGeneration 3/20 → Best Fitness = 0.9342\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/980637972.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;31m# Compute fitness\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpopulation_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mfitness_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfitness_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0mbest_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfitness_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_47/980637972.py\u001b[0m in \u001b[0;36mfitness_fn\u001b[0;34m(position)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGradientBoostingClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0;31m# fit the boosting stages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 538\u001b[0;31m         n_stages = self._fit_stages(\n\u001b[0m\u001b[1;32m    539\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36m_fit_stages\u001b[0;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[0m\n\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0;31m# fit next stage of trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             raw_predictions = self._fit_stage(\n\u001b[0m\u001b[1;32m    616\u001b[0m                 \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36m_fit_stage\u001b[0;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m             \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresidual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0;31m# update tree leaves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m   1245\u001b[0m         \"\"\"\n\u001b[1;32m   1246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1247\u001b[0;31m         super().fit(\n\u001b[0m\u001b[1;32m   1248\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    377\u001b[0m             )\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# ---- Step 1: Load the dataset ----\ndf = pd.read_csv(\"/kaggle/input/loan-final-normalized-csv/loan_final_normalized.csv\")\n\n# ---- Step 2: Print first few rows (optional) ----\nprint(\"Preview of data:\")\nprint(df.head(), \"\\n\")\n\n# ---- Step 3: Print dataset info ----\nprint(\"INFO:\")\nprint(df.info(), \"\\n\")\n\n# ---- Step 4: Print statistical summary ----\nprint(\"DESCRIBE:\")\nprint(df.describe(include='all'))  # include='all' to show categorical stats also","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T13:00:24.628996Z","iopub.execute_input":"2025-11-27T13:00:24.629679Z","iopub.status.idle":"2025-11-27T13:00:25.163369Z","shell.execute_reply.started":"2025-11-27T13:00:24.629653Z","shell.execute_reply":"2025-11-27T13:00:25.162755Z"}},"outputs":[{"name":"stdout","text":"Preview of data:\n   customer_id       age  occupation_status  years_employed  annual_income  \\\n0            0  0.423077                  0        0.431078       0.045017   \n1            1  0.288462                  0        0.182957       0.119519   \n2            2  0.461538                  2        0.027569       0.024851   \n3            3  0.673077                  2        0.012531       0.060200   \n4            4  0.269231                  0        0.313283       0.207051   \n\n   credit_score  credit_history_years  savings_assets  current_debt  \\\n0      0.685259              0.176667        0.002983      0.065897   \n1      0.555777              0.116667        0.000563      0.100990   \n2      0.679283              0.280000        0.000057      0.047721   \n3      0.685259              0.326667        0.004933      0.070693   \n4      0.561753              0.240000        0.000697      0.075721   \n\n   defaults_on_file  delinquencies_last_2yrs  derogatory_marks  product_type  \\\n0               0.0                 0.000000               0.0             0   \n1               0.0                 0.111111               0.0             2   \n2               0.0                 0.000000               0.0             0   \n3               0.0                 0.111111               0.0             0   \n4               0.0                 0.000000               0.0             2   \n\n   loan_intent  loan_amount  interest_rate  debt_to_income_ratio  \\\n0            0     0.001005       0.648235              0.527569   \n1            3     0.530653       0.476471              0.478697   \n2            1     0.016080       0.725294              0.469925   \n3            0     0.024121       0.749412              0.496241   \n4            2     0.995980       0.465882              0.241855   \n\n   loan_to_income_ratio  payment_to_income_ratio  loan_status  \n0              0.007526                 0.007530          1.0  \n1              0.616658                 0.615964          0.0  \n2              0.046663                 0.046687          1.0  \n3              0.045660                 0.045181          1.0  \n4              0.781234                 0.781627          1.0   \n\nINFO:\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 50000 entries, 0 to 49999\nData columns (total 20 columns):\n #   Column                   Non-Null Count  Dtype  \n---  ------                   --------------  -----  \n 0   customer_id              50000 non-null  int64  \n 1   age                      50000 non-null  float64\n 2   occupation_status        50000 non-null  int64  \n 3   years_employed           50000 non-null  float64\n 4   annual_income            50000 non-null  float64\n 5   credit_score             50000 non-null  float64\n 6   credit_history_years     50000 non-null  float64\n 7   savings_assets           50000 non-null  float64\n 8   current_debt             50000 non-null  float64\n 9   defaults_on_file         50000 non-null  float64\n 10  delinquencies_last_2yrs  50000 non-null  float64\n 11  derogatory_marks         50000 non-null  float64\n 12  product_type             50000 non-null  int64  \n 13  loan_intent              50000 non-null  int64  \n 14  loan_amount              50000 non-null  float64\n 15  interest_rate            50000 non-null  float64\n 16  debt_to_income_ratio     50000 non-null  float64\n 17  loan_to_income_ratio     50000 non-null  float64\n 18  payment_to_income_ratio  50000 non-null  float64\n 19  loan_status              50000 non-null  float64\ndtypes: float64(16), int64(4)\nmemory usage: 7.6 MB\nNone \n\nDESCRIBE:\n        customer_id           age  occupation_status  years_employed  \\\ncount  50000.000000  50000.000000       50000.000000    50000.000000   \nmean   24999.500000      0.326097           0.397580        0.186839   \nstd    14433.901067      0.213819           0.658421        0.190779   \nmin        0.000000      0.000000           0.000000        0.000000   \n25%    12499.750000      0.153846           0.000000        0.032581   \n50%    24999.500000      0.326923           0.000000        0.122807   \n75%    37499.250000      0.480769           1.000000        0.285714   \nmax    49999.000000      1.000000           2.000000        1.000000   \n\n       annual_income  credit_score  credit_history_years  savings_assets  \\\ncount   50000.000000  50000.000000          50000.000000    50000.000000   \nmean        0.149204      0.588874              0.272276        0.011985   \nstd         0.138853      0.128947              0.240252        0.044108   \nmin         0.000000      0.000000              0.000000        0.000000   \n25%         0.052257      0.501992              0.066667        0.000433   \n50%         0.113223      0.587649              0.203333        0.001893   \n75%         0.203078      0.675299              0.420000        0.007570   \nmax         1.000000      1.000000              1.000000        1.000000   \n\n       current_debt  defaults_on_file  delinquencies_last_2yrs  \\\ncount  50000.000000      50000.000000             50000.000000   \nmean       0.087151          0.053480                 0.061627   \nstd        0.081109          0.224991                 0.093894   \nmin        0.000000          0.000000                 0.000000   \n25%        0.033812          0.000000                 0.000000   \n50%        0.063233          0.000000                 0.000000   \n75%        0.112621          0.000000                 0.111111   \nmax        1.000000          1.000000                 1.000000   \n\n       derogatory_marks  product_type   loan_intent   loan_amount  \\\ncount      50000.000000  50000.000000  50000.000000  50000.000000   \nmean           0.036910      0.901360      2.801620      0.327054   \nstd            0.103249      0.888733      1.745694      0.262474   \nmin            0.000000      0.000000      0.000000      0.000000   \n25%            0.000000      0.000000      2.000000      0.118593   \n50%            0.000000      1.000000      3.000000      0.257286   \n75%            0.000000      2.000000      4.000000      0.482412   \nmax            1.000000      2.000000      5.000000      1.000000   \n\n       interest_rate  debt_to_income_ratio  loan_to_income_ratio  \\\ncount   50000.000000          50000.000000          50000.000000   \nmean        0.558741              0.355544              0.348218   \nstd         0.239291              0.200234              0.233712   \nmin         0.000000              0.000000              0.000000   \n25%         0.363529              0.199248              0.163071   \n50%         0.555294              0.329574              0.308078   \n75%         0.757059              0.484962              0.502885   \nmax         1.000000              1.000000              1.000000   \n\n       payment_to_income_ratio   loan_status  \ncount             50000.000000  50000.000000  \nmean                  0.347884      0.550460  \nstd                   0.233837      0.497452  \nmin                   0.000000      0.000000  \n25%                   0.162651      0.000000  \n50%                   0.307229      1.000000  \n75%                   0.503012      1.000000  \nmax                   1.000000      1.000000  \n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# ============================================================\n# Harmony Search (HS) + Gradient Boosting for Feature Selection\n# ============================================================\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score\n)\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n# ============================================================\n# 1. Prepare dataset\n# ============================================================\nX = df.drop(\"loan_status\", axis=1)\ny = df[\"loan_status\"].astype(int)\n\nfeature_count = X.shape[1]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\n# ============================================================\n# 2. Harmony Search Parameters\n# ============================================================\nhms = 15                # Harmony Memory Size\niterations = 10\nhmcr = 0.9             # Harmony Memory Consideration Rate\npar = 0.3              # Pitch Adjustment Rate\nbw = 0.1               # Bandwidth (small perturbation)\n\n# ============================================================\n# 3. Fitness Function (GradientBoosting)\n# ============================================================\ndef fitness(solution):\n    if np.sum(solution) == 0:\n        return 0  # invalid solution\n\n    selected_cols = X.columns[solution == 1]\n\n    Xtr = X_train[selected_cols]\n    Xte = X_test[selected_cols]\n\n    model = GradientBoostingClassifier(\n        n_estimators=200,\n        learning_rate=0.05,\n        max_depth=3,\n        random_state=42\n    )\n\n    model.fit(Xtr, y_train)\n    pred = model.predict(Xte)\n\n    acc = accuracy_score(y_test, pred)\n    prec = precision_score(y_test, pred, zero_division=0)\n    rec = recall_score(y_test, pred, zero_division=0)\n    f1 = f1_score(y_test, pred, zero_division=0)\n\n    return (acc + prec + rec + f1) / 4\n\n\n# ============================================================\n# 4. Initialize Harmony Memory (HM)\n# ============================================================\ndef initialize_harmony_memory():\n    HM = []\n    scores = []\n\n    for _ in range(hms):\n        sol = np.random.randint(0, 2, feature_count)\n        score = fitness(sol)\n        HM.append(sol)\n        scores.append(score)\n\n    return np.array(HM), np.array(scores)\n\n\n# ============================================================\n# 5. Generate New Harmony\n# ============================================================\ndef generate_new_harmony(HM):\n    new_harmony = np.zeros(feature_count)\n\n    for f in range(feature_count):\n\n        if np.random.rand() < hmcr:  \n            # choose from memory\n            idx = np.random.randint(0, hms)\n            value = HM[idx][f]\n\n            # Pitch Adjustment\n            if np.random.rand() < par:\n                value = 1 - value  # flip bit\n\n        else:\n            value = np.random.randint(0, 2)\n\n        new_harmony[f] = value\n\n    return new_harmony\n\n\n# ============================================================\n# 6. Harmony Search Optimization\n# ============================================================\ndef harmony_search():\n\n    HM, scores = initialize_harmony_memory()\n    best_index = np.argmax(scores)\n    best_solution = HM[best_index].copy()\n    best_score = scores[best_index]\n\n    for itr in range(iterations):\n\n        new_harmony = generate_new_harmony(HM)\n        new_score = fitness(new_harmony)\n\n        # Replace worst if new harmony is better\n        worst_index = np.argmin(scores)\n        if new_score > scores[worst_index]:\n            HM[worst_index] = new_harmony\n            scores[worst_index] = new_score\n\n        # Update global best\n        if new_score > best_score:\n            best_solution = new_harmony.copy()\n            best_score = new_score\n\n        print(f\"Iteration {itr+1}/{iterations} - Best Fitness = {best_score:.4f}\")\n\n    return best_solution, best_score\n\n\n# ============================================================\n# 7. Run Harmony Search\n# ============================================================\nbest_solution, best_score = harmony_search()\n\nselected_features = X.columns[best_solution == 1].tolist()\n\nprint(\"\\n==============================\")\nprint(\" BEST FEATURES SELECTED BY HS\")\nprint(\"==============================\")\nprint(\"Total Features :\", len(X.columns))\nprint(\"Selected       :\", len(selected_features))\nprint(selected_features)\n\n\n# ============================================================\n# 8. Final Gradient Boosting Model on Selected Features\n# ============================================================\nXtr_final = X_train[selected_features]\nXte_final = X_test[selected_features]\n\nfinal_model = GradientBoostingClassifier(\n    n_estimators=200,\n    learning_rate=0.05,\n    max_depth=3,\n    random_state=42\n)\n\nfinal_model.fit(Xtr_final, y_train)\nfinal_pred = final_model.predict(Xte_final)\n\nacc = accuracy_score(y_test, final_pred)\nprec = precision_score(y_test, final_pred, zero_division=0)\nrec = recall_score(y_test, final_pred, zero_division=0)\nf1 = f1_score(y_test, final_pred, zero_division=0)\n\nprint(\"\\n==============================\")\nprint(\"   FINAL GBC RESULTS\")\nprint(\"==============================\")\nprint(f\"Accuracy :  {acc:.4f}\")\nprint(f\"Precision: {prec:.4f}\")\nprint(f\"Recall   : {rec:.4f}\")\nprint(f\"F1-score : {f1:.4f}\")\nprint(f\"AVG Score: {(acc + prec + rec + f1) / 4:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T13:00:58.896864Z","iopub.execute_input":"2025-11-27T13:00:58.897162Z","iopub.status.idle":"2025-11-27T13:07:14.573006Z","shell.execute_reply.started":"2025-11-27T13:00:58.897142Z","shell.execute_reply":"2025-11-27T13:07:14.572329Z"}},"outputs":[{"name":"stdout","text":"Iteration 1/10 - Best Fitness = 0.9081\nIteration 2/10 - Best Fitness = 0.9081\nIteration 3/10 - Best Fitness = 0.9081\nIteration 4/10 - Best Fitness = 0.9081\nIteration 5/10 - Best Fitness = 0.9081\nIteration 6/10 - Best Fitness = 0.9081\nIteration 7/10 - Best Fitness = 0.9081\nIteration 8/10 - Best Fitness = 0.9081\nIteration 9/10 - Best Fitness = 0.9081\nIteration 10/10 - Best Fitness = 0.9115\n\n==============================\n BEST FEATURES SELECTED BY HS\n==============================\nTotal Features : 19\nSelected       : 12\n['customer_id', 'age', 'occupation_status', 'years_employed', 'credit_score', 'savings_assets', 'current_debt', 'defaults_on_file', 'delinquencies_last_2yrs', 'loan_intent', 'debt_to_income_ratio', 'loan_to_income_ratio']\n\n==============================\n   FINAL GBC RESULTS\n==============================\nAccuracy :  0.9037\nPrecision: 0.8995\nRecall   : 0.9288\nF1-score : 0.9139\nAVG Score: 0.9115\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# ============================================================\n# Whale Optimization Algorithm (WOA) + Gradient Boosting\n# ============================================================\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score\n)\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n# ============================================================\n# 1. Dataset\n# ============================================================\nX = df.drop(\"loan_status\", axis=1)\ny = df[\"loan_status\"].astype(int)\n\nfeature_count = X.shape[1]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\n# ============================================================\n# 2. WOA Parameters\n# ============================================================\npopulation_size = 20\niterations = 10\n\n# ============================================================\n# 3. Fitness Function (GradientBoosting)\n# ============================================================\ndef fitness(solution):\n\n    if np.sum(solution) == 0:\n        return 0\n\n    selected_cols = X.columns[solution == 1]\n\n    Xtr = X_train[selected_cols]\n    Xte = X_test[selected_cols]\n\n    model = GradientBoostingClassifier(\n        n_estimators=200,\n        learning_rate=0.05,\n        max_depth=3,\n        random_state=42\n    )\n\n    model.fit(Xtr, y_train)\n    pred = model.predict(Xte)\n\n    acc = accuracy_score(y_test, pred)\n    prec = precision_score(y_test, pred, zero_division=0)\n    rec = recall_score(y_test, pred, zero_division=0)\n    f1 = f1_score(y_test, pred, zero_division=0)\n\n    return (acc + prec + rec + f1) / 4\n\n\n# ============================================================\n# 4. Initialize Population\n# ============================================================\ndef initialize_population():\n    pop = np.random.randint(0, 2, (population_size, feature_count))\n    scores = np.array([fitness(ind) for ind in pop])\n    return pop, scores\n\n\n# ============================================================\n# 5. Whale Optimization Algorithm\n# ============================================================\ndef woa():\n\n    population, scores = initialize_population()\n\n    # Best whale\n    best_index = np.argmax(scores)\n    best_solution = population[best_index].copy()\n    best_score = scores[best_index]\n\n    for itr in range(iterations):\n\n        a = 2 - itr * (2 / iterations)   # linearly decreasing a\n\n        for i in range(population_size):\n\n            A = 2 * a * np.random.rand() - a\n            C = 2 * np.random.rand()\n            p = np.random.rand()\n            b = 1\n            l = np.random.uniform(-1, 1)\n\n            whale = population[i].copy()\n\n            if p < 0.5:\n\n                if abs(A) < 1:\n                    # Exploitation\n                    D = abs(C * best_solution - whale)\n                    new_position = best_solution - A * D\n\n                else:\n                    # Select random whale\n                    rand_idx = np.random.randint(0, population_size)\n                    random_whale = population[rand_idx]\n\n                    D = abs(C * random_whale - whale)\n                    new_position = random_whale - A * D\n\n            else:\n                # Spiral updating\n                D = abs(best_solution - whale)\n                new_position = (D * np.exp(b * l) * np.cos(2 * np.pi * l)) + best_solution\n\n            # Convert to binary (sigmoid threshold)\n            new_binary = 1 / (1 + np.exp(-new_position))\n            new_binary = (new_binary > 0.5).astype(int)\n\n            population[i] = new_binary\n            scores[i] = fitness(new_binary)\n\n        # Update global best\n        best_index = np.argmax(scores)\n        if scores[best_index] > best_score:\n            best_score = scores[best_index]\n            best_solution = population[best_index].copy()\n\n        print(f\"Iteration {itr+1}/{iterations} - Best Fitness = {best_score:.4f}\")\n\n    return best_solution, best_score\n\n\n# ============================================================\n# 6. Run WOA Feature Selection\n# ============================================================\nbest_solution, best_score = woa()\n\nselected_features = X.columns[best_solution == 1].tolist()\n\nprint(\"\\n==============================\")\nprint(\" BEST FEATURES SELECTED BY WOA\")\nprint(\"==============================\")\nprint(\"Total Features :\", len(X.columns))\nprint(\"Selected       :\", len(selected_features))\nprint(selected_features)\n\n\n# ============================================================\n# 7. Final Gradient Boosting Model\n# ============================================================\nXtr_final = X_train[selected_features]\nXte_final = X_test[selected_features]\n\nfinal_model = GradientBoostingClassifier(\n    n_estimators=200,\n    learning_rate=0.05,\n    max_depth=3,\n    random_state=42\n)\n\nfinal_model.fit(Xtr_final, y_train)\nfinal_pred = final_model.predict(Xte_final)\n\nacc = accuracy_score(y_test, final_pred)\nprec = precision_score(y_test, final_pred, zero_division=0)\nrec = recall_score(y_test, final_pred, zero_division=0)\nf1 = f1_score(y_test, final_pred, zero_division=0)\n\nprint(\"\\n==============================\")\nprint(\"   FINAL GBC RESULTS\")\nprint(\"==============================\")\nprint(f\"Accuracy :  {acc:.4f}\")\nprint(f\"Precision: {prec:.4f}\")\nprint(f\"Recall   : {rec:.4f}\")\nprint(f\"F1-score : {f1:.4f}\")\nprint(f\"AVG Score: {(acc + prec + rec + f1) / 4:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T13:08:13.446006Z","iopub.execute_input":"2025-11-27T13:08:13.446744Z","iopub.status.idle":"2025-11-27T14:29:28.598168Z","shell.execute_reply.started":"2025-11-27T13:08:13.446693Z","shell.execute_reply":"2025-11-27T14:29:28.597338Z"}},"outputs":[{"name":"stdout","text":"Iteration 1/10 - Best Fitness = 0.9244\nIteration 2/10 - Best Fitness = 0.9252\nIteration 3/10 - Best Fitness = 0.9252\nIteration 4/10 - Best Fitness = 0.9252\nIteration 5/10 - Best Fitness = 0.9252\nIteration 6/10 - Best Fitness = 0.9252\nIteration 7/10 - Best Fitness = 0.9252\nIteration 8/10 - Best Fitness = 0.9252\nIteration 9/10 - Best Fitness = 0.9252\nIteration 10/10 - Best Fitness = 0.9252\n\n==============================\n BEST FEATURES SELECTED BY WOA\n==============================\nTotal Features : 19\nSelected       : 19\n['customer_id', 'age', 'occupation_status', 'years_employed', 'annual_income', 'credit_score', 'credit_history_years', 'savings_assets', 'current_debt', 'defaults_on_file', 'delinquencies_last_2yrs', 'derogatory_marks', 'product_type', 'loan_intent', 'loan_amount', 'interest_rate', 'debt_to_income_ratio', 'loan_to_income_ratio', 'payment_to_income_ratio']\n\n==============================\n   FINAL GBC RESULTS\n==============================\nAccuracy :  0.9186\nPrecision: 0.9124\nRecall   : 0.9426\nF1-score : 0.9273\nAVG Score: 0.9252\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# ============================================================\n# Grey Wolf Optimizer (GWO) + Gradient Boosting Classifier\n# ============================================================\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score\n)\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n# ============================================================\n# 1. Prepare dataset\n# ============================================================\nX = df.drop(\"loan_status\", axis=1)\ny = df[\"loan_status\"].astype(int)\n\nfeature_count = X.shape[1]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\n# ============================================================\n# 2. GWO Parameters\n# ============================================================\nwolves = 12\niterations = 5\n\n\n# ============================================================\n# 3. Fitness Function (Gradient Boosting)\n# ============================================================\ndef fitness(solution):\n\n    if np.sum(solution) == 0:\n        return 0  # invalid → no features selected\n\n    selected_cols = X.columns[solution == 1]\n\n    Xtr = X_train[selected_cols]\n    Xte = X_test[selected_cols]\n\n    model = GradientBoostingClassifier(\n        n_estimators=200,\n        learning_rate=0.05,\n        max_depth=3,\n        random_state=42\n    )\n\n    model.fit(Xtr, y_train)\n    pred = model.predict(Xte)\n\n    acc = accuracy_score(y_test, pred)\n    prec = precision_score(y_test, pred, zero_division=0)\n    rec = recall_score(y_test, pred, zero_division=0)\n    f1 = f1_score(y_test, pred, zero_division=0)\n\n    return (acc + prec + rec + f1) / 4\n\n\n# ============================================================\n# 4. Initialize Population\n# ============================================================\ndef init_population():\n    return np.random.randint(0, 2, (wolves, feature_count))\n\n\n# ============================================================\n# 5. Binary Grey Wolf Optimizer\n# ============================================================\ndef gwo_feature_selection():\n\n    population = init_population()\n    fitness_scores = np.zeros(wolves)\n\n    Alpha = Beta = Delta = None\n    Alpha_score = Beta_score = Delta_score = -1\n\n    for itr in range(iterations):\n\n        # Evaluate population\n        for i in range(wolves):\n\n            fitness_scores[i] = fitness(population[i])\n\n            if fitness_scores[i] > Alpha_score:\n                Alpha_score = fitness_scores[i]\n                Alpha = population[i].copy()\n\n            elif fitness_scores[i] > Beta_score:\n                Beta_score = fitness_scores[i]\n                Beta = population[i].copy()\n\n            elif fitness_scores[i] > Delta_score:\n                Delta_score = fitness_scores[i]\n                Delta = population[i].copy()\n\n        print(f\"Iteration {itr+1}/{iterations} - Best Fitness = {Alpha_score:.4f}\")\n\n        a = 2 - itr * (2 / iterations)  # a decreases from 2 → 0\n\n        # Update positions\n        for i in range(wolves):\n            for d in range(feature_count):\n\n                # ---------------- Alpha Update ----------------\n                r1, r2 = np.random.rand(), np.random.rand()\n                A1 = 2 * a * r1 - a\n                C1 = 2 * r2\n                D_alpha = abs(C1 * Alpha[d] - population[i][d])\n                X1 = Alpha[d] - A1 * D_alpha\n\n                # ---------------- Beta Update ----------------\n                r1, r2 = np.random.rand(), np.random.rand()\n                A2 = 2 * a * r1 - a\n                C2 = 2 * r2\n                D_beta = abs(C2 * Beta[d] - population[i][d])\n                X2 = Beta[d] - A2 * D_beta\n\n                # ---------------- Delta Update ----------------\n                r1, r2 = np.random.rand(), np.random.rand()\n                A3 = 2 * a * r1 - a\n                C3 = 2 * r2\n                D_delta = abs(C3 * Delta[d] - population[i][d])\n                X3 = Delta[d] - A3 * D_delta\n\n                new_pos = (X1 + X2 + X3) / 3\n\n                # ---------------- Binary Sigmoid ----------------\n                s = 1 / (1 + np.exp(-new_pos))\n                population[i][d] = 1 if np.random.rand() < s else 0\n\n    return Alpha, Alpha_score\n\n\n# ============================================================\n# 6. Run GWO Feature Selection\n# ============================================================\nbest_solution, best_score = gwo_feature_selection()\n\nselected_features = X.columns[best_solution == 1].tolist()\n\nprint(\"\\n==============================\")\nprint(\" BEST FEATURES SELECTED BY GWO\")\nprint(\"==============================\")\nprint(\"Total Features :\", len(X.columns))\nprint(\"Selected       :\", len(selected_features))\nprint(selected_features)\n\n\n# ============================================================\n# 7. Final Gradient Boosting Model on Selected Features\n# ============================================================\nXtr_final = X_train[selected_features]\nXte_final = X_test[selected_features]\n\nfinal_model = GradientBoostingClassifier(\n    n_estimators=200,\n    learning_rate=0.05,\n    max_depth=3,\n    random_state=42\n)\n\nfinal_model.fit(Xtr_final, y_train)\nfinal_pred = final_model.predict(Xte_final)\n\nacc = accuracy_score(y_test, final_pred)\nprec = precision_score(y_test, final_pred, zero_division=0)\nrec = recall_score(y_test, final_pred, zero_division=0)\nf1 = f1_score(y_test, final_pred, zero_division=0)\n\nprint(\"\\n==============================\")\nprint(\" FINAL GRADIENT BOOSTING RESULTS\")\nprint(\"==============================\")\nprint(f\"Accuracy :  {acc:.4f}\")\nprint(f\"Precision: {prec:.4f}\")\nprint(f\"Recall   : {rec:.4f}\")\nprint(f\"F1-score : {f1:.4f}\")\nprint(f\"AVG Score: {(acc + prec + rec + f1) / 4:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T14:31:25.542752Z","iopub.execute_input":"2025-11-27T14:31:25.543315Z","iopub.status.idle":"2025-11-27T14:48:32.201190Z","shell.execute_reply.started":"2025-11-27T14:31:25.543288Z","shell.execute_reply":"2025-11-27T14:48:32.200532Z"}},"outputs":[{"name":"stdout","text":"Iteration 1/5 - Best Fitness = 0.8769\nIteration 2/5 - Best Fitness = 0.9188\nIteration 3/5 - Best Fitness = 0.9215\nIteration 4/5 - Best Fitness = 0.9250\nIteration 5/5 - Best Fitness = 0.9250\n\n==============================\n BEST FEATURES SELECTED BY GWO\n==============================\nTotal Features : 19\nSelected       : 17\n['customer_id', 'age', 'occupation_status', 'years_employed', 'annual_income', 'credit_score', 'credit_history_years', 'current_debt', 'defaults_on_file', 'delinquencies_last_2yrs', 'derogatory_marks', 'loan_intent', 'loan_amount', 'interest_rate', 'debt_to_income_ratio', 'loan_to_income_ratio', 'payment_to_income_ratio']\n\n==============================\n FINAL GRADIENT BOOSTING RESULTS\n==============================\nAccuracy :  0.9184\nPrecision: 0.9133\nRecall   : 0.9411\nF1-score : 0.9270\nAVG Score: 0.9250\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# ============================================================\n# Bat Algorithm (BA) + Gradient Boosting for Feature Selection\n# ============================================================\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score\n)\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n# ============================================================\n# 1. Prepare dataset\n# ============================================================\nX = df.drop(\"loan_status\", axis=1)\ny = df[\"loan_status\"].astype(int)\n\nfeature_count = X.shape[1]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\n\n# ============================================================\n# 2. Bat Algorithm Parameters\n# ============================================================\nn_bats = 20\niterations = 5\n\nA = 0.5       # Loudness\nr = 0.5       # Pulse emission rate\nfmin, fmax = 0, 2  # Frequency range\n\n\n# ============================================================\n# 3. Fitness Function using Gradient Boosting\n# ============================================================\ndef fitness(solution):\n    if np.sum(solution) == 0:\n        return 0  # no features selected → invalid\n\n    selected_cols = X.columns[solution == 1]\n    Xtr = X_train[selected_cols]\n    Xte = X_test[selected_cols]\n\n    model = GradientBoostingClassifier(\n        n_estimators=200,\n        learning_rate=0.05,\n        max_depth=3,\n        random_state=42\n    )\n\n    model.fit(Xtr, y_train)\n    pred = model.predict(Xte)\n\n    acc = accuracy_score(y_test, pred)\n    prec = precision_score(y_test, pred, zero_division=0)\n    rec = recall_score(y_test, pred, zero_division=0)\n    f1 = f1_score(y_test, pred, zero_division=0)\n\n    return (acc + prec + rec + f1) / 4\n\n\n# ============================================================\n# 4. Initialize Bat Population\n# ============================================================\ndef init_population():\n    pop = np.random.randint(0, 2, (n_bats, feature_count))\n    vel = np.random.rand(n_bats, feature_count)\n    freq = np.zeros(n_bats)\n    return pop, vel, freq\n\n\n# ============================================================\n# 5. Bat Algorithm Main Loop\n# ============================================================\ndef bat_feature_selection():\n\n    population, velocity, freq = init_population()\n    fitness_vals = np.zeros(n_bats)\n\n    # Evaluate initial\n    for i in range(n_bats):\n        fitness_vals[i] = fitness(population[i])\n\n    best_idx = np.argmax(fitness_vals)\n    best_solution = population[best_idx].copy()\n    best_fitness = fitness_vals[best_idx]\n\n    for itr in range(iterations):\n\n        for i in range(n_bats):\n\n            # Update frequency\n            freq[i] = fmin + (fmax - fmin) * np.random.rand()\n\n            # Velocity update\n            velocity[i] += (population[i] ^ best_solution) * freq[i]\n\n            # Position update (Binary using Sigmoid)\n            temp = population[i] + velocity[i]\n            prob = 1 / (1 + np.exp(-temp))\n            new_solution = (np.random.rand(feature_count) < prob).astype(int)\n\n            # Local search\n            if np.random.rand() > r:\n                # Flip some bits (local improvement)\n                flip_idx = np.random.randint(0, feature_count)\n                new_solution[flip_idx] = 1 - new_solution[flip_idx]\n\n            new_fitness = fitness(new_solution)\n\n            # Accept new solution based on loudness A\n            if (new_fitness > fitness_vals[i]) and (np.random.rand() < A):\n                population[i] = new_solution\n                fitness_vals[i] = new_fitness\n\n            # Update global best\n            if new_fitness > best_fitness:\n                best_solution = new_solution.copy()\n                best_fitness = new_fitness\n\n        print(f\"Iteration {itr+1}/{iterations} - Best Fitness = {best_fitness:.4f}\")\n\n    return best_solution, best_fitness\n\n\n# ============================================================\n# 6. Run Bat Algorithm for Feature Selection\n# ============================================================\nbest_solution, best_score = bat_feature_selection()\n\nselected_features = X.columns[best_solution == 1].tolist()\n\nprint(\"\\n==============================\")\nprint(\"  BEST FEATURES SELECTED BY BAT\")\nprint(\"==============================\")\nprint(\"Total Features :\", len(X.columns))\nprint(\"Selected       :\", len(selected_features))\nprint(selected_features)\n\n\n# ============================================================\n# 7. Final Gradient Boosting on Selected Features\n# ============================================================\nXtr_final = X_train[selected_features]\nXte_final = X_test[selected_features]\n\nfinal_model = GradientBoostingClassifier(\n    n_estimators=200,\n    learning_rate=0.05,\n    max_depth=3,\n    random_state=42\n)\n\nfinal_model.fit(Xtr_final, y_train)\npred_final = final_model.predict(Xte_final)\n\nacc = accuracy_score(y_test, pred_final)\nprec = precision_score(y_test, pred_final, zero_division=0)\nrec = recall_score(y_test, pred_final, zero_division=0)\nf1 = f1_score(y_test, pred_final, zero_division=0)\n\nprint(\"\\n==============================\")\nprint(\" FINAL GRADIENT BOOSTING RESULTS\")\nprint(\"==============================\")\nprint(f\"Accuracy :  {acc:.4f}\")\nprint(f\"Precision: {prec:.4f}\")\nprint(f\"Recall   : {rec:.4f}\")\nprint(f\"F1-score : {f1:.4f}\")\nprint(f\"AVG Score: {(acc + prec + rec + f1) / 4:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T14:49:20.967061Z","iopub.execute_input":"2025-11-27T14:49:20.967782Z","iopub.status.idle":"2025-11-27T15:30:15.330490Z","shell.execute_reply.started":"2025-11-27T14:49:20.967747Z","shell.execute_reply":"2025-11-27T15:30:15.329737Z"}},"outputs":[{"name":"stdout","text":"Iteration 1/5 - Best Fitness = 0.9245\nIteration 2/5 - Best Fitness = 0.9252\nIteration 3/5 - Best Fitness = 0.9252\nIteration 4/5 - Best Fitness = 0.9252\nIteration 5/5 - Best Fitness = 0.9252\n\n==============================\n  BEST FEATURES SELECTED BY BAT\n==============================\nTotal Features : 19\nSelected       : 18\n['age', 'occupation_status', 'years_employed', 'annual_income', 'credit_score', 'credit_history_years', 'savings_assets', 'current_debt', 'defaults_on_file', 'delinquencies_last_2yrs', 'derogatory_marks', 'product_type', 'loan_intent', 'loan_amount', 'interest_rate', 'debt_to_income_ratio', 'loan_to_income_ratio', 'payment_to_income_ratio']\n\n==============================\n FINAL GRADIENT BOOSTING RESULTS\n==============================\nAccuracy :  0.9186\nPrecision: 0.9124\nRecall   : 0.9426\nF1-score : 0.9273\nAVG Score: 0.9252\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# ============================================================\n# Poor & Rich Optimization (PRO) + Gradient Boosting for Feature Selection\n# ============================================================\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score\n)\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n# ============================================================\n# 1. Prepare dataset\n# ============================================================\nX = df.drop(\"loan_status\", axis=1)\ny = df[\"loan_status\"].astype(int)\n\nfeature_count = X.shape[1]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\n\n# ============================================================\n# 2. Fitness Function using Gradient Boosting\n# ============================================================\ndef fitness(solution):\n    if np.sum(solution) == 0:\n        return 0  # No feature selected\n\n    selected = X.columns[solution == 1]\n    Xtr = X_train[selected]\n    Xte = X_test[selected]\n\n    model = GradientBoostingClassifier(\n        n_estimators=200,\n        learning_rate=0.05,\n        max_depth=3,\n        random_state=42\n    )\n\n    model.fit(Xtr, y_train)\n    pred = model.predict(Xte)\n\n    acc = accuracy_score(y_test, pred)\n    prec = precision_score(y_test, pred, zero_division=0)\n    rec = recall_score(y_test, pred, zero_division=0)\n    f1 = f1_score(y_test, pred, zero_division=0)\n\n    return (acc + prec + rec + f1) / 4\n\n\n# ============================================================\n# 3. PRO Parameters\n# ============================================================\npop_size = 20\niterations = 5\n\n\n# ============================================================\n# 4. Initialize Population\n# ============================================================\ndef init_population():\n    pop = np.random.randint(0, 2, (pop_size, feature_count))\n    return pop\n\n\n# ============================================================\n# 5. Poor & Rich Optimization (PRO) algorithm\n# ============================================================\ndef PRO_feature_selection():\n\n    population = init_population()\n    fitness_vals = np.zeros(pop_size)\n\n    # Evaluate initial\n    for i in range(pop_size):\n        fitness_vals[i] = fitness(population[i])\n\n    best_idx = np.argmax(fitness_vals)\n    best_solution = population[best_idx].copy()\n    best_fitness = fitness_vals[best_idx]\n\n    for itr in range(iterations):\n\n        median_fit = np.median(fitness_vals)\n\n        rich_group = population[fitness_vals >= median_fit]\n        poor_group = population[fitness_vals < median_fit]\n\n        mean_rich = np.mean(rich_group, axis=0) if len(rich_group) > 0 else np.random.rand(feature_count)\n        mean_poor = np.mean(poor_group, axis=0) if len(poor_group) > 0 else np.random.rand(feature_count)\n\n        new_population = population.copy()\n\n        for i in range(pop_size):\n\n            if fitness_vals[i] >= median_fit:\n                # Rich → exploit near rich mean\n                new_vec = population[i] + np.random.uniform(-1, 1) * (mean_rich - population[i])\n            else:\n                # Poor → move toward rich\n                new_vec = population[i] + np.random.uniform(0, 1) * (mean_rich - population[i])\n\n            # Sigmoid → Binary conversion\n            prob = 1 / (1 + np.exp(-new_vec))\n            new_solution = (np.random.rand(feature_count) < prob).astype(int)\n\n            new_fitness = fitness(new_solution)\n\n            # Accept if improves\n            if new_fitness > fitness_vals[i]:\n                new_population[i] = new_solution\n                fitness_vals[i] = new_fitness\n\n            # Global best update\n            if new_fitness > best_fitness:\n                best_fitness = new_fitness\n                best_solution = new_solution.copy()\n\n        population = new_population\n\n        print(f\"Iteration {itr+1}/{iterations} - Best Fitness = {best_fitness:.4f}\")\n\n    return best_solution, best_fitness\n\n\n# ============================================================\n# 6. Run PRO Feature Selection\n# ============================================================\nbest_solution, best_score = PRO_feature_selection()\n\nselected_features = X.columns[best_solution == 1].tolist()\n\nprint(\"\\n==============================\")\nprint(\"  BEST FEATURES SELECTED BY PRO\")\nprint(\"==============================\")\nprint(\"Total Features :\", len(X.columns))\nprint(\"Selected       :\", len(selected_features))\nprint(selected_features)\n\n\n# ============================================================\n# 7. Final Gradient Boosting on Selected Features\n# ============================================================\nXtr_final = X_train[selected_features]\nXte_final = X_test[selected_features]\n\nfinal_model = GradientBoostingClassifier(\n    n_estimators=200,\n    learning_rate=0.05,\n    max_depth=3,\n    random_state=42\n)\n\nfinal_model.fit(Xtr_final, y_train)\npred_final = final_model.predict(Xte_final)\n\nacc = accuracy_score(y_test, pred_final)\nprec = precision_score(y_test, pred_final, zero_division=0)\nrec = recall_score(y_test, pred_final, zero_division=0)\nf1 = f1_score(y_test, pred_final, zero_division=0)\n\nprint(\"\\n==============================\")\nprint(\" FINAL GRADIENT BOOSTING RESULTS (PRO)\")\nprint(\"==============================\")\nprint(f\"Accuracy :  {acc:.4f}\")\nprint(f\"Precision: {prec:.4f}\")\nprint(f\"Recall   : {rec:.4f}\")\nprint(f\"F1-score : {f1:.4f}\")\nprint(f\"AVG Score: {(acc + prec + rec + f1) / 4:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T15:41:39.568129Z","iopub.execute_input":"2025-11-27T15:41:39.568940Z","iopub.status.idle":"2025-11-27T16:13:32.391576Z","shell.execute_reply.started":"2025-11-27T15:41:39.568905Z","shell.execute_reply":"2025-11-27T16:13:32.390725Z"}},"outputs":[{"name":"stdout","text":"Iteration 1/5 - Best Fitness = 0.9220\nIteration 2/5 - Best Fitness = 0.9221\nIteration 3/5 - Best Fitness = 0.9248\nIteration 4/5 - Best Fitness = 0.9248\nIteration 5/5 - Best Fitness = 0.9248\n\n==============================\n  BEST FEATURES SELECTED BY PRO\n==============================\nTotal Features : 19\nSelected       : 18\n['customer_id', 'age', 'occupation_status', 'years_employed', 'annual_income', 'credit_score', 'credit_history_years', 'savings_assets', 'current_debt', 'defaults_on_file', 'delinquencies_last_2yrs', 'derogatory_marks', 'product_type', 'loan_intent', 'loan_amount', 'interest_rate', 'debt_to_income_ratio', 'payment_to_income_ratio']\n\n==============================\n FINAL GRADIENT BOOSTING RESULTS (PRO)\n==============================\nAccuracy :  0.9182\nPrecision: 0.9132\nRecall   : 0.9408\nF1-score : 0.9268\nAVG Score: 0.9248\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# ===============================\n# Hybrid PSO + XGBoost + LightGBM\n# Feature Selection + Evaluation\n# ===============================\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# ===============================\n# 1. Load / Prepare Data\n# ===============================\n\n# Your DataFrame must already exist as df\n# Example: df = pd.read_csv(\"your_dataset.csv\")\ndf=pd.read_csv(\"/kaggle/input/loan-final-normalized-csv/loan_final_normalized.csv\")\nTARGET_COL = \"loan_status\"    \n\nX = df.drop(columns=[TARGET_COL])\ny = df[TARGET_COL].astype(int)   # assuming binary labels like 0/1\n\nfeature_names = X.columns.to_numpy()\nn_features = X.shape[1]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\nprint(\"Train shape:\", X_train.shape)\nprint(\"Test shape :\", X_test.shape)\nprint(\"No. of features:\", n_features)\n\n\n# ===============================\n# 2. Helper: Model Evaluation\n# ===============================\n\ndef evaluate_model(model, X_tr, X_te, y_tr, y_te, label=\"MODEL\", average_type=\"binary\"):\n    \"\"\"\n    Train model and print Accuracy, Precision, Recall, F1.\n    For multi-class classification, set average_type=\"macro\".\n    \"\"\"\n    model.fit(X_tr, y_tr)\n    y_pred = model.predict(X_te)\n\n    acc = accuracy_score(y_te, y_pred)\n    prec = precision_score(y_te, y_pred, average=average_type, zero_division=0)\n    rec = recall_score(y_te, y_pred, average=average_type, zero_division=0)\n    f1 = f1_score(y_te, y_pred, average=average_type, zero_division=0)\n\n    print(f\"\\n==== {label} ====\")\n    print(f\"Accuracy : {acc:.4f}\")\n    print(f\"Precision: {prec:.4f}\")\n    print(f\"Recall   : {rec:.4f}\")\n    print(f\"F1-score : {f1:.4f}\")\n\n    return {\"acc\": acc, \"prec\": prec, \"rec\": rec, \"f1\": f1}\n\n\n# ===============================\n# 3. PSO for Feature Selection\n#    – Fitness = XGBoost CV F1\n# ===============================\n\nclass PSOFeatureSelector:\n    \"\"\"\n    Binary PSO for feature selection using XGBoost as evaluator.\n    Each particle encodes a bitmask over features.\n    \"\"\"\n\n    def __init__(\n        self,\n        n_particles,\n        n_iterations,\n        inertia=0.7,\n        cognitive=1.5,\n        social=1.5,\n        random_state=42,\n        cv_splits=3\n    ):\n        self.n_particles = n_particles\n        self.n_iterations = n_iterations\n        self.w = inertia\n        self.c1 = cognitive\n        self.c2 = social\n        self.random_state = random_state\n        self.cv_splits = cv_splits\n        np.random.seed(self.random_state)\n\n    def _init_swarm(self, n_dims):\n        # positions: (n_particles, n_features) in {0,1}\n        positions = np.random.randint(0, 2, size=(self.n_particles, n_dims))\n        # avoid all-zero vectors\n        for i in range(self.n_particles):\n            if positions[i].sum() == 0:\n                positions[i, np.random.randint(0, n_dims)] = 1\n\n        velocities = np.random.uniform(-1, 1, size=(self.n_particles, n_dims))\n\n        return positions.astype(float), velocities.astype(float)\n\n    def _fitness(self, mask, X, y):\n        if mask.sum() == 0:\n            return 0.0\n\n        X_sel = X[:, mask == 1]\n\n        model = XGBClassifier(\n            n_estimators=200,\n            max_depth=4,\n            learning_rate=0.1,\n            subsample=0.8,\n            colsample_bytree=0.8,\n            eval_metric=\"logloss\",\n            n_jobs=-1\n        )\n\n        cv = StratifiedKFold(n_splits=self.cv_splits, shuffle=True, random_state=self.random_state)\n        f1 = cross_val_score(model, X_sel, y, cv=cv, scoring=\"f1\").mean()\n\n        # NEW: penalty for too many or too few features\n        k = mask.sum()                           # number of selected features\n        penalty = 0.002 * abs(k - (X.shape[1] * 0.4))   # optimal = 40% features\n\n        return f1 - penalty\n\n\n    def fit(self, X, y):\n        \"\"\"\n        Run PSO to find best feature subset.\n        X: np.array (n_samples, n_features)\n        y: np.array (n_samples,)\n        \"\"\"\n        n_dims = X.shape[1]\n        positions, velocities = self._init_swarm(n_dims)\n\n        # personal best\n        pbest_pos = positions.copy()\n        pbest_scores = np.array([self._fitness(p, X, y) for p in positions])\n\n        # global best\n        gbest_idx = np.argmax(pbest_scores)\n        gbest_pos = pbest_pos[gbest_idx].copy()\n        gbest_score = pbest_scores[gbest_idx]\n\n        print(\"\\n[PSO] Initial best F1:\", gbest_score)\n\n        for it in range(self.n_iterations):\n            for i in range(self.n_particles):\n                # Update velocity\n                r1 = np.random.rand(n_dims)\n                r2 = np.random.rand(n_dims)\n\n                velocities[i] = (\n                    self.w * velocities[i]\n                    + self.c1 * r1 * (pbest_pos[i] - positions[i])\n                    + self.c2 * r2 * (gbest_pos - positions[i])\n                )\n\n                # Binary PSO using sigmoid\n                sigmoid = 1 / (1 + np.exp(-velocities[i]))\n                new_pos = (np.random.rand(n_dims) < sigmoid).astype(float)\n\n                # Avoid no-feature case\n                if new_pos.sum() == 0:\n                    new_pos[np.random.randint(0, n_dims)] = 1\n\n                positions[i] = new_pos\n\n                # Evaluate new fitness\n                score = self._fitness(positions[i], X, y)\n\n                # Update personal best\n                if score > pbest_scores[i]:\n                    pbest_scores[i] = score\n                    pbest_pos[i] = positions[i].copy()\n\n            # Update global best\n            best_idx = np.argmax(pbest_scores)\n            if pbest_scores[best_idx] > gbest_score:\n                gbest_score = pbest_scores[best_idx]\n                gbest_pos = pbest_pos[best_idx].copy()\n\n            print(f\"[PSO] Iter {it+1}/{self.n_iterations} - Best F1: {gbest_score:.4f}  | Selected features: {int(gbest_pos.sum())}\")\n\n        self.best_mask_ = gbest_pos.astype(int)\n        self.best_score_ = gbest_score\n        return self\n\n    def transform(self, X):\n        return X[:, self.best_mask_ == 1]\n\n    def fit_transform(self, X, y):\n        self.fit(X, y)\n        return self.transform(X)\n\n\n# ===============================\n# 4. Run PSO Feature Selection\n# ===============================\n\n# Convert to numpy for speed\nX_train_np = X_train.to_numpy()\ny_train_np = y_train.to_numpy()\n\npso_selector = PSOFeatureSelector(\n    n_particles=20,\n    n_iterations=20,\n    inertia=0.7,\n    cognitive=1.5,\n    social=1.5,\n    random_state=42,\n    cv_splits=3\n)\n\nX_train_fs = pso_selector.fit_transform(X_train_np, y_train_np)\nmask = pso_selector.best_mask_.astype(bool)\nselected_features = feature_names[mask]\n\nprint(\"\\n===============================\")\nprint(\"PSO + XGBoost Feature Selection\")\nprint(\"Best CV F1-score:\", pso_selector.best_score_)\nprint(\"Selected feature count:\", len(selected_features))\nprint(\"Selected features:\")\nprint(list(selected_features))\nprint(\"===============================\")\n\n# Apply same feature subset to test data\nX_test_np = X_test.to_numpy()\nX_test_fs = X_test_np[:, mask]\n\n\n# ===============================\n# 5. Train & Evaluate Models\n# ===============================\n\n# 5.1 Baseline: LightGBM on ALL features\nlgb_all = LGBMClassifier(\n    n_estimators=300,\n    learning_rate=0.05,\n    max_depth=-1,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    random_state=42,\n    n_jobs=-1\n)\nresults_lgb_all = evaluate_model(\n    lgb_all, X_train, X_test, y_train, y_test,\n    label=\"LightGBM (All Features)\", \n    average_type=\"binary\"  # for multi-class use \"macro\"\n)\n\n# 5.2 Baseline: XGBoost on ALL features\nxgb_all = XGBClassifier(\n    n_estimators=300,\n    max_depth=5,\n    learning_rate=0.05,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    eval_metric=\"logloss\",\n    random_state=42,\n    n_jobs=-1\n)\nresults_xgb_all = evaluate_model(\n    xgb_all, X_train, X_test, y_train, y_test,\n    label=\"XGBoost (All Features)\",\n    average_type=\"binary\"\n)\n\n# 5.3 HYBRID: PSO (XGBoost-based feature eval) + LightGBM\nlgb_pso = LGBMClassifier(\n    n_estimators=300,\n    learning_rate=0.05,\n    max_depth=-1,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    random_state=42,\n    n_jobs=-1\n)\nresults_lgb_pso = evaluate_model(\n    lgb_pso, X_train_fs, X_test_fs, y_train, y_test,\n    label=\"Hybrid: PSO + XGBoost (FS) + LightGBM\",\n    average_type=\"binary\"\n)\n\nprint(\"\\n\\n======= SUMMARY =======\")\nprint(\"LightGBM (All):\", results_lgb_all)\nprint(\"XGBoost (All):\", results_xgb_all)\nprint(\"Hybrid PSO+XGB+LGB:\", results_lgb_pso)\nprint(\"========================\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T03:58:46.081237Z","iopub.execute_input":"2025-11-28T03:58:46.081771Z","iopub.status.idle":"2025-11-28T04:05:02.128967Z","shell.execute_reply.started":"2025-11-28T03:58:46.081742Z","shell.execute_reply":"2025-11-28T04:05:02.128177Z"}},"outputs":[{"name":"stdout","text":"Train shape: (40000, 19)\nTest shape : (10000, 19)\nNo. of features: 19\n\n[PSO] Initial best F1: 0.9253162974218366\n[PSO] Iter 1/20 - Best F1: 0.9324  | Selected features: 14\n[PSO] Iter 2/20 - Best F1: 0.9324  | Selected features: 14\n[PSO] Iter 3/20 - Best F1: 0.9354  | Selected features: 15\n[PSO] Iter 4/20 - Best F1: 0.9354  | Selected features: 15\n[PSO] Iter 5/20 - Best F1: 0.9354  | Selected features: 15\n[PSO] Iter 6/20 - Best F1: 0.9354  | Selected features: 15\n[PSO] Iter 7/20 - Best F1: 0.9354  | Selected features: 15\n[PSO] Iter 8/20 - Best F1: 0.9354  | Selected features: 15\n[PSO] Iter 9/20 - Best F1: 0.9354  | Selected features: 15\n[PSO] Iter 10/20 - Best F1: 0.9355  | Selected features: 14\n[PSO] Iter 11/20 - Best F1: 0.9355  | Selected features: 14\n[PSO] Iter 12/20 - Best F1: 0.9360  | Selected features: 14\n[PSO] Iter 13/20 - Best F1: 0.9360  | Selected features: 14\n[PSO] Iter 14/20 - Best F1: 0.9360  | Selected features: 14\n[PSO] Iter 15/20 - Best F1: 0.9360  | Selected features: 14\n[PSO] Iter 16/20 - Best F1: 0.9360  | Selected features: 14\n[PSO] Iter 17/20 - Best F1: 0.9360  | Selected features: 14\n[PSO] Iter 18/20 - Best F1: 0.9366  | Selected features: 15\n[PSO] Iter 19/20 - Best F1: 0.9366  | Selected features: 15\n[PSO] Iter 20/20 - Best F1: 0.9366  | Selected features: 15\n\n===============================\nPSO + XGBoost Feature Selection\nBest CV F1-score: 0.9366400673385248\nSelected feature count: 15\nSelected features:\n['age', 'years_employed', 'annual_income', 'credit_score', 'credit_history_years', 'savings_assets', 'current_debt', 'defaults_on_file', 'delinquencies_last_2yrs', 'derogatory_marks', 'loan_intent', 'loan_amount', 'interest_rate', 'debt_to_income_ratio', 'payment_to_income_ratio']\n===============================\n[LightGBM] [Info] Number of positive: 22018, number of negative: 17982\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005506 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3140\n[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 19\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.550450 -> initscore=0.202489\n[LightGBM] [Info] Start training from score 0.202489\n\n==== LightGBM (All Features) ====\nAccuracy : 0.9267\nPrecision: 0.9271\nRecall   : 0.9408\nF1-score : 0.9339\n\n==== XGBoost (All Features) ====\nAccuracy : 0.9272\nPrecision: 0.9246\nRecall   : 0.9448\nF1-score : 0.9346\n[LightGBM] [Info] Number of positive: 22018, number of negative: 17982\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003179 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2624\n[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 15\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.550450 -> initscore=0.202489\n[LightGBM] [Info] Start training from score 0.202489\n\n==== Hybrid: PSO + XGBoost (FS) + LightGBM ====\nAccuracy : 0.9268\nPrecision: 0.9273\nRecall   : 0.9408\nF1-score : 0.9340\n\n\n======= SUMMARY =======\nLightGBM (All): {'acc': 0.9267, 'prec': 0.927139276763337, 'rec': 0.9407811080835604, 'f1': 0.9339103777837888}\nXGBoost (All): {'acc': 0.9272, 'prec': 0.9246222222222222, 'rec': 0.9447774750227066, 'f1': 0.9345911949685535}\nHybrid PSO+XGB+LGB: {'acc': 0.9268, 'prec': 0.9273052820053715, 'rec': 0.9407811080835604, 'f1': 0.9339945897204689}\n========================\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n# -----------------------------\n# Combined PSO + GWO Optimization\n# -----------------------------\nclass PSO_GWO_FeatureSelector:\n    def __init__(self, num_particles, num_features, max_iter):\n        self.num_particles = num_particles\n        self.num_features = num_features\n        self.max_iter = max_iter\n        \n    def random_mask(self):\n        mask = np.random.randint(0, 2, self.num_features)\n        if mask.sum() == 0:\n            mask[np.random.randint(0, self.num_features)] = 1\n        return mask\n    \n    def evaluate_mask(self, mask, X, y):\n        selected = np.where(mask == 1)[0]\n        if len(selected) == 0:\n            return 0.0\n        \n        X_sel = X[:, selected]\n        \n        model = XGBClassifier(\n            n_estimators=100,\n            max_depth=5,\n            learning_rate=0.1,\n            eval_metric=\"logloss\",\n            n_jobs=-1,\n            subsample=0.8\n        )\n        \n        cv = StratifiedKFold(n_splits=3, shuffle=True)\n        scores = []\n        \n        for tr, va in cv.split(X_sel, y):\n            model.fit(X_sel[tr], y[tr])\n            pred = model.predict(X_sel[va])\n            scores.append(f1_score(y[va], pred))\n        \n        return np.mean(scores)\n    \n    def optimize(self, X, y):\n        population = np.array([self.random_mask() for _ in range(self.num_particles)])\n        fitness = np.array([self.evaluate_mask(m, X, y) for m in population])\n        \n        alpha, beta, delta = population[np.argsort(-fitness)[:3]]\n\n        # PSO parameters\n        velocity = np.random.uniform(-1, 1, population.shape)\n        w, c1, c2 = 0.7, 1.4, 1.4\n        \n        for it in range(self.max_iter):\n            for i in range(self.num_particles):\n                \n                # ========== PSO Update ==========\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocity[i] = (\n                    w * velocity[i]\n                    + c1 * r1 * (alpha - population[i])\n                    + c2 * r2 * (beta - population[i])\n                )\n                new_pso = (np.random.rand(self.num_features) < 1/(1+np.exp(-velocity[i]))).astype(int)\n                \n                # ========== GWO Update ==========\n                a = 2 * (1 - it / self.max_iter)\n                dist_alpha = abs(alpha - population[i])\n                X1 = alpha - a * dist_alpha\n                \n                dist_beta = abs(beta - population[i])\n                X2 = beta - a * dist_beta\n                \n                dist_delta = abs(delta - population[i])\n                X3 = delta - a * dist_delta\n                \n                new_gwo = ((X1 + X2 + X3) / 3 >= 0.5).astype(int)\n                \n                # Combine two explorers: weighted hybrid\n                new_mask = ((new_pso + new_gwo) / 2 >= 0.5).astype(int)\n                \n                if new_mask.sum() == 0:\n                    new_mask[np.random.randint(0, self.num_features)] = 1\n                \n                # Evaluate\n                new_fit = self.evaluate_mask(new_mask, X, y)\n                \n                if new_fit > fitness[i]:\n                    population[i] = new_mask\n                    fitness[i] = new_fit\n            \n            # Update alpha, beta, delta wolves\n            alpha, beta, delta = population[np.argsort(-fitness)[:3]]\n            \n            print(f\"Iter {it+1}/{self.max_iter} - Best F1: {fitness.max():.4f}\")\n        \n        return alpha, fitness.max()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T04:28:30.553580Z","iopub.execute_input":"2025-11-28T04:28:30.554191Z","iopub.status.idle":"2025-11-28T04:28:30.799397Z","shell.execute_reply.started":"2025-11-28T04:28:30.554166Z","shell.execute_reply":"2025-11-28T04:28:30.798840Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# ===================================================================\n# FULL HYBRID PIPELINE: PSO + GWO + Feature Fusion + Bayesian Opt +\n# XGBoost + LightGBM + CatBoost + Metrics\n# ===================================================================\n\nimport numpy as np\nimport pandas as pd\nimport random\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\n\nfrom bayes_opt import BayesianOptimization\n\n# ============================================================\n# LOAD DATA\n# df MUST BE ALREADY LOADED\n# TARGET COL MUST BE CHANGED HERE\n# ============================================================\nTARGET = \"loan_status\"\n\nX = df.drop(columns=[TARGET]).to_numpy()\ny = df[TARGET].to_numpy()\nfeature_names = df.drop(columns=[TARGET]).columns\n\n\n# ============================================================\n# TRAIN/VAL/TEST SPLIT\n# ============================================================\nX_train_full, X_test, y_train_full, y_test = train_test_split(\n    X, y, test_size=0.20, stratify=y, random_state=42\n)\n\nX_train, X_val, y_train, y_val = train_test_split(\n    X_train_full, y_train_full, test_size=0.20, stratify=y_train_full, random_state=42\n)\n\n\n# ============================================================\n# FEATURE IMPORTANCE FUSION (XGB + CAT)\n# ============================================================\ndef fused_feature_importance(X, y):\n    xgb = XGBClassifier(n_estimators=300, eval_metric=\"logloss\", n_jobs=-1)\n    cat = CatBoostClassifier(iterations=300, verbose=False)\n\n    xgb.fit(X, y)\n    cat.fit(X, y)\n\n    imp_xgb = xgb.feature_importances_ / (xgb.feature_importances_.max() + 1e-9)\n    imp_cat = cat.get_feature_importance() / (cat.get_feature_importance().max() + 1e-9)\n\n    fused = 0.6 * imp_xgb + 0.4 * imp_cat\n    return fused\n\n\nfused_scores = fused_feature_importance(X_train_full, y_train_full)\nranked = np.argsort(-fused_scores)\nprint(\"Top 20 fused features:\", feature_names[ranked[:20]])\n\n\n# ============================================================\n# HYBRID PSO + GWO FEATURE SELECTION\n# ============================================================\nclass PSO_GWO_FeatureSelector:\n    def __init__(self, n_particles, n_features, max_iter):\n        self.n_particles = n_particles\n        self.n_features = n_features\n        self.max_iter = max_iter\n\n    def random_mask(self):\n        mask = np.random.randint(0, 2, self.n_features)\n        if mask.sum() == 0:\n            mask[random.randint(0, self.n_features - 1)] = 1\n        return mask\n\n    def evaluate_mask(self, mask, X, y):\n        idx = np.where(mask == 1)[0]\n        if len(idx) == 0:\n            return 0\n\n        X_sel = X[:, idx]\n        model = XGBClassifier(\n            n_estimators=200, learning_rate=0.1, max_depth=5,\n            eval_metric=\"logloss\", n_jobs=-1\n        )\n\n        cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n        scores = []\n        for tr, va in cv.split(X_sel, y):\n            model.fit(X_sel[tr], y[tr])\n            pred = model.predict(X_sel[va])\n            scores.append(f1_score(y[va], pred))\n        return np.mean(scores)\n\n    def optimize(self, X, y):\n        pop = np.array([self.random_mask() for _ in range(self.n_particles)])\n        fitness = np.array([self.evaluate_mask(m, X, y) for m in pop])\n\n        alpha, beta, delta = pop[np.argsort(-fitness)[:3]]\n\n        # PSO velocity\n        velocity = np.random.uniform(-1, 1, pop.shape)\n        w, c1, c2 = 0.7, 1.5, 1.5\n\n        for it in range(self.max_iter):\n            for i in range(self.n_particles):\n\n                # ---------------- PSO update ----------------\n                r1, r2 = np.random.rand(), np.random.rand()\n                velocity[i] = (\n                    w * velocity[i]\n                    + c1 * r1 * (alpha - pop[i])\n                    + c2 * r2 * (beta - pop[i])\n                )\n                pso_mask = (np.random.rand(self.n_features) < 1/(1+np.exp(-velocity[i]))).astype(int)\n\n                # ---------------- GWO update ----------------\n                a = 2 * (1 - it / self.max_iter)\n                X1 = alpha - a * abs(alpha - pop[i])\n                X2 = beta - a * abs(beta - pop[i])\n                X3 = delta - a * abs(delta - pop[i])\n                gwo_mask = ((X1 + X2 + X3) / 3 >= 0.5).astype(int)\n\n                # Combine\n                new_mask = ((pso_mask + gwo_mask) / 2 >= 0.5).astype(int)\n\n                if new_mask.sum() == 0:\n                    new_mask[random.randint(0, self.n_features - 1)] = 1\n\n                new_fit = self.evaluate_mask(new_mask, X, y)\n                if new_fit > fitness[i]:\n                    pop[i] = new_mask\n                    fitness[i] = new_fit\n\n            best_idx = np.argmax(fitness)\n            alpha, beta, delta = pop[np.argsort(-fitness)[:3]]\n\n            print(f\"Iter {it+1}/{self.max_iter} — Best F1: {fitness.max():.4f}\")\n\n        return pop[np.argmax(fitness)], fitness.max()\n\n\nselector = PSO_GWO_FeatureSelector(\n    n_particles=25,\n    n_features=X_train.shape[1],\n    max_iter=20\n)\n\nbest_mask, best_f1 = selector.optimize(X_train, y_train)\nbest_features = np.where(best_mask == 1)[0]\nprint(\"Selected features:\", feature_names[best_features])\n\n\n# ============================================================\n# APPLY FEATURE SELECTION\n# ============================================================\nX_train_sel = X_train[:, best_features]\nX_val_sel   = X_val[:, best_features]\nX_test_sel  = X_test[:, best_features]\n\n\n# ============================================================\n# BAYESIAN OPTIMIZATION FOR LIGHTGBM\n# ============================================================\ndef lgb_eval(num_leaves, max_depth, learning_rate):\n    params = {\n        \"num_leaves\": int(num_leaves),\n        \"max_depth\": int(max_depth),\n        \"learning_rate\": float(learning_rate),\n        \"n_estimators\": 300,\n        \"objective\": \"binary\"\n    }\n    model = LGBMClassifier(**params)\n    model.fit(X_train_sel, y_train)\n    pred = model.predict(X_val_sel)\n    return f1_score(y_val, pred)\n\noptimizer = BayesianOptimization(\n    f=lgb_eval,\n    pbounds={\n        \"num_leaves\": (20, 200),\n        \"max_depth\": (4, 12),\n        \"learning_rate\": (0.03, 0.3)\n    },\n    random_state=42\n)\n\noptimizer.maximize(init_points=5, n_iter=20)\nbest_params = optimizer.max[\"params\"]\n\nbest_params[\"num_leaves\"] = int(best_params[\"num_leaves\"])\nbest_params[\"max_depth\"]  = int(best_params[\"max_depth\"])\nbest_params[\"learning_rate\"] = float(best_params[\"learning_rate\"])\n\nprint(\"Best params:\", best_params)\n\n\n# ============================================================\n# FINAL MODELS\n# ============================================================\nmodels = {\n    \"XGB\": XGBClassifier(n_estimators=300, eval_metric=\"logloss\"),\n    \"LGB\": LGBMClassifier(**best_params, n_estimators=300),\n    \"CAT\": CatBoostClassifier(iterations=300, verbose=False)\n}\n\nfor name, model in models.items():\n    model.fit(X_train_sel, y_train)\n    pred = model.predict(X_test_sel)\n\n    print(f\"\\n===== {name} =====\")\n    print(\"Accuracy :\", accuracy_score(y_test, pred))\n    print(\"Precision:\", precision_score(y_test, pred))\n    print(\"Recall   :\", recall_score(y_test, pred))\n    print(\"F1 Score :\", f1_score(y_test, pred))\n    print(\"AUC      :\", roc_auc_score(y_test, pred))\n\n\n'''output :\n===== LGB =====\nAccuracy : 0.9283\nPrecision: 0.9250710227272727\nRecall   : 0.9464123524069028\nF1 Score : 0.9356200053874472\nAUC      : 0.9262651305972223\n\n===== CAT =====\nAccuracy : 0.9282\nPrecision: 0.9262689225289403\nRecall   : 0.9447774750227066\nF1 Score : 0.935431654676259\nAUC      : 0.9263375695469485'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================\n# HYBRID PIPELINE:\n#   - GA Feature Selection\n#   - PSO Feature Selection\n#   - XGBoost / LightGBM / CatBoost models\n#   - Multiple combinations compared\n# ==============================================================\n\nimport numpy as np\nimport pandas as pd\nimport random\nimport warnings\nimport logging\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n)\n\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\n\n# -------------------------\n# 0. GLOBAL SETTINGS\n# -------------------------\nRANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE)\nrandom.seed(RANDOM_STATE)\n\n# Suppress warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Suppress LightGBM logs\nimport lightgbm as lgb\nlogging.getLogger(\"lightgbm\").setLevel(logging.ERROR)\n\n\n# ==============================================================\n# 1. LOAD DATA\n# ==============================================================\n\n# --- YOU MUST HAVE df ALREADY LOADED ---\n# Example:\n# df = pd.read_csv(\"your_data.csv\")\n\nTARGET_COL = \"loan_status\"   # <-- change if needed\n\nX_df = df.drop(columns=[TARGET_COL])\ny = df[TARGET_COL].astype(int).values\nfeature_names = X_df.columns.values\nX = X_df.values\n\nprint(\"Total samples:\", X.shape[0])\nprint(\"Total features:\", X.shape[1])\n\n\n# ==============================================================\n# 2. SPLIT INTO TRAIN / VAL / TEST\n# ==============================================================\n\nX_train_full, X_test, y_train_full, y_test = train_test_split(\n    X, y, test_size=0.20, stratify=y, random_state=RANDOM_STATE\n)\n\nX_train, X_val, y_train, y_val = train_test_split(\n    X_train_full, y_train_full, test_size=0.20,\n    stratify=y_train_full, random_state=RANDOM_STATE\n)\n\nprint(\"\\nTrain shape:\", X_train.shape)\nprint(\"Val shape  :\", X_val.shape)\nprint(\"Test shape :\", X_test.shape)\n\n\n# ==============================================================\n# 3. METRIC HELPER\n# ==============================================================\n\ndef evaluate_model(model, X_tr, X_te, y_tr, y_te, label):\n    model.fit(X_tr, y_tr)\n    y_pred = model.predict(X_te)\n\n    acc  = accuracy_score(y_te, y_pred)\n    prec = precision_score(y_te, y_pred, zero_division=0)\n    rec  = recall_score(y_te, y_pred, zero_division=0)\n    f1   = f1_score(y_te, y_pred, zero_division=0)\n    auc  = roc_auc_score(y_te, y_pred)\n\n    print(f\"\\n==== {label} ====\")\n    print(f\"Accuracy : {acc:.4f}\")\n    print(f\"Precision: {prec:.4f}\")\n    print(f\"Recall   : {rec:.4f}\")\n    print(f\"F1-score : {f1:.4f}\")\n    print(f\"AUC      : {auc:.4f}\")\n\n    return {\"acc\": acc, \"prec\": prec, \"rec\": rec, \"f1\": f1, \"auc\": auc}\n\n\n# ==============================================================\n# 4. GENETIC ALGORITHM FEATURE SELECTION\n# ==============================================================\n\nclass GAFeatureSelector:\n    \"\"\"\n    Binary Genetic Algorithm for feature selection.\n    Chromosome: binary mask over features.\n    Fitness: mean CV F1-score using XGBoost.\n    \"\"\"\n\n    def __init__(\n        self,\n        n_features,\n        pop_size=20,\n        n_generations=15,\n        crossover_rate=0.8,\n        mutation_rate=0.02,\n        random_state=42\n    ):\n        self.n_features = n_features\n        self.pop_size = pop_size\n        self.n_generations = n_generations\n        self.crossover_rate = crossover_rate\n        self.mutation_rate = mutation_rate\n        self.random_state = random_state\n        np.random.seed(self.random_state)\n        random.seed(self.random_state)\n\n    def _init_population(self):\n        pop = np.random.randint(0, 2, size=(self.pop_size, self.n_features))\n        # Ensure no all-zero individuals\n        for i in range(self.pop_size):\n            if pop[i].sum() == 0:\n                pop[i, np.random.randint(0, self.n_features)] = 1\n        return pop\n\n    def _fitness(self, mask, X, y):\n        idx = np.where(mask == 1)[0]\n        if len(idx) == 0:\n            return 0.0\n\n        X_sel = X[:, idx]\n        model = XGBClassifier(\n            n_estimators=200,\n            max_depth=4,\n            learning_rate=0.1,\n            subsample=0.8,\n            colsample_bytree=0.8,\n            eval_metric=\"logloss\",\n            n_jobs=-1,\n            random_state=self.random_state\n        )\n\n        cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=self.random_state)\n        scores = []\n        for tr, va in cv.split(X_sel, y):\n            model.fit(X_sel[tr], y[tr])\n            pred = model.predict(X_sel[va])\n            scores.append(f1_score(y[va], pred))\n        return np.mean(scores)\n\n    def _tournament_selection(self, population, fitness, k=3):\n        selected = np.random.choice(len(population), k, replace=False)\n        best_idx = selected[np.argmax(fitness[selected])]\n        return population[best_idx].copy()\n\n    def _crossover(self, parent1, parent2):\n        if np.random.rand() > self.crossover_rate:\n            return parent1.copy(), parent2.copy()\n\n        point = np.random.randint(1, self.n_features - 1)\n        child1 = np.concatenate([parent1[:point], parent2[point:]])\n        child2 = np.concatenate([parent2[:point], parent1[point:]])\n        return child1, child2\n\n    def _mutate(self, individual):\n        for i in range(self.n_features):\n            if np.random.rand() < self.mutation_rate:\n                individual[i] = 1 - individual[i]\n        if individual.sum() == 0:\n            individual[np.random.randint(0, self.n_features)] = 1\n        return individual\n\n    def fit(self, X, y):\n        pop = self._init_population()\n        fitness = np.array([self._fitness(ind, X, y) for ind in pop])\n\n        best_idx = np.argmax(fitness)\n        best_ind = pop[best_idx].copy()\n        best_fit = fitness[best_idx]\n\n        print(\"\\n[GA] Initial best F1:\", best_fit)\n\n        for gen in range(self.n_generations):\n            new_pop = []\n            while len(new_pop) < self.pop_size:\n                parent1 = self._tournament_selection(pop, fitness)\n                parent2 = self._tournament_selection(pop, fitness)\n\n                child1, child2 = self._crossover(parent1, parent2)\n                child1 = self._mutate(child1)\n                child2 = self._mutate(child2)\n\n                new_pop.extend([child1, child2])\n\n            pop = np.array(new_pop[:self.pop_size])\n            fitness = np.array([self._fitness(ind, X, y) for ind in pop])\n\n            gen_best_idx = np.argmax(fitness)\n            gen_best_fit = fitness[gen_best_idx]\n            if gen_best_fit > best_fit:\n                best_fit = gen_best_fit\n                best_ind = pop[gen_best_idx].copy()\n\n            print(f\"[GA] Generation {gen+1}/{self.n_generations} - Best F1: {best_fit:.4f}\")\n\n        self.best_mask_ = best_ind\n        self.best_score_ = best_fit\n        return self\n\n    def transform(self, X):\n        return X[:, self.best_mask_ == 1]\n\n    def fit_transform(self, X, y):\n        self.fit(X, y)\n        return self.transform(X)\n\n\n# ==============================================================\n# 5. PSO FEATURE SELECTION\n# ==============================================================\n\nclass PSOFeatureSelector:\n    \"\"\"\n    Binary PSO for feature selection.\n    Fitness: mean CV F1-score using XGBoost.\n    \"\"\"\n\n    def __init__(\n        self,\n        n_features,\n        n_particles=20,\n        n_iterations=15,\n        inertia=0.7,\n        cognitive=1.5,\n        social=1.5,\n        random_state=42\n    ):\n        self.n_features = n_features\n        self.n_particles = n_particles\n        self.n_iterations = n_iterations\n        self.w = inertia\n        self.c1 = cognitive\n        self.c2 = social\n        self.random_state = random_state\n        np.random.seed(self.random_state)\n\n    def _init_swarm(self):\n        positions = np.random.randint(0, 2, size=(self.n_particles, self.n_features)).astype(float)\n        for i in range(self.n_particles):\n            if positions[i].sum() == 0:\n                positions[i, np.random.randint(0, self.n_features)] = 1\n        velocities = np.random.uniform(-1, 1, size=(self.n_particles, self.n_features))\n        return positions, velocities\n\n    def _fitness(self, pos, X, y):\n        mask = (pos >= 0.5).astype(int)\n        idx = np.where(mask == 1)[0]\n        if len(idx) == 0:\n            return 0.0\n\n        X_sel = X[:, idx]\n        model = XGBClassifier(\n            n_estimators=200,\n            max_depth=4,\n            learning_rate=0.1,\n            subsample=0.8,\n            colsample_bytree=0.8,\n            eval_metric=\"logloss\",\n            n_jobs=-1,\n            random_state=self.random_state\n        )\n        cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=self.random_state)\n        scores = []\n        for tr, va in cv.split(X_sel, y):\n            model.fit(X_sel[tr], y[tr])\n            pred = model.predict(X_sel[va])\n            scores.append(f1_score(y[va], pred))\n        return np.mean(scores)\n\n    def fit(self, X, y):\n        positions, velocities = self._init_swarm()\n\n        pbest_pos = positions.copy()\n        pbest_scores = np.array([self._fitness(p, X, y) for p in positions])\n\n        gbest_idx = np.argmax(pbest_scores)\n        gbest_pos = pbest_pos[gbest_idx].copy()\n        gbest_score = pbest_scores[gbest_idx]\n\n        print(\"\\n[PSO] Initial best F1:\", gbest_score)\n\n        for it in range(self.n_iterations):\n            for i in range(self.n_particles):\n                r1 = np.random.rand(self.n_features)\n                r2 = np.random.rand(self.n_features)\n\n                velocities[i] = (\n                    self.w * velocities[i]\n                    + self.c1 * r1 * (pbest_pos[i] - positions[i])\n                    + self.c2 * r2 * (gbest_pos - positions[i])\n                )\n\n                sigmoid = 1 / (1 + np.exp(-velocities[i]))\n                new_pos = (np.random.rand(self.n_features) < sigmoid).astype(float)\n\n                if new_pos.sum() == 0:\n                    new_pos[np.random.randint(0, self.n_features)] = 1\n\n                positions[i] = new_pos\n                score = self._fitness(positions[i], X, y)\n\n                if score > pbest_scores[i]:\n                    pbest_scores[i] = score\n                    pbest_pos[i] = positions[i].copy()\n\n            best_idx = np.argmax(pbest_scores)\n            if pbest_scores[best_idx] > gbest_score:\n                gbest_score = pbest_scores[best_idx]\n                gbest_pos = pbest_pos[best_idx].copy()\n\n            print(f\"[PSO] Iter {it+1}/{self.n_iterations} - Best F1: {gbest_score:.4f}\")\n\n        self.best_pos_ = gbest_pos\n        self.best_score_ = gbest_score\n        return self\n\n    def transform(self, X):\n        mask = (self.best_pos_ >= 0.5).astype(int)\n        return X[:, mask == 1]\n\n    def fit_transform(self, X, y):\n        self.fit(X, y)\n        return self.transform(X)\n\n\n# ==============================================================\n# 6. RUN GA FEATURE SELECTION\n# ==============================================================\n\nga_selector = GAFeatureSelector(\n    n_features=X_train.shape[1],\n    pop_size=20,\n    n_generations=10,\n    crossover_rate=0.8,\n    mutation_rate=0.02,\n    random_state=RANDOM_STATE\n)\n\nX_train_ga = ga_selector.fit_transform(X_train, y_train)\nX_val_ga   = ga_selector.transform(X_val)\nX_test_ga  = ga_selector.transform(X_test)\n\nga_selected_features = feature_names[ga_selector.best_mask_ == 1]\nprint(\"\\n[GA] Best F1 (CV):\", ga_selector.best_score_)\nprint(\"[GA] Selected features count:\", len(ga_selected_features))\nprint(\"[GA] Features:\", list(ga_selected_features))\n\n\n# ==============================================================\n# 7. RUN PSO FEATURE SELECTION\n# ==============================================================\n\npso_selector = PSOFeatureSelector(\n    n_features=X_train.shape[1],\n    n_particles=20,\n    n_iterations=10,\n    inertia=0.7,\n    cognitive=1.5,\n    social=1.5,\n    random_state=RANDOM_STATE\n)\n\nX_train_pso = pso_selector.fit_transform(X_train, y_train)\nX_val_pso   = pso_selector.transform(X_val)\nX_test_pso  = pso_selector.transform(X_test)\n\npso_mask = (pso_selector.best_pos_ >= 0.5).astype(int)\npso_selected_features = feature_names[pso_mask == 1]\n\nprint(\"\\n[PSO] Best F1 (CV):\", pso_selector.best_score_)\nprint(\"[PSO] Selected features count:\", len(pso_selected_features))\nprint(\"[PSO] Features:\", list(pso_selected_features))\n\n\n# ==============================================================\n# 8. DEFINE MODELS (NO LGB WARNINGS)\n# ==============================================================\n\ndef get_models():\n    models = {\n        \"XGBoost\": XGBClassifier(\n            n_estimators=300,\n            max_depth=5,\n            learning_rate=0.05,\n            subsample=0.8,\n            colsample_bytree=0.8,\n            eval_metric=\"logloss\",\n            n_jobs=-1,\n            random_state=RANDOM_STATE\n        ),\n        \"LightGBM\": LGBMClassifier(\n            n_estimators=300,\n            learning_rate=0.05,\n            max_depth=-1,\n            subsample=0.8,\n            colsample_bytree=0.8,\n            objective=\"binary\",\n            random_state=RANDOM_STATE,\n            verbose=-1  # suppress logs/warnings\n        ),\n        \"CatBoost\": CatBoostClassifier(\n            iterations=300,\n            learning_rate=0.05,\n            depth=6,\n            verbose=False,\n            random_state=RANDOM_STATE\n        )\n    }\n    return models\n\n\n# ==============================================================\n# 9. TRAIN & EVALUATE ALL COMBINATIONS\n# ==============================================================\n\nresults = {}\n\n# --- 9.1 Baseline: All Features ---\nmodels_all = get_models()\nfor name, model in models_all.items():\n    res = evaluate_model(\n        model,\n        X_train_full,\n        X_test,\n        y_train_full,\n        y_test,\n        label=f\"{name} (All Features)\"\n    )\n    results[f\"{name}_all\"] = res\n\n# --- 9.2 GA-selected Features ---\nmodels_ga = get_models()\nfor name, model in models_ga.items():\n    res = evaluate_model(\n        model,\n        np.vstack([X_train_ga, X_val_ga]),\n        X_test_ga,\n        np.concatenate([y_train, y_val]),\n        y_test,\n        label=f\"{name} (GA Features)\"\n    )\n    results[f\"{name}_ga\"] = res\n\n# --- 9.3 PSO-selected Features ---\nmodels_pso = get_models()\nfor name, model in models_pso.items():\n    res = evaluate_model(\n        model,\n        np.vstack([X_train_pso, X_val_pso]),\n        X_test_pso,\n        np.concatenate([y_train, y_val]),\n        y_test,\n        label=f\"{name} (PSO Features)\"\n    )\n    results[f\"{name}_pso\"] = res\n\nprint(\"\\n\\n========== SUMMARY DICT ==========\")\nfor k, v in results.items():\n    print(k, \"->\", v)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T04:50:01.583592Z","iopub.execute_input":"2025-11-28T04:50:01.583874Z","iopub.status.idle":"2025-11-28T04:56:07.446507Z","shell.execute_reply.started":"2025-11-28T04:50:01.583855Z","shell.execute_reply":"2025-11-28T04:56:07.445847Z"}},"outputs":[{"name":"stdout","text":"Total samples: 50000\nTotal features: 19\n\nTrain shape: (32000, 19)\nVal shape  : (8000, 19)\nTest shape : (10000, 19)\n\n[GA] Initial best F1: 0.9246193188832371\n[GA] Generation 1/10 - Best F1: 0.9328\n[GA] Generation 2/10 - Best F1: 0.9329\n[GA] Generation 3/10 - Best F1: 0.9329\n[GA] Generation 4/10 - Best F1: 0.9332\n[GA] Generation 5/10 - Best F1: 0.9332\n[GA] Generation 6/10 - Best F1: 0.9351\n[GA] Generation 7/10 - Best F1: 0.9351\n[GA] Generation 8/10 - Best F1: 0.9355\n[GA] Generation 9/10 - Best F1: 0.9355\n[GA] Generation 10/10 - Best F1: 0.9355\n\n[GA] Best F1 (CV): 0.9355251946405058\n[GA] Selected features count: 15\n[GA] Features: ['age', 'years_employed', 'annual_income', 'credit_score', 'credit_history_years', 'savings_assets', 'current_debt', 'defaults_on_file', 'delinquencies_last_2yrs', 'derogatory_marks', 'loan_intent', 'loan_amount', 'interest_rate', 'debt_to_income_ratio', 'payment_to_income_ratio']\n\n[PSO] Initial best F1: 0.9246193188832371\n[PSO] Iter 1/10 - Best F1: 0.9325\n[PSO] Iter 2/10 - Best F1: 0.9325\n[PSO] Iter 3/10 - Best F1: 0.9344\n[PSO] Iter 4/10 - Best F1: 0.9344\n[PSO] Iter 5/10 - Best F1: 0.9344\n[PSO] Iter 6/10 - Best F1: 0.9344\n[PSO] Iter 7/10 - Best F1: 0.9344\n[PSO] Iter 8/10 - Best F1: 0.9344\n[PSO] Iter 9/10 - Best F1: 0.9344\n[PSO] Iter 10/10 - Best F1: 0.9344\n\n[PSO] Best F1 (CV): 0.9344153160349083\n[PSO] Selected features count: 15\n[PSO] Features: ['occupation_status', 'years_employed', 'annual_income', 'credit_score', 'credit_history_years', 'current_debt', 'defaults_on_file', 'delinquencies_last_2yrs', 'derogatory_marks', 'loan_intent', 'loan_amount', 'interest_rate', 'debt_to_income_ratio', 'loan_to_income_ratio', 'payment_to_income_ratio']\n\n==== XGBoost (All Features) ====\nAccuracy : 0.9272\nPrecision: 0.9246\nRecall   : 0.9448\nF1-score : 0.9346\nAUC      : 0.9252\n\n==== LightGBM (All Features) ====\nAccuracy : 0.9267\nPrecision: 0.9271\nRecall   : 0.9408\nF1-score : 0.9339\nAUC      : 0.9251\n\n==== CatBoost (All Features) ====\nAccuracy : 0.9279\nPrecision: 0.9253\nRecall   : 0.9453\nF1-score : 0.9352\nAUC      : 0.9259\n\n==== XGBoost (GA Features) ====\nAccuracy : 0.9260\nPrecision: 0.9230\nRecall   : 0.9444\nF1-score : 0.9336\nAUC      : 0.9239\n\n==== LightGBM (GA Features) ====\nAccuracy : 0.9268\nPrecision: 0.9273\nRecall   : 0.9408\nF1-score : 0.9340\nAUC      : 0.9252\n\n==== CatBoost (GA Features) ====\nAccuracy : 0.9290\nPrecision: 0.9256\nRecall   : 0.9471\nF1-score : 0.9363\nAUC      : 0.9270\n\n==== XGBoost (PSO Features) ====\nAccuracy : 0.9261\nPrecision: 0.9230\nRecall   : 0.9446\nF1-score : 0.9337\nAUC      : 0.9240\n\n==== LightGBM (PSO Features) ====\nAccuracy : 0.9250\nPrecision: 0.9245\nRecall   : 0.9406\nF1-score : 0.9325\nAUC      : 0.9232\n\n==== CatBoost (PSO Features) ====\nAccuracy : 0.9272\nPrecision: 0.9239\nRecall   : 0.9457\nF1-score : 0.9346\nAUC      : 0.9251\n\n\n========== SUMMARY DICT ==========\nXGBoost_all -> {'acc': 0.9272, 'prec': 0.9246222222222222, 'rec': 0.9447774750227066, 'f1': 0.9345911949685535, 'auc': 0.9252252224946682}\nLightGBM_all -> {'acc': 0.9267, 'prec': 0.927139276763337, 'rec': 0.9407811080835604, 'f1': 0.9339103777837888, 'auc': 0.9251180290139716}\nCatBoost_all -> {'acc': 0.9279, 'prec': 0.9253200568990043, 'rec': 0.945322434150772, 'f1': 0.9352143049689998, 'auc': 0.925942640879613}\nXGBoost_ga -> {'acc': 0.926, 'prec': 0.9229540209479851, 'rec': 0.9444141689373297, 'f1': 0.9335607829053689, 'auc': 0.9239312223996993}\nLightGBM_ga -> {'acc': 0.9268, 'prec': 0.9273052820053715, 'rec': 0.9407811080835604, 'f1': 0.9339945897204689, 'auc': 0.9252292637191996}\nCatBoost_ga -> {'acc': 0.929, 'prec': 0.9256169004083081, 'rec': 0.9471389645776567, 'f1': 0.9362542646794757, 'auc': 0.9269621407982832}\nXGBoost_pso -> {'acc': 0.9261, 'prec': 0.9229676961306355, 'rec': 0.9445958219800181, 'f1': 0.933656522129455, 'auc': 0.9240220489210436}\nLightGBM_pso -> {'acc': 0.925, 'prec': 0.9244777718264595, 'rec': 0.940599455040872, 'f1': 0.9324689357104268, 'auc': 0.9232474472089787}\nCatBoost_pso -> {'acc': 0.9272, 'prec': 0.923868677905945, 'rec': 0.9456857402361489, 'f1': 0.9346499102333933, 'auc': 0.9251231815752491}\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# ==============================================================\n# HYBRID PIPELINE (Corrected & Updated):\n#   - GA Feature Selection (Wrapper = CatBoost)\n#   - PSO Feature Selection (Wrapper = CatBoost)\n#   - XGBoost / LightGBM / CatBoost models\n#   - Multiple combinations compared\n# ==============================================================\n\nimport numpy as np\nimport pandas as pd\nimport random\nimport warnings\nimport logging\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n)\n\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\n\n# -------------------------\n# GLOBAL SETTINGS\n# -------------------------\ndf=pd.read_csv(\"/kaggle/input/loan-final-normalized-csv/loan_final_normalized.csv\")\nRANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE)\nrandom.seed(RANDOM_STATE)\n\nwarnings.filterwarnings(\"ignore\")\nlogging.getLogger(\"lightgbm\").setLevel(logging.ERROR)\n\n# ==============================================================\n# LOAD DATA\n# ==============================================================\n\nTARGET_COL = \"loan_status\"  # Change if needed\n\nX_df = df.drop(columns=[TARGET_COL])\ny = df[TARGET_COL].astype(int).values\nfeature_names = X_df.columns.values\nX = X_df.values\n\nprint(\"Total samples:\", X.shape[0])\nprint(\"Total features:\", X.shape[1])\n\n# ==============================================================\n# TRAIN / VAL / TEST SPLIT\n# ==============================================================\n\nX_train_full, X_test, y_train_full, y_test = train_test_split(\n    X, y, test_size=0.20, stratify=y, random_state=RANDOM_STATE\n)\n\nX_train, X_val, y_train, y_val = train_test_split(\n    X_train_full, y_train_full, test_size=0.20,\n    stratify=y_train_full, random_state=RANDOM_STATE\n)\n\nprint(\"\\nTrain:\", X_train.shape, \n      \"\\nVal  :\", X_val.shape, \n      \"\\nTest :\", X_test.shape)\n\n# ==============================================================\n# METRIC FUNCTION\n# ==============================================================\n\ndef evaluate_model(model, X_tr, X_te, y_tr, y_te, label):\n    model.fit(X_tr, y_tr)\n    y_pred = model.predict(X_te)\n\n    acc  = accuracy_score(y_te, y_pred)\n    prec = precision_score(y_te, y_pred, zero_division=0)\n    rec  = recall_score(y_te, y_pred, zero_division=0)\n    f1   = f1_score(y_te, y_pred, zero_division=0)\n    auc  = roc_auc_score(y_te, y_pred)\n\n    print(f\"\\n==== {label} ====\")\n    print(f\"Accuracy : {acc:.4f}\")\n    print(f\"Precision: {prec:.4f}\")\n    print(f\"Recall   : {rec:.4f}\")\n    print(f\"F1-score : {f1:.4f}\")\n    print(f\"AUC      : {auc:.4f}\")\n\n    return {\"acc\": acc, \"prec\": prec, \"rec\": rec, \"f1\": f1, \"auc\": auc}\n\n# ==============================================================\n# GENETIC ALGORITHM (Wrapper = CatBoost)\n# ==============================================================\n\nclass GAFeatureSelector:\n    def __init__(\n        self,\n        n_features,\n        pop_size=20,\n        n_generations=15,\n        crossover_rate=0.8,\n        mutation_rate=0.02,\n        random_state=42\n    ):\n        self.n_features = n_features\n        self.pop_size = pop_size\n        self.n_generations = n_generations\n        self.crossover_rate = crossover_rate\n        self.mutation_rate = mutation_rate\n        self.random_state = random_state\n        np.random.seed(random_state)\n        random.seed(random_state)\n\n    def _init_population(self):\n        pop = np.random.randint(0, 2, size=(self.pop_size, self.n_features))\n        for i in range(self.pop_size):\n            if pop[i].sum() == 0:\n                pop[i, np.random.randint(0, self.n_features)] = 1\n        return pop\n\n    def _fitness(self, mask, X, y):\n        idx = np.where(mask == 1)[0]\n        if len(idx) == 0:\n            return 0.0\n\n        X_sel = X[:, idx]\n\n        model = CatBoostClassifier(\n            iterations=200,\n            learning_rate=0.08,\n            depth=6,\n            verbose=False,\n            random_state=self.random_state\n        )\n\n        cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=self.random_state)\n        scores = []\n        for tr, va in cv.split(X_sel, y):\n            model.fit(X_sel[tr], y[tr])\n            pred = model.predict(X_sel[va])\n            scores.append(f1_score(y[va], pred))\n\n        return np.mean(scores)\n\n    def _tournament_selection(self, population, fitness, k=3):\n        selected = np.random.choice(len(population), k, replace=False)\n        best_idx = selected[np.argmax(fitness[selected])]\n        return population[best_idx].copy()\n\n    def _crossover(self, p1, p2):\n        if np.random.rand() > self.crossover_rate:\n            return p1.copy(), p2.copy()\n\n        point = np.random.randint(1, self.n_features - 1)\n        return (np.concatenate([p1[:point], p2[point:]]),\n                np.concatenate([p2[:point], p1[point:]]))\n\n    def _mutate(self, indiv):\n        for i in range(self.n_features):\n            if np.random.rand() < self.mutation_rate:\n                indiv[i] = 1 - indiv[i]\n        if indiv.sum() == 0:\n            indiv[np.random.randint(0, self.n_features)] = 1\n        return indiv\n\n    def fit(self, X, y):\n        pop = self._init_population()\n        fitness = np.array([self._fitness(ind, X, y) for ind in pop])\n\n        best_ind = pop[np.argmax(fitness)].copy()\n        best_fit = fitness.max()\n\n        print(\"\\n[GA] Initial best F1:\", best_fit)\n\n        for gen in range(self.n_generations):\n            new_pop = []\n            while len(new_pop) < self.pop_size:\n                p1 = self._tournament_selection(pop, fitness)\n                p2 = self._tournament_selection(pop, fitness)\n                c1, c2 = self._crossover(p1, p2)\n                new_pop.extend([self._mutate(c1), self._mutate(c2)])\n\n            pop = np.array(new_pop[:self.pop_size])\n            fitness = np.array([self._fitness(ind, X, y) for ind in pop])\n\n            if fitness.max() > best_fit:\n                best_fit = fitness.max()\n                best_ind = pop[np.argmax(fitness)].copy()\n\n            print(f\"[GA] Generation {gen+1}/{self.n_generations} - Best F1: {best_fit:.4f}\")\n\n        self.best_mask_ = best_ind\n        self.best_score_ = best_fit\n        return self\n\n    def transform(self, X):\n        return X[:, self.best_mask_ == 1]\n\n    def fit_transform(self, X, y):\n        self.fit(X, y)\n        return self.transform(X)\n\n# ==============================================================\n# PSO (Wrapper = CatBoost)\n# ==============================================================\n\nclass PSOFeatureSelector:\n    def __init__(\n        self,\n        n_features,\n        n_particles=20,\n        n_iterations=15,\n        inertia=0.7,\n        cognitive=1.5,\n        social=1.5,\n        random_state=42\n    ):\n        self.n_features = n_features\n        self.n_particles = n_particles\n        self.n_iterations = n_iterations\n        self.w = inertia\n        self.c1 = cognitive\n        self.c2 = social\n        self.random_state = random_state\n        np.random.seed(random_state)\n\n    def _init_swarm(self):\n        positions = np.random.randint(0, 2, (self.n_particles, self.n_features)).astype(float)\n        for i in range(self.n_particles):\n            if positions[i].sum() == 0:\n                positions[i, np.random.randint(0, self.n_features)] = 1\n        velocities = np.random.uniform(-1, 1, (self.n_particles, self.n_features))\n        return positions, velocities\n\n    def _fitness(self, pos, X, y):\n        mask = (pos >= 0.5).astype(int)\n        idx = np.where(mask == 1)[0]\n        if len(idx) == 0:\n            return 0.0\n\n        X_sel = X[:, idx]\n\n        model = CatBoostClassifier(\n            iterations=200,\n            learning_rate=0.08,\n            depth=6,\n            verbose=False,\n            random_state=self.random_state\n        )\n\n        cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=self.random_state)\n\n        scores = []\n        for tr, va in cv.split(X_sel, y):\n            model.fit(X_sel[tr], y[tr])\n            pred = model.predict(X_sel[va])\n            scores.append(f1_score(y[va], pred))\n\n        return np.mean(scores)\n\n    def fit(self, X, y):\n        positions, velocities = self._init_swarm()\n\n        pbest_pos = positions.copy()\n        pbest_scores = np.array([self._fitness(p, X, y) for p in positions])\n\n        gbest_pos = pbest_pos[np.argmax(pbest_scores)].copy()\n        gbest_score = pbest_scores.max()\n\n        print(\"\\n[PSO] Initial best F1:\", gbest_score)\n\n        for it in range(self.n_iterations):\n            for i in range(self.n_particles):\n                r1 = np.random.rand(self.n_features)\n                r2 = np.random.rand(self.n_features)\n\n                velocities[i] = (\n                    self.w * velocities[i]\n                    + self.c1 * r1 * (pbest_pos[i] - positions[i])\n                    + self.c2 * r2 * (gbest_pos - positions[i])\n                )\n\n                sigmoid = 1 / (1 + np.exp(-velocities[i]))\n                new_pos = (np.random.rand(self.n_features) < sigmoid).astype(float)\n\n                if new_pos.sum() == 0:\n                    new_pos[np.random.randint(0, self.n_features)] = 1\n\n                positions[i] = new_pos\n\n                score = self._fitness(new_pos, X, y)\n\n                if score > pbest_scores[i]:\n                    pbest_scores[i] = score\n                    pbest_pos[i] = new_pos.copy()\n\n            if pbest_scores.max() > gbest_score:\n                gbest_score = pbest_scores.max()\n                gbest_pos = pbest_pos[np.argmax(pbest_scores)].copy()\n\n            print(f\"[PSO] Iter {it+1}/{self.n_iterations} - Best F1: {gbest_score:.4f}\")\n\n        self.best_pos_ = gbest_pos\n        self.best_score_ = gbest_score\n        return self\n\n    def transform(self, X):\n        mask = (self.best_pos_ >= 0.5).astype(int)\n        return X[:, mask == 1]\n\n    def fit_transform(self, X, y):\n        self.fit(X, y)\n        return self.transform(X)\n\n# ==============================================================\n# RUN GA\n# ==============================================================\n\nga_selector = GAFeatureSelector(\n    n_features=X_train.shape[1],\n    pop_size=20,\n    n_generations=10,\n    crossover_rate=0.8,\n    mutation_rate=0.02,\n    random_state=RANDOM_STATE\n)\n\nX_train_ga = ga_selector.fit_transform(X_train, y_train)\nX_val_ga   = ga_selector.transform(X_val)\nX_test_ga  = ga_selector.transform(X_test)\n\nga_selected_features = feature_names[ga_selector.best_mask_ == 1]\n\nprint(\"\\n[GA] Best F1 (CV):\", ga_selector.best_score_)\nprint(\"[GA] Selected:\", len(ga_selected_features))\n\n# ==============================================================\n# RUN PSO\n# ==============================================================\n\npso_selector = PSOFeatureSelector(\n    n_features=X_train.shape[1],\n    n_particles=20,\n    n_iterations=10,\n    inertia=0.7,\n    cognitive=1.5,\n    social=1.5,\n    random_state=RANDOM_STATE\n)\n\nX_train_pso = pso_selector.fit_transform(X_train, y_train)\nX_val_pso   = pso_selector.transform(X_val)\nX_test_pso  = pso_selector.transform(X_test)\n\npso_mask = (pso_selector.best_pos_ >= 0.5).astype(int)\npso_selected_features = feature_names[pso_mask == 1]\n\nprint(\"\\n[PSO] Best F1 (CV):\", pso_selector.best_score_)\nprint(\"[PSO] Selected:\", len(pso_selected_features))\n\n# ==============================================================\n# DEFINE FINAL MODELS\n# ==============================================================\n\ndef get_models():\n    return {\n        \"XGBoost\": XGBClassifier(\n            n_estimators=300,\n            max_depth=6,\n            learning_rate=0.05,\n            subsample=0.8,\n            colsample_bytree=0.8,\n            eval_metric=\"logloss\",\n            n_jobs=-1,\n            random_state=RANDOM_STATE\n        ),\n        \"LightGBM\": LGBMClassifier(\n            n_estimators=300,\n            learning_rate=0.05,\n            max_depth=-1,\n            subsample=0.8,\n            colsample_bytree=0.8,\n            objective=\"binary\",\n            verbose=-1,\n            random_state=RANDOM_STATE\n        ),\n        \"CatBoost\": CatBoostClassifier(\n            iterations=300,\n            learning_rate=0.05,\n            depth=6,\n            verbose=False,\n            random_state=RANDOM_STATE\n        )\n    }\n\n# ==============================================================\n# TRAIN & EVALUATE\n# ==============================================================\n\nresults = {}\n\n# 1. All features\nmodels_all = get_models()\nfor name, model in models_all.items():\n    results[name+\"_all\"] = evaluate_model(\n        model, X_train_full, X_test, y_train_full, y_test,\n        f\"{name} (All)\"\n    )\n\n# 2. GA features\nmodels_ga = get_models()\nfor name, model in models_ga.items():\n    results[name+\"_ga\"] = evaluate_model(\n        model,\n        np.vstack([X_train_ga, X_val_ga]),\n        X_test_ga,\n        np.concatenate([y_train, y_val]),\n        y_test,\n        f\"{name} (GA)\"\n    )\n\n# 3. PSO features\nmodels_pso = get_models()\nfor name, model in models_pso.items():\n    results[name+\"_pso\"] = evaluate_model(\n        model,\n        np.vstack([X_train_pso, X_val_pso]),\n        X_test_pso,\n        np.concatenate([y_train, y_val]),\n        y_test,\n        f\"{name} (PSO)\"\n    )\n\nprint(\"\\n========= SUMMARY =========\")\nfor k, v in results.items():\n    print(k, \"=>\", v)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T06:01:07.205840Z","iopub.execute_input":"2025-11-28T06:01:07.206383Z"}},"outputs":[{"name":"stdout","text":"Total samples: 50000\nTotal features: 19\n\nTrain: (32000, 19) \nVal  : (8000, 19) \nTest : (10000, 19)\n\n[GA] Initial best F1: 0.9252879113377773\n[GA] Generation 1/10 - Best F1: 0.9322\n[GA] Generation 2/10 - Best F1: 0.9322\n[GA] Generation 3/10 - Best F1: 0.9326\n[GA] Generation 4/10 - Best F1: 0.9329\n[GA] Generation 5/10 - Best F1: 0.9329\n[GA] Generation 6/10 - Best F1: 0.9345\n[GA] Generation 7/10 - Best F1: 0.9352\n[GA] Generation 8/10 - Best F1: 0.9352\n[GA] Generation 9/10 - Best F1: 0.9356\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# ============================================================\n# HYBRID ENSEMBLE:\n# 1) PSO + XGBoost (features + model)\n# 2) GA + CatBoost (features + model)\n# 3) GWO + CatBoost (features + model)\n# -> Meta Neural Network on their probabilities\n# -> Save all models + feature masks + zip file\n# ============================================================\n\nimport numpy as np\nimport pandas as pd\nimport random\nimport time\nimport warnings\nimport pickle\nimport zipfile\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score, make_scorer\n)\nfrom sklearn.base import clone\n\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\n\nwarnings.filterwarnings(\"ignore\")\n\n# ---------------- GLOBAL SETTINGS ----------------\nRANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE)\nrandom.seed(RANDOM_STATE)\n\n# ---------------- LOAD DATA ----------------\n# Adjust this path if needed\nCSV_PATH = \"/kaggle/input/loan-final-normalized-csv/loan_final_normalized.csv\"\nTARGET_COL = \"loan_status\"\n\ndf = pd.read_csv(CSV_PATH)\nX = df.drop(columns=[TARGET_COL])\ny = df[TARGET_COL].astype(int).values\n\nfeature_names = np.array(X.columns)\nn_features = X.shape[1]\n\nprint(\"Data loaded.\")\nprint(\"Samples:\", X.shape[0], \"Features:\", X.shape[1])\n\n# ---------------- TRAIN / VAL / TEST SPLIT ----------------\nX_temp, X_test, y_temp, y_test = train_test_split(\n    X, y, test_size=0.2, stratify=y, random_state=RANDOM_STATE\n)\n\nX_train, X_val, y_train, y_val = train_test_split(\n    X_temp, y_temp, test_size=0.25, stratify=y_temp, random_state=RANDOM_STATE\n)\n# Now: train 60%, val 20%, test 20%\n\nprint(\"\\nTrain:\", X_train.shape, \"Val:\", X_val.shape, \"Test:\", X_test.shape)\n\n# ============================================================\n# FITNESS HELPERS\n# ============================================================\n\ndef fitness_xgb(mask, X_data, y_data, cv=3):\n    \"\"\"Fitness for PSO: XGBoost F1 CV.\"\"\"\n    idx = np.where(mask == 1)[0]\n    if len(idx) == 0:\n        return 0.0\n    X_sel = X_data[:, idx]\n\n    model = XGBClassifier(\n        n_estimators=200,\n        max_depth=5,\n        learning_rate=0.05,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        eval_metric=\"logloss\",\n        n_jobs=-1,\n        random_state=RANDOM_STATE\n    )\n\n    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=RANDOM_STATE)\n    scores = cross_val_score(\n        model, X_sel, y_data, cv=skf,\n        scoring=make_scorer(f1_score, zero_division=0),\n        n_jobs=-1\n    )\n    return scores.mean()\n\n\ndef fitness_cat(mask, X_data, y_data, cv=3):\n    \"\"\"Fitness for GA and GWO: CatBoost F1 CV.\"\"\"\n    idx = np.where(mask == 1)[0]\n    if len(idx) == 0:\n        return 0.0\n    X_sel = X_data[:, idx]\n\n    model = CatBoostClassifier(\n        iterations=200,\n        learning_rate=0.05,\n        depth=6,\n        verbose=False,\n        random_seed=RANDOM_STATE\n    )\n\n    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=RANDOM_STATE)\n    scores = cross_val_score(\n        model, X_sel, y_data, cv=skf,\n        scoring=make_scorer(f1_score, zero_division=0),\n        n_jobs=-1\n    )\n    return scores.mean()\n\n# Convert to numpy arrays for faster indexing\nX_train_np = X_train.values\nX_val_np   = X_val.values\nX_test_np  = X_test.values\n\n# ============================================================\n# 1) PSO + XGBoost PATH\n# ============================================================\n\ndef run_pso_feature_selection(\n    n_particles=15, n_iters=15, w=0.7, c1=1.5, c2=1.5\n):\n    print(\"\\n=== PSO Feature Selection (Wrapper: XGBoost) ===\")\n    dim = n_features\n\n    positions = np.random.randint(0, 2, (n_particles, dim)).astype(int)\n    velocities = np.random.uniform(-1, 1, (n_particles, dim))\n\n    personal_best = positions.copy()\n    personal_best_scores = np.array([\n        fitness_xgb(p, X_train_np, y_train) for p in positions\n    ])\n    global_best = personal_best[np.argmax(personal_best_scores)].copy()\n    global_best_score = personal_best_scores.max()\n\n    for it in range(n_iters):\n        print(f\" PSO iter {it+1}/{n_iters} - best F1: {global_best_score:.4f}\")\n        for i in range(n_particles):\n            r1 = np.random.rand(dim)\n            r2 = np.random.rand(dim)\n\n            velocities[i] = (\n                w * velocities[i]\n                + c1 * r1 * (personal_best[i] - positions[i])\n                + c2 * r2 * (global_best - positions[i])\n            )\n            sigmoid = 1 / (1 + np.exp(-velocities[i]))\n            positions[i] = (np.random.rand(dim) < sigmoid).astype(int)\n\n            score = fitness_xgb(positions[i], X_train_np, y_train)\n\n            if score > personal_best_scores[i]:\n                personal_best_scores[i] = score\n                personal_best[i] = positions[i].copy()\n\n        global_best = personal_best[np.argmax(personal_best_scores)].copy()\n        global_best_score = personal_best_scores.max()\n\n    print(\"PSO final best F1:\", global_best_score)\n    return global_best, global_best_score\n\n# ============================================================\n# 2) GA + CatBoost PATH\n# ============================================================\n\ndef run_ga_feature_selection(\n    pop_size=25, n_gens=15, crossover_rate=0.7, mutation_rate=0.1\n):\n    print(\"\\n=== GA Feature Selection (Wrapper: CatBoost) ===\")\n    dim = n_features\n\n    population = np.random.randint(0, 2, (pop_size, dim)).astype(int)\n    fitness_scores = np.array([\n        fitness_cat(ind, X_train_np, y_train) for ind in population\n    ])\n\n    def tournament_select(k=3):\n        idxs = np.random.randint(0, pop_size, k)\n        return idxs[np.argmax(fitness_scores[idxs])]\n\n    for gen in range(n_gens):\n        print(f\" GA gen {gen+1}/{n_gens} - best F1: {fitness_scores.max():.4f}\")\n        new_pop = []\n        # Elitism\n        elite_idxs = np.argsort(fitness_scores)[-2:]\n        new_pop.extend(population[elite_idxs].tolist())\n\n        while len(new_pop) < pop_size:\n            p1 = population[tournament_select()].copy()\n            p2 = population[tournament_select()].copy()\n\n            # Crossover\n            if np.random.rand() < crossover_rate:\n                point = np.random.randint(1, dim)\n                c1 = np.concatenate([p1[:point], p2[point:]])\n                c2 = np.concatenate([p2[:point], p1[point:]])\n            else:\n                c1, c2 = p1, p2\n\n            # Mutation\n            for child in (c1, c2):\n                for d in range(dim):\n                    if np.random.rand() < mutation_rate:\n                        child[d] = 1 - child[d]\n                new_pop.append(child)\n                if len(new_pop) >= pop_size:\n                    break\n\n        population = np.array(new_pop[:pop_size])\n        fitness_scores = np.array([\n            fitness_cat(ind, X_train_np, y_train) for ind in population\n        ])\n\n    best_ind = population[np.argmax(fitness_scores)].copy()\n    best_score = fitness_scores.max()\n    print(\"GA final best F1:\", best_score)\n    return best_ind, best_score\n\n# ============================================================\n# 3) GWO + CatBoost PATH\n# ============================================================\n\ndef run_gwo_feature_selection(\n    n_wolves=15, n_iters=15\n):\n    print(\"\\n=== GWO Feature Selection (Wrapper: CatBoost) ===\")\n    dim = n_features\n\n    population = np.random.randint(0, 2, (n_wolves, dim)).astype(int)\n    fitness_scores = np.array([\n        fitness_cat(ind, X_train_np, y_train) for ind in population\n    ])\n\n    Alpha = Beta = Delta = None\n    Alpha_score = Beta_score = Delta_score = -1.0\n\n    for it in range(n_iters):\n        print(f\" GWO iter {it+1}/{n_iters} - best Alpha F1: {Alpha_score:.4f}\")\n        # Update Alpha, Beta, Delta\n        for i, score in enumerate(fitness_scores):\n            if score > Alpha_score:\n                Delta_score, Beta_score, Alpha_score = Beta_score, Alpha_score, score\n                Delta, Beta, Alpha = Beta, Alpha, population[i].copy()\n            elif score > Beta_score:\n                Delta_score, Beta_score = Beta_score, score\n                Delta, Beta = Beta, population[i].copy()\n            elif score > Delta_score:\n                Delta_score = score\n                Delta = population[i].copy()\n\n        a = 2 - it * (2 / n_iters)\n\n        if Alpha is None:\n            continue\n\n        for i in range(n_wolves):\n            new_pos = population[i].copy()\n            for d in range(dim):\n                r1, r2 = np.random.rand(), np.random.rand()\n                A1 = 2 * a * r1 - a\n                C1 = 2 * r2\n                D_alpha = abs(C1 * Alpha[d] - population[i][d])\n                X1 = Alpha[d] - A1 * D_alpha\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                A2 = 2 * a * r1 - a\n                C2 = 2 * r2\n                D_beta = abs(C2 * Beta[d] - population[i][d])\n                X2 = Beta[d] - A2 * D_beta\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                A3 = 2 * a * r1 - a\n                C3 = 2 * r2\n                D_delta = abs(C3 * Delta[d] - population[i][d])\n                X3 = Delta[d] - A3 * D_delta\n\n                X_mean = (X1 + X2 + X3) / 3.0\n                s = 1 / (1 + np.exp(-X_mean))\n                new_pos[d] = 1 if np.random.rand() < s else 0\n\n            population[i] = new_pos\n\n        fitness_scores = np.array([\n            fitness_cat(ind, X_train_np, y_train) for ind in population\n        ])\n\n    best_ind = population[np.argmax(fitness_scores)].copy()\n    best_score = fitness_scores.max()\n    print(\"GWO final best F1:\", best_score)\n    return best_ind, best_score\n\n# ============================================================\n# RUN ALL THREE OPTIMIZERS AND TRAIN BASE MODELS\n# ============================================================\n\n# 1) PSO → best features → XGBoost\npso_mask, pso_score = run_pso_feature_selection()\npso_feat_idx = np.where(pso_mask == 1)[0]\npso_features = feature_names[pso_feat_idx]\nprint(\"\\n[PSO] Selected features:\", len(pso_features))\n\nmodel_pso_xgb = XGBClassifier(\n    n_estimators=300,\n    max_depth=5,\n    learning_rate=0.05,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    eval_metric=\"logloss\",\n    n_jobs=-1,\n    random_state=RANDOM_STATE\n)\n\nmodel_pso_xgb.fit(X_train_np[:, pso_feat_idx], y_train)\n\n# 2) GA → best features → CatBoost\nga_mask, ga_score = run_ga_feature_selection()\nga_feat_idx = np.where(ga_mask == 1)[0]\nga_features = feature_names[ga_feat_idx]\nprint(\"\\n[GA] Selected features:\", len(ga_features))\n\nmodel_ga_cat = CatBoostClassifier(\n    iterations=300,\n    learning_rate=0.05,\n    depth=6,\n    verbose=False,\n    random_seed=RANDOM_STATE\n)\nmodel_ga_cat.fit(X_train_np[:, ga_feat_idx], y_train)\n\n# 3) GWO → best features → CatBoost\ngwo_mask, gwo_score = run_gwo_feature_selection()\ngwo_feat_idx = np.where(gwo_mask == 1)[0]\ngwo_features = feature_names[gwo_feat_idx]\nprint(\"\\n[GWO] Selected features:\", len(gwo_features))\n\nmodel_gwo_cat = CatBoostClassifier(\n    iterations=300,\n    learning_rate=0.05,\n    depth=6,\n    verbose=False,\n    random_seed=RANDOM_STATE\n)\nmodel_gwo_cat.fit(X_train_np[:, gwo_feat_idx], y_train)\n\n# ============================================================\n# BUILD META DATASET FROM 3 MODELS (STACKING INPUT)\n# ============================================================\n\nprint(\"\\n=== Building meta-features for Neural Network (using validation set) ===\")\n\n# Base model probabilities on validation set\npso_val_proba = model_pso_xgb.predict_proba(X_val_np[:, pso_feat_idx])[:, 1]\nga_val_proba  = model_ga_cat.predict_proba(X_val_np[:, ga_feat_idx])[:, 1]\ngwo_val_proba = model_gwo_cat.predict_proba(X_val_np[:, gwo_feat_idx])[:, 1]\n\nX_meta_train = np.vstack([pso_val_proba, ga_val_proba, gwo_val_proba]).T  # shape (n_val, 3)\ny_meta_train = y_val\n\nprint(\"Meta-data shape:\", X_meta_train.shape)\n\n# ============================================================\n# SMALL NEURAL NETWORK META-MODEL\n# ============================================================\n\nprint(\"\\n=== Training Meta Neural Network (stacking) ===\")\n\nmeta_model = Sequential([\n    Dense(16, activation=\"relu\", input_shape=(3,)),\n    Dropout(0.2),\n    Dense(8, activation=\"relu\"),\n    Dense(1, activation=\"sigmoid\")\n])\n\nmeta_model.compile(\n    optimizer=Adam(learning_rate=0.01),\n    loss=\"binary_crossentropy\",\n    metrics=[\"accuracy\"]\n)\n\nhistory = meta_model.fit(\n    X_meta_train, y_meta_train,\n    epochs=30,\n    batch_size=32,\n    validation_split=0.2,\n    verbose=0\n)\n\nprint(\"Meta NN training done.\")\n\n# ============================================================\n# EVALUATE FINAL ENSEMBLE ON TEST SET\n# ============================================================\n\nprint(\"\\n=== Evaluating final ensemble on TEST set ===\")\n\npso_test_proba = model_pso_xgb.predict_proba(X_test_np[:, pso_feat_idx])[:, 1]\nga_test_proba  = model_ga_cat.predict_proba(X_test_np[:, ga_feat_idx])[:, 1]\ngwo_test_proba = model_gwo_cat.predict_proba(X_test_np[:, gwo_feat_idx])[:, 1]\n\nX_meta_test = np.vstack([pso_test_proba, ga_test_proba, gwo_test_proba]).T\n\nmeta_test_proba = meta_model.predict(X_meta_test).ravel()\nmeta_test_pred = (meta_test_proba >= 0.5).astype(int)\n\nacc  = accuracy_score(y_test, meta_test_pred)\nprec = precision_score(y_test, meta_test_pred, zero_division=0)\nrec  = recall_score(y_test, meta_test_pred, zero_division=0)\nf1   = f1_score(y_test, meta_test_pred, zero_division=0)\n\nprint(f\"Final Ensemble Accuracy : {acc:.4f}\")\nprint(f\"Final Ensemble Precision: {prec:.4f}\")\nprint(f\"Final Ensemble Recall   : {rec:.4f}\")\nprint(f\"Final Ensemble F1-score : {f1:.4f}\")\n\n# ============================================================\n# SAVE MODELS + FEATURE MASKS + ZIP\n# ============================================================\n\nprint(\"\\n=== Saving models and masks ===\")\n\n# Save XGBoost model\nmodel_pso_xgb.save_model(\"model_pso_xgb.json\")\n\n# Save CatBoost models\nmodel_ga_cat.save_model(\"model_ga_cat.cbm\")\nmodel_gwo_cat.save_model(\"model_gwo_cat.cbm\")\n\n# Save meta NN\nmeta_model.save(\"meta_meta_nn.h5\")\n\n# Save feature indices / masks\nfeature_info = {\n    \"pso_feat_idx\": pso_feat_idx,\n    \"ga_feat_idx\": ga_feat_idx,\n    \"gwo_feat_idx\": gwo_feat_idx,\n    \"pso_features\": pso_features.tolist(),\n    \"ga_features\": ga_features.tolist(),\n    \"gwo_features\": gwo_features.tolist()\n}\nwith open(\"feature_masks.pkl\", \"wb\") as f:\n    pickle.dump(feature_info, f)\n\n# Zip everything\nzip_filename = \"final_hybrid_ensemble_models.zip\"\nwith zipfile.ZipFile(zip_filename, \"w\") as zf:\n    zf.write(\"model_pso_xgb.json\")\n    zf.write(\"model_ga_cat.cbm\")\n    zf.write(\"model_gwo_cat.cbm\")\n    zf.write(\"meta_meta_nn.h5\")\n    zf.write(\"feature_masks.pkl\")\n\nprint(\"Saved models and zipped file:\", zip_filename)\nprint(\"Done.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T15:17:10.627399Z","iopub.execute_input":"2025-11-28T15:17:10.627699Z","iopub.status.idle":"2025-11-28T16:08:54.899063Z","shell.execute_reply.started":"2025-11-28T15:17:10.627645Z","shell.execute_reply":"2025-11-28T16:08:54.898414Z"}},"outputs":[{"name":"stderr","text":"2025-11-28 15:17:19.292777: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1764343039.767135      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1764343039.905356      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stdout","text":"Data loaded.\nSamples: 50000 Features: 19\n\nTrain: (30000, 19) Val: (10000, 19) Test: (10000, 19)\n\n=== PSO Feature Selection (Wrapper: XGBoost) ===\n PSO iter 1/15 - best F1: 0.9216\n PSO iter 2/15 - best F1: 0.9216\n PSO iter 3/15 - best F1: 0.9271\n PSO iter 4/15 - best F1: 0.9271\n PSO iter 5/15 - best F1: 0.9271\n PSO iter 6/15 - best F1: 0.9271\n PSO iter 7/15 - best F1: 0.9271\n PSO iter 8/15 - best F1: 0.9277\n PSO iter 9/15 - best F1: 0.9277\n PSO iter 10/15 - best F1: 0.9290\n PSO iter 11/15 - best F1: 0.9290\n PSO iter 12/15 - best F1: 0.9311\n PSO iter 13/15 - best F1: 0.9311\n PSO iter 14/15 - best F1: 0.9311\n PSO iter 15/15 - best F1: 0.9328\nPSO final best F1: 0.9328121953263177\n\n[PSO] Selected features: 17\n\n=== GA Feature Selection (Wrapper: CatBoost) ===\n GA gen 1/15 - best F1: 0.9006\n GA gen 2/15 - best F1: 0.9120\n GA gen 3/15 - best F1: 0.9231\n GA gen 4/15 - best F1: 0.9310\n GA gen 5/15 - best F1: 0.9310\n GA gen 6/15 - best F1: 0.9315\n GA gen 7/15 - best F1: 0.9324\n GA gen 8/15 - best F1: 0.9324\n GA gen 9/15 - best F1: 0.9328\n GA gen 10/15 - best F1: 0.9328\n GA gen 11/15 - best F1: 0.9334\n GA gen 12/15 - best F1: 0.9334\n GA gen 13/15 - best F1: 0.9334\n GA gen 14/15 - best F1: 0.9334\n GA gen 15/15 - best F1: 0.9334\nGA final best F1: 0.9333754024894927\n\n[GA] Selected features: 14\n\n=== GWO Feature Selection (Wrapper: CatBoost) ===\n GWO iter 1/15 - best Alpha F1: -1.0000\n GWO iter 2/15 - best Alpha F1: 0.8995\n GWO iter 3/15 - best Alpha F1: 0.9334\n GWO iter 4/15 - best Alpha F1: 0.9334\n GWO iter 5/15 - best Alpha F1: 0.9334\n GWO iter 6/15 - best Alpha F1: 0.9334\n GWO iter 7/15 - best Alpha F1: 0.9334\n GWO iter 8/15 - best Alpha F1: 0.9334\n GWO iter 9/15 - best Alpha F1: 0.9334\n GWO iter 10/15 - best Alpha F1: 0.9334\n GWO iter 11/15 - best Alpha F1: 0.9334\n GWO iter 12/15 - best Alpha F1: 0.9334\n GWO iter 13/15 - best Alpha F1: 0.9334\n GWO iter 14/15 - best Alpha F1: 0.9334\n GWO iter 15/15 - best Alpha F1: 0.9334\nGWO final best F1: 0.9304786545359681\n\n[GWO] Selected features: 16\n\n=== Building meta-features for Neural Network (using validation set) ===\nMeta-data shape: (10000, 3)\n\n=== Training Meta Neural Network (stacking) ===\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1764346109.250399      47 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1764346109.251083      47 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1764346112.349524   11679 service.cc:148] XLA service 0x7ebd08009320 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1764346112.350799   11679 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1764346112.350819   11679 service.cc:156]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\nI0000 00:00:1764346112.682782   11679 cuda_dnn.cc:529] Loaded cuDNN version 90300\nI0000 00:00:1764346113.921015   11679 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"Meta NN training done.\n\n=== Evaluating final ensemble on TEST set ===\n\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"Final Ensemble Accuracy : 0.9099\nFinal Ensemble Precision: 0.9725\nFinal Ensemble Recall   : 0.8607\nFinal Ensemble F1-score : 0.9132\n\n=== Saving models and masks ===\nSaved models and zipped file: final_hybrid_ensemble_models.zip\nDone.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ============================================================\n# HYBRID ENSEMBLE WITH PROPER STACKING (PSO + GA + GWO)\n# BASE MODELS: XGBoost, CatBoost, CatBoost\n# META MODEL: Logistic Regression (correct stacking)\n# ============================================================\n\nimport numpy as np\nimport pandas as pd\nimport warnings, time, pickle, zipfile\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.base import clone\n\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\n\n# ============================================================\n# LOAD DATA\n# ============================================================\ndf = pd.read_csv(\"/kaggle/input/loan-final-normalized-csv/loan_final_normalized.csv\")\n\nX = df.drop(\"loan_status\", axis=1).values\ny = df[\"loan_status\"].astype(int).values\nfeature_names = df.drop(\"loan_status\", axis=1).columns\n\nN_FEATURES = X.shape[1]\nRANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE)\n\nprint(\"Data loaded.\")\n\n\n# ============================================================\n# FITNESS FUNCTIONS\n# ============================================================\n\ndef eval_xgb(mask):\n    \"\"\"Evaluate mask using XGBoost.\"\"\"\n    idx = np.where(mask == 1)[0]\n    if len(idx) == 0:\n        return 0.0\n    X_sel = X[:, idx]\n    \n    model = XGBClassifier(\n        n_estimators=200, learning_rate=0.05, max_depth=6,\n        subsample=0.8, colsample_bytree=0.8, n_jobs=-1,\n        eval_metric=\"logloss\", random_state=RANDOM_STATE\n    )\n    \n    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)\n    scores = []\n    for tr, va in cv.split(X_sel, y):\n        model.fit(X_sel[tr], y[tr])\n        pred = model.predict(X_sel[va])\n        scores.append(f1_score(y[va], pred))\n    return np.mean(scores)\n\ndef eval_cat(mask):\n    \"\"\"Evaluate mask using CatBoost.\"\"\"\n    idx = np.where(mask == 1)[0]\n    if len(idx) == 0:\n        return 0.0\n    X_sel = X[:, idx]\n    \n    model = CatBoostClassifier(\n        iterations=200, depth=6, learning_rate=0.05,\n        verbose=False, random_seed=RANDOM_STATE\n    )\n    \n    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)\n    scores = []\n    for tr, va in cv.split(X_sel, y):\n        model.fit(X_sel[tr], y[tr])\n        pred = model.predict(X_sel[va])\n        scores.append(f1_score(y[va], pred))\n    return np.mean(scores)\n\n\n# ============================================================\n# PSO FEATURE SELECTION\n# ============================================================\ndef run_pso():\n    swarm = 20\n    iters = 20\n    dim = N_FEATURES\n    \n    pos = np.random.randint(0, 2, (swarm, dim))\n    vel = np.random.rand(swarm, dim)\n    \n    pbest = pos.copy()\n    pbest_scores = np.array([eval_xgb(m) for m in pos])\n    \n    gbest = pbest[np.argmax(pbest_scores)].copy()\n    gbest_score = np.max(pbest_scores)\n    \n    for t in range(iters):\n        print(f\"PSO Iter={t+1} BestF1={gbest_score:.4f}\")\n        for i in range(swarm):\n            r1, r2 = np.random.rand(dim), np.random.rand(dim)\n            vel[i] = 0.7*vel[i] + 1.5*r1*(pbest[i]-pos[i]) + 1.5*r2*(gbest-pos[i])\n            pos[i] = (1/(1+np.exp(-vel[i])) > 0.5).astype(int)\n            \n            sc = eval_xgb(pos[i])\n            if sc > pbest_scores[i]:\n                pbest_scores[i] = sc\n                pbest[i] = pos[i].copy()\n            \n            if sc > gbest_score:\n                gbest_score = sc\n                gbest = pos[i].copy()\n    \n    return gbest\n\n\n# ============================================================\n# GENETIC ALGORITHM FEATURE SELECTION\n# ============================================================\ndef run_ga():\n    pop = 30\n    gens = 20\n    dim = N_FEATURES\n    \n    population = np.random.randint(0, 2, (pop, dim))\n    \n    for g in range(gens):\n        scores = np.array([eval_cat(ind) for ind in population])\n        print(f\"GA Gen {g+1} BestF1={scores.max():.4f}\")\n        \n        parents = population[scores.argsort()[-10:]]\n        \n        children = []\n        for _ in range(pop // 2):\n            p1, p2 = parents[np.random.randint(10)], parents[np.random.randint(10)]\n            cut = np.random.randint(1, dim - 1)\n            child = np.concatenate([p1[:cut], p2[cut:]])\n            children.append(child)\n        \n        children = np.array(children)\n        \n        mutation = np.random.rand(*children.shape) < 0.1\n        children = np.abs(children - mutation.astype(int))\n        \n        population = np.vstack((parents, children))\n    \n    best = population[np.argmax([eval_cat(ind) for ind in population])]\n    return best\n\n\n# ============================================================\n# GWO FEATURE SELECTION\n# ============================================================\ndef run_gwo():\n    wolves = 20\n    iters = 20\n    dim = N_FEATURES\n    \n    pop = np.random.randint(0, 2, (wolves, dim))\n    scores = np.array([eval_cat(ind) for ind in pop])\n    \n    alpha, beta, delta = pop[scores.argsort()[-3:]]\n    \n    for t in range(iters):\n        print(f\"GWO Iter={t+1} BestF1={scores.max():.4f}\")\n        \n        a = 2 - t*(2/iters)\n        \n        for i in range(wolves):\n            for leader in [alpha, beta, delta]:\n                r1, r2 = np.random.rand(), np.random.rand()\n                A = 2*a*r1 - a\n                C = 2*r2\n                D = abs(C*leader - pop[i])\n                X = leader - A*D\n                pop[i] = (1/(1+np.exp(-X)) > 0.5).astype(int)\n        \n        scores = np.array([eval_cat(ind) for ind in pop])\n        alpha, beta, delta = pop[scores.argsort()[-3:]]\n    \n    return alpha\n\n\n# ============================================================\n# TRAIN BASE MODELS WITH FINAL SELECTED FEATURES\n# ============================================================\ndef train_base_model(model, mask, X_train, y_train):\n    idx = np.where(mask == 1)[0]\n    model.fit(X_train[:, idx], y_train)\n    return model\n\n\n# ============================================================\n# PROPER STACKING USING OUT-OF-FOLD PREDICTIONS\n# ============================================================\ndef generate_oof_predictions(mask, model, X, y):\n    idx = np.where(mask == 1)[0]\n    X_sel = X[:, idx]\n    \n    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    oof = np.zeros(len(X))\n    \n    for tr, va in skf.split(X_sel, y):\n        m = clone(model)\n        m.fit(X_sel[tr], y[tr])\n        oof[va] = m.predict_proba(X_sel[va])[:, 1]\n    \n    return oof\n\n\n# ============================================================\n# RUN OPTIMIZERS\n# ============================================================\nprint(\"\\nRunning PSO...\")\nmask_pso = run_pso()\n\nprint(\"\\nRunning GA...\")\nmask_ga = run_ga()\n\nprint(\"\\nRunning GWO...\")\nmask_gwo = run_gwo()\n\n\n# ============================================================\n# BASE MODELS\n# ============================================================\nxgb_model = XGBClassifier(\n    n_estimators=300, learning_rate=0.05, max_depth=6,\n    subsample=0.8, colsample_bytree=0.8, eval_metric=\"logloss\",\n    random_state=42, n_jobs=-1\n)\n\ncat_model1 = CatBoostClassifier(iterations=300, depth=6, learning_rate=0.05, verbose=False)\ncat_model2 = CatBoostClassifier(iterations=300, depth=6, learning_rate=0.05, verbose=False)\n\n\n# ============================================================\n# OOF STACKING\n# ============================================================\nprint(\"\\nGenerating OOF predictions...\")\n\noof_pso = generate_oof_predictions(mask_pso, xgb_model, X, y)\noof_ga = generate_oof_predictions(mask_ga, cat_model1, X, y)\noof_gwo = generate_oof_predictions(mask_gwo, cat_model2, X, y)\n\nstack_X = np.vstack([oof_pso, oof_ga, oof_gwo]).T\n\n\n# ============================================================\n# TRAIN META-MODEL (LOGISTIC REGRESSION)\n# ============================================================\nprint(\"\\nTraining Logistic Regression Meta Model...\")\n\nmeta_model = LogisticRegression()\nmeta_model.fit(stack_X, y)\n\nprint(\"Meta model training done.\")\n\n\n# ============================================================\n# FINAL TEST SPLIT AND EVALUATION\n# ============================================================\nX_train_final, X_test_final, y_train_final, y_test_final = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\ntest_pso = xgb_model.fit(X_train_final[:, mask_pso==1], y_train_final)\\\n                    .predict_proba(X_test_final[:, mask_pso==1])[:,1]\n\ntest_ga = cat_model1.fit(X_train_final[:, mask_ga==1], y_train_final)\\\n                    .predict_proba(X_test_final[:, mask_ga==1])[:,1]\n\ntest_gwo = cat_model2.fit(X_train_final[:, mask_gwo==1], y_train_final)\\\n                     .predict_proba(X_test_final[:, mask_gwo==1])[:,1]\n\ntest_stack = np.vstack([test_pso, test_ga, test_gwo]).T\n\nmeta_pred = meta_model.predict(test_stack)\nmeta_proba = meta_model.predict_proba(test_stack)[:,1]\n\nacc = accuracy_score(y_test_final, meta_pred)\nprec = precision_score(y_test_final, meta_pred)\nrec = recall_score(y_test_final, meta_pred)\nf1 = f1_score(y_test_final, meta_pred)\n\nprint(\"\\n===== FINAL STACKED ENSEMBLE RESULTS =====\")\nprint(f\"Accuracy  : {acc:.4f}\")\nprint(f\"Precision : {prec:.4f}\")\nprint(f\"Recall    : {rec:.4f}\")\nprint(f\"F1 Score  : {f1:.4f}\")\n\n\n# ============================================================\n# SAVE ALL MODELS AND MASKS\n# ============================================================\nprint(\"\\nSaving models...\")\n\nwith open(\"mask_pso.pkl\", \"wb\") as f: pickle.dump(mask_pso, f)\nwith open(\"mask_ga.pkl\", \"wb\") as f: pickle.dump(mask_ga, f)\nwith open(\"mask_gwo.pkl\", \"wb\") as f: pickle.dump(mask_gwo, f)\nwith open(\"meta_model.pkl\", \"wb\") as f: pickle.dump(meta_model, f)\n\n# Zip everything\nwith zipfile.ZipFile(\"final_stacked_ensemble.zip\", \"w\") as zf:\n    zf.write(\"mask_pso.pkl\")\n    zf.write(\"mask_ga.pkl\")\n    zf.write(\"mask_gwo.pkl\")\n    zf.write(\"meta_model.pkl\")\n\nprint(\"Saved final_stacked_ensemble.zip successfully.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:18:50.808925Z","iopub.execute_input":"2025-11-28T16:18:50.809283Z","iopub.status.idle":"2025-11-28T17:53:41.852637Z","shell.execute_reply.started":"2025-11-28T16:18:50.809260Z","shell.execute_reply":"2025-11-28T17:53:41.851897Z"}},"outputs":[{"name":"stdout","text":"Data loaded.\n\nRunning PSO...\nPSO Iter=1 BestF1=0.9244\nPSO Iter=2 BestF1=0.9342\nPSO Iter=3 BestF1=0.9342\nPSO Iter=4 BestF1=0.9342\nPSO Iter=5 BestF1=0.9342\nPSO Iter=6 BestF1=0.9342\nPSO Iter=7 BestF1=0.9342\nPSO Iter=8 BestF1=0.9342\nPSO Iter=9 BestF1=0.9342\nPSO Iter=10 BestF1=0.9342\nPSO Iter=11 BestF1=0.9342\nPSO Iter=12 BestF1=0.9342\nPSO Iter=13 BestF1=0.9342\nPSO Iter=14 BestF1=0.9342\nPSO Iter=15 BestF1=0.9342\nPSO Iter=16 BestF1=0.9342\nPSO Iter=17 BestF1=0.9342\nPSO Iter=18 BestF1=0.9342\nPSO Iter=19 BestF1=0.9342\nPSO Iter=20 BestF1=0.9342\n\nRunning GA...\nGA Gen 1 BestF1=0.9255\nGA Gen 2 BestF1=0.9255\nGA Gen 3 BestF1=0.9313\nGA Gen 4 BestF1=0.9313\nGA Gen 5 BestF1=0.9335\nGA Gen 6 BestF1=0.9335\nGA Gen 7 BestF1=0.9335\nGA Gen 8 BestF1=0.9340\nGA Gen 9 BestF1=0.9340\nGA Gen 10 BestF1=0.9340\nGA Gen 11 BestF1=0.9340\nGA Gen 12 BestF1=0.9340\nGA Gen 13 BestF1=0.9340\nGA Gen 14 BestF1=0.9340\nGA Gen 15 BestF1=0.9340\nGA Gen 16 BestF1=0.9340\nGA Gen 17 BestF1=0.9340\nGA Gen 18 BestF1=0.9340\nGA Gen 19 BestF1=0.9340\nGA Gen 20 BestF1=0.9340\n\nRunning GWO...\nGWO Iter=1 BestF1=0.9119\nGWO Iter=2 BestF1=0.9337\nGWO Iter=3 BestF1=0.9337\nGWO Iter=4 BestF1=0.9337\nGWO Iter=5 BestF1=0.9337\nGWO Iter=6 BestF1=0.9337\nGWO Iter=7 BestF1=0.9337\nGWO Iter=8 BestF1=0.9337\nGWO Iter=9 BestF1=0.9337\nGWO Iter=10 BestF1=0.9337\nGWO Iter=11 BestF1=0.9337\nGWO Iter=12 BestF1=0.9337\nGWO Iter=13 BestF1=0.9337\nGWO Iter=14 BestF1=0.9337\nGWO Iter=15 BestF1=0.9337\nGWO Iter=16 BestF1=0.9337\nGWO Iter=17 BestF1=0.9337\nGWO Iter=18 BestF1=0.9337\nGWO Iter=19 BestF1=0.9337\nGWO Iter=20 BestF1=0.9337\n\nGenerating OOF predictions...\n\nTraining Logistic Regression Meta Model...\nMeta model training done.\n\n===== FINAL STACKED ENSEMBLE RESULTS =====\nAccuracy  : 0.9284\nPrecision : 0.9280\nRecall    : 0.9431\nF1 Score  : 0.9355\n\nSaving models...\nSaved final_stacked_ensemble.zip successfully.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# ==========================================================\n#  FULL PIPELINE: FEATURE SELECTION + LOADED MODEL PREDICTION\n# ==========================================================\n\nimport pandas as pd\nimport numpy as np\nimport joblib\nimport logging\nimport warnings\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.preprocessing import MinMaxScaler\n\n# -----------------------------\n# Disable warnings\n# -----------------------------\nwarnings.filterwarnings(\"ignore\")\n\n# -----------------------------\n# Setup Logger (show all logs)\n# -----------------------------\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s — %(levelname)s — %(message)s\")\nlog = logging.getLogger()\n\nlog.info(\"Starting diabetes feature selection + loan model prediction pipeline...\")\n\n# ==========================================================\n# 1. LOAD DIABETES DATASET\n# ==========================================================\nlog.info(\"Loading Diabetes dataset...\")\ndf = pd.read_csv(\"/kaggle/input/balanced/balanced_20percent.csv\")\n\nX = df.drop(\"Diabetes_binary\", axis=1)\ny = df[\"Diabetes_binary\"]\n\nlog.info(f\"Dataset loaded. Shape: {df.shape}\")\nlog.info(f\"Target Value Counts:\\n{y.value_counts()}\")\n\n# ==========================================================\n# 2. FEATURE SELECTION\n# ==========================================================\nlog.info(\"Performing scaling + feature selection...\")\n\nscaler = MinMaxScaler()\nX_scaled = scaler.fit_transform(X)\n\nselector = SelectKBest(score_func=chi2, k=10)\nX_selected = selector.fit_transform(X_scaled, y)\n\nselected_mask = selector.get_support()\nselected_features = X.columns[selected_mask]\n\nlog.info(f\"Selected Features ({len(selected_features)}): {list(selected_features)}\")\n\n# Convert back to DataFrame\nX_selected_df = pd.DataFrame(X_selected, columns=selected_features)\n\n# ==========================================================\n# 3. LOAD SAVED MODEL (TRAINED ON LOAN DATASET)\n# ==========================================================\nmodel_path = \"/kaggle/input/saved-models/meta_model.pkl\"\nlog.info(f\"Loading trained model from: {model_path}\")\n\nmodel = joblib.load(model_path)\n\n# Model has NO feature_names_in_ — use coef_ shape\nn_features = model.coef_.shape[1]\nlog.info(f\"Model expects {n_features} input features.\")\n\n# ==========================================================\n# 4. ALIGN FEATURES BY PAD/TRIM ONLY\n# ==========================================================\nlog.info(\"Aligning diabetes selected features to required number of input features...\")\n\n# If selected feature count < expected → pad with zeros\nif X_selected_df.shape[1] < n_features:\n    missing = n_features - X_selected_df.shape[1]\n    for i in range(missing):\n        X_selected_df[f\"pad_{i}\"] = 0\n    log.info(f\"Padded with {missing} zero-features.\")\n\n# If selected feature count > expected → truncate\nif X_selected_df.shape[1] > n_features:\n    X_selected_df = X_selected_df.iloc[:, :n_features]\n    log.info(f\"Trimmed extra features.\")\n\n# Final aligned input\nX_final = X_selected_df\nlog.info(f\"Final aligned dataset shape: {X_final.shape}\")\n\n# ==========================================================\n# 5. PREDICTION\n# ==========================================================\nlog.info(\"Running predictions...\")\n\npred = model.predict(X_final)\nprob = model.predict_proba(X_final)[:, 1]\n\n# ==========================================================\n# 6. METRICS & REPORT\n# ==========================================================\nlog.info(\"Computing evaluation metrics...\")\n\nacc = accuracy_score(y, pred)\nprec = precision_score(y, pred, zero_division=0)\nrec = recall_score(y, pred, zero_division=0)\nf1 = f1_score(y, pred, zero_division=0)\ncm = confusion_matrix(y, pred)\ncls_report = classification_report(y, pred)\n\nprint(\"\\n========== MODEL EVALUATION ==========\")\nprint(f\"Accuracy       : {acc:.4f}\")\nprint(f\"Precision      : {prec:.4f}\")\nprint(f\"Recall         : {rec:.4f}\")\nprint(f\"F1-score       : {f1:.4f}\")\nprint(\"\\nConfusion Matrix:\")\nprint(cm)\nprint(\"\\nClassification Report:\")\nprint(cls_report)\n\nlog.info(\"Pipeline completed successfully.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T15:03:44.655374Z","iopub.execute_input":"2025-11-29T15:03:44.655965Z","iopub.status.idle":"2025-11-29T15:03:44.794479Z","shell.execute_reply.started":"2025-11-29T15:03:44.655940Z","shell.execute_reply":"2025-11-29T15:03:44.793925Z"}},"outputs":[{"name":"stderr","text":"2025-11-29 15:03:44,663 — INFO — Starting diabetes feature selection + loan model prediction pipeline...\n2025-11-29 15:03:44,665 — INFO — Loading Diabetes dataset...\n2025-11-29 15:03:44,694 — INFO — Dataset loaded. Shape: (14098, 22)\n2025-11-29 15:03:44,696 — INFO — Target Value Counts:\nDiabetes_binary\n0.0    7049\n1.0    7049\nName: count, dtype: int64\n2025-11-29 15:03:44,697 — INFO — Performing scaling + feature selection...\n2025-11-29 15:03:44,709 — INFO — Selected Features (10): ['HighBP', 'HighChol', 'Stroke', 'HeartDiseaseorAttack', 'PhysActivity', 'HvyAlcoholConsump', 'GenHlth', 'PhysHlth', 'DiffWalk', 'Income']\n2025-11-29 15:03:44,709 — INFO — Loading trained model from: /kaggle/input/saved-models/meta_model.pkl\n2025-11-29 15:03:44,712 — INFO — Model expects 3 input features.\n2025-11-29 15:03:44,713 — INFO — Aligning diabetes selected features to required number of input features...\n2025-11-29 15:03:44,714 — INFO — Trimmed extra features.\n2025-11-29 15:03:44,714 — INFO — Final aligned dataset shape: (14098, 3)\n2025-11-29 15:03:44,715 — INFO — Running predictions...\n2025-11-29 15:03:44,728 — INFO — Computing evaluation metrics...\n2025-11-29 15:03:44,791 — INFO — Pipeline completed successfully.\n","output_type":"stream"},{"name":"stdout","text":"\n========== MODEL EVALUATION ==========\nAccuracy       : 0.5322\nPrecision      : 0.7528\nRecall         : 0.0959\nF1-score       : 0.1701\n\nConfusion Matrix:\n[[6827  222]\n [6373  676]]\n\nClassification Report:\n              precision    recall  f1-score   support\n\n         0.0       0.52      0.97      0.67      7049\n         1.0       0.75      0.10      0.17      7049\n\n    accuracy                           0.53     14098\n   macro avg       0.63      0.53      0.42     14098\nweighted avg       0.63      0.53      0.42     14098\n\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# ==========================================================\n# FINAL PIPELINE: Drop 2 features → Apply GA/GWO/PSO masks\n# ==========================================================\n\nimport pandas as pd\nimport numpy as np\nimport joblib\nimport warnings\nimport logging\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n\nwarnings.filterwarnings(\"ignore\")\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s — %(levelname)s — %(message)s\")\nlog = logging.getLogger()\n\n\n# ==========================================================\n# 1. LOAD DIABETES DATASET\n# ==========================================================\ndf = pd.read_csv(\"/kaggle/input/balanced/balanced_20percent.csv\")\n\n# Drop 2 columns to match GA/PSO/GWO mask size = 19\ndrop_cols = [\"DiffWalk\", \"Sex\"]   # you can change if needed\ndf = df.drop(columns=drop_cols)\n\nlog.info(f\"After dropping {drop_cols}, shape = {df.shape}\")\n\nX = df.drop(\"Diabetes_binary\", axis=1)\ny = df[\"Diabetes_binary\"]\n\nfeature_names = list(X.columns)\nlog.info(f\"Final feature count (should be 19): {len(feature_names)}\")\n\n\n# ==========================================================\n# 2. LOAD GA/GWO/PSO MASK ARRAYS\n# ==========================================================\npaths = [\n    \"/kaggle/input/saved-models/mask_ga.pkl\",\n    \"/kaggle/input/saved-models/mask_gwo.pkl\",\n    \"/kaggle/input/saved-models/mask_pso.pkl\"\n]\n\nmasks = [joblib.load(p) for p in paths]\n\nfor i, mask in enumerate(masks):\n    log.info(f\"Mask {i+1}: length={len(mask)} values={mask}\")\n\n\n# ==========================================================\n# 3. APPLY MASKS TO GET SELECTED FEATURES\n# ==========================================================\nselected_lists = []\n\nfor i, mask in enumerate(masks):\n    selected = list(np.array(feature_names)[mask == 1])\n    selected_lists.append(selected)\n    log.info(f\"Model {i+1} selected: {selected}\")\n\n\n# ==========================================================\n# 4. FEATURE VOTING (Majority rule: appear in ≥ 2 models)\n# ==========================================================\nvotes = {}\nfor feats in selected_lists:\n    for f in feats:\n        votes[f] = votes.get(f, 0) + 1\n\nfinal_features = [f for f, c in votes.items() if c >= 2]\n\n# fallback: select top 10 most voted\nif len(final_features) < 3:\n    final_features = sorted(votes, key=votes.get, reverse=True)[:10]\n\nlog.info(f\"FINAL VOTED FEATURES: {final_features}\")\n\n\n# ==========================================================\n# 5. ALIGN FOR LOGISTIC REGRESSION MODEL (PAD/TRIM)\n# ==========================================================\nX_selected = X[final_features]\n\n# Load logistic regression model\nlr_model = joblib.load(\"/kaggle/input/saved-models/meta_model.pkl\")\n\nn_features = lr_model.coef_.shape[1]\nlog.info(f\"Logistic model expects {n_features} features.\")\n\n# pad / trim\nif X_selected.shape[1] < n_features:\n    missing = n_features - X_selected.shape[1]\n    for i in range(missing):\n        X_selected[f\"pad_{i}\"] = 0\n    log.info(f\"Padded {missing} missing columns.\")\n\nif X_selected.shape[1] > n_features:\n    X_selected = X_selected.iloc[:, :n_features]\n    log.info(\"Trimmed extra columns.\")\n\nX_final = X_selected\nlog.info(f\"Final input shape = {X_final.shape}\")\n\n\n# ==========================================================\n# 6. PREDICT\n# ==========================================================\npred = lr_model.predict(X_final)\nprob = lr_model.predict_proba(X_final)[:, 1]\n\n\n# ==========================================================\n# 7. METRICS\n# ==========================================================\nprint(\"\\n======== MODEL PERFORMANCE ========\")\nprint(\"Accuracy :\", round(accuracy_score(y, pred), 4))\nprint(\"Precision:\", round(precision_score(y, pred), 4))\nprint(\"Recall   :\", round(recall_score(y, pred), 4))\nprint(\"F1 Score :\", round(f1_score(y, pred), 4))\n\nprint(\"\\nConfusion Matrix:\\n\", confusion_matrix(y, pred))\nprint(\"\\nClassification Report:\\n\", classification_report(y, pred))\n\nlog.info(\"Pipeline completed successfully.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T15:25:54.422338Z","iopub.execute_input":"2025-11-29T15:25:54.422920Z","iopub.status.idle":"2025-11-29T15:25:54.524300Z","shell.execute_reply.started":"2025-11-29T15:25:54.422895Z","shell.execute_reply":"2025-11-29T15:25:54.523680Z"}},"outputs":[{"name":"stderr","text":"2025-11-29 15:25:54,466 — INFO — After dropping ['DiffWalk', 'Sex'], shape = (14098, 20)\n2025-11-29 15:25:54,469 — INFO — Final feature count (should be 19): 19\n2025-11-29 15:25:54,475 — INFO — Mask 1: length=19 values=[1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1]\n2025-11-29 15:25:54,476 — INFO — Mask 2: length=19 values=[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n2025-11-29 15:25:54,476 — INFO — Mask 3: length=19 values=[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n2025-11-29 15:25:54,477 — INFO — Model 1 selected: ['HighBP', 'HighChol', 'CholCheck', 'BMI', 'Stroke', 'HeartDiseaseorAttack', 'Fruits', 'Veggies', 'HvyAlcoholConsump', 'AnyHealthcare', 'NoDocbcCost', 'GenHlth', 'MentHlth', 'PhysHlth', 'Age', 'Education', 'Income']\n2025-11-29 15:25:54,478 — INFO — Model 2 selected: ['HighBP', 'HighChol', 'CholCheck', 'BMI', 'Smoker', 'Stroke', 'HeartDiseaseorAttack', 'PhysActivity', 'Fruits', 'Veggies', 'HvyAlcoholConsump', 'AnyHealthcare', 'NoDocbcCost', 'GenHlth', 'MentHlth', 'PhysHlth', 'Age', 'Education', 'Income']\n2025-11-29 15:25:54,478 — INFO — Model 3 selected: ['HighBP', 'HighChol', 'CholCheck', 'BMI', 'Smoker', 'Stroke', 'HeartDiseaseorAttack', 'PhysActivity', 'Fruits', 'Veggies', 'HvyAlcoholConsump', 'AnyHealthcare', 'NoDocbcCost', 'GenHlth', 'MentHlth', 'PhysHlth', 'Age', 'Education', 'Income']\n2025-11-29 15:25:54,479 — INFO — FINAL VOTED FEATURES: ['HighBP', 'HighChol', 'CholCheck', 'BMI', 'Stroke', 'HeartDiseaseorAttack', 'Fruits', 'Veggies', 'HvyAlcoholConsump', 'AnyHealthcare', 'NoDocbcCost', 'GenHlth', 'MentHlth', 'PhysHlth', 'Age', 'Education', 'Income', 'Smoker', 'PhysActivity']\n2025-11-29 15:25:54,484 — INFO — Logistic model expects 3 features.\n2025-11-29 15:25:54,485 — INFO — Trimmed extra columns.\n2025-11-29 15:25:54,486 — INFO — Final input shape = (14098, 3)\n2025-11-29 15:25:54,521 — INFO — Pipeline completed successfully.\n","output_type":"stream"},{"name":"stdout","text":"\n======== MODEL PERFORMANCE ========\nAccuracy : 0.5174\nPrecision: 0.5089\nRecall   : 0.994\nF1 Score : 0.6732\n\nConfusion Matrix:\n [[ 287 6762]\n [  42 7007]]\n\nClassification Report:\n               precision    recall  f1-score   support\n\n         0.0       0.87      0.04      0.08      7049\n         1.0       0.51      0.99      0.67      7049\n\n    accuracy                           0.52     14098\n   macro avg       0.69      0.52      0.38     14098\nweighted avg       0.69      0.52      0.38     14098\n\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import joblib\n\npaths = [\n    \"/kaggle/input/saved-models/mask_ga.pkl\",\n    \"/kaggle/input/saved-models/mask_gwo.pkl\",\n    \"/kaggle/input/saved-models/mask_pso.pkl\"\n]\n\nfor p in paths:\n    print(\"\\n==============================\")\n    print(\"FILE:\", p)\n    print(\"==============================\")\n    obj = joblib.load(p)\n    print(\"TYPE:\", type(obj))\n\n    # If dict, print keys\n    if isinstance(obj, dict):\n        print(\"DICT KEYS:\", obj.keys())\n\n    # If list/array\n    if isinstance(obj, (list, tuple)):\n        print(\"LIST LENGTH:\", len(obj))\n        print(\"FIRST 5:\", obj[:5])\n\n    # If numpy array\n    try:\n        import numpy as np\n        if isinstance(obj, np.ndarray):\n            print(\"ARRAY SHAPE:\", obj.shape)\n            print(\"FIRST ROW:\", obj[:5])\n    except:\n        pass\n\n    print(obj)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T16:34:27.263375Z","iopub.execute_input":"2025-11-29T16:34:27.264105Z","iopub.status.idle":"2025-11-29T16:34:27.326786Z","shell.execute_reply.started":"2025-11-29T16:34:27.264077Z","shell.execute_reply":"2025-11-29T16:34:27.326226Z"}},"outputs":[{"name":"stdout","text":"\n==============================\nFILE: /kaggle/input/saved-models/mask_ga.pkl\n==============================\nTYPE: <class 'numpy.ndarray'>\nARRAY SHAPE: (19,)\nFIRST ROW: [1 1 1 1 0]\n[1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1]\n\n==============================\nFILE: /kaggle/input/saved-models/mask_gwo.pkl\n==============================\nTYPE: <class 'numpy.ndarray'>\nARRAY SHAPE: (19,)\nFIRST ROW: [1 1 1 1 1]\n[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n\n==============================\nFILE: /kaggle/input/saved-models/mask_pso.pkl\n==============================\nTYPE: <class 'numpy.ndarray'>\nARRAY SHAPE: (19,)\nFIRST ROW: [1 1 1 1 1]\n[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ==========================================================\n# FINAL PIPELINE: Drop 2 features → Apply GA/GWO/PSO masks\n# ==========================================================\n\nimport pandas as pd\nimport numpy as np\nimport joblib\nimport warnings\nimport logging\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n\nwarnings.filterwarnings(\"ignore\")\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s — %(levelname)s — %(message)s\")\nlog = logging.getLogger()\n\n\n# ==========================================================\n# 1. LOAD DIABETES DATASET\n# ==========================================================\ndf = pd.read_csv(\"/kaggle/input/balanced/balanced_20percent.csv\")\n\n# Drop 2 columns to match GA/PSO/GWO mask size = 19\ndrop_cols = [\"DiffWalk\", \"Sex\"]   # you can change if needed\ndf = df.drop(columns=drop_cols)\n\nlog.info(f\"After dropping {drop_cols}, shape = {df.shape}\")\n\nX = df.drop(\"Diabetes_binary\", axis=1)\ny = df[\"Diabetes_binary\"]\n\nfeature_names = list(X.columns)\nlog.info(f\"Final feature count (should be 19): {len(feature_names)}\")\n\n\n# ==========================================================\n# 2. LOAD GA/GWO/PSO MASK ARRAYS\n# ==========================================================\npaths = [\n    \"/kaggle/input/saved-models/mask_ga.pkl\",\n    \"/kaggle/input/saved-models/mask_gwo.pkl\",\n    \"/kaggle/input/saved-models/mask_pso.pkl\"\n]\n\nmasks = [joblib.load(p) for p in paths]\n\nfor i, mask in enumerate(masks):\n    log.info(f\"Mask {i+1}: length={len(mask)} values={mask}\")\n\n\n# ==========================================================\n# 3. APPLY MASKS TO GET SELECTED FEATURES\n# ==========================================================\nselected_lists = []\n\nfor i, mask in enumerate(masks):\n    selected = list(np.array(feature_names)[mask == 1])\n    selected_lists.append(selected)\n    log.info(f\"Model {i+1} selected: {selected}\")\n\n\n# ==========================================================\n# 4. FEATURE VOTING (Majority rule: appear in ≥ 2 models)\n# ==========================================================\nvotes = {}\nfor feats in selected_lists:\n    for f in feats:\n        votes[f] = votes.get(f, 0) + 1\n\nfinal_features = [f for f, c in votes.items() if c >= 2]\n\n# fallback: select top 10 most voted\nif len(final_features) < 3:\n    final_features = sorted(votes, key=votes.get, reverse=True)[:10]\n\nlog.info(f\"FINAL VOTED FEATURES: {final_features}\")\n\n\n# ==========================================================\n# 5. ALIGN FOR LOGISTIC REGRESSION MODEL (PAD/TRIM)\n# ==========================================================\nX_selected = X[final_features]\n\n# Load logistic regression model\nlr_model = joblib.load(\"/kaggle/input/saved-models/meta_model.pkl\")\n\nn_features = lr_model.coef_.shape[1]\nlog.info(f\"Logistic model expects {n_features} features.\")\n\n# pad / trim\nif X_selected.shape[1] < n_features:\n    missing = n_features - X_selected.shape[1]\n    for i in range(missing):\n        X_selected[f\"pad_{i}\"] = 0\n    log.info(f\"Padded {missing} missing columns.\")\n\nif X_selected.shape[1] > n_features:\n    X_selected = X_selected.iloc[:, :n_features]\n    log.info(\"Trimmed extra columns.\")\n\nX_final = X_selected\nlog.info(f\"Final input shape = {X_final.shape}\")\n\n\n# ==========================================================\n# 6. PREDICT\n# ==========================================================\npred = lr_model.predict(X_final)\nprob = lr_model.predict_proba(X_final)[:, 1]\n\n\n# ==========================================================\n# 7. METRICS\n# ==========================================================\nprint(\"\\n======== MODEL PERFORMANCE ========\")\nprint(\"Accuracy :\", round(accuracy_score(y, pred), 4))\nprint(\"Precision:\", round(precision_score(y, pred), 4))\nprint(\"Recall   :\", round(recall_score(y, pred), 4))\nprint(\"F1 Score :\", round(f1_score(y, pred), 4))\n\nprint(\"\\nConfusion Matrix:\\n\", confusion_matrix(y, pred))\nprint(\"\\nClassification Report:\\n\", classification_report(y, pred))\n\nlog.info(\"Pipeline completed successfully.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T18:20:30.806385Z","iopub.execute_input":"2025-11-29T18:20:30.806690Z","iopub.status.idle":"2025-11-29T18:20:30.901690Z","shell.execute_reply.started":"2025-11-29T18:20:30.806668Z","shell.execute_reply":"2025-11-29T18:20:30.900847Z"}},"outputs":[{"name":"stderr","text":"2025-11-29 18:20:30,850 — INFO — Diabetes dataset loaded with 19 features.\n2025-11-29 18:20:30,856 — INFO — Loaded GA mask length  = 19\n2025-11-29 18:20:30,856 — INFO — Loaded GWO mask length = 19\n2025-11-29 18:20:30,857 — INFO — Loaded PSO mask length = 19\n2025-11-29 18:20:30,862 — INFO — Loaded model for union: <class 'catboost.core.CatBoostClassifier'>\n2025-11-29 18:20:30,866 — INFO — Loaded model for intersection: <class 'catboost.core.CatBoostClassifier'>\n2025-11-29 18:20:30,870 — INFO — Loaded model for voting: <class 'catboost.core.CatBoostClassifier'>\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mCatBoostError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_46/4091317430.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0mX_sel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mselected_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_sel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     results[name] = {\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, prediction_type, ntree_start, ntree_end, thread_count, verbose, task_type)\u001b[0m\n\u001b[1;32m   5305\u001b[0m                   \u001b[0;32mwith\u001b[0m \u001b[0mlog\u001b[0m \u001b[0mprobability\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mevery\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5306\u001b[0m         \"\"\"\n\u001b[0;32m-> 5307\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntree_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntree_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthread_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'predict'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntree_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntree_end\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthread_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"CPU\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36m_predict\u001b[0;34m(self, data, prediction_type, ntree_start, ntree_end, thread_count, verbose, parent_method_name, task_type)\u001b[0m\n\u001b[1;32m   2621\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_prediction_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2623\u001b[0;31m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_base_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntree_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntree_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthread_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2624\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdata_is_single_object\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36m_base_predict\u001b[0;34m(self, pool, prediction_type, ntree_start, ntree_end, thread_count, verbose, task_type)\u001b[0m\n\u001b[1;32m   1840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1841\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_base_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntree_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntree_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthread_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1842\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_base_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntree_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntree_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthread_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1844\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_base_virtual_ensembles_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntree_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvirtual_ensembles_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthread_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._CatBoost._base_predict\u001b[0;34m()\u001b[0m\n","\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._CatBoost._base_predict\u001b[0;34m()\u001b[0m\n","\u001b[0;31mCatBoostError\u001b[0m: catboost/libs/data/model_dataset_compatibility.cpp:81: At position 0 should be feature with name years_employed (found HighBP)."],"ename":"CatBoostError","evalue":"catboost/libs/data/model_dataset_compatibility.cpp:81: At position 0 should be feature with name years_employed (found HighBP).","output_type":"error"}],"execution_count":6},{"cell_type":"code","source":"import pandas as pd\n\ndf=pd.read_csv(\"/kaggle/input/new-ids/UNSW_NB15_testing-set.csv\")\nprint(df.columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T12:39:31.322902Z","iopub.execute_input":"2025-12-02T12:39:31.323258Z","iopub.status.idle":"2025-12-02T12:39:33.414619Z","shell.execute_reply.started":"2025-12-02T12:39:31.323233Z","shell.execute_reply":"2025-12-02T12:39:33.413959Z"}},"outputs":[{"name":"stdout","text":"Index(['id', 'dur', 'proto', 'service', 'state', 'spkts', 'dpkts', 'sbytes',\n       'dbytes', 'rate', 'sttl', 'dttl', 'sload', 'dload', 'sloss', 'dloss',\n       'sinpkt', 'dinpkt', 'sjit', 'djit', 'swin', 'stcpb', 'dtcpb', 'dwin',\n       'tcprtt', 'synack', 'ackdat', 'smean', 'dmean', 'trans_depth',\n       'response_body_len', 'ct_srv_src', 'ct_state_ttl', 'ct_dst_ltm',\n       'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm',\n       'is_ftp_login', 'ct_ftp_cmd', 'ct_flw_http_mthd', 'ct_src_ltm',\n       'ct_srv_dst', 'is_sm_ips_ports', 'attack_cat', 'label'],\n      dtype='object')\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ==========================================================\n# FINAL PIPELINE: Apply GA/GWO/PSO masks on DDOS → Test with saved LR model\n# ==========================================================\n\nimport pandas as pd\nimport numpy as np\nimport joblib\nimport warnings\nimport logging\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n\nwarnings.filterwarnings(\"ignore\")\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s — %(levelname)s — %(message)s\")\nlog = logging.getLogger()\n\n\n# ==========================================================\n# 1. LOAD DDOS DATASET  (YOU SELECT EXACTLY 19 COLUMNS)\n# ==========================================================\ndf = pd.read_csv(\"/kaggle/input/new-ids/UNSW_NB15_testing-set.csv\")\n\n# 👉 YOU SELECT EXACTLY 19 FEATURES FROM DDOS\nddos_features_19 = [\n    \"spkts\",\"dpkts\",\"sbytes\",\"dbytes\",\"rate\",\n    \"sttl\",\"dttl\",\"sload\",\"dload\",\"sloss\",\n    \"dloss\",\"synack\",\"ackdat\",\"smean\",\"dmean\",\n    \"sinpkt\",\"dinpkt\",\"sjit\",\"djit\"\n]\n\ndf = df[ddos_features_19].copy()\nlog.info(f\"DDOS dataset loaded with 19 selected numeric features: {df.shape}\")\n\n\n# ==========================================================\n# 2. LOAD SAVED MASKS (GA / GWO / PSO)\n# ==========================================================\npaths = [\n    \"/kaggle/input/saved-models/mask_ga.pkl\",\n    \"/kaggle/input/saved-models/mask_gwo.pkl\",\n    \"/kaggle/input/saved-models/mask_pso.pkl\"\n]\n\nmasks = [joblib.load(p) for p in paths]\n\nfor i, mask in enumerate(masks):\n    log.info(f\"Mask {i+1}: length={len(mask)} values={mask}\")\n\n\n# ==========================================================\n# 3. RENAME DDOS COLUMNS → MATCH EXPECTED FEATURE NAMES FROM MODEL\n# ==========================================================\nlr_model = joblib.load(\"/kaggle/input/saved-models/meta_model.pkl\")\nexpected_names = list(lr_model.feature_names_in_)\n\nif len(expected_names) != 19:\n    raise ValueError(\"Your logistic regression model does NOT expect 19 features!\")\n\ndf.columns = expected_names\nlog.info(\"DDOS columns renamed to match model feature names.\")\n\n\n# ==========================================================\n# 4. APPLY MASKS + VOTING (just like diabetes)\n# ==========================================================\nfeature_names = list(df.columns)\n\nselected_lists = []\n\nfor i, mask in enumerate(masks):\n    selected = list(np.array(feature_names)[mask == 1])\n    selected_lists.append(selected)\n    log.info(f\"Model {i+1} selected: {selected}\")\n\n# voting\nvotes = {}\nfor feats in selected_lists:\n    for f in feats:\n        votes[f] = votes.get(f, 0) + 1\n\nfinal_features = [f for f, c in votes.items() if c >= 2]\n\nif len(final_features) < 3:\n    final_features = sorted(votes, key=votes.get, reverse=True)[:10]\n\nlog.info(f\"FINAL VOTED FEATURES: {final_features}\")\n\n\n# ==========================================================\n# 5. ALIGN DDOS SELECTED FEATURES FOR LOGISTIC MODEL\n# ==========================================================\nX_selected = df[final_features]\n\nn_features = lr_model.coef_.shape[1]\nlog.info(f\"Logistic model expects {n_features} features.\")\n\n# pad if needed\nif X_selected.shape[1] < n_features:\n    missing = n_features - X_selected.shape[1]\n    for i in range(missing):\n        X_selected[f\"pad_{i}\"] = 0\n    log.info(f\"Padded {missing} missing columns.\")\n\n# trim if needed\nif X_selected.shape[1] > n_features:\n    X_selected = X_selected.iloc[:, :n_features]\n    log.info(\"Trimmed extra columns.\")\n\nX_final = X_selected.copy()\nlog.info(f\"Final DDOS input shape for model: {X_final.shape}\")\n\n\n# ==========================================================\n# 6. PREDICT USING TRAINED MODEL\n# ==========================================================\npred = lr_model.predict(X_final)\nprob = lr_model.predict_proba(X_final)[:, 1]\n\n\n# ==========================================================\n# 7. IF YOU HAVE DDOS LABELS, LOAD THEM\n# ==========================================================\ntry:\n    y = pd.read_csv(\"/kaggle/input/ddos/ddos_labels.csv\")[\"label\"]\nexcept:\n    y = None\n\n\n# ==========================================================\n# 8. METRICS (ONLY IF LABEL AVAILABLE)\n# ==========================================================\nif y is not None:\n    print(\"\\n======== MODEL PERFORMANCE (DDOS) ========\")\n    print(\"Accuracy :\", round(accuracy_score(y, pred), 4))\n    print(\"Precision:\", round(precision_score(y, pred), 4))\n    print(\"Recall   :\", round(recall_score(y, pred), 4))\n    print(\"F1 Score :\", round(f1_score(y, pred), 4))\n\n    print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y, pred))\n    print(\"\\nClassification Report:\\n\", classification_report(y, pred))\nelse:\n    print(\"\\nNo DDOS labels → showing first 20 predictions:\")\n    print(pred[:20])\n\nlog.info(\"DDOS pipeline completed successfully.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T12:41:15.046480Z","iopub.execute_input":"2025-12-02T12:41:15.047127Z","iopub.status.idle":"2025-12-02T12:41:16.013698Z","shell.execute_reply.started":"2025-12-02T12:41:15.047098Z","shell.execute_reply":"2025-12-02T12:41:16.012780Z"}},"outputs":[{"name":"stderr","text":"2025-12-02 12:41:15,903 — INFO — DDOS dataset loaded with 19 selected numeric features: (82332, 19)\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/2027535947.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m ]\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mmasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_47/2027535947.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     41\u001b[0m ]\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mmasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode, ensure_native_byte_order)\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_native_byte_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_native_byte_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    736\u001b[0m             with _validate_fileobject_and_memmap(f, filename, mmap_mode) as (\n\u001b[1;32m    737\u001b[0m                 \u001b[0mfobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/saved-models/mask_ga.pkl'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/input/saved-models/mask_ga.pkl'","output_type":"error"}],"execution_count":2},{"cell_type":"code","source":"# ==========================================================\n# AUTO FEATURE SELECTION ON FULL DDOS DATASET (47 COLS)\n# Using GA/GWO/PSO masks → voting → align → test with LOAN LR model\n# ==========================================================\n\nimport pandas as pd\nimport numpy as np\nimport joblib\nimport warnings\nimport logging\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n\nwarnings.filterwarnings(\"ignore\")\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s — %(levelname)s — %(message)s\")\nlog = logging.getLogger()\n\n# ==========================================================\n# 1. LOAD FULL DDOS DATASET (47+ columns)\n# ==========================================================\nddos_csv_path = \"/kaggle/working/ddos_final_preprocessed.csv\"    # <-- change here\ndf = pd.read_csv(ddos_csv_path)\n\nTARGET_COL = \"label\"\nif TARGET_COL not in df.columns:\n    raise ValueError(f\"{TARGET_COL} not found in dataset.\")\n\nX_full = df.drop(TARGET_COL, axis=1)\ny = df[TARGET_COL].astype(int)\n\nfeature_names_full = list(X_full.columns)\nlog.info(f\"DDOS dataset loaded: {X_full.shape[1]} features present.\")\n\n\n# ==========================================================\n# 2. LOAD 19-length GA/GWO/PSO MASK ARRAYS\n# ==========================================================\npaths = [\n    \"/kaggle/input/saved-model/mask_ga.pkl\",\n    \"/kaggle/input/saved-model/mask_gwo.pkl\",\n    \"/kaggle/input/saved-model/mask_pso.pkl\"\n]\n\nmasks = [joblib.load(p) for p in paths]\n\nfor i, mask in enumerate(masks):\n    log.info(f\"Mask {i+1}: length={len(mask)} values={mask}\")\n\n# These masks correspond to LOAN dataset feature ordering.\n# We must select the FIRST 19 columns from DDOS for mapping.\n# You can reorder differently if needed.\n\n\n# ==========================================================\n# 3. PICK FIRST 19 NUMERIC DDOS FEATURES AUTOMATICALLY\n# ==========================================================\n# FILTER NUMERIC ONLY\nnumeric_cols = X_full.select_dtypes(include=['number']).columns.tolist()\n\nif len(numeric_cols) < 19:\n    raise ValueError(\"Your DDOS dataset does not have 19 numeric columns!\")\n\n# Take first 19 numeric columns\nddos_19_features = numeric_cols[:19]\n\nlog.info(f\"Auto-selected DDOS 19 numeric columns: {ddos_19_features}\")\n\nX_19 = X_full[ddos_19_features]\nfeature_names = ddos_19_features   # these 19 will be mapped to masks\n\n\n# ==========================================================\n# 4. APPLY MASKS TO THESE 19 DDOS FEATURES\n# ==========================================================\nselected_lists = []\n\nfor i, mask in enumerate(masks):\n    selected = list(np.array(feature_names)[mask == 1])\n    selected_lists.append(selected)\n    log.info(f\"Model {i+1} selected: {selected}\")\n\n\n# ==========================================================\n# 5. FEATURE VOTING (majority: ≥2 masks)\n# ==========================================================\nvotes = {}\nfor feats in selected_lists:\n    for f in feats:\n        votes[f] = votes.get(f, 0) + 1\n\nfinal_features = [f for f, c in votes.items() if c >= 2]\n\n# fallback\nif len(final_features) < 3:\n    final_features = sorted(votes, key=votes.get, reverse=True)[:10]\n\nlog.info(f\"FINAL VOTED FEATURES: {final_features}\")\n\n\n# ==========================================================\n# 6. LOAD LOGISTIC REGRESSION MODEL (TRAINED ON LOAN)\n# ==========================================================\nlr_model = joblib.load(\"/kaggle/input/saved-model/meta_model.pkl\")\n\nn_features_expected = lr_model.coef_.shape[1]\nlog.info(f\"LR model expects {n_features_expected} features.\")\n\n\n# ==========================================================\n# 7. ALIGN DDOS FEATURES TO MODEL EXPECTED SIZE\n# ==========================================================\nX_sel = X_19[final_features]\n\n# pad\nif X_sel.shape[1] < n_features_expected:\n    missing = n_features_expected - X_sel.shape[1]\n    for i in range(missing):\n        X_sel[f\"pad_{i}\"] = 0\n    log.info(f\"Padded {missing} missing columns.\")\n\n# trim\nif X_sel.shape[1] > n_features_expected:\n    X_sel = X_sel.iloc[:, :n_features_expected]\n    log.info(\"Trimmed extra columns.\")\n\nX_final = X_sel\nlog.info(f\"Aligned DDOS input shape: {X_final.shape}\")\n\n\n# ==========================================================\n# 8. PREDICT USING LOAN MODEL\n# ==========================================================\npred = lr_model.predict(X_final)\nprob = lr_model.predict_proba(X_final)[:, 1]\n\n\n# ==========================================================\n# 9. METRICS\n# ==========================================================\nprint(\"\\n======== DDOS TESTING WITH LOAN MODEL ========\")\nprint(\"Accuracy :\", round(accuracy_score(y, pred), 4))\nprint(\"Precision:\", round(precision_score(y, pred), 4))\nprint(\"Recall   :\", round(recall_score(y, pred), 4))\nprint(\"F1 Score :\", round(f1_score(y, pred), 4))\n\nprint(\"\\nConfusion Matrix:\\n\", confusion_matrix(y, pred))\nprint(\"\\nClassification Report:\\n\", classification_report(y, pred))\n\nlog.info(\"DDOS Testing using LOAN model completed successfully.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T13:39:08.980468Z","iopub.execute_input":"2025-12-02T13:39:08.981162Z","iopub.status.idle":"2025-12-02T13:39:09.489007Z","shell.execute_reply.started":"2025-12-02T13:39:08.981137Z","shell.execute_reply":"2025-12-02T13:39:09.488438Z"}},"outputs":[{"name":"stderr","text":"2025-12-02 13:39:09,217 — INFO — DDOS dataset loaded: 19 features present.\n2025-12-02 13:39:09,239 — INFO — Mask 1: length=19 values=[1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1]\n2025-12-02 13:39:09,240 — INFO — Mask 2: length=19 values=[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n2025-12-02 13:39:09,240 — INFO — Mask 3: length=19 values=[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n2025-12-02 13:39:09,244 — INFO — Auto-selected DDOS 19 numeric columns: ['id', 'dload', 'sinpkt', 'ct_srv_src', 'attack_cat', 'state', 'sbytes', 'rate', 'dttl', 'sload', 'sjit', 'dwin', 'synack', 'ackdat', 'trans_depth', 'ct_state_ttl', 'ct_dst_ltm', 'ct_src_dport_ltm', 'ct_src_ltm']\n2025-12-02 13:39:09,247 — INFO — Model 1 selected: ['id', 'dload', 'sinpkt', 'ct_srv_src', 'state', 'sbytes', 'dttl', 'sload', 'sjit', 'dwin', 'synack', 'ackdat', 'trans_depth', 'ct_state_ttl', 'ct_dst_ltm', 'ct_src_dport_ltm', 'ct_src_ltm']\n2025-12-02 13:39:09,248 — INFO — Model 2 selected: ['id', 'dload', 'sinpkt', 'ct_srv_src', 'attack_cat', 'state', 'sbytes', 'rate', 'dttl', 'sload', 'sjit', 'dwin', 'synack', 'ackdat', 'trans_depth', 'ct_state_ttl', 'ct_dst_ltm', 'ct_src_dport_ltm', 'ct_src_ltm']\n2025-12-02 13:39:09,248 — INFO — Model 3 selected: ['id', 'dload', 'sinpkt', 'ct_srv_src', 'attack_cat', 'state', 'sbytes', 'rate', 'dttl', 'sload', 'sjit', 'dwin', 'synack', 'ackdat', 'trans_depth', 'ct_state_ttl', 'ct_dst_ltm', 'ct_src_dport_ltm', 'ct_src_ltm']\n2025-12-02 13:39:09,249 — INFO — FINAL VOTED FEATURES: ['id', 'dload', 'sinpkt', 'ct_srv_src', 'state', 'sbytes', 'dttl', 'sload', 'sjit', 'dwin', 'synack', 'ackdat', 'trans_depth', 'ct_state_ttl', 'ct_dst_ltm', 'ct_src_dport_ltm', 'ct_src_ltm', 'attack_cat', 'rate']\n2025-12-02 13:39:09,252 — INFO — LR model expects 3 features.\n2025-12-02 13:39:09,256 — INFO — Trimmed extra columns.\n2025-12-02 13:39:09,256 — INFO — Aligned DDOS input shape: (82332, 3)\n","output_type":"stream"},{"name":"stdout","text":"\n======== DDOS TESTING WITH LOAN MODEL ========\nAccuracy : 0.4389\nPrecision: 0.0069\nRecall   : 0.0001\nF1 Score : 0.0003\n\nConfusion Matrix:\n [[36133   867]\n [45326     6]]\n","output_type":"stream"},{"name":"stderr","text":"2025-12-02 13:39:09,486 — INFO — DDOS Testing using LOAN model completed successfully.\n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.44      0.98      0.61     37000\n           1       0.01      0.00      0.00     45332\n\n    accuracy                           0.44     82332\n   macro avg       0.23      0.49      0.31     82332\nweighted avg       0.20      0.44      0.27     82332\n\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# ==========================================================\n#   DDOS FEATURE SELECTION (PSO + GA + GWO) on ALL COLUMNS\n#   Converts all categorical → numeric, then selects TOP-19\n#   Saves ddos_selected19.csv\n# ==========================================================\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\nfrom sklearn.metrics import make_scorer, f1_score\nfrom sklearn.base import clone\nfrom catboost import CatBoostClassifier\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# -------------------------\n# LOAD DDOS DATASET\n# -------------------------\nddos_csv_path = \"/kaggle/input/new-ids/UNSW_NB15_testing-set.csv\"     # <-- change\nTARGET_COL = \"label\"\n\ndf = pd.read_csv(ddos_csv_path)\nprint(\"\\nLoaded DDOS dataset:\", df.shape)\n\n# ------------------------------------------------------\n# 1. CONVERT ALL NON-NUMERIC COLUMNS → LABEL ENCODED\n# ------------------------------------------------------\nencoders = {}\nfor col in df.columns:\n    if col != TARGET_COL and df[col].dtype == \"object\":\n        encoder = LabelEncoder()\n        df[col] = encoder.fit_transform(df[col].astype(str))\n        encoders[col] = encoder\n\nprint(\"Converted categorical columns to numeric.\")\n\n# ------------------------------------------------------\n# PREPARE DATA\n# ------------------------------------------------------\nX_full = df.drop(TARGET_COL, axis=1)\ny = df[TARGET_COL].astype(int)\n\nfeature_names = list(X_full.columns)\nN = len(feature_names)\n\nprint(f\"Total encoded feature count = {N}\")\n\n\n# ============================================================\n# FITNESS FUNCTION (CatBoost)\n# ============================================================\ndef get_model():\n    return CatBoostClassifier(\n        iterations=200,\n        depth=6,\n        learning_rate=0.05,\n        verbose=0,\n        random_seed=42\n    )\n\ndef fitness(mask):\n    idx = np.where(mask == 1)[0]\n    if len(idx) == 0:\n        return 0\n\n    X_sel = X_full.iloc[:, idx]\n    model = get_model()\n\n    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n\n    scores = cross_val_score(\n        clone(model), X_sel, y, cv=skf,\n        scoring=make_scorer(f1_score)\n    )\n    return scores.mean()\n\n\n# ============================================================\n# PSO / GA / GWO  (same logic, works on ALL features)\n# ============================================================\n\ndef run_pso(swarm=15, iters=10):\n    print(\"\\n=== PSO ===\")\n    dim = N\n    pos = np.random.randint(0,2,(swarm,dim))\n    vel = np.random.uniform(-1,1,(swarm,dim))\n\n    best = pos[0]\n    best_score = fitness(best)\n\n    for t in range(iters):\n        for i in range(swarm):\n            r1, r2 = np.random.rand(dim), np.random.rand(dim)\n            vel[i] = 0.5*vel[i] + 1.5*r1*(best-pos[i]) + 1.5*r2*(best-pos[i])\n            s = 1/(1+np.exp(-vel[i]))\n            pos[i] = (np.random.rand(dim) < s).astype(int)\n\n            score = fitness(pos[i])\n            if score > best_score:\n                best_score = score\n                best = pos[i].copy()\n\n        print(f\" iter {t+1}/{iters} best={best_score:.4f}\")\n\n    return best\n\n\ndef run_ga(pop=20, gens=10):\n    print(\"\\n=== GA ===\")\n    dim = N\n    population = np.random.randint(0,2,(pop,dim))\n\n    def mutate(ind):\n        for i in range(dim):\n            if np.random.rand() < 0.05:\n                ind[i] = 1 - ind[i]\n        return ind\n\n    for g in range(gens):\n        fitnesses = np.array([fitness(ind) for ind in population])\n        best_idx = np.argmax(fitnesses)\n\n        new_pop = [population[best_idx].copy()]  # elitism\n\n        while len(new_pop) < pop:\n            parents = population[np.random.choice(pop, 2)]\n            pt = np.random.randint(1, dim)\n            child = np.concatenate([parents[0][:pt], parents[1][pt:]])\n            new_pop.append(mutate(child))\n\n        population = np.array(new_pop)\n        print(f\" gen {g+1}/{gens} best={fitnesses[best_idx]:.4f}\")\n\n    return population[np.argmax(fitnesses)]\n\n\ndef run_gwo(wolves=15, iters=10):\n    print(\"\\n=== GWO ===\")\n    dim = N\n    pack = np.random.randint(0,2,(wolves,dim))\n\n    for t in range(iters):\n        scores = np.array([fitness(w) for w in pack])\n        idx = scores.argsort()[::-1]\n\n        alpha, beta, delta = pack[idx[:3]]\n        a = 2 - t*(2/iters)\n\n        for i in range(wolves):\n            X = pack[i]\n\n            D1 = abs(np.random.rand(dim)*alpha - X)\n            D2 = abs(np.random.rand(dim)*beta - X)\n            D3 = abs(np.random.rand(dim)*delta - X)\n\n            X1 = alpha - a*D1\n            X2 = beta  - a*D2\n            X3 = delta - a*D3\n\n            X_new = (X1+X2+X3)/3\n            prob = 1/(1+np.exp(-X_new))\n            pack[i] = (np.random.rand(dim) < prob).astype(int)\n\n        print(f\" iter {t+1}/{iters} best={scores[idx[0]]:.4f}\")\n\n    return pack[idx[0]]\n\n\n# ============================================================\n# RUN ALL OPTIMIZERS\n# ============================================================\nmask_pso = run_pso()\nmask_ga  = run_ga()\nmask_gwo = run_gwo()\n\nprint(\"\\nMasks obtained:\")\nprint(\"PSO:\", mask_pso)\nprint(\"GA :\", mask_ga)\nprint(\"GWO:\", mask_gwo)\n\n\n# ============================================================\n# FEATURE VOTING → Select TOP 19\n# ============================================================\nvote_count = {}\nfor m in [mask_pso, mask_ga, mask_gwo]:\n    feats = np.array(feature_names)[m == 1]\n    for f in feats:\n        vote_count[f] = vote_count.get(f, 0) + 1\n\nsorted_feats = sorted(vote_count.items(), key=lambda x: x[1], reverse=True)\ntop19 = [f for f,_ in sorted_feats[:19]]\n\nprint(\"\\nTOP-19 SELECTED FEATURES:\")\nprint(top19)\n\n# Save CSV\ndf_sel = df[top19 + [TARGET_COL]]\ndf_sel.to_csv(\"ddos_selected19.csv\", index=False)\n\nprint(\"\\nSaved: ddos_selected19.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T14:51:13.143579Z","iopub.execute_input":"2025-12-02T14:51:13.143922Z","iopub.status.idle":"2025-12-02T15:35:09.117026Z","shell.execute_reply.started":"2025-12-02T14:51:13.143898Z","shell.execute_reply":"2025-12-02T15:35:09.116219Z"}},"outputs":[{"name":"stdout","text":"\nLoaded DDOS dataset: (82332, 45)\nConverted categorical columns to numeric.\nTotal encoded feature count = 44\n\n=== PSO ===\n iter 1/10 best=1.0000\n iter 2/10 best=1.0000\n iter 3/10 best=1.0000\n iter 4/10 best=1.0000\n iter 5/10 best=1.0000\n iter 6/10 best=1.0000\n iter 7/10 best=1.0000\n iter 8/10 best=1.0000\n iter 9/10 best=1.0000\n iter 10/10 best=1.0000\n\n=== GA ===\n gen 1/10 best=1.0000\n gen 2/10 best=1.0000\n gen 3/10 best=1.0000\n gen 4/10 best=1.0000\n gen 5/10 best=1.0000\n gen 6/10 best=1.0000\n gen 7/10 best=1.0000\n gen 8/10 best=1.0000\n gen 9/10 best=1.0000\n gen 10/10 best=1.0000\n\n=== GWO ===\n iter 1/10 best=1.0000\n iter 2/10 best=1.0000\n iter 3/10 best=1.0000\n iter 4/10 best=1.0000\n iter 5/10 best=1.0000\n iter 6/10 best=1.0000\n iter 7/10 best=1.0000\n iter 8/10 best=1.0000\n iter 9/10 best=1.0000\n iter 10/10 best=1.0000\n\nMasks obtained:\nPSO: [1 0 0 0 1 1 1 0 0 0 1 1 1 0 1 1 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 0 1 0\n 1 1 0 1 0 1 1]\nGA : [1 0 0 1 0 1 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 1 0 1 1 0 0 0 1\n 1 0 0 0 1 1 1]\nGWO: [0 1 1 0 0 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1 1 1 0 1\n 1 1 1 1 0 0 1]\n\nTOP-19 SELECTED FEATURES:\n['dpkts', 'dttl', 'djit', 'ackdat', 'ct_state_ttl', 'is_ftp_login', 'attack_cat', 'id', 'spkts', 'sttl', 'dloss', 'dinpkt', 'stcpb', 'response_body_len', 'ct_dst_ltm', 'ct_ftp_cmd', 'ct_src_ltm', 'is_sm_ips_ports', 'sinpkt']\n\nSaved: ddos_selected19.csv\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\n\n# =========================================\n# 1️⃣ Load dataset (after feature selection)\n# =========================================\ndf = pd.read_csv(\"/kaggle/working/ddos_selected19.csv\")   # <-- change path only if needed\nprint(\"Original shape:\", df.shape)\n\n# =========================================\n# 2️⃣ Remove columns where ALL values are NaN\n# =========================================\ndf = df.dropna(axis=1, how='all')\n\n# =========================================\n# 3️⃣ Remove columns where ALL values are 0\n# =========================================\ndf = df.loc[:, (df != 0).any(axis=0)]\n\n# =========================================\n# 4️⃣ Remove duplicate rows\n# =========================================\ndf = df.drop_duplicates()\n\n# =========================================\n# 5️⃣ Identify numeric & categorical columns\n# =========================================\nnum_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\ncat_cols = df.select_dtypes(include=['object']).columns.tolist()\n\nprint(\"Numeric cols:\", num_cols)\nprint(\"Categorical cols:\", cat_cols)\n\n# =========================================\n# 6️⃣ Handle missing values\n# =========================================\nif len(num_cols) > 0:\n    df[num_cols] = df[num_cols].fillna(df[num_cols].median())\n\nif len(cat_cols) > 0:\n    df[cat_cols] = df[cat_cols].fillna(df[cat_cols].mode().iloc[0])\n\n# =========================================\n# 7️⃣ Label encode categorical columns\n# =========================================\nfor col in cat_cols:\n    le = LabelEncoder()\n    df[col] = le.fit_transform(df[col].astype(str))\n\n# =========================================\n# 8️⃣ Min-Max scale numeric columns\n# =========================================\nscaler = MinMaxScaler()\ndf[num_cols] = scaler.fit_transform(df[num_cols])\n\n# =========================================\n# 9️⃣ Save final output\n# =========================================\noutput_filename = \"ddos_final_preprocessed.csv\"\ndf.to_csv(output_filename, index=False)\n\nprint(f\"✅ DDOS preprocessing complete! Saved as: {output_filename}\")\nprint(\"Final shape:\", df.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T15:35:46.458706Z","iopub.execute_input":"2025-12-02T15:35:46.459568Z","iopub.status.idle":"2025-12-02T15:35:48.346072Z","shell.execute_reply.started":"2025-12-02T15:35:46.459534Z","shell.execute_reply":"2025-12-02T15:35:48.345295Z"}},"outputs":[{"name":"stdout","text":"Original shape: (82332, 20)\nNumeric cols: ['dpkts', 'dttl', 'djit', 'ackdat', 'ct_state_ttl', 'is_ftp_login', 'attack_cat', 'id', 'spkts', 'sttl', 'dloss', 'dinpkt', 'stcpb', 'response_body_len', 'ct_dst_ltm', 'ct_ftp_cmd', 'ct_src_ltm', 'is_sm_ips_ports', 'sinpkt', 'label']\nCategorical cols: []\n✅ DDOS preprocessing complete! Saved as: ddos_final_preprocessed.csv\nFinal shape: (82332, 20)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom catboost import CatBoostClassifier\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\nimport joblib\n\n# Load final selected 19-feature DDOS dataset\ndf = pd.read_csv(\"/kaggle/working/ddos_selected19.csv\")\nprint(\"Loaded:\", df.shape)\n\nTARGET_COL = \"label\"\n\n# Encode categorical columns if any\nfor col in df.columns:\n    if col != TARGET_COL and df[col].dtype == object:\n        df[col] = LabelEncoder().fit_transform(df[col].astype(str))\n\n# Normalization\nnum_cols = df.drop(TARGET_COL, axis=1).columns\ndf[num_cols] = MinMaxScaler().fit_transform(df[num_cols])\n\nX = df.drop(TARGET_COL, axis=1)\ny = df[TARGET_COL].astype(int)\n\nprint(\"Training shape:\", X.shape)\n\n# Train  NEW DDOS model\nmodel = CatBoostClassifier(\n    iterations=800,\n    depth=6,\n    learning_rate=0.05,\n    verbose=0\n)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\nmodel.fit(X_train, y_train)\n\n# Evaluate\npred = model.predict(X_test)\n\nprint(\"\\nAccuracy:\", accuracy_score(y_test, pred))\nprint(\"Precision:\", precision_score(y_test, pred))\nprint(\"Recall:\", recall_score(y_test, pred))\nprint(\"F1 Score:\", f1_score(y_test, pred))\n\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, pred))\n\n# save\njoblib.dump(model, \"ddos_19features_trained_model.pkl\")\nprint(\"Saved ddos_19features_trained_model.pkl\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T15:49:53.332647Z","iopub.execute_input":"2025-12-02T15:49:53.333243Z","iopub.status.idle":"2025-12-02T15:50:00.062958Z","shell.execute_reply.started":"2025-12-02T15:49:53.333222Z","shell.execute_reply":"2025-12-02T15:50:00.062286Z"}},"outputs":[{"name":"stdout","text":"Loaded: (82332, 20)\nTraining shape: (82332, 19)\n\nAccuracy: 1.0\nPrecision: 1.0\nRecall: 1.0\nF1 Score: 1.0\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00      7400\n           1       1.00      1.00      1.00      9067\n\n    accuracy                           1.00     16467\n   macro avg       1.00      1.00      1.00     16467\nweighted avg       1.00      1.00      1.00     16467\n\nSaved ddos_19features_trained_model.pkl\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"#USING NEW DATA SET CIC-DDOS2019 FROM KAGGLE DIRECTLY ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import kagglehub\n\n# Download latest version\npath = kagglehub.dataset_download(\"sizlingdhairya1/cicddos2019\")\n\nprint(\"Path to dataset files:\", path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T18:11:16.561492Z","iopub.execute_input":"2025-12-03T18:11:16.562164Z","iopub.status.idle":"2025-12-03T18:11:16.812872Z","shell.execute_reply.started":"2025-12-03T18:11:16.562141Z","shell.execute_reply":"2025-12-03T18:11:16.812217Z"}},"outputs":[{"name":"stdout","text":"Path to dataset files: /kaggle/input/cicddos2019\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"#CIC-DDOS2019 WITH ONLY MODEL USED CATBOOST\n\n\n# ==============================================================\n# CICDDoS2019 CLEANING + CATBOOST CLASSIFICATION (80–20 SPLIT)\n# ==============================================================\n\nimport kagglehub\nimport os\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\nfrom catboost import CatBoostClassifier\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# --------------------------------------------------------------\n# 1. DOWNLOAD CICDDoS2019 DATASET\n# --------------------------------------------------------------\npath = kagglehub.dataset_download(\"sizlingdhairya1/cicddos2019\")\nprint(\"Dataset downloaded to:\", path)\n\n# Load all CSVs\nfiles = [os.path.join(path, f) for f in os.listdir(path) if f.endswith(\".csv\")]\n\nprint(f\"\\nFound {len(files)} CSV files. Loading...\")\n\ndf_list = []\nfor f in files:\n    print(\"Loading:\", f)\n    df_list.append(pd.read_csv(f, low_memory=False))\n\ndf = pd.concat(df_list, axis=0, ignore_index=True)\nprint(\"\\nMerged dataset shape:\", df.shape)\n\n\n# --------------------------------------------------------------\n# 2. BASIC CLEANING\n# --------------------------------------------------------------\n\n# Remove duplicate rows\ndf = df.drop_duplicates()\nprint(\"After dropping duplicates:\", df.shape)\n\n# Drop all-NaN columns\ndf = df.dropna(axis=1, how='all')\nprint(\"After dropping all-NaN columns:\", df.shape)\n\n# Remove useless index-like columns\nfor col in [\"Unnamed: 0\", \"Flow ID\"]:\n    if col in df.columns:\n        df = df.drop(columns=[col])\n\nprint(\"After dropping index/FlowID columns:\", df.shape)\n\n# Fill missing numeric values\nnum_cols = df.select_dtypes(include=[np.number]).columns\ndf[num_cols] = df[num_cols].fillna(df[num_cols].median())\n\n# Fill missing categorical values\ncat_cols = df.select_dtypes(include=[\"object\"]).columns\nfor col in cat_cols:\n    df[col] = df[col].fillna(df[col].mode()[0])\n\nprint(\"Missing values handled.\")\n\n\n# --------------------------------------------------------------\n# 3. LABEL ENCODE CATEGORICAL COLUMNS\n# --------------------------------------------------------------\nle = LabelEncoder()\nfor col in cat_cols:\n    df[col] = le.fit_transform(df[col].astype(str))\n\nprint(\"Categorical → numeric encoding complete.\")\n\n\n# --------------------------------------------------------------\n# 4. SELECT TARGET COLUMN\n# --------------------------------------------------------------\nTARGET_COL = \" Label\"    # <-- EXACT MATCH\n\nif TARGET_COL not in df.columns:\n    raise ValueError(f\"Target column '{TARGET_COL}' not found!\")\n\nprint(\"\\nUsing target column:\", TARGET_COL)\n\nX = df.drop(TARGET_COL, axis=1)\ny = df[TARGET_COL].astype(int)\n\nprint(\"Final feature shape:\", X.shape)\n\n\n# --------------------------------------------------------------\n# 5. TRAIN-TEST SPLIT\n# --------------------------------------------------------------\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.2,\n    random_state=42,\n    stratify=y\n)\n\nprint(\"\\nTrain size:\", X_train.shape, \" Test size:\", X_test.shape)\n\n\n# --------------------------------------------------------------\n# 6. TRAIN CATBOOST\n# --------------------------------------------------------------\nmodel = CatBoostClassifier(\n    iterations=400,\n    learning_rate=0.05,\n    depth=8,\n    verbose=50,\n    random_seed=42\n)\n\nmodel.fit(X_train, y_train)\n\n\n# --------------------------------------------------------------\n# 7. EVALUATE MODEL\n# --------------------------------------------------------------\npred = model.predict(X_test)\n\nacc = accuracy_score(y_test, pred)\nprec = precision_score(y_test, pred, average=\"weighted\")\nrec  = recall_score(y_test, pred, average=\"weighted\")\nf1   = f1_score(y_test, pred, average=\"weighted\")\n\nprint(\"\\n================= CICDDoS2019 CATBOOST RESULTS =================\")\nprint(\"Accuracy :\", round(acc, 4))\nprint(\"Precision:\", round(prec, 4))\nprint(\"Recall   :\", round(rec, 4))\nprint(\"F1 Score :\", round(f1, 4))\n\nprint(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, pred))\nprint(\"\\nClassification Report:\\n\", classification_report(y_test, pred))\n\nprint(\"================================================================\\n\")\n\nimport pickle\n\nsave_path = \"cicddos2019_catboost_model.pkl\"\n\nwith open(save_path, \"wb\") as f:\n    pickle.dump({\n        \"model\": model,\n        \"features\": X.columns.tolist()\n    }, f)\n\nprint(f\"\\nModel saved successfully as: {save_path}\")\nprint(\"You can now load it later using pickle.load(...)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T18:27:29.881562Z","iopub.execute_input":"2025-12-03T18:27:29.881862Z","iopub.status.idle":"2025-12-03T18:42:55.138374Z","shell.execute_reply.started":"2025-12-03T18:27:29.881840Z","shell.execute_reply":"2025-12-03T18:42:55.137657Z"}},"outputs":[{"name":"stdout","text":"Dataset downloaded to: /kaggle/input/cicddos2019\n\nFound 1 CSV files. Loading...\nLoading: /kaggle/input/cicddos2019/Random_combine_final.csv\n\nMerged dataset shape: (300000, 88)\nAfter dropping duplicates: (299991, 88)\nAfter dropping all-NaN columns: (299991, 88)\nAfter dropping index/FlowID columns: (299991, 86)\nMissing values handled.\nCategorical → numeric encoding complete.\n\nUsing target column:  Label\nFinal feature shape: (299991, 85)\n\nTrain size: (239992, 85)  Test size: (59999, 85)\n0:\tlearn: 2.1430835\ttotal: 2.3s\tremaining: 15m 16s\n50:\tlearn: 0.1182699\ttotal: 1m 56s\tremaining: 13m 18s\n100:\tlearn: 0.0380642\ttotal: 3m 50s\tremaining: 11m 22s\n150:\tlearn: 0.0259132\ttotal: 5m 45s\tremaining: 9m 30s\n200:\tlearn: 0.0216656\ttotal: 7m 41s\tremaining: 7m 36s\n250:\tlearn: 0.0195842\ttotal: 9m 35s\tremaining: 5m 41s\n300:\tlearn: 0.0182737\ttotal: 11m 30s\tremaining: 3m 47s\n350:\tlearn: 0.0171308\ttotal: 13m 24s\tremaining: 1m 52s\n399:\tlearn: 0.0163033\ttotal: 15m 17s\tremaining: 0us\n\n================= CICDDoS2019 CATBOOST RESULTS =================\nAccuracy : 0.9927\nPrecision: 0.993\nRecall   : 0.9927\nF1 Score : 0.9927\n\nConfusion Matrix:\n [[   95     0     0     0     0     1     0     0     0     0     0     0\n      1     0     0     0     0     0     0]\n [    0  4314    19     0     4     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0]\n [    0     0  1817     1     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0]\n [    0     0    27  3830     0     4     0     0     0     0     0     0\n      0     1     0     0     0     0     0]\n [    0    10     0     0  1015     0     0     0     0     0     0     0\n      0     1     0     0     0     0     0]\n [    0     0     0    33     0  3452    33     0     0     0     0     0\n      0     0     0     0     0     0     0]\n [    0     0     0     0     0    41  4319    11     0     0     0     0\n      0     0     0     0     0     0     0]\n [    0     0     0     0     0     0    13  2206    18     0     0     0\n      0     0     0     0     0     0     0]\n [    0     0     0     0     0     0     0    58  2589     0     0     0\n      0     0     0     0     0     0     0]\n [    0     0     0     0     0     0     0     0     0  1646     0     0\n      0     0     0     0     0     0     0]\n [    0     0     0     0     0     0     0     0     0     0  4930     0\n      0     0     0     0     0     0     0]\n [    0     0     0     0     0     0     0     0     0     0     0  3062\n     51     0     0     0     0     0     0]\n [    0     0     0     0     0     0     0     0     0     0     0     6\n    154     1     0     0     0     0     0]\n [    0     0     0     0     0     0     0     0     0     0     0     0\n      0  5529     0     3     0     0     0]\n [    0     0     0     0     0     0     0     0     0     0     0     0\n      0    12 17091     0     0     0     0]\n [    0     0     0     0     0     0     0     0     0     0     2     0\n      0     1     0  3286     0     0     0]\n [    0     0     0     0     0     0     0     0    12     0     0     0\n      0    69     0     0   228     0     0]\n [    0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     1     0     1     0]\n [    0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     1     0     0]]\n\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       1.00      0.98      0.99        97\n           1       1.00      0.99      1.00      4337\n           2       0.98      1.00      0.99      1818\n           3       0.99      0.99      0.99      3862\n           4       1.00      0.99      0.99      1026\n           5       0.99      0.98      0.98      3518\n           6       0.99      0.99      0.99      4371\n           7       0.97      0.99      0.98      2237\n           8       0.99      0.98      0.98      2647\n           9       1.00      1.00      1.00      1646\n          10       1.00      1.00      1.00      4930\n          11       1.00      0.98      0.99      3113\n          12       0.75      0.96      0.84       161\n          13       0.98      1.00      0.99      5532\n          14       1.00      1.00      1.00     17103\n          15       1.00      1.00      1.00      3289\n          16       1.00      0.74      0.85       309\n          17       1.00      0.50      0.67         2\n          18       0.00      0.00      0.00         1\n\n    accuracy                           0.99     59999\n   macro avg       0.93      0.90      0.91     59999\nweighted avg       0.99      0.99      0.99     59999\n\n================================================================\n\n\nModel saved successfully as: cicddos2019_catboost_model.pkl\nYou can now load it later using pickle.load(...)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"#TESTING USING JUST MODEL\n\n\nimport pickle\nimport pandas as pd\nimport numpy as np\nimport kagglehub\nimport glob, os\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n\n# -----------------------------------------------------\n# 1) Load saved model\n# -----------------------------------------------------\nmodel_path = \"/kaggle/working/cicddos2019_catboost_model.pkl\"\nsaved = pickle.load(open(model_path, \"rb\"))\n\nmodel = saved[\"model\"]\ntrain_features_raw = saved[\"features\"]\n\nprint(\"\\nLoaded model.\")\nprint(\"Training feature count:\", len(train_features_raw))\n\n# Create clean versions (strip spaces)\ntrain_features_clean = [f.strip() for f in train_features_raw]\n\n# Mapping clean → ORIGINAL (with hidden spaces)\nfeature_map = dict(zip(train_features_clean, train_features_raw))\n\n\n# -----------------------------------------------------\n# 2) Load Kaggle dataset\n# -----------------------------------------------------\nDATA_PATH = kagglehub.dataset_download(\"sizlingdhairya1/cicddos2019\")\ncsv_files = glob.glob(os.path.join(DATA_PATH, \"*.csv\"))\n\ndfs = []\nfor f in csv_files:\n    dfs.append(pd.read_csv(f, low_memory=False))\n\ndf = pd.concat(dfs, ignore_index=True)\ndf.columns = df.columns.astype(str).str.strip()\n\n# Fix the label column\nlabel_candidates = [c for c in df.columns if c.lower().strip() == \"label\"]\nlabel_col = label_candidates[0]\ndf.rename(columns={label_col: \"Label\"}, inplace=True)\n\n\n# -----------------------------------------------------\n# 3) Clean missing values (same as training)\n# -----------------------------------------------------\nnum_cols = df.select_dtypes(include=[np.number]).columns\ncat_cols = df.select_dtypes(include=[\"object\"]).columns\n\ndf[num_cols] = df[num_cols].fillna(df[num_cols].median())\nfor c in cat_cols:\n    df[c] = df[c].fillna(df[c].mode()[0])\n    df[c] = LabelEncoder().fit_transform(df[c].astype(str))\n\n\n# -----------------------------------------------------\n# 4) RENAME test features to EXACT model names\n# -----------------------------------------------------\ndf_cols = df.columns.tolist()\ndf_cols_clean = [c.strip() for c in df_cols]\n\nrename_dict = {}\n\nfor clean_name, orig_name in feature_map.items():\n    if clean_name in df_cols_clean:\n        found_index = df_cols_clean.index(clean_name)\n        df_name = df_cols[found_index]\n        rename_dict[df_name] = orig_name\n\ndf.rename(columns=rename_dict, inplace=True)\n\n\n# -----------------------------------------------------\n# 5) Check for missing features\n# -----------------------------------------------------\nmissing = [f for f in train_features_raw if f not in df.columns]\n\nif missing:\n    raise RuntimeError(\n        \"Still missing features:\\n\" + str(missing) +\n        \"\\n\\nThis means training column names contained hidden unicode symbols.\"\n    )\n\n\n# -----------------------------------------------------\n# 6) Prepare X, y\n# -----------------------------------------------------\ndf = df[train_features_raw + [\"Label\"]]\nX = df.drop(\"Label\", axis=1)\ny = df[\"Label\"].astype(int)\n\nprint(\"\\nFinal Test Shape:\", X.shape)\n\n\n# -----------------------------------------------------\n# 7) Predict\n# -----------------------------------------------------\ny_pred = model.predict(X)\n\nacc  = accuracy_score(y, y_pred)\nprec = precision_score(y, y_pred, average=\"weighted\", zero_division=0)\nrec  = recall_score(y, y_pred, average=\"weighted\", zero_division=0)\nf1   = f1_score(y, y_pred, average=\"weighted\", zero_division=0)\n\nprint(\"\\n========== FINAL TEST RESULTS ==========\")\nprint(\"Accuracy :\", acc)\nprint(\"Precision:\", prec)\nprint(\"Recall   :\", rec)\nprint(\"F1 Score :\", f1)\n\nprint(\"\\nClassification Report:\\n\")\nprint(classification_report(y, y_pred, zero_division=0))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T19:13:04.611351Z","iopub.execute_input":"2025-12-03T19:13:04.612058Z","iopub.status.idle":"2025-12-03T19:13:12.612421Z","shell.execute_reply.started":"2025-12-03T19:13:04.612034Z","shell.execute_reply":"2025-12-03T19:13:12.611767Z"}},"outputs":[{"name":"stdout","text":"\nLoaded model.\nTraining feature count: 85\n\nFinal Test Shape: (300000, 85)\n\n========== FINAL TEST RESULTS ==========\nAccuracy : 0.99376\nPrecision: 0.9939438457235572\nRecall   : 0.99376\nF1 Score : 0.9937173594773673\n\nClassification Report:\n\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00       487\n           1       1.00      1.00      1.00     21686\n           2       0.98      1.00      0.99      9090\n           3       0.99      0.99      0.99     19308\n           4       1.00      0.99      1.00      5131\n           5       0.99      0.98      0.99     17588\n           6       0.99      0.99      0.99     21856\n           7       0.97      0.99      0.98     11184\n           8       0.99      0.98      0.99     13237\n           9       1.00      1.00      1.00      8228\n          10       1.00      1.00      1.00     24652\n          11       1.00      0.99      0.99     15563\n          12       0.78      0.98      0.87       805\n          13       0.99      1.00      0.99     27668\n          14       1.00      1.00      1.00     85512\n          15       1.00      1.00      1.00     16447\n          16       1.00      0.76      0.86      1547\n          17       1.00      0.25      0.40         8\n          18       0.00      0.00      0.00         3\n\n    accuracy                           0.99    300000\n   macro avg       0.93      0.89      0.90    300000\nweighted avg       0.99      0.99      0.99    300000\n\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"print(\"Columns in dataset:\")\nprint(df.columns.tolist())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T13:13:22.928909Z","iopub.execute_input":"2025-12-03T13:13:22.929275Z","iopub.status.idle":"2025-12-03T13:13:22.933624Z","shell.execute_reply.started":"2025-12-03T13:13:22.929248Z","shell.execute_reply":"2025-12-03T13:13:22.932823Z"}},"outputs":[{"name":"stdout","text":"Columns in dataset:\n['Unnamed: 0', 'Flow ID', ' Source IP', ' Source Port', ' Destination IP', ' Destination Port', ' Protocol', ' Timestamp', ' Flow Duration', ' Total Fwd Packets', ' Total Backward Packets', 'Total Length of Fwd Packets', ' Total Length of Bwd Packets', ' Fwd Packet Length Max', ' Fwd Packet Length Min', ' Fwd Packet Length Mean', ' Fwd Packet Length Std', 'Bwd Packet Length Max', ' Bwd Packet Length Min', ' Bwd Packet Length Mean', ' Bwd Packet Length Std', 'Flow Bytes/s', ' Flow Packets/s', ' Flow IAT Mean', ' Flow IAT Std', ' Flow IAT Max', ' Flow IAT Min', 'Fwd IAT Total', ' Fwd IAT Mean', ' Fwd IAT Std', ' Fwd IAT Max', ' Fwd IAT Min', 'Bwd IAT Total', ' Bwd IAT Mean', ' Bwd IAT Std', ' Bwd IAT Max', ' Bwd IAT Min', 'Fwd PSH Flags', ' Bwd PSH Flags', ' Fwd URG Flags', ' Bwd URG Flags', ' Fwd Header Length', ' Bwd Header Length', 'Fwd Packets/s', ' Bwd Packets/s', ' Min Packet Length', ' Max Packet Length', ' Packet Length Mean', ' Packet Length Std', ' Packet Length Variance', 'FIN Flag Count', ' SYN Flag Count', ' RST Flag Count', ' PSH Flag Count', ' ACK Flag Count', ' URG Flag Count', ' CWE Flag Count', ' ECE Flag Count', ' Down/Up Ratio', ' Average Packet Size', ' Avg Fwd Segment Size', ' Avg Bwd Segment Size', ' Fwd Header Length.1', 'Fwd Avg Bytes/Bulk', ' Fwd Avg Packets/Bulk', ' Fwd Avg Bulk Rate', ' Bwd Avg Bytes/Bulk', ' Bwd Avg Packets/Bulk', 'Bwd Avg Bulk Rate', 'Subflow Fwd Packets', ' Subflow Fwd Bytes', ' Subflow Bwd Packets', ' Subflow Bwd Bytes', 'Init_Win_bytes_forward', ' Init_Win_bytes_backward', ' act_data_pkt_fwd', ' min_seg_size_forward', 'Active Mean', ' Active Std', ' Active Max', ' Active Min', 'Idle Mean', ' Idle Std', ' Idle Max', ' Idle Min', 'SimillarHTTP', ' Inbound', ' Label']\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"#VOTING FOR 6 ITERATIONS\n\n\n# hybrid_voting_hlo_ddos_pipeline.py\n# Single-file: PSO + GA + GWO -> VOTING -> HLO -> Hill-climb -> Final CatBoost\n# Option A: optimization subset = 3000 rows (1500 benign + 1500 attack)\n\nimport kagglehub\nimport glob, os, time, pickle, warnings\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\nfrom sklearn.model_selection import StratifiedKFold, train_test_split, cross_val_score\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, make_scorer\nfrom sklearn.base import clone\nfrom catboost import CatBoostClassifier\n\nwarnings.filterwarnings(\"ignore\")\nnp.random.seed(42)\n\n# -----------------------\n# USER CONFIG\n# -----------------------\nDATA_PATH = kagglehub.dataset_download(\"sizlingdhairya1/cicddos2019\")  # your loader\nOPT_SUBSET_PER_CLASS = 1500     # 1500 benign + 1500 attack => 3000 rows\nPSO_SWARM = 8\nPSO_ITERS = 6\nGA_POP = 12\nGA_GENS = 6\nGWO_WOLVES = 8\nGWO_ITERS = 6\nHLO_POP = 8\nHLO_ITERS = 8\nFIT_CB_ITERS_OPT = 80    # CatBoost iterations used inside fitness (fast)\nCV_OPT = 2               # cheap CV inside optimizer\nFINAL_CB_ITERS = 1000    # final model iterations (early stopping used)\nFINAL_EARLY_STOP = 50\nSAVE_PREFIX = \"ddos_hybrid_voting_hlo\"\nLEAKAGE_SINGLE_FEATURE_THRESHOLD = 0.99999  # single-feature accuracy threshold to treat as leakage\n\nprint(\"DATA_PATH:\", DATA_PATH)\n\n# -----------------------\n# 1) load all CSVs from dataset path\n# -----------------------\ncsv_files = glob.glob(os.path.join(DATA_PATH, \"*.csv\"))\nif len(csv_files) == 0:\n    raise RuntimeError(\"No CSV files found in DATA_PATH: \" + DATA_PATH)\n\nprint(f\"Found {len(csv_files)} CSV files. Loading & merging (may take a bit)...\")\ndfs = []\nfor f in csv_files:\n    print(\" ->\", os.path.basename(f))\n    dfs.append(pd.read_csv(f, low_memory=False))\ndf = pd.concat(dfs, ignore_index=True)\nprint(\"Merged dataset shape:\", df.shape)\n\n# -----------------------\n# 2) normalize columns and find label\n# -----------------------\ndf.columns = df.columns.str.strip()\nTARGET_CANDIDATES = [\"Label\", \"label\", \" Attack\", \"attack_cat\", \"Label \"]\nfound_label = None\nfor c in [\"Label\", \"label\", \"Attack\", \"attack\", \"attack_cat\"]:\n    if c in df.columns:\n        found_label = c\n        break\nif found_label is None:\n    # try case-insensitive lookup\n    for c in df.columns:\n        if c.strip().lower() == \"label\" or c.strip().lower() == \"attack\":\n            found_label = c\n            break\nif found_label is None:\n    raise RuntimeError(\"Cannot find label column. Columns available: \" + \", \".join(df.columns[:30]))\n\n# normalize label column name\ndf.rename(columns={found_label: \"Label\"}, inplace=True)\nprint(\"Using target column 'Label' (original: {})\".format(found_label))\n\n# -----------------------\n# 3) keep only rows with non-null label and make binary target\n# -----------------------\ndf = df[df[\"Label\"].notna()].copy()\ndf[\"Label\"] = df[\"Label\"].astype(str).str.strip().str.lower()\n# convert benign -> 0 else -> 1 (attack)\ndf[\"Label\"] = df[\"Label\"].apply(lambda x: 0 if x == \"benign\" else 1)\nprint(\"Label counts (full):\\n\", df[\"Label\"].value_counts())\n\n# -----------------------\n# 4) drop obviously leaking columns if present (IDs, IPs, timestamps)\n# -----------------------\npossible_leak_cols = [c for c in df.columns if c.strip().lower() in (\n    \"id\", \"flow id\", \"flowid\", \"timestamp\", \"ts\", \"source ip\", \"destination ip\",\n    \"src ip\", \"dst ip\", \"sourceip\", \"destinationip\", \"srcip\", \"dstip\")]\nif possible_leak_cols:\n    print(\"Dropping likely-leakage columns (ids/timestamps/ips):\", possible_leak_cols)\n    df.drop(columns=[c for c in possible_leak_cols if c in df.columns], inplace=True)\n\n# -----------------------\n# 5) basic cleaning: drop all-empty columns, replace inf, drop rows with NaN\n# -----------------------\ndf.replace([np.inf, -np.inf], np.nan, inplace=True)\ndf.dropna(axis=1, how=\"all\", inplace=True)\ndf.dropna(axis=0, how=\"any\", inplace=True)   # safe because we'll use full dataset for final, but optimizer needs no missing\nprint(\"After basic cleaning:\", df.shape)\n\n# -----------------------\n# 6) create balanced small subset for optimization: OPT_SUBSET_PER_CLASS * 2 rows\n# -----------------------\ncounts = df[\"Label\"].value_counts().to_dict()\nn_attack = counts.get(1, 0)\nn_benign = counts.get(0, 0)\ntake_attack = min(OPT_SUBSET_PER_CLASS, n_attack)\ntake_benign = min(OPT_SUBSET_PER_CLASS, n_benign)\nif take_attack < 10 or take_benign < 10:\n    raise RuntimeError(\"Not enough rows in one class to form the optimization subset. counts=\" + str(counts))\n\ndf_attack = df[df[\"Label\"] == 1].sample(take_attack, random_state=42)\ndf_benign = df[df[\"Label\"] == 0].sample(take_benign, random_state=42)\ndf_sub = pd.concat([df_attack, df_benign], ignore_index=True).sample(frac=1.0, random_state=42).reset_index(drop=True)\nprint(\"Optimization subset shape:\", df_sub.shape, \"Label counts:\", df_sub[\"Label\"].value_counts().to_dict())\n\n# -----------------------\n# 7) preprocess subset: encode categorical & scale numeric\n# -----------------------\nTARGET_COL = \"Label\"\nX_sub = df_sub.drop(columns=[TARGET_COL]).copy()\ny_sub = df_sub[TARGET_COL].astype(int).copy()\n\n# encode object columns\nobj_cols = X_sub.select_dtypes(include=[\"object\"]).columns.tolist()\nfor c in obj_cols:\n    X_sub[c] = LabelEncoder().fit_transform(X_sub[c].astype(str))\n# numeric scaling\nnum_cols_sub = X_sub.select_dtypes(include=[np.number]).columns.tolist()\nif len(num_cols_sub) > 0:\n    X_sub[num_cols_sub] = MinMaxScaler().fit_transform(X_sub[num_cols_sub])\n\nFEATURE_NAMES = X_sub.columns.tolist()\nN_FEATURES = len(FEATURE_NAMES)\nprint(\"Subset features:\", N_FEATURES)\n\n# -----------------------\n# 8) CatBoost factory & fitness function with caching\n# -----------------------\ndef get_catboost_model(iterations=FIT_CB_ITERS_OPT):\n    return CatBoostClassifier(iterations=iterations, learning_rate=0.05, depth=6,\n                              verbose=0, random_seed=42)\n\nfitness_cache = {}\ndef evaluate_mask(mask_bool, cv=CV_OPT, cb_iter=FIT_CB_ITERS_OPT):\n    key = tuple(int(x) for x in mask_bool)\n    if key in fitness_cache:\n        return fitness_cache[key]\n    idxs = [i for i,b in enumerate(key) if b==1]\n    if len(idxs) == 0:\n        fitness_cache[key] = 0.0\n        return 0.0\n    Xsel = X_sub.iloc[:, idxs]\n    model = get_catboost_model(iterations=cb_iter)\n    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=42)\n    try:\n        scores = cross_val_score(clone(model), Xsel, y_sub, cv=skf, scoring=make_scorer(f1_score), n_jobs=-1)\n    except Exception as e:\n        # if CatBoost fails (e.g. unexpected types), return 0\n        fitness_cache[key] = 0.0\n        return 0.0\n    val = float(np.mean(scores))\n    fitness_cache[key] = val\n    return val\n\n# -----------------------\n# 9) PSO (binary) - reduced\n# -----------------------\ndef run_pso(swarm_size=PSO_SWARM, iters=PSO_ITERS):\n    print(\"[PSO] start: swarm\", swarm_size, \"iters\", iters)\n    dim = N_FEATURES\n    pos = np.random.randint(0,2,(swarm_size,dim))\n    vel = np.random.uniform(-1,1,(swarm_size,dim))\n    pbest = pos.copy()\n    pbest_scores = np.array([evaluate_mask(p) for p in pbest])\n    gbest_idx = int(np.argmax(pbest_scores))\n    gbest = pbest[gbest_idx].copy()\n    gbest_score = pbest_scores[gbest_idx]\n    w = 0.6; c1 = c2 = 1.5\n    for t in range(iters):\n        print(\" PSO iter\", t+1, \"/\", iters, \"best\", gbest_score)\n        for i in range(swarm_size):\n            r1 = np.random.rand(dim); r2 = np.random.rand(dim)\n            vel[i] = w*vel[i] + c1*r1*(pbest[i] - pos[i]) + c2*r2*(gbest - pos[i])\n            s = 1.0 / (1.0 + np.exp(-vel[i]))\n            pos[i] = (np.random.rand(dim) < s).astype(int)\n            sc = evaluate_mask(pos[i])\n            if sc > pbest_scores[i]:\n                pbest[i] = pos[i].copy(); pbest_scores[i] = sc\n            if sc > gbest_score:\n                gbest = pos[i].copy(); gbest_score = sc\n        w = max(0.2, w*0.97)\n    print(\"[PSO] done best score\", gbest_score, \"selected\", int(np.sum(gbest)))\n    return gbest\n\n# -----------------------\n# 10) GA (binary) - reduced\n# -----------------------\ndef run_ga(pop_size=GA_POP, gens=GA_GENS):\n    print(\"[GA] start: pop\", pop_size, \"gens\", gens)\n    dim = N_FEATURES\n    pop = np.random.randint(0,2,(pop_size, dim))\n    fitnesses = np.array([evaluate_mask(ind) for ind in pop])\n    for g in range(gens):\n        print(\" GA gen\", g+1, \"/\", gens, \"best\", fitnesses.max())\n        elite_idxs = np.argsort(fitnesses)[-2:]\n        new_pop = [pop[elite_idxs[0]].copy(), pop[elite_idxs[1]].copy()]\n        while len(new_pop) < pop_size:\n            p1 = pop[np.random.randint(pop_size)].copy()\n            p2 = pop[np.random.randint(pop_size)].copy()\n            if np.random.rand() < 0.7:\n                pt = np.random.randint(1, dim)\n                child = np.concatenate([p1[:pt], p2[pt:]])\n            else:\n                child = p1\n            # mutation\n            for d in range(dim):\n                if np.random.rand() < 0.05:\n                    child[d] = 1-child[d]\n            new_pop.append(child)\n        pop = np.array(new_pop[:pop_size])\n        fitnesses = np.array([evaluate_mask(ind) for ind in pop])\n    best = pop[np.argmax(fitnesses)]\n    print(\"[GA] done best score\", fitnesses.max(), \"selected\", int(np.sum(best)))\n    return best\n\n# -----------------------\n# 11) GWO (binary) - reduced\n# -----------------------\ndef run_gwo(wolves=GWO_WOLVES, iters=GWO_ITERS):\n    print(\"[GWO] start: wolves\", wolves, \"iters\", iters)\n    dim = N_FEATURES\n    pack = np.random.randint(0,2,(wolves, dim))\n    fitnesses = np.array([evaluate_mask(ind) for ind in pack])\n    Alpha = Beta = Delta = None\n    Alpha_score = Beta_score = Delta_score = -1.0\n    for itr in range(iters):\n        print(\" GWO iter\", itr+1, \"/\", iters, \"best\", Alpha_score)\n        # update alpha/beta/delta\n        for i in range(wolves):\n            sc = fitnesses[i]\n            if sc > Alpha_score:\n                Delta_score, Beta_score, Alpha_score = Beta_score, Alpha_score, sc\n                Delta, Beta, Alpha = Beta, Alpha, pack[i].copy()\n            elif sc > Beta_score:\n                Delta_score, Beta_score = Beta_score, sc\n                Delta, Beta = Beta, pack[i].copy()\n            elif sc > Delta_score:\n                Delta_score = sc; Delta = pack[i].copy()\n        a = 2 - itr*(2.0/iters)\n        for i in range(wolves):\n            if Alpha is None:\n                continue\n            for d in range(dim):\n                r1, r2 = np.random.rand(), np.random.rand()\n                A1 = 2*a*r1 - a; C1 = 2*r2\n                D_alpha = abs(C1*Alpha[d] - pack[i][d])\n                X1 = Alpha[d] - A1*D_alpha\n                # use X1 approx only (keeps it simple + fast)\n                s = 1.0/(1.0+np.exp(-X1))\n                pack[i][d] = 1 if np.random.rand() < s else 0\n        fitnesses = np.array([evaluate_mask(ind) for ind in pack])\n    best = pack[np.argmax(fitnesses)]\n    print(\"[GWO] done best score\", fitnesses.max(), \"selected\", int(np.sum(best)))\n    return best\n\n# -----------------------\n# 12) RUN OPTIMIZERS (PSO, GA, GWO)\n# -----------------------\nt0 = time.time()\nmask_pso = run_pso()\nmask_ga = run_ga()\nmask_gwo = run_gwo()\nt1 = time.time()\nprint(\"Optimizers finished in\", int(t1-t0), \"s\")\n\n# Save raw masks\nos.makedirs(\"outputs\", exist_ok=True)\npickle.dump({\"mask_pso\": mask_pso.tolist(), \"mask_ga\": mask_ga.tolist(), \"mask_gwo\": mask_gwo.tolist()}, open(os.path.join(\"outputs\", SAVE_PREFIX + \"_raw_masks.pkl\"), \"wb\"))\n\n# -----------------------\n# 13) VOTING (majority >= 2)\n# -----------------------\nvotes = np.array(mask_pso) + np.array(mask_ga) + np.array(mask_gwo)\nvoting_mask = (votes >= 2).astype(int)\nselected_indices = list(np.where(voting_mask == 1)[0])\nselected_features_voting = [FEATURE_NAMES[i] for i in selected_indices]\nprint(\"Voting selected features count:\", len(selected_indices))\nprint(\"Voting selected:\", selected_features_voting)\n\n# Save voting mask\npickle.dump({\"voting_mask\": voting_mask.tolist(), \"selected_features_voting\": selected_features_voting},\n            open(os.path.join(\"outputs\", SAVE_PREFIX + \"_voting.pkl\"), \"wb\"))\n\n# -----------------------\n# 14) HLO on candidate set (candidates = voting selected)\n# -----------------------\ndef hlo_on_candidates(candidate_mask, pop_size=HLO_POP, iters=HLO_ITERS):\n    cand_idxs = np.where(np.array(candidate_mask).astype(bool))[0].tolist()\n    k = len(cand_idxs)\n    if k == 0:\n        raise RuntimeError(\"No candidates for HLO\")\n    print(\"[HLO] start on\", k, \"candidates\")\n    pop = np.random.randint(0,2,(pop_size, k))\n    def fitness_local(bitmask):\n        full = np.zeros(N_FEATURES, dtype=int)\n        for j,b in enumerate(bitmask):\n            if int(b)==1:\n                full[cand_idxs[j]] = 1\n        return evaluate_mask(full)\n    fitness_scores = np.array([fitness_local(ind) for ind in pop])\n    best_idx = int(np.argmax(fitness_scores))\n    best_solution = pop[best_idx].copy()\n    best_score = fitness_scores[best_idx]\n    for it in range(iters):\n        print(\" HLO iter\", it+1, \"/\", iters, \"best\", best_score)\n        teacher = pop[int(np.argmax([fitness_local(x) for x in pop]))].copy()\n        new_pop = []\n        for i in range(pop_size):\n            learner = pop[i].copy()\n            # teaching\n            for d in range(k):\n                if np.random.rand() < 0.75:\n                    learner[d] = teacher[d]\n            # peer learning\n            partner = pop[np.random.randint(pop_size)].copy()\n            for d in range(k):\n                if learner[d] != partner[d] and np.random.rand() < 0.5:\n                    learner[d] = partner[d]\n            # mutation\n            for d in range(k):\n                if np.random.rand() < 0.12:\n                    learner[d] = 1 - learner[d]\n            new_pop.append(learner)\n        pop = np.array(new_pop)\n        fitness_scores = np.array([fitness_local(ind) for ind in pop])\n        gen_best_idx = int(np.argmax(fitness_scores))\n        gen_best_score = fitness_scores[gen_best_idx]\n        gen_best_sol = pop[gen_best_idx].copy()\n        if gen_best_score > best_score:\n            best_score = gen_best_score\n            best_solution = gen_best_sol.copy()\n    final_full = np.zeros(N_FEATURES, dtype=int)\n    for j,b in enumerate(best_solution):\n        if int(b)==1:\n            final_full[cand_idxs[j]] = 1\n    print(\"[HLO] done best local score\", best_score, \"selected\", int(final_full.sum()))\n    return final_full, best_score\n\nhlo_mask, hlo_score = hlo_on_candidates(voting_mask)\npickle.dump({\"hlo_mask\": hlo_mask.tolist(), \"hlo_score\": hlo_score},\n            open(os.path.join(\"outputs\", SAVE_PREFIX + \"_hlo.pkl\"), \"wb\"))\n\n# -----------------------\n# 15) Greedy hill-climb restricted to candidate indices\n# -----------------------\ndef hill_climb(initial_mask, candidate_mask, max_steps=100, eval_cap=500):\n    cand_idxs = np.where(np.array(candidate_mask).astype(bool))[0].tolist()\n    cur = initial_mask.copy()\n    cur_score = evaluate_mask(cur)\n    steps = 0\n    evals = 0\n    improved = True\n    print(\"[HC] start: candidates\", len(cand_idxs))\n    while improved and steps < max_steps and evals < eval_cap:\n        improved = False\n        for idx in np.random.permutation(cand_idxs):\n            trial = cur.copy()\n            trial[idx] = 1 - trial[idx]\n            sc = evaluate_mask(trial)\n            evals += 1\n            if sc > cur_score + 1e-8:\n                cur = trial\n                cur_score = sc\n                improved = True\n                steps += 1\n                print(f\" HC step {steps}: flipped {FEATURE_NAMES[idx]} -> new_score {cur_score:.4f} (evals={evals})\")\n                break\n    print(\"[HC] done steps\", steps, \"evals\", evals, \"final_score\", cur_score, \"selected\", int(cur.sum()))\n    return cur, cur_score\n\nhc_mask, hc_score = hill_climb(hlo_mask, voting_mask)\npickle.dump({\"hc_mask\": hc_mask.tolist(), \"hc_score\": hc_score},\n            open(os.path.join(\"outputs\", SAVE_PREFIX + \"_hc.pkl\"), \"wb\"))\n\n# -----------------------\n# 16) Selected features after hill-climb (final_mask)\n# -----------------------\nfinal_mask = hc_mask\nfinal_selected_indices = np.where(np.array(final_mask).astype(bool))[0].tolist()\nfinal_selected = [FEATURE_NAMES[i] for i in final_selected_indices]\nprint(\"Final selected features:\", final_selected, \"count:\", len(final_selected))\n\n# -----------------------\n# 17) Leakage check: drop single-feature perfect predictors\n# -----------------------\ndef single_feature_predictive_accuracy(feature_series, labels):\n    # map each feature value to most common label for that value, compute accuracy\n    mapping = feature_series.groupby(feature_series).apply(lambda s: labels[s.index].mode().iloc[0])\n    preds = feature_series.map(mapping)\n    return (preds.values == labels.values).mean()\n\n# check each final feature; if single-feature accuracy >= threshold, drop it\nto_drop = []\nfor f in final_selected:\n    acc = single_feature_predictive_accuracy(X_sub[f], y_sub)\n    if acc >= LEAKAGE_SINGLE_FEATURE_THRESHOLD or acc == 1.0:\n        print(f\"Leakage-suspect feature '{f}' single-feature accuracy={acc:.6f} -> will drop\")\n        to_drop.append(f)\n\nif to_drop:\n    final_selected = [f for f in final_selected if f not in to_drop]\n    final_selected_indices = [FEATURE_NAMES.index(f) for f in final_selected]\n    print(\"After dropping leakage suspects, final features:\", final_selected)\n\nif len(final_selected) == 0:\n    raise RuntimeError(\"No safe features remain after leakage check. Consider lowering threshold or manual check.\")\n\n# Save final selected features\npickle.dump({\"final_selected\": final_selected, \"final_mask\": final_mask.tolist()},\n            open(os.path.join(\"outputs\", SAVE_PREFIX + \"_final_selected.pkl\"), \"wb\"))\n\n# -----------------------\n# 18) Prepare FULL dataset with same preprocessing for final training\n# -----------------------\n# Reuse df (full merged) earlier but ensure the same preprocessing as subset\ndf_full = df.copy()\n# Already dropped leak columns earlier and trimmed nulls; ensure same features exist\nmissing_in_full = [f for f in final_selected if f not in df_full.columns]\nif missing_in_full:\n    raise RuntimeError(\"Selected features missing from full dataset: \" + str(missing_in_full))\n\n# Keep only final selected + label\ndf_full = df_full[final_selected + [\"Label\"]].copy()\n\n# Convert object columns to numeric (LabelEncode) and fill NaN\nfor c in df_full.columns:\n    if c != \"Label\" and df_full[c].dtype == \"object\":\n        df_full[c] = LabelEncoder().fit_transform(df_full[c].astype(str))\ndf_full.replace([np.inf, -np.inf], np.nan, inplace=True)\ndf_full.fillna(0, inplace=True)\n\n# Scale numeric columns (MinMax) using full data\nnum_cols = [c for c in final_selected if pd.api.types.is_numeric_dtype(df_full[c])]\nif len(num_cols) > 0:\n    df_full[num_cols] = MinMaxScaler().fit_transform(df_full[num_cols])\n\nX_full = df_full.drop(columns=[\"Label\"])\ny_full = df_full[\"Label\"].astype(int)\nprint(\"Full final training shape:\", X_full.shape, \"Label dist:\", y_full.value_counts().to_dict())\n\n# -----------------------\n# 19) Final train/test split (80/20 stratified) and final CatBoost training with regularization + early stopping\n# -----------------------\nminclass = y_full.value_counts().min()\nif minclass < 10:\n    print(\"Warning: small class size after selecting features:\", minclass)\n\nX_train, X_test, y_train, y_test = train_test_split(X_full, y_full, test_size=0.2, stratify=y_full, random_state=42)\nX_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.15, stratify=y_train, random_state=42)\n\nfinal_params = {\n    \"iterations\": FINAL_CB_ITERS,\n    \"learning_rate\": 0.03,\n    \"depth\": 6,\n    \"l2_leaf_reg\": 7.0,\n    \"bootstrap_type\": \"Bernoulli\",\n    \"subsample\": 0.8,\n    \"random_strength\": 1.0,\n    \"verbose\": 50,\n    \"random_seed\": 42\n}\nfinal_model = CatBoostClassifier(**final_params)\n\nprint(\"Training final model on full data with early stopping...\")\nfinal_model.fit(X_tr, y_tr, eval_set=(X_val, y_val), early_stopping_rounds=FINAL_EARLY_STOP, use_best_model=True)\n\n# Evaluate on hold-out test\ny_pred = final_model.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred, zero_division=0)\nrec = recall_score(y_test, y_pred, zero_division=0)\nf1 = f1_score(y_test, y_pred, zero_division=0)\nprint(\"\\n=== FINAL HOLDOUT METRICS ===\")\nprint(\"Accuracy:\", acc)\nprint(\"Precision:\", prec)\nprint(\"Recall:\", rec)\nprint(\"F1:\", f1)\nprint(\"\\nClassification report:\\n\", classification_report(y_test, y_pred))\n\n# Quick 5-fold CV estimate (fast: reduced iters)\ncv_model = CatBoostClassifier(iterations=200, learning_rate=0.03, depth=6, l2_leaf_reg=7.0,\n                              bootstrap_type=\"Bernoulli\", subsample=0.8, random_seed=42, verbose=0)\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\naccs = cross_val_score(cv_model, X_full, y_full, cv=skf, scoring=\"accuracy\", n_jobs=-1)\nf1s = cross_val_score(cv_model, X_full, y_full, cv=skf, scoring=make_scorer(f1_score), n_jobs=-1)\nprint(\"\\n5-fold CV (quick estimate) -> Accuracy: %.4f ± %.4f ; F1: %.4f ± %.4f\" % (accs.mean(), accs.std(), f1s.mean(), f1s.std()))\n\n# -----------------------\n# 20) Save final model & selected features\n# -----------------------\npickle.dump({\"model\": final_model, \"features\": final_selected, \"mask\": final_mask.tolist()},\n            open(os.path.join(\"outputs\", SAVE_PREFIX + \"_final_model.pkl\"), \"wb\"))\nprint(\"Saved final model + features -> outputs/{}_final_model.pkl\".format(SAVE_PREFIX))\n\nprint(\"PIPELINE COMPLETE\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T17:22:47.241878Z","iopub.execute_input":"2025-12-03T17:22:47.242503Z","iopub.status.idle":"2025-12-03T17:26:16.794841Z","shell.execute_reply.started":"2025-12-03T17:22:47.242477Z","shell.execute_reply":"2025-12-03T17:26:16.794176Z"}},"outputs":[{"name":"stdout","text":"DATA_PATH: /kaggle/input/cicddos2019\nFound 1 CSV files. Loading & merging (may take a bit)...\n -> Random_combine_final.csv\nMerged dataset shape: (300000, 88)\nUsing target column 'Label' (original: Label)\nLabel counts (full):\n Label\n1    299513\n0       487\nName: count, dtype: int64\nDropping likely-leakage columns (ids/timestamps/ips): ['Flow ID', 'Source IP', 'Destination IP', 'Timestamp']\nAfter basic cleaning: (290753, 84)\nOptimization subset shape: (1980, 84) Label counts: {1: 1500, 0: 480}\nSubset features: 83\n[PSO] start: swarm 8 iters 6\n PSO iter 1 / 6 best 0.9993342210386151\n PSO iter 2 / 6 best 0.9993342210386151\n PSO iter 3 / 6 best 0.9993342210386151\n PSO iter 4 / 6 best 0.9993342210386151\n PSO iter 5 / 6 best 0.9993342210386151\n PSO iter 6 / 6 best 0.9993342210386151\n[PSO] done best score 1.0 selected 40\n[GA] start: pop 12 gens 6\n GA gen 1 / 6 best 0.9996668887408395\n GA gen 2 / 6 best 0.9996668887408395\n GA gen 3 / 6 best 1.0\n GA gen 4 / 6 best 1.0\n GA gen 5 / 6 best 1.0\n GA gen 6 / 6 best 1.0\n[GA] done best score 1.0 selected 46\n[GWO] start: wolves 8 iters 6\n GWO iter 1 / 6 best -1.0\n GWO iter 2 / 6 best 1.0\n GWO iter 3 / 6 best 1.0\n GWO iter 4 / 6 best 1.0\n GWO iter 5 / 6 best 1.0\n GWO iter 6 / 6 best 1.0\n[GWO] done best score 1.0 selected 52\nOptimizers finished in 91 s\nVoting selected features count: 50\nVoting selected: ['Unnamed: 0', 'Source Port', 'Destination Port', 'Total Fwd Packets', 'Total Backward Packets', 'Fwd Packet Length Max', 'Fwd Packet Length Min', 'Fwd Packet Length Mean', 'Bwd Packet Length Max', 'Bwd Packet Length Mean', 'Flow Packets/s', 'Flow IAT Std', 'Flow IAT Min', 'Fwd IAT Total', 'Bwd IAT Total', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Header Length', 'Bwd Header Length', 'Min Packet Length', 'Packet Length Std', 'Packet Length Variance', 'SYN Flag Count', 'RST Flag Count', 'PSH Flag Count', 'ACK Flag Count', 'URG Flag Count', 'ECE Flag Count', 'Down/Up Ratio', 'Average Packet Size', 'Avg Bwd Segment Size', 'Fwd Header Length.1', 'Fwd Avg Packets/Bulk', 'Fwd Avg Bulk Rate', 'Bwd Avg Bulk Rate', 'Subflow Fwd Packets', 'Subflow Bwd Packets', 'Subflow Bwd Bytes', 'Init_Win_bytes_forward', 'min_seg_size_forward', 'Active Mean', 'Active Max', 'Idle Mean', 'Idle Std', 'Idle Min', 'SimillarHTTP']\n[HLO] start on 50 candidates\n HLO iter 1 / 8 best 0.9996668887408395\n HLO iter 2 / 8 best 1.0\n HLO iter 3 / 8 best 1.0\n HLO iter 4 / 8 best 1.0\n HLO iter 5 / 8 best 1.0\n HLO iter 6 / 8 best 1.0\n HLO iter 7 / 8 best 1.0\n HLO iter 8 / 8 best 1.0\n[HLO] done best local score 1.0 selected 26\n[HC] start: candidates 50\n[HC] done steps 0 evals 50 final_score 1.0 selected 26\nFinal selected features: ['Unnamed: 0', 'Source Port', 'Destination Port', 'Fwd Packet Length Mean', 'Bwd Packet Length Mean', 'Fwd IAT Total', 'Bwd IAT Total', 'Bwd IAT Max', 'Fwd URG Flags', 'Bwd URG Flags', 'Bwd Header Length', 'Min Packet Length', 'Packet Length Variance', 'ECE Flag Count', 'Average Packet Size', 'Avg Bwd Segment Size', 'Fwd Header Length.1', 'Fwd Avg Packets/Bulk', 'Fwd Avg Bulk Rate', 'Subflow Fwd Packets', 'Subflow Bwd Bytes', 'Init_Win_bytes_forward', 'min_seg_size_forward', 'Idle Mean', 'Idle Std', 'SimillarHTTP'] count: 26\nFull final training shape: (290753, 26) Label dist: {1: 290273, 0: 480}\nTraining final model on full data with early stopping...\n0:\tlearn: 0.6025174\ttest: 0.6026089\tbest: 0.6026089 (0)\ttotal: 21.2ms\tremaining: 21.1s\n50:\tlearn: 0.0018134\ttest: 0.0019100\tbest: 0.0019100 (50)\ttotal: 1.04s\tremaining: 19.4s\n100:\tlearn: 0.0007193\ttest: 0.0007965\tbest: 0.0007965 (100)\ttotal: 1.96s\tremaining: 17.5s\n150:\tlearn: 0.0006161\ttest: 0.0006954\tbest: 0.0006954 (150)\ttotal: 2.81s\tremaining: 15.8s\n200:\tlearn: 0.0005720\ttest: 0.0006472\tbest: 0.0006472 (199)\ttotal: 3.65s\tremaining: 14.5s\n250:\tlearn: 0.0005350\ttest: 0.0006074\tbest: 0.0006074 (248)\ttotal: 4.5s\tremaining: 13.4s\n300:\tlearn: 0.0005238\ttest: 0.0005975\tbest: 0.0005975 (300)\ttotal: 5.38s\tremaining: 12.5s\n350:\tlearn: 0.0005061\ttest: 0.0005815\tbest: 0.0005815 (350)\ttotal: 6.18s\tremaining: 11.4s\n400:\tlearn: 0.0004774\ttest: 0.0005567\tbest: 0.0005567 (400)\ttotal: 7s\tremaining: 10.5s\n450:\tlearn: 0.0004740\ttest: 0.0005532\tbest: 0.0005532 (450)\ttotal: 7.81s\tremaining: 9.51s\n500:\tlearn: 0.0004669\ttest: 0.0005458\tbest: 0.0005458 (500)\ttotal: 8.61s\tremaining: 8.58s\n550:\tlearn: 0.0004600\ttest: 0.0005379\tbest: 0.0005379 (550)\ttotal: 9.42s\tremaining: 7.67s\n600:\tlearn: 0.0004506\ttest: 0.0005272\tbest: 0.0005272 (600)\ttotal: 10.2s\tremaining: 6.79s\n650:\tlearn: 0.0004453\ttest: 0.0005223\tbest: 0.0005223 (650)\ttotal: 11s\tremaining: 5.92s\n700:\tlearn: 0.0004306\ttest: 0.0005056\tbest: 0.0005056 (700)\ttotal: 11.9s\tremaining: 5.05s\n750:\tlearn: 0.0004255\ttest: 0.0005016\tbest: 0.0005016 (749)\ttotal: 12.7s\tremaining: 4.19s\n800:\tlearn: 0.0004243\ttest: 0.0005003\tbest: 0.0005003 (800)\ttotal: 13.5s\tremaining: 3.35s\n850:\tlearn: 0.0004228\ttest: 0.0004982\tbest: 0.0004982 (850)\ttotal: 14.3s\tremaining: 2.5s\n900:\tlearn: 0.0004170\ttest: 0.0004920\tbest: 0.0004920 (900)\ttotal: 15.1s\tremaining: 1.66s\n950:\tlearn: 0.0004095\ttest: 0.0004840\tbest: 0.0004840 (950)\ttotal: 16s\tremaining: 823ms\n999:\tlearn: 0.0004090\ttest: 0.0004836\tbest: 0.0004836 (999)\ttotal: 16.8s\tremaining: 0us\n\nbestTest = 0.0004835560535\nbestIteration = 999\n\n\n=== FINAL HOLDOUT METRICS ===\nAccuracy: 0.9997764440852264\nPrecision: 0.9999138703231586\nRecall: 0.999862199638274\nF1: 0.9998880343131766\n\nClassification report:\n               precision    recall  f1-score   support\n\n           0       0.92      0.95      0.93        96\n           1       1.00      1.00      1.00     58055\n\n    accuracy                           1.00     58151\n   macro avg       0.96      0.97      0.97     58151\nweighted avg       1.00      1.00      1.00     58151\n\n\n5-fold CV (quick estimate) -> Accuracy: 0.9998 ± 0.0001 ; F1: 0.9999 ± 0.0000\nSaved final model + features -> outputs/ddos_hybrid_voting_hlo_final_model.pkl\nPIPELINE COMPLETE\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"#VOTING FOR 20 ITERATIONS\n\n\n# hybrid_voting_hlo_ddos_pipeline.py\n# Single-file: PSO + GA + GWO -> VOTING -> HLO -> Hill-climb -> Final CatBoost\n# Option A: optimization subset = 3000 rows (1500 benign + 1500 attack)\n\nimport kagglehub\nimport glob, os, time, pickle, warnings\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\nfrom sklearn.model_selection import StratifiedKFold, train_test_split, cross_val_score\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, make_scorer\nfrom sklearn.base import clone\nfrom catboost import CatBoostClassifier\n\nwarnings.filterwarnings(\"ignore\")\nnp.random.seed(42)\n\n# -----------------------\n# USER CONFIG\n# -----------------------\nDATA_PATH = kagglehub.dataset_download(\"sizlingdhairya1/cicddos2019\")  # your loader\nOPT_SUBSET_PER_CLASS = 1500     # 1500 benign + 1500 attack => 3000 rows\nPSO_SWARM = 8\nPSO_ITERS = 20\nGA_POP = 12\nGA_GENS = 20\nGWO_WOLVES = 8\nGWO_ITERS = 20\nHLO_POP = 8\nHLO_ITERS = 10\nFIT_CB_ITERS_OPT = 80    # CatBoost iterations used inside fitness (fast)\nCV_OPT = 2               # cheap CV inside optimizer\nFINAL_CB_ITERS = 1000    # final model iterations (early stopping used)\nFINAL_EARLY_STOP = 50\nSAVE_PREFIX = \"ddos_hybrid_voting_hlo\"\nLEAKAGE_SINGLE_FEATURE_THRESHOLD = 0.99999  # single-feature accuracy threshold to treat as leakage\n\nprint(\"DATA_PATH:\", DATA_PATH)\n\n# -----------------------\n# 1) load all CSVs from dataset path\n# -----------------------\ncsv_files = glob.glob(os.path.join(DATA_PATH, \"*.csv\"))\nif len(csv_files) == 0:\n    raise RuntimeError(\"No CSV files found in DATA_PATH: \" + DATA_PATH)\n\nprint(f\"Found {len(csv_files)} CSV files. Loading & merging (may take a bit)...\")\ndfs = []\nfor f in csv_files:\n    print(\" ->\", os.path.basename(f))\n    dfs.append(pd.read_csv(f, low_memory=False))\ndf = pd.concat(dfs, ignore_index=True)\nprint(\"Merged dataset shape:\", df.shape)\n\n# -----------------------\n# 2) normalize columns and find label\n# -----------------------\ndf.columns = df.columns.str.strip()\nTARGET_CANDIDATES = [\"Label\", \"label\", \" Attack\", \"attack_cat\", \"Label \"]\nfound_label = None\nfor c in [\"Label\", \"label\", \"Attack\", \"attack\", \"attack_cat\"]:\n    if c in df.columns:\n        found_label = c\n        break\nif found_label is None:\n    # try case-insensitive lookup\n    for c in df.columns:\n        if c.strip().lower() == \"label\" or c.strip().lower() == \"attack\":\n            found_label = c\n            break\nif found_label is None:\n    raise RuntimeError(\"Cannot find label column. Columns available: \" + \", \".join(df.columns[:30]))\n\n# normalize label column name\ndf.rename(columns={found_label: \"Label\"}, inplace=True)\nprint(\"Using target column 'Label' (original: {})\".format(found_label))\n\n# -----------------------\n# 3) keep only rows with non-null label and make binary target\n# -----------------------\ndf = df[df[\"Label\"].notna()].copy()\ndf[\"Label\"] = df[\"Label\"].astype(str).str.strip().str.lower()\n# convert benign -> 0 else -> 1 (attack)\ndf[\"Label\"] = df[\"Label\"].apply(lambda x: 0 if x == \"benign\" else 1)\nprint(\"Label counts (full):\\n\", df[\"Label\"].value_counts())\n\n# -----------------------\n# 4) drop obviously leaking columns if present (IDs, IPs, timestamps)\n# -----------------------\npossible_leak_cols = [c for c in df.columns if c.strip().lower() in (\n    \"id\", \"flow id\", \"flowid\", \"timestamp\", \"ts\", \"source ip\", \"destination ip\",\n    \"src ip\", \"dst ip\", \"sourceip\", \"destinationip\", \"srcip\", \"dstip\")]\nif possible_leak_cols:\n    print(\"Dropping likely-leakage columns (ids/timestamps/ips):\", possible_leak_cols)\n    df.drop(columns=[c for c in possible_leak_cols if c in df.columns], inplace=True)\n\n# -----------------------\n# 5) basic cleaning: drop all-empty columns, replace inf, drop rows with NaN\n# -----------------------\ndf.replace([np.inf, -np.inf], np.nan, inplace=True)\ndf.dropna(axis=1, how=\"all\", inplace=True)\ndf.dropna(axis=0, how=\"any\", inplace=True)   # safe because we'll use full dataset for final, but optimizer needs no missing\nprint(\"After basic cleaning:\", df.shape)\n\n# -----------------------\n# 6) create balanced small subset for optimization: OPT_SUBSET_PER_CLASS * 2 rows\n# -----------------------\ncounts = df[\"Label\"].value_counts().to_dict()\nn_attack = counts.get(1, 0)\nn_benign = counts.get(0, 0)\ntake_attack = min(OPT_SUBSET_PER_CLASS, n_attack)\ntake_benign = min(OPT_SUBSET_PER_CLASS, n_benign)\nif take_attack < 10 or take_benign < 10:\n    raise RuntimeError(\"Not enough rows in one class to form the optimization subset. counts=\" + str(counts))\n\ndf_attack = df[df[\"Label\"] == 1].sample(take_attack, random_state=42)\ndf_benign = df[df[\"Label\"] == 0].sample(take_benign, random_state=42)\ndf_sub = pd.concat([df_attack, df_benign], ignore_index=True).sample(frac=1.0, random_state=42).reset_index(drop=True)\nprint(\"Optimization subset shape:\", df_sub.shape, \"Label counts:\", df_sub[\"Label\"].value_counts().to_dict())\n\n# -----------------------\n# 7) preprocess subset: encode categorical & scale numeric\n# -----------------------\nTARGET_COL = \"Label\"\nX_sub = df_sub.drop(columns=[TARGET_COL]).copy()\ny_sub = df_sub[TARGET_COL].astype(int).copy()\n\n# encode object columns\nobj_cols = X_sub.select_dtypes(include=[\"object\"]).columns.tolist()\nfor c in obj_cols:\n    X_sub[c] = LabelEncoder().fit_transform(X_sub[c].astype(str))\n# numeric scaling\nnum_cols_sub = X_sub.select_dtypes(include=[np.number]).columns.tolist()\nif len(num_cols_sub) > 0:\n    X_sub[num_cols_sub] = MinMaxScaler().fit_transform(X_sub[num_cols_sub])\n\nFEATURE_NAMES = X_sub.columns.tolist()\nN_FEATURES = len(FEATURE_NAMES)\nprint(\"Subset features:\", N_FEATURES)\n\n# -----------------------\n# 8) CatBoost factory & fitness function with caching\n# -----------------------\ndef get_catboost_model(iterations=FIT_CB_ITERS_OPT):\n    return CatBoostClassifier(iterations=iterations, learning_rate=0.05, depth=6,\n                              verbose=0, random_seed=42)\n\nfitness_cache = {}\ndef evaluate_mask(mask_bool, cv=CV_OPT, cb_iter=FIT_CB_ITERS_OPT):\n    key = tuple(int(x) for x in mask_bool)\n    if key in fitness_cache:\n        return fitness_cache[key]\n    idxs = [i for i,b in enumerate(key) if b==1]\n    if len(idxs) == 0:\n        fitness_cache[key] = 0.0\n        return 0.0\n    Xsel = X_sub.iloc[:, idxs]\n    model = get_catboost_model(iterations=cb_iter)\n    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=42)\n    try:\n        scores = cross_val_score(clone(model), Xsel, y_sub, cv=skf, scoring=make_scorer(f1_score), n_jobs=-1)\n    except Exception as e:\n        # if CatBoost fails (e.g. unexpected types), return 0\n        fitness_cache[key] = 0.0\n        return 0.0\n    val = float(np.mean(scores))\n    fitness_cache[key] = val\n    return val\n\n# -----------------------\n# 9) PSO (binary) - reduced\n# -----------------------\ndef run_pso(swarm_size=PSO_SWARM, iters=PSO_ITERS):\n    print(\"[PSO] start: swarm\", swarm_size, \"iters\", iters)\n    dim = N_FEATURES\n    pos = np.random.randint(0,2,(swarm_size,dim))\n    vel = np.random.uniform(-1,1,(swarm_size,dim))\n    pbest = pos.copy()\n    pbest_scores = np.array([evaluate_mask(p) for p in pbest])\n    gbest_idx = int(np.argmax(pbest_scores))\n    gbest = pbest[gbest_idx].copy()\n    gbest_score = pbest_scores[gbest_idx]\n    w = 0.6; c1 = c2 = 1.5\n    for t in range(iters):\n        print(\" PSO iter\", t+1, \"/\", iters, \"best\", gbest_score)\n        for i in range(swarm_size):\n            r1 = np.random.rand(dim); r2 = np.random.rand(dim)\n            vel[i] = w*vel[i] + c1*r1*(pbest[i] - pos[i]) + c2*r2*(gbest - pos[i])\n            s = 1.0 / (1.0 + np.exp(-vel[i]))\n            pos[i] = (np.random.rand(dim) < s).astype(int)\n            sc = evaluate_mask(pos[i])\n            if sc > pbest_scores[i]:\n                pbest[i] = pos[i].copy(); pbest_scores[i] = sc\n            if sc > gbest_score:\n                gbest = pos[i].copy(); gbest_score = sc\n        w = max(0.2, w*0.97)\n    print(\"[PSO] done best score\", gbest_score, \"selected\", int(np.sum(gbest)))\n    return gbest\n\n# -----------------------\n# 10) GA (binary) - reduced\n# -----------------------\ndef run_ga(pop_size=GA_POP, gens=GA_GENS):\n    print(\"[GA] start: pop\", pop_size, \"gens\", gens)\n    dim = N_FEATURES\n    pop = np.random.randint(0,2,(pop_size, dim))\n    fitnesses = np.array([evaluate_mask(ind) for ind in pop])\n    for g in range(gens):\n        print(\" GA gen\", g+1, \"/\", gens, \"best\", fitnesses.max())\n        elite_idxs = np.argsort(fitnesses)[-2:]\n        new_pop = [pop[elite_idxs[0]].copy(), pop[elite_idxs[1]].copy()]\n        while len(new_pop) < pop_size:\n            p1 = pop[np.random.randint(pop_size)].copy()\n            p2 = pop[np.random.randint(pop_size)].copy()\n            if np.random.rand() < 0.7:\n                pt = np.random.randint(1, dim)\n                child = np.concatenate([p1[:pt], p2[pt:]])\n            else:\n                child = p1\n            # mutation\n            for d in range(dim):\n                if np.random.rand() < 0.05:\n                    child[d] = 1-child[d]\n            new_pop.append(child)\n        pop = np.array(new_pop[:pop_size])\n        fitnesses = np.array([evaluate_mask(ind) for ind in pop])\n    best = pop[np.argmax(fitnesses)]\n    print(\"[GA] done best score\", fitnesses.max(), \"selected\", int(np.sum(best)))\n    return best\n\n# -----------------------\n# 11) GWO (binary) - reduced\n# -----------------------\ndef run_gwo(wolves=GWO_WOLVES, iters=GWO_ITERS):\n    print(\"[GWO] start: wolves\", wolves, \"iters\", iters)\n    dim = N_FEATURES\n    pack = np.random.randint(0,2,(wolves, dim))\n    fitnesses = np.array([evaluate_mask(ind) for ind in pack])\n    Alpha = Beta = Delta = None\n    Alpha_score = Beta_score = Delta_score = -1.0\n    for itr in range(iters):\n        print(\" GWO iter\", itr+1, \"/\", iters, \"best\", Alpha_score)\n        # update alpha/beta/delta\n        for i in range(wolves):\n            sc = fitnesses[i]\n            if sc > Alpha_score:\n                Delta_score, Beta_score, Alpha_score = Beta_score, Alpha_score, sc\n                Delta, Beta, Alpha = Beta, Alpha, pack[i].copy()\n            elif sc > Beta_score:\n                Delta_score, Beta_score = Beta_score, sc\n                Delta, Beta = Beta, pack[i].copy()\n            elif sc > Delta_score:\n                Delta_score = sc; Delta = pack[i].copy()\n        a = 2 - itr*(2.0/iters)\n        for i in range(wolves):\n            if Alpha is None:\n                continue\n            for d in range(dim):\n                r1, r2 = np.random.rand(), np.random.rand()\n                A1 = 2*a*r1 - a; C1 = 2*r2\n                D_alpha = abs(C1*Alpha[d] - pack[i][d])\n                X1 = Alpha[d] - A1*D_alpha\n                # use X1 approx only (keeps it simple + fast)\n                s = 1.0/(1.0+np.exp(-X1))\n                pack[i][d] = 1 if np.random.rand() < s else 0\n        fitnesses = np.array([evaluate_mask(ind) for ind in pack])\n    best = pack[np.argmax(fitnesses)]\n    print(\"[GWO] done best score\", fitnesses.max(), \"selected\", int(np.sum(best)))\n    return best\n\n# -----------------------\n# 12) RUN OPTIMIZERS (PSO, GA, GWO)\n# -----------------------\nt0 = time.time()\nmask_pso = run_pso()\nmask_ga = run_ga()\nmask_gwo = run_gwo()\nt1 = time.time()\nprint(\"Optimizers finished in\", int(t1-t0), \"s\")\n\n# Save raw masks\nos.makedirs(\"outputs\", exist_ok=True)\npickle.dump({\"mask_pso\": mask_pso.tolist(), \"mask_ga\": mask_ga.tolist(), \"mask_gwo\": mask_gwo.tolist()}, open(os.path.join(\"outputs\", SAVE_PREFIX + \"_raw_masks.pkl\"), \"wb\"))\n\n# -----------------------\n# 13) VOTING (majority >= 2)\n# -----------------------\nvotes = np.array(mask_pso) + np.array(mask_ga) + np.array(mask_gwo)\nvoting_mask = (votes >= 2).astype(int)\nselected_indices = list(np.where(voting_mask == 1)[0])\nselected_features_voting = [FEATURE_NAMES[i] for i in selected_indices]\nprint(\"Voting selected features count:\", len(selected_indices))\nprint(\"Voting selected:\", selected_features_voting)\n\n# Save voting mask\npickle.dump({\"voting_mask\": voting_mask.tolist(), \"selected_features_voting\": selected_features_voting},\n            open(os.path.join(\"outputs\", SAVE_PREFIX + \"_voting.pkl\"), \"wb\"))\n\n# -----------------------\n# 14) HLO on candidate set (candidates = voting selected)\n# -----------------------\ndef hlo_on_candidates(candidate_mask, pop_size=HLO_POP, iters=HLO_ITERS):\n    cand_idxs = np.where(np.array(candidate_mask).astype(bool))[0].tolist()\n    k = len(cand_idxs)\n    if k == 0:\n        raise RuntimeError(\"No candidates for HLO\")\n    print(\"[HLO] start on\", k, \"candidates\")\n    pop = np.random.randint(0,2,(pop_size, k))\n    def fitness_local(bitmask):\n        full = np.zeros(N_FEATURES, dtype=int)\n        for j,b in enumerate(bitmask):\n            if int(b)==1:\n                full[cand_idxs[j]] = 1\n        return evaluate_mask(full)\n    fitness_scores = np.array([fitness_local(ind) for ind in pop])\n    best_idx = int(np.argmax(fitness_scores))\n    best_solution = pop[best_idx].copy()\n    best_score = fitness_scores[best_idx]\n    for it in range(iters):\n        print(\" HLO iter\", it+1, \"/\", iters, \"best\", best_score)\n        teacher = pop[int(np.argmax([fitness_local(x) for x in pop]))].copy()\n        new_pop = []\n        for i in range(pop_size):\n            learner = pop[i].copy()\n            # teaching\n            for d in range(k):\n                if np.random.rand() < 0.75:\n                    learner[d] = teacher[d]\n            # peer learning\n            partner = pop[np.random.randint(pop_size)].copy()\n            for d in range(k):\n                if learner[d] != partner[d] and np.random.rand() < 0.5:\n                    learner[d] = partner[d]\n            # mutation\n            for d in range(k):\n                if np.random.rand() < 0.12:\n                    learner[d] = 1 - learner[d]\n            new_pop.append(learner)\n        pop = np.array(new_pop)\n        fitness_scores = np.array([fitness_local(ind) for ind in pop])\n        gen_best_idx = int(np.argmax(fitness_scores))\n        gen_best_score = fitness_scores[gen_best_idx]\n        gen_best_sol = pop[gen_best_idx].copy()\n        if gen_best_score > best_score:\n            best_score = gen_best_score\n            best_solution = gen_best_sol.copy()\n    final_full = np.zeros(N_FEATURES, dtype=int)\n    for j,b in enumerate(best_solution):\n        if int(b)==1:\n            final_full[cand_idxs[j]] = 1\n    print(\"[HLO] done best local score\", best_score, \"selected\", int(final_full.sum()))\n    return final_full, best_score\n\nhlo_mask, hlo_score = hlo_on_candidates(voting_mask)\npickle.dump({\"hlo_mask\": hlo_mask.tolist(), \"hlo_score\": hlo_score},\n            open(os.path.join(\"outputs\", SAVE_PREFIX + \"_hlo.pkl\"), \"wb\"))\n\n# -----------------------\n# 15) Greedy hill-climb restricted to candidate indices\n# -----------------------\ndef hill_climb(initial_mask, candidate_mask, max_steps=100, eval_cap=500):\n    cand_idxs = np.where(np.array(candidate_mask).astype(bool))[0].tolist()\n    cur = initial_mask.copy()\n    cur_score = evaluate_mask(cur)\n    steps = 0\n    evals = 0\n    improved = True\n    print(\"[HC] start: candidates\", len(cand_idxs))\n    while improved and steps < max_steps and evals < eval_cap:\n        improved = False\n        for idx in np.random.permutation(cand_idxs):\n            trial = cur.copy()\n            trial[idx] = 1 - trial[idx]\n            sc = evaluate_mask(trial)\n            evals += 1\n            if sc > cur_score + 1e-8:\n                cur = trial\n                cur_score = sc\n                improved = True\n                steps += 1\n                print(f\" HC step {steps}: flipped {FEATURE_NAMES[idx]} -> new_score {cur_score:.4f} (evals={evals})\")\n                break\n    print(\"[HC] done steps\", steps, \"evals\", evals, \"final_score\", cur_score, \"selected\", int(cur.sum()))\n    return cur, cur_score\n\nhc_mask, hc_score = hill_climb(hlo_mask, voting_mask)\npickle.dump({\"hc_mask\": hc_mask.tolist(), \"hc_score\": hc_score},\n            open(os.path.join(\"outputs\", SAVE_PREFIX + \"_hc.pkl\"), \"wb\"))\n\n# -----------------------\n# 16) Selected features after hill-climb (final_mask)\n# -----------------------\nfinal_mask = hc_mask\nfinal_selected_indices = np.where(np.array(final_mask).astype(bool))[0].tolist()\nfinal_selected = [FEATURE_NAMES[i] for i in final_selected_indices]\nprint(\"Final selected features:\", final_selected, \"count:\", len(final_selected))\n\n# -----------------------\n# 17) Leakage check: drop single-feature perfect predictors\n# -----------------------\ndef single_feature_predictive_accuracy(feature_series, labels):\n    # map each feature value to most common label for that value, compute accuracy\n    mapping = feature_series.groupby(feature_series).apply(lambda s: labels[s.index].mode().iloc[0])\n    preds = feature_series.map(mapping)\n    return (preds.values == labels.values).mean()\n\n# check each final feature; if single-feature accuracy >= threshold, drop it\nto_drop = []\nfor f in final_selected:\n    acc = single_feature_predictive_accuracy(X_sub[f], y_sub)\n    if acc >= LEAKAGE_SINGLE_FEATURE_THRESHOLD or acc == 1.0:\n        print(f\"Leakage-suspect feature '{f}' single-feature accuracy={acc:.6f} -> will drop\")\n        to_drop.append(f)\n\nif to_drop:\n    final_selected = [f for f in final_selected if f not in to_drop]\n    final_selected_indices = [FEATURE_NAMES.index(f) for f in final_selected]\n    print(\"After dropping leakage suspects, final features:\", final_selected)\n\nif len(final_selected) == 0:\n    raise RuntimeError(\"No safe features remain after leakage check. Consider lowering threshold or manual check.\")\n\n# Save final selected features\npickle.dump({\"final_selected\": final_selected, \"final_mask\": final_mask.tolist()},\n            open(os.path.join(\"outputs\", SAVE_PREFIX + \"_final_selected.pkl\"), \"wb\"))\n\n# -----------------------\n# 18) Prepare FULL dataset with same preprocessing for final training\n# -----------------------\n# Reuse df (full merged) earlier but ensure the same preprocessing as subset\ndf_full = df.copy()\n# Already dropped leak columns earlier and trimmed nulls; ensure same features exist\nmissing_in_full = [f for f in final_selected if f not in df_full.columns]\nif missing_in_full:\n    raise RuntimeError(\"Selected features missing from full dataset: \" + str(missing_in_full))\n\n# Keep only final selected + label\ndf_full = df_full[final_selected + [\"Label\"]].copy()\n\n# Convert object columns to numeric (LabelEncode) and fill NaN\nfor c in df_full.columns:\n    if c != \"Label\" and df_full[c].dtype == \"object\":\n        df_full[c] = LabelEncoder().fit_transform(df_full[c].astype(str))\ndf_full.replace([np.inf, -np.inf], np.nan, inplace=True)\ndf_full.fillna(0, inplace=True)\n\n# Scale numeric columns (MinMax) using full data\nnum_cols = [c for c in final_selected if pd.api.types.is_numeric_dtype(df_full[c])]\nif len(num_cols) > 0:\n    df_full[num_cols] = MinMaxScaler().fit_transform(df_full[num_cols])\n\nX_full = df_full.drop(columns=[\"Label\"])\ny_full = df_full[\"Label\"].astype(int)\nprint(\"Full final training shape:\", X_full.shape, \"Label dist:\", y_full.value_counts().to_dict())\n\n# -----------------------\n# 19) Final train/test split (80/20 stratified) and final CatBoost training with regularization + early stopping\n# -----------------------\nminclass = y_full.value_counts().min()\nif minclass < 10:\n    print(\"Warning: small class size after selecting features:\", minclass)\n\nX_train, X_test, y_train, y_test = train_test_split(X_full, y_full, test_size=0.2, stratify=y_full, random_state=42)\nX_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.15, stratify=y_train, random_state=42)\n\nfinal_params = {\n    \"iterations\": FINAL_CB_ITERS,\n    \"learning_rate\": 0.03,\n    \"depth\": 6,\n    \"l2_leaf_reg\": 7.0,\n    \"bootstrap_type\": \"Bernoulli\",\n    \"subsample\": 0.8,\n    \"random_strength\": 1.0,\n    \"verbose\": 50,\n    \"random_seed\": 42\n}\nfinal_model = CatBoostClassifier(**final_params)\n\nprint(\"Training final model on full data with early stopping...\")\nfinal_model.fit(X_tr, y_tr, eval_set=(X_val, y_val), early_stopping_rounds=FINAL_EARLY_STOP, use_best_model=True)\n\n# Evaluate on hold-out test\ny_pred = final_model.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred, zero_division=0)\nrec = recall_score(y_test, y_pred, zero_division=0)\nf1 = f1_score(y_test, y_pred, zero_division=0)\nprint(\"\\n=== FINAL HOLDOUT METRICS ===\")\nprint(\"Accuracy:\", acc)\nprint(\"Precision:\", prec)\nprint(\"Recall:\", rec)\nprint(\"F1:\", f1)\nprint(\"\\nClassification report:\\n\", classification_report(y_test, y_pred))\n\n# Quick 5-fold CV estimate (fast: reduced iters)\ncv_model = CatBoostClassifier(iterations=200, learning_rate=0.03, depth=6, l2_leaf_reg=7.0,\n                              bootstrap_type=\"Bernoulli\", subsample=0.8, random_seed=42, verbose=0)\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\naccs = cross_val_score(cv_model, X_full, y_full, cv=skf, scoring=\"accuracy\", n_jobs=-1)\nf1s = cross_val_score(cv_model, X_full, y_full, cv=skf, scoring=make_scorer(f1_score), n_jobs=-1)\nprint(\"\\n5-fold CV (quick estimate) -> Accuracy: %.4f ± %.4f ; F1: %.4f ± %.4f\" % (accs.mean(), accs.std(), f1s.mean(), f1s.std()))\n\n# -----------------------\n# 20) Save final model & selected features\n# -----------------------\npickle.dump({\"model\": final_model, \"features\": final_selected, \"mask\": final_mask.tolist()},\n            open(os.path.join(\"outputs\", SAVE_PREFIX + \"_final_model.pkl\"), \"wb\"))\nprint(\"Saved final model + features -> outputs/{}_final_model.pkl\".format(SAVE_PREFIX))\n\nprint(\"PIPELINE COMPLETE\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T17:30:08.904623Z","iopub.execute_input":"2025-12-03T17:30:08.905406Z","iopub.status.idle":"2025-12-03T17:36:25.415705Z","shell.execute_reply.started":"2025-12-03T17:30:08.905380Z","shell.execute_reply":"2025-12-03T17:36:25.414924Z"}},"outputs":[{"name":"stdout","text":"DATA_PATH: /kaggle/input/cicddos2019\nFound 1 CSV files. Loading & merging (may take a bit)...\n -> Random_combine_final.csv\nMerged dataset shape: (300000, 88)\nUsing target column 'Label' (original: Label)\nLabel counts (full):\n Label\n1    299513\n0       487\nName: count, dtype: int64\nDropping likely-leakage columns (ids/timestamps/ips): ['Flow ID', 'Source IP', 'Destination IP', 'Timestamp']\nAfter basic cleaning: (290753, 84)\nOptimization subset shape: (1980, 84) Label counts: {1: 1500, 0: 480}\nSubset features: 83\n[PSO] start: swarm 8 iters 20\n PSO iter 1 / 20 best 0.9993342210386151\n PSO iter 2 / 20 best 0.9993342210386151\n PSO iter 3 / 20 best 0.9993342210386151\n PSO iter 4 / 20 best 0.9993342210386151\n PSO iter 5 / 20 best 0.9993342210386151\n PSO iter 6 / 20 best 0.9993342210386151\n PSO iter 7 / 20 best 1.0\n PSO iter 8 / 20 best 1.0\n PSO iter 9 / 20 best 1.0\n PSO iter 10 / 20 best 1.0\n PSO iter 11 / 20 best 1.0\n PSO iter 12 / 20 best 1.0\n PSO iter 13 / 20 best 1.0\n PSO iter 14 / 20 best 1.0\n PSO iter 15 / 20 best 1.0\n PSO iter 16 / 20 best 1.0\n PSO iter 17 / 20 best 1.0\n PSO iter 18 / 20 best 1.0\n PSO iter 19 / 20 best 1.0\n PSO iter 20 / 20 best 1.0\n[PSO] done best score 1.0 selected 40\n[GA] start: pop 12 gens 20\n GA gen 1 / 20 best 0.9996668887408395\n GA gen 2 / 20 best 0.9996668887408395\n GA gen 3 / 20 best 1.0\n GA gen 4 / 20 best 1.0\n GA gen 5 / 20 best 1.0\n GA gen 6 / 20 best 1.0\n GA gen 7 / 20 best 1.0\n GA gen 8 / 20 best 1.0\n GA gen 9 / 20 best 1.0\n GA gen 10 / 20 best 1.0\n GA gen 11 / 20 best 1.0\n GA gen 12 / 20 best 1.0\n GA gen 13 / 20 best 1.0\n GA gen 14 / 20 best 1.0\n GA gen 15 / 20 best 1.0\n GA gen 16 / 20 best 1.0\n GA gen 17 / 20 best 1.0\n GA gen 18 / 20 best 1.0\n GA gen 19 / 20 best 1.0\n GA gen 20 / 20 best 1.0\n[GA] done best score 1.0 selected 38\n[GWO] start: wolves 8 iters 20\n GWO iter 1 / 20 best -1.0\n GWO iter 2 / 20 best 0.9993342210386151\n GWO iter 3 / 20 best 0.9996668887408395\n GWO iter 4 / 20 best 1.0\n GWO iter 5 / 20 best 1.0\n GWO iter 6 / 20 best 1.0\n GWO iter 7 / 20 best 1.0\n GWO iter 8 / 20 best 1.0\n GWO iter 9 / 20 best 1.0\n GWO iter 10 / 20 best 1.0\n GWO iter 11 / 20 best 1.0\n GWO iter 12 / 20 best 1.0\n GWO iter 13 / 20 best 1.0\n GWO iter 14 / 20 best 1.0\n GWO iter 15 / 20 best 1.0\n GWO iter 16 / 20 best 1.0\n GWO iter 17 / 20 best 1.0\n GWO iter 18 / 20 best 1.0\n GWO iter 19 / 20 best 1.0\n GWO iter 20 / 20 best 1.0\n[GWO] done best score 0.9996668887408395 selected 52\nOptimizers finished in 262 s\nVoting selected features count: 41\nVoting selected: ['Source Port', 'Destination Port', 'Total Fwd Packets', 'Total Backward Packets', 'Total Length of Bwd Packets', 'Fwd Packet Length Min', 'Fwd Packet Length Std', 'Bwd Packet Length Min', 'Flow IAT Std', 'Flow IAT Min', 'Fwd IAT Mean', 'Fwd IAT Max', 'Bwd IAT Total', 'Bwd IAT Min', 'Fwd PSH Flags', 'Fwd URG Flags', 'Min Packet Length', 'Max Packet Length', 'Packet Length Variance', 'SYN Flag Count', 'CWE Flag Count', 'ECE Flag Count', 'Down/Up Ratio', 'Avg Fwd Segment Size', 'Avg Bwd Segment Size', 'Fwd Avg Packets/Bulk', 'Fwd Avg Bulk Rate', 'Bwd Avg Bytes/Bulk', 'Bwd Avg Packets/Bulk', 'Bwd Avg Bulk Rate', 'Subflow Fwd Bytes', 'Subflow Bwd Packets', 'Subflow Bwd Bytes', 'Init_Win_bytes_forward', 'Init_Win_bytes_backward', 'act_data_pkt_fwd', 'min_seg_size_forward', 'Active Mean', 'Active Max', 'Idle Std', 'Idle Min']\n[HLO] start on 41 candidates\n HLO iter 1 / 10 best 1.0\n HLO iter 2 / 10 best 1.0\n HLO iter 3 / 10 best 1.0\n HLO iter 4 / 10 best 1.0\n HLO iter 5 / 10 best 1.0\n HLO iter 6 / 10 best 1.0\n HLO iter 7 / 10 best 1.0\n HLO iter 8 / 10 best 1.0\n HLO iter 9 / 10 best 1.0\n HLO iter 10 / 10 best 1.0\n[HLO] done best local score 1.0 selected 22\n[HC] start: candidates 41\n[HC] done steps 0 evals 41 final_score 1.0 selected 22\nFinal selected features: ['Source Port', 'Destination Port', 'Total Backward Packets', 'Total Length of Bwd Packets', 'Fwd Packet Length Std', 'Bwd Packet Length Min', 'Flow IAT Min', 'Fwd IAT Max', 'Bwd IAT Min', 'Min Packet Length', 'Packet Length Variance', 'ECE Flag Count', 'Down/Up Ratio', 'Avg Fwd Segment Size', 'Fwd Avg Bulk Rate', 'Bwd Avg Packets/Bulk', 'Subflow Fwd Bytes', 'Subflow Bwd Bytes', 'Init_Win_bytes_forward', 'min_seg_size_forward', 'Active Mean', 'Active Max'] count: 22\nFull final training shape: (290753, 22) Label dist: {1: 290273, 0: 480}\nTraining final model on full data with early stopping...\n0:\tlearn: 0.5824022\ttest: 0.5824279\tbest: 0.5824279 (0)\ttotal: 21.4ms\tremaining: 21.4s\n50:\tlearn: 0.0018616\ttest: 0.0020712\tbest: 0.0020712 (50)\ttotal: 1.01s\tremaining: 18.8s\n100:\tlearn: 0.0007316\ttest: 0.0009125\tbest: 0.0009125 (100)\ttotal: 1.88s\tremaining: 16.7s\n150:\tlearn: 0.0005956\ttest: 0.0007714\tbest: 0.0007714 (150)\ttotal: 2.69s\tremaining: 15.1s\n200:\tlearn: 0.0005388\ttest: 0.0007091\tbest: 0.0007091 (200)\ttotal: 3.5s\tremaining: 13.9s\n250:\tlearn: 0.0005129\ttest: 0.0006829\tbest: 0.0006829 (250)\ttotal: 4.3s\tremaining: 12.8s\n300:\tlearn: 0.0004932\ttest: 0.0006619\tbest: 0.0006619 (299)\ttotal: 5.11s\tremaining: 11.9s\n350:\tlearn: 0.0004852\ttest: 0.0006538\tbest: 0.0006538 (349)\ttotal: 5.9s\tremaining: 10.9s\n400:\tlearn: 0.0004708\ttest: 0.0006377\tbest: 0.0006377 (400)\ttotal: 6.68s\tremaining: 9.99s\n450:\tlearn: 0.0004633\ttest: 0.0006293\tbest: 0.0006293 (450)\ttotal: 7.49s\tremaining: 9.11s\n500:\tlearn: 0.0004529\ttest: 0.0006202\tbest: 0.0006202 (499)\ttotal: 8.3s\tremaining: 8.27s\n550:\tlearn: 0.0004456\ttest: 0.0006133\tbest: 0.0006133 (549)\ttotal: 9.11s\tremaining: 7.42s\n600:\tlearn: 0.0004418\ttest: 0.0006089\tbest: 0.0006089 (599)\ttotal: 9.9s\tremaining: 6.57s\n650:\tlearn: 0.0004399\ttest: 0.0006070\tbest: 0.0006070 (650)\ttotal: 10.7s\tremaining: 5.73s\n700:\tlearn: 0.0004391\ttest: 0.0006060\tbest: 0.0006060 (700)\ttotal: 11.5s\tremaining: 4.89s\n750:\tlearn: 0.0004362\ttest: 0.0006036\tbest: 0.0006036 (750)\ttotal: 12.3s\tremaining: 4.06s\n800:\tlearn: 0.0004179\ttest: 0.0005811\tbest: 0.0005811 (800)\ttotal: 13.1s\tremaining: 3.25s\n850:\tlearn: 0.0004130\ttest: 0.0005759\tbest: 0.0005759 (849)\ttotal: 13.9s\tremaining: 2.43s\n900:\tlearn: 0.0004104\ttest: 0.0005736\tbest: 0.0005736 (900)\ttotal: 14.7s\tremaining: 1.61s\n950:\tlearn: 0.0004104\ttest: 0.0005735\tbest: 0.0005735 (950)\ttotal: 15.5s\tremaining: 797ms\n999:\tlearn: 0.0004071\ttest: 0.0005693\tbest: 0.0005693 (999)\ttotal: 16.2s\tremaining: 0us\n\nbestTest = 0.0005692890232\nbestIteration = 999\n\n\n=== FINAL HOLDOUT METRICS ===\nAccuracy: 0.9998452305205413\nPrecision: 0.9999310986323079\nRecall: 0.9999138747739212\nF1: 0.9999224866289435\n\nClassification report:\n               precision    recall  f1-score   support\n\n           0       0.95      0.96      0.95        96\n           1       1.00      1.00      1.00     58055\n\n    accuracy                           1.00     58151\n   macro avg       0.97      0.98      0.98     58151\nweighted avg       1.00      1.00      1.00     58151\n\n\n5-fold CV (quick estimate) -> Accuracy: 0.9998 ± 0.0001 ; F1: 0.9999 ± 0.0000\nSaved final model + features -> outputs/ddos_hybrid_voting_hlo_final_model.pkl\nPIPELINE COMPLETE\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"#testing voting output saved model on CIC-DDOS2019\n\nimport pickle\nimport pandas as pd\nimport numpy as np\nimport glob, os\nimport kagglehub\n\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\nimport pandas.api.types as ptypes\n\n# ======================================================\n# 1) LOAD SAVED MODEL\n# ======================================================\nmodel_path = \"outputs/ddos_hybrid_voting_hlo_final_model.pkl\"\n\nsaved = pickle.load(open(model_path, \"rb\"))\nfinal_model = saved[\"model\"]\nsel_features = saved[\"features\"]\n\nprint(\"Loaded saved model.\")\nprint(\"Selected features:\", sel_features)\n\n# ======================================================\n# 2) LOAD DATASET FROM KAGGLE\n# ======================================================\nDATA_PATH = kagglehub.dataset_download(\"sizlingdhairya1/cicddos2019\")\nprint(\"DATA_PATH:\", DATA_PATH)\n\ncsv_files = glob.glob(os.path.join(DATA_PATH, \"*.csv\"))\n\nif len(csv_files) == 0:\n    raise RuntimeError(\"ERROR: No CSV files found!\")\n\ndfs = []\nfor f in csv_files:\n    print(\"Loading:\", os.path.basename(f))\n    dfs.append(pd.read_csv(f, low_memory=False))\n\ndf_full = pd.concat(dfs, ignore_index=True)\nprint(\"Merged dataset shape:\", df_full.shape)\n\n# ======================================================\n# 3) CLEAN COLUMN NAMES\n# ======================================================\ndf_full.columns = df_full.columns.str.strip()\n\n# Fix label column name (normalize case)\nlabel_cols = [c for c in df_full.columns if c.strip().lower() == \"label\"]\nif len(label_cols) == 0:\n    raise RuntimeError(\"ERROR: No label column detected!\")\nlabel_name = label_cols[0]\n\nif label_name != \"Label\":\n    df_full.rename(columns={label_name: \"Label\"}, inplace=True)\n\n# Drop missing labels\ndf_full = df_full[df_full[\"Label\"].notna()]\n\n# Binary label conversion\ndf_full[\"Label\"] = df_full[\"Label\"].astype(str).str.strip().str.lower()\ndf_full[\"Label\"] = df_full[\"Label\"].map(lambda x: 0 if x == \"benign\" else 1)\n\n# ======================================================\n# 4) KEEP ONLY SELECTED FEATURES (+Label)\n# ======================================================\nmissing = [c for c in sel_features if c not in df_full.columns]\nif missing:\n    raise RuntimeError(\"Selected features missing in dataset: \" + str(missing))\n\ndf_full = df_full[sel_features + [\"Label\"]].copy()\n\nprint(\"After selecting features:\", df_full.shape)\n\n# ======================================================\n# 5) ENCODE CATEGORICAL COLS\n# ======================================================\nfor c in sel_features:\n    if df_full[c].dtype == object:\n        df_full[c] = LabelEncoder().fit_transform(df_full[c].astype(str))\n\n# Replace infinities and missing values\ndf_full.replace([np.inf, -np.inf], np.nan, inplace=True)\ndf_full.fillna(0, inplace=True)\n\n# ======================================================\n# 6) SCALE NUMERIC FEATURES\n# ======================================================\nnum_cols = [c for c in sel_features if ptypes.is_numeric_dtype(df_full[c])]\nif num_cols:\n    df_full[num_cols] = MinMaxScaler().fit_transform(df_full[num_cols])\n\nX_full = df_full.drop(\"Label\", axis=1)\ny_full = df_full[\"Label\"].astype(int)\n\nprint(\"Final data shape for inference:\", X_full.shape)\n\n# ======================================================\n# 7) RECREATE SAME 80/20 SPLIT USED BEFORE\n# ======================================================\nX_train, X_test, y_train, y_test = train_test_split(\n    X_full, y_full,\n    test_size=0.20, random_state=42,\n    stratify=y_full\n)\n\nprint(\"Test set shape:\", X_test.shape)\n\n# ======================================================\n# 8) RUN PREDICTIONS\n# ======================================================\ny_pred = final_model.predict(X_test)\n\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred, zero_division=0)\nrec = recall_score(y_test, y_pred, zero_division=0)\nf1  = f1_score(y_test, y_pred, zero_division=0)\n\nprint(\"\\n========== TEST SET PERFORMANCE ==========\")\nprint(\"Accuracy :\", acc)\nprint(\"Precision:\", prec)\nprint(\"Recall   :\", rec)\nprint(\"F1 Score :\", f1)\n\nprint(\"\\n----- CLASSIFICATION REPORT -----\\n\")\nprint(classification_report(y_test, y_pred))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T17:42:14.075244Z","iopub.execute_input":"2025-12-03T17:42:14.075547Z","iopub.status.idle":"2025-12-03T17:42:18.857502Z","shell.execute_reply.started":"2025-12-03T17:42:14.075523Z","shell.execute_reply":"2025-12-03T17:42:18.856724Z"}},"outputs":[{"name":"stdout","text":"Loaded saved model.\nSelected features: ['Source Port', 'Destination Port', 'Total Backward Packets', 'Total Length of Bwd Packets', 'Fwd Packet Length Std', 'Bwd Packet Length Min', 'Flow IAT Min', 'Fwd IAT Max', 'Bwd IAT Min', 'Min Packet Length', 'Packet Length Variance', 'ECE Flag Count', 'Down/Up Ratio', 'Avg Fwd Segment Size', 'Fwd Avg Bulk Rate', 'Bwd Avg Packets/Bulk', 'Subflow Fwd Bytes', 'Subflow Bwd Bytes', 'Init_Win_bytes_forward', 'min_seg_size_forward', 'Active Mean', 'Active Max']\nDATA_PATH: /kaggle/input/cicddos2019\nLoading: Random_combine_final.csv\nMerged dataset shape: (300000, 88)\nAfter selecting features: (300000, 23)\nFinal data shape for inference: (300000, 22)\nTest set shape: (60000, 22)\n\n========== TEST SET PERFORMANCE ==========\nAccuracy : 0.9999666666666667\nPrecision: 1.0\nRecall   : 0.9999666126905163\nF1 Score : 0.9999833060665755\n\n----- CLASSIFICATION REPORT -----\n\n              precision    recall  f1-score   support\n\n           0       0.98      1.00      0.99        97\n           1       1.00      1.00      1.00     59903\n\n    accuracy                           1.00     60000\n   macro avg       0.99      1.00      0.99     60000\nweighted avg       1.00      1.00      1.00     60000\n\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"#UNION WITH ALL SELECTIONS\n\n\n# hybrid_union_hlo_ddos_pipeline.py\n# PSO + GA + GWO -> UNION -> HLO -> Hill-climb -> final CatBoost\n# Uses kagglehub.dataset_download(...) to fetch CICDDoS2019\n# Balanced optimization subset (OPT_SUBSET_PER_CLASS per class)\n# Reduced optimizer budgets for speed; full logic retained.\n\nimport kagglehub\nimport glob, os, time, pickle, warnings\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\nfrom sklearn.model_selection import StratifiedKFold, train_test_split, cross_val_score\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, make_scorer\nfrom sklearn.base import clone\nfrom catboost import CatBoostClassifier\n\nwarnings.filterwarnings(\"ignore\")\nnp.random.seed(42)\n\n# -----------------------\n# USER CONFIG (tune if needed)\n# -----------------------\nDATA_PATH = kagglehub.dataset_download(\"sizlingdhairya1/cicddos2019\")\nOPT_SUBSET_PER_CLASS = 1500    # target per-class rows for optimization subset (will use min available)\nPSO_SWARM = 8\nPSO_ITERS = 10\nGA_POP = 12\nGA_GENS = 10\nGWO_WOLVES = 8\nGWO_ITERS = 10\nHLO_POP = 8\nHLO_ITERS = 8\nFIT_CB_ITERS_OPT = 80    # iterations used inside fitness (fast)\nCV_OPT = 2               # cheap CV inside optimizer\nFINAL_CB_ITERS = 1000    # large but we'll use early stopping\nFINAL_EARLY_STOP = 50\nFINAL_TEST_SIZE = 0.20\nSAVE_PREFIX = \"ddos_hybrid_union\"\nLEAKAGE_SINGLE_FEATURE_THRESHOLD = 0.99999  # drop features with single-feature perf >= this\n\nOUT_DIR = \"outputs\"\nos.makedirs(OUT_DIR, exist_ok=True)\n\nprint(\"[INFO] DATA_PATH:\", DATA_PATH)\n\n# -----------------------\n# 1) load & merge CSVs from dataset path\n# -----------------------\ncsv_files = glob.glob(os.path.join(DATA_PATH, \"*.csv\"))\nif len(csv_files) == 0:\n    raise RuntimeError(\"No CSV files found in DATA_PATH: \" + str(DATA_PATH))\n\nprint(f\"[INFO] Found {len(csv_files)} CSV files — merging...\")\ndfs = []\nfor f in csv_files:\n    print(\" -> Loading\", os.path.basename(f))\n    dfs.append(pd.read_csv(f, low_memory=False))\ndf = pd.concat(dfs, ignore_index=True)\nprint(\"[INFO] Merged dataset:\", df.shape)\n\n# -----------------------\n# 2) find label column (robust)\n# -----------------------\ndf.columns = df.columns.astype(str).str.strip()  # normalize\nfound_label = None\nfor cand in [\"Label\", \"label\", \"Attack\", \"attack\", \"attack_cat\", \" Label\", \" Label \"]:\n    if cand in df.columns:\n        found_label = cand\n        break\nif found_label is None:\n    for c in df.columns:\n        if c.strip().lower() in (\"label\", \"attack\", \"attack_cat\"):\n            found_label = c\n            break\nif found_label is None:\n    raise RuntimeError(\"Could not find label column in dataset. Columns: \" + \", \".join(df.columns[:30]))\n\nprint(\"[INFO] Using label column:\", repr(found_label))\ndf.rename(columns={found_label: \"Label\"}, inplace=True)\n\n# -----------------------\n# 3) basic cleaning & binary target\n# -----------------------\ndf = df[df[\"Label\"].notna()].copy()\ndf[\"Label\"] = df[\"Label\"].astype(str).str.strip().str.lower()\n# convert benign -> 0, else -> 1\ndf[\"Label\"] = df[\"Label\"].apply(lambda x: 0 if x == \"benign\" else 1)\nprint(\"[INFO] Label distribution (full):\\n\", df[\"Label\"].value_counts())\n\n# -----------------------\n# 4) drop obvious leakage columns (IDs / IPs / timestamps) if present\n# -----------------------\npossible_leak_cols = [c for c in df.columns if c.strip().lower() in (\n    \"id\", \"flow id\", \"flowid\", \"timestamp\", \"ts\", \"source ip\", \"destination ip\",\n    \"src ip\", \"dst ip\", \"sourceip\", \"destinationip\", \"srcip\", \"dstip\", \"flow_id\")]\nif possible_leak_cols:\n    print(\"[INFO] Dropping likely leakage columns:\", possible_leak_cols)\n    df.drop(columns=[c for c in possible_leak_cols if c in df.columns], inplace=True)\n\n# -----------------------\n# 5) drop empty cols, replace inf, drop rows with NaN (optimizer needs clean subset)\n# -----------------------\ndf.replace([np.inf, -np.inf], np.nan, inplace=True)\ndf.dropna(axis=1, how=\"all\", inplace=True)\ndf.dropna(axis=0, how=\"any\", inplace=True)\nprint(\"[INFO] After cleaning (full):\", df.shape)\n\n# -----------------------\n# 6) balanced small subset for optimization\n# -----------------------\ncounts = df[\"Label\"].value_counts().to_dict()\nn_attack = counts.get(1, 0)\nn_benign = counts.get(0, 0)\ntake_attack = min(OPT_SUBSET_PER_CLASS, n_attack)\ntake_benign = min(OPT_SUBSET_PER_CLASS, n_benign)\nif take_attack < 10 or take_benign < 10:\n    raise RuntimeError(f\"Not enough rows in one class to form optimization subset. counts={counts}\")\n\ndf_attack = df[df[\"Label\"] == 1].sample(take_attack, random_state=42)\ndf_benign = df[df[\"Label\"] == 0].sample(take_benign, random_state=42)\ndf_sub = pd.concat([df_attack, df_benign], ignore_index=True).sample(frac=1.0, random_state=42).reset_index(drop=True)\nprint(\"[INFO] Optimization subset shape:\", df_sub.shape, \"Label counts:\", df_sub[\"Label\"].value_counts().to_dict())\n\n# -----------------------\n# 7) preprocess subset: encode categorical & scale numeric\n# -----------------------\nTARGET_COL = \"Label\"\nX_sub = df_sub.drop(columns=[TARGET_COL]).copy()\ny_sub = df_sub[TARGET_COL].astype(int).copy()\n\n# encode object columns\nobj_cols = X_sub.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\nfor c in obj_cols:\n    X_sub[c] = LabelEncoder().fit_transform(X_sub[c].astype(str))\n\n# scale numeric columns\nnum_cols_sub = X_sub.select_dtypes(include=[np.number]).columns.tolist()\nif len(num_cols_sub) > 0:\n    X_sub[num_cols_sub] = MinMaxScaler().fit_transform(X_sub[num_cols_sub])\n\nFEATURE_NAMES = X_sub.columns.tolist()\nN_FEATURES = len(FEATURE_NAMES)\nprint(\"[INFO] Subset features:\", N_FEATURES)\n\n# -----------------------\n# 8) CatBoost factory & fitness function (with caching)\n# -----------------------\ndef get_catboost_model(iterations=FIT_CB_ITERS_OPT):\n    return CatBoostClassifier(iterations=iterations, learning_rate=0.05, depth=6,\n                              verbose=0, random_seed=42)\n\nfitness_cache = {}\ndef evaluate_mask(mask_bool, cv=CV_OPT, cb_iter=FIT_CB_ITERS_OPT):\n    # mask_bool: array-like of 0/1 length == N_FEATURES\n    key = tuple(int(x) for x in mask_bool)\n    if key in fitness_cache:\n        return fitness_cache[key]\n    idxs = [i for i,b in enumerate(key) if b==1]\n    if len(idxs) == 0:\n        fitness_cache[key] = 0.0\n        return 0.0\n    Xsel = X_sub.iloc[:, idxs]\n    model = get_catboost_model(iterations=cb_iter)\n    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=42)\n    try:\n        scores = cross_val_score(clone(model), Xsel, y_sub, cv=skf, scoring=make_scorer(f1_score), n_jobs=-1)\n    except Exception:\n        fitness_cache[key] = 0.0\n        return 0.0\n    val = float(np.mean(scores))\n    fitness_cache[key] = val\n    return val\n\n# -----------------------\n# 9) PSO (binary) - reduced\n# -----------------------\ndef run_pso(swarm_size=PSO_SWARM, iters=PSO_ITERS):\n    print(\"[PSO] start: swarm\", swarm_size, \"iters\", iters)\n    dim = N_FEATURES\n    pos = np.random.randint(0,2,(swarm_size,dim))\n    vel = np.random.uniform(-1,1,(swarm_size,dim))\n    pbest = pos.copy()\n    pbest_scores = np.array([evaluate_mask(p) for p in pbest])\n    gbest_idx = int(np.argmax(pbest_scores))\n    gbest = pbest[gbest_idx].copy()\n    gbest_score = pbest_scores[gbest_idx]\n    w = 0.6; c1 = c2 = 1.5\n    for t in range(iters):\n        print(\" PSO iter\", t+1, \"/\", iters, \"best\", gbest_score)\n        for i in range(swarm_size):\n            r1 = np.random.rand(dim); r2 = np.random.rand(dim)\n            vel[i] = w*vel[i] + c1*r1*(pbest[i] - pos[i]) + c2*r2*(gbest - pos[i])\n            s = 1.0 / (1.0 + np.exp(-vel[i]))\n            pos[i] = (np.random.rand(dim) < s).astype(int)\n            sc = evaluate_mask(pos[i])\n            if sc > pbest_scores[i]:\n                pbest[i] = pos[i].copy(); pbest_scores[i] = sc\n            if sc > gbest_score:\n                gbest = pos[i].copy(); gbest_score = sc\n        w = max(0.2, w*0.97)\n    print(\"[PSO] done best score\", gbest_score, \"selected\", int(np.sum(gbest)))\n    return gbest\n\n# -----------------------\n# 10) GA (binary) - reduced\n# -----------------------\ndef run_ga(pop_size=GA_POP, gens=GA_GENS):\n    print(\"[GA] start: pop\", pop_size, \"gens\", gens)\n    dim = N_FEATURES\n    pop = np.random.randint(0,2,(pop_size, dim))\n    fitnesses = np.array([evaluate_mask(ind) for ind in pop])\n    for g in range(gens):\n        print(\" GA gen\", g+1, \"/\", gens, \"best\", fitnesses.max())\n        elite_idxs = np.argsort(fitnesses)[-2:]\n        new_pop = [pop[elite_idxs[0]].copy(), pop[elite_idxs[1]].copy()]\n        while len(new_pop) < pop_size:\n            p1 = pop[np.random.randint(pop_size)].copy()\n            p2 = pop[np.random.randint(pop_size)].copy()\n            if np.random.rand() < 0.7:\n                pt = np.random.randint(1, dim)\n                child = np.concatenate([p1[:pt], p2[pt:]])\n            else:\n                child = p1\n            # mutation\n            for d in range(dim):\n                if np.random.rand() < 0.05:\n                    child[d] = 1-child[d]\n            new_pop.append(child)\n        pop = np.array(new_pop[:pop_size])\n        fitnesses = np.array([evaluate_mask(ind) for ind in pop])\n    best = pop[np.argmax(fitnesses)]\n    print(\"[GA] done best score\", fitnesses.max(), \"selected\", int(np.sum(best)))\n    return best\n\n# -----------------------\n# 11) GWO (binary) - reduced\n# -----------------------\ndef run_gwo(wolves=GWO_WOLVES, iters=GWO_ITERS):\n    print(\"[GWO] start: wolves\", wolves, \"iters\", iters)\n    dim = N_FEATURES\n    pack = np.random.randint(0,2,(wolves, dim))\n    fitnesses = np.array([evaluate_mask(ind) for ind in pack])\n    Alpha = Beta = Delta = None\n    Alpha_score = Beta_score = Delta_score = -1.0\n    for itr in range(iters):\n        print(\" GWO iter\", itr+1, \"/\", iters, \"best\", Alpha_score)\n        for i in range(wolves):\n            sc = fitnesses[i]\n            if sc > Alpha_score:\n                Delta_score, Beta_score, Alpha_score = Beta_score, Alpha_score, sc\n                Delta, Beta, Alpha = Beta, Alpha, pack[i].copy()\n            elif sc > Beta_score:\n                Delta_score, Beta_score = Beta_score, sc\n                Delta, Beta = Beta, pack[i].copy()\n            elif sc > Delta_score:\n                Delta_score = sc; Delta = pack[i].copy()\n        a = 2 - itr*(2.0/iters)\n        for i in range(wolves):\n            if Alpha is None:\n                continue\n            for d in range(dim):\n                r1, r2 = np.random.rand(), np.random.rand()\n                A1 = 2*a*r1 - a; C1 = 2*r2\n                D_alpha = abs(C1*Alpha[d] - pack[i][d])\n                X1 = Alpha[d] - A1*D_alpha\n                s = 1.0/(1.0+np.exp(-X1))\n                pack[i][d] = 1 if np.random.rand() < s else 0\n        fitnesses = np.array([evaluate_mask(ind) for ind in pack])\n    best = pack[np.argmax(fitnesses)]\n    print(\"[GWO] done best score\", fitnesses.max(), \"selected\", int(np.sum(best)))\n    return best\n\n# -----------------------\n# 12) RUN OPTIMIZERS\n# -----------------------\nt0 = time.time()\nmask_pso = run_pso()\nmask_ga = run_ga()\nmask_gwo = run_gwo()\nt1 = time.time()\nprint(\"[INFO] Optimizers finished in\", int(t1-t0), \"s\")\n\n# save raw masks\npickle.dump({\"mask_pso\": mask_pso.tolist(), \"mask_ga\": mask_ga.tolist(), \"mask_gwo\": mask_gwo.tolist()},\n            open(os.path.join(OUT_DIR, SAVE_PREFIX + \"_raw_masks.pkl\"), \"wb\"))\n\n# -----------------------\n# 13) UNION (majority not used; union keeps any selected by any optimizer)\n# -----------------------\nunion_mask = ((np.array(mask_pso) == 1) | (np.array(mask_ga) == 1) | (np.array(mask_gwo) == 1)).astype(int)\nselected_indices = list(np.where(union_mask == 1)[0])\nselected_features_union = [FEATURE_NAMES[i] for i in selected_indices]\nprint(\"[INFO] UNION selected features count:\", len(selected_indices))\nprint(\"[INFO] UNION selected:\", selected_features_union)\n\npickle.dump({\"union_mask\": union_mask.tolist(), \"selected_features_union\": selected_features_union},\n            open(os.path.join(OUT_DIR, SAVE_PREFIX + \"_union.pkl\"), \"wb\"))\n\n# -----------------------\n# 14) HLO (on UNION candidates)\n# -----------------------\ndef hlo_on_candidates(candidate_mask, pop_size=HLO_POP, iters=HLO_ITERS):\n    cand_idxs = np.where(np.array(candidate_mask).astype(bool))[0].tolist()\n    k = len(cand_idxs)\n    if k == 0:\n        raise RuntimeError(\"No candidates for HLO\")\n    print(\"[HLO] start on\", k, \"candidates\")\n    pop = np.random.randint(0,2,(pop_size, k))\n    def fitness_local(bitmask):\n        full = np.zeros(N_FEATURES, dtype=int)\n        for j,b in enumerate(bitmask):\n            if int(b) == 1:\n                full[cand_idxs[j]] = 1\n        return evaluate_mask(full)\n    fitness_scores = np.array([fitness_local(ind) for ind in pop])\n    best_idx = int(np.argmax(fitness_scores))\n    best_solution = pop[best_idx].copy()\n    best_score = fitness_scores[best_idx]\n    for it in range(iters):\n        print(\" HLO iter\", it+1, \"/\", iters, \"best\", best_score)\n        teacher = pop[int(np.argmax([fitness_local(x) for x in pop]))].copy()\n        new_pop = []\n        for i in range(pop_size):\n            learner = pop[i].copy()\n            # teaching\n            for d in range(k):\n                if np.random.rand() < 0.75:\n                    learner[d] = teacher[d]\n            # peer learning\n            partner = pop[np.random.randint(pop_size)].copy()\n            for d in range(k):\n                if learner[d] != partner[d] and np.random.rand() < 0.5:\n                    learner[d] = partner[d]\n            # mutation\n            for d in range(k):\n                if np.random.rand() < 0.12:\n                    learner[d] = 1 - learner[d]\n            new_pop.append(learner)\n        pop = np.array(new_pop)\n        fitness_scores = np.array([fitness_local(ind) for ind in pop])\n        gen_best_idx = int(np.argmax(fitness_scores))\n        gen_best_score = fitness_scores[gen_best_idx]\n        gen_best_sol = pop[gen_best_idx].copy()\n        if gen_best_score > best_score:\n            best_score = gen_best_score\n            best_solution = gen_best_sol.copy()\n    final_full = np.zeros(N_FEATURES, dtype=int)\n    for j,b in enumerate(best_solution):\n        if int(b) == 1:\n            final_full[cand_idxs[j]] = 1\n    print(\"[HLO] done best local score\", best_score, \"selected\", int(final_full.sum()))\n    return final_full, best_score\n\nhlo_mask, hlo_score = hlo_on_candidates(union_mask)\npickle.dump({\"hlo_mask\": hlo_mask.tolist(), \"hlo_score\": hlo_score},\n            open(os.path.join(OUT_DIR, SAVE_PREFIX + \"_hlo.pkl\"), \"wb\"))\n\n# -----------------------\n# 15) Greedy hill-climb restricted to union candidate indices\n# -----------------------\ndef hill_climb(initial_mask, candidate_mask, max_steps=100, eval_cap=500):\n    cand_idxs = np.where(np.array(candidate_mask).astype(bool))[0].tolist()\n    cur = initial_mask.copy()\n    cur_score = evaluate_mask(cur)\n    steps = 0\n    evals = 0\n    improved = True\n    print(\"[HC] start: candidates\", len(cand_idxs))\n    while improved and steps < max_steps and evals < eval_cap:\n        improved = False\n        for idx in np.random.permutation(cand_idxs):\n            trial = cur.copy()\n            trial[idx] = 1 - trial[idx]\n            sc = evaluate_mask(trial)\n            evals += 1\n            if sc > cur_score + 1e-8:\n                cur = trial\n                cur_score = sc\n                improved = True\n                steps += 1\n                print(f\" HC step {steps}: flipped {FEATURE_NAMES[idx]} -> new_score {cur_score:.4f} (evals={evals})\")\n                break\n    print(\"[HC] done steps\", steps, \"evals\", evals, \"final_score\", cur_score, \"selected\", int(cur.sum()))\n    return cur, cur_score\n\nhc_mask, hc_score = hill_climb(hlo_mask, union_mask)\npickle.dump({\"hc_mask\": hc_mask.tolist(), \"hc_score\": hc_score},\n            open(os.path.join(OUT_DIR, SAVE_PREFIX + \"_hc.pkl\"), \"wb\"))\n\n# -----------------------\n# 16) Final selected features after hill-climb\n# -----------------------\nfinal_mask = hc_mask\nfinal_selected_indices = np.where(np.array(final_mask).astype(bool))[0].tolist()\nfinal_selected = [FEATURE_NAMES[i] for i in final_selected_indices]\nprint(\"[INFO] Final selected features:\", final_selected, \"count:\", len(final_selected))\n\n# -----------------------\n# 17) Leakage check: drop single-feature perfect predictors\n# -----------------------\ndef single_feature_predictive_accuracy(feature_series, labels):\n    # for each value map to most common label and compute accuracy\n    mapping = feature_series.groupby(feature_series).apply(lambda s: labels[s.index].mode().iloc[0])\n    preds = feature_series.map(mapping)\n    return (preds.values == labels.values).mean()\n\nto_drop = []\nfor f in final_selected:\n    acc = single_feature_predictive_accuracy(X_sub[f], y_sub)\n    if acc >= LEAKAGE_SINGLE_FEATURE_THRESHOLD or acc == 1.0:\n        print(f\"[LEAK] Dropping '{f}' single-feature accuracy={acc:.6f}\")\n        to_drop.append(f)\nif to_drop:\n    final_selected = [f for f in final_selected if f not in to_drop]\n    final_selected_indices = [FEATURE_NAMES.index(f) for f in final_selected]\n    print(\"[INFO] After dropping leakage suspects, final features:\", final_selected)\n\nif len(final_selected) == 0:\n    raise RuntimeError(\"No features remain after leakage check. Lower threshold or inspect features.\")\n\npickle.dump({\"final_selected\": final_selected, \"final_mask\": final_mask.tolist()},\n            open(os.path.join(OUT_DIR, SAVE_PREFIX + \"_final_selected.pkl\"), \"wb\"))\n\n# -----------------------\n# 18) Prepare FULL dataset with same preprocessing for final training\n# -----------------------\ndf_full = df.copy()\nmissing_in_full = [f for f in final_selected if f not in df_full.columns]\nif missing_in_full:\n    raise RuntimeError(\"Selected features missing from full dataset: \" + str(missing_in_full))\n\ndf_full = df_full[final_selected + [\"Label\"]].copy()\nfor c in df_full.columns:\n    if c != \"Label\" and df_full[c].dtype == \"object\":\n        df_full[c] = LabelEncoder().fit_transform(df_full[c].astype(str))\ndf_full.replace([np.inf, -np.inf], np.nan, inplace=True)\ndf_full.fillna(0, inplace=True)\n\nnum_cols = [c for c in final_selected if pd.api.types.is_numeric_dtype(df_full[c])]\nif len(num_cols) > 0:\n    df_full[num_cols] = MinMaxScaler().fit_transform(df_full[num_cols])\n\nX_full = df_full.drop(columns=[\"Label\"])\ny_full = df_full[\"Label\"].astype(int)\nprint(\"[INFO] Full final training shape:\", X_full.shape, \"Label dist:\", y_full.value_counts().to_dict())\n\n# -----------------------\n# 19) Final train/test (80/20 stratified) and CatBoost training\n# -----------------------\nminclass = y_full.value_counts().min()\nif minclass < 10:\n    print(\"[WARN] Small class size after selecting features:\", minclass)\n\nX_train, X_test, y_train, y_test = train_test_split(X_full, y_full, test_size=FINAL_TEST_SIZE, stratify=y_full, random_state=42)\nX_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.15, stratify=y_train, random_state=42)\n\nfinal_params = {\n    \"iterations\": FINAL_CB_ITERS,\n    \"learning_rate\": 0.03,\n    \"depth\": 6,\n    \"l2_leaf_reg\": 7.0,\n    \"bootstrap_type\": \"Bernoulli\",\n    \"subsample\": 0.8,\n    \"random_strength\": 1.0,\n    \"verbose\": 50,\n    \"random_seed\": 42\n}\nfinal_model = CatBoostClassifier(**final_params)\n\nprint(\"[INFO] Training final model with early stopping...\")\nfinal_model.fit(X_tr, y_tr, eval_set=(X_val, y_val), early_stopping_rounds=FINAL_EARLY_STOP, use_best_model=True)\n\n# Evaluate on hold-out test\ny_pred = final_model.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred, zero_division=0)\nrec = recall_score(y_test, y_pred, zero_division=0)\nf1 = f1_score(y_test, y_pred, zero_division=0)\nprint(\"\\n=== FINAL HOLDOUT METRICS ===\")\nprint(\"Accuracy:\", acc)\nprint(\"Precision:\", prec)\nprint(\"Recall:\", rec)\nprint(\"F1:\", f1)\nprint(\"\\nClassification report:\\n\", classification_report(y_test, y_pred))\n\n# Quick 5-fold CV (reduced iters) as a sanity check\ncv_model = CatBoostClassifier(iterations=200, learning_rate=0.03, depth=6, l2_leaf_reg=7.0,\n                              bootstrap_type=\"Bernoulli\", subsample=0.8, random_seed=42, verbose=0)\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\naccs = cross_val_score(cv_model, X_full, y_full, cv=skf, scoring=\"accuracy\", n_jobs=-1)\nf1s = cross_val_score(cv_model, X_full, y_full, cv=skf, scoring=make_scorer(f1_score), n_jobs=-1)\nprint(\"\\n5-fold CV estimate -> Accuracy: %.4f ± %.4f ; F1: %.4f ± %.4f\" % (accs.mean(), accs.std(), f1s.mean(), f1s.std()))\n\n# -----------------------\n# 20) Save final model & selected features\n# -----------------------\nfinal_model_path = os.path.join(OUT_DIR, f\"{SAVE_PREFIX}_final_model.pkl\")\nwith open(final_model_path, \"wb\") as f:\n    pickle.dump({\"model\": final_model, \"features\": final_selected, \"mask\": final_mask.tolist()}, f)\nprint(\"[INFO] Saved final model ->\", final_model_path)\n\nprint(\"PIPELINE COMPLETE\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T18:18:15.999729Z","iopub.execute_input":"2025-12-03T18:18:16.000340Z","iopub.status.idle":"2025-12-03T18:23:10.552092Z","shell.execute_reply.started":"2025-12-03T18:18:16.000317Z","shell.execute_reply":"2025-12-03T18:23:10.551201Z"}},"outputs":[{"name":"stdout","text":"[INFO] DATA_PATH: /kaggle/input/cicddos2019\n[INFO] Found 1 CSV files — merging...\n -> Loading Random_combine_final.csv\n[INFO] Merged dataset: (300000, 88)\n[INFO] Using label column: 'Label'\n[INFO] Label distribution (full):\n Label\n1    299513\n0       487\nName: count, dtype: int64\n[INFO] Dropping likely leakage columns: ['Flow ID', 'Source IP', 'Destination IP', 'Timestamp']\n[INFO] After cleaning (full): (290753, 84)\n[INFO] Optimization subset shape: (1980, 84) Label counts: {1: 1500, 0: 480}\n[INFO] Subset features: 83\n[PSO] start: swarm 8 iters 10\n PSO iter 1 / 10 best 0.9993342210386151\n PSO iter 2 / 10 best 0.9993342210386151\n PSO iter 3 / 10 best 0.9993342210386151\n PSO iter 4 / 10 best 0.9993342210386151\n PSO iter 5 / 10 best 0.9993342210386151\n PSO iter 6 / 10 best 0.9993342210386151\n PSO iter 7 / 10 best 1.0\n PSO iter 8 / 10 best 1.0\n PSO iter 9 / 10 best 1.0\n PSO iter 10 / 10 best 1.0\n[PSO] done best score 1.0 selected 40\n[GA] start: pop 12 gens 10\n GA gen 1 / 10 best 1.0\n GA gen 2 / 10 best 1.0\n GA gen 3 / 10 best 1.0\n GA gen 4 / 10 best 1.0\n GA gen 5 / 10 best 1.0\n GA gen 6 / 10 best 1.0\n GA gen 7 / 10 best 1.0\n GA gen 8 / 10 best 1.0\n GA gen 9 / 10 best 1.0\n GA gen 10 / 10 best 1.0\n[GA] done best score 1.0 selected 39\n[GWO] start: wolves 8 iters 10\n GWO iter 1 / 10 best -1.0\n GWO iter 2 / 10 best 0.9993342210386151\n GWO iter 3 / 10 best 1.0\n GWO iter 4 / 10 best 1.0\n GWO iter 5 / 10 best 1.0\n GWO iter 6 / 10 best 1.0\n GWO iter 7 / 10 best 1.0\n GWO iter 8 / 10 best 1.0\n GWO iter 9 / 10 best 1.0\n GWO iter 10 / 10 best 1.0\n[GWO] done best score 1.0 selected 52\n[INFO] Optimizers finished in 142 s\n[INFO] UNION selected features count: 72\n[INFO] UNION selected: ['Unnamed: 0', 'Source Port', 'Destination Port', 'Protocol', 'Flow Duration', 'Total Fwd Packets', 'Total Backward Packets', 'Total Length of Fwd Packets', 'Total Length of Bwd Packets', 'Fwd Packet Length Max', 'Fwd Packet Length Min', 'Fwd Packet Length Mean', 'Fwd Packet Length Std', 'Bwd Packet Length Max', 'Bwd Packet Length Min', 'Bwd Packet Length Std', 'Flow Bytes/s', 'Flow Packets/s', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Total', 'Fwd IAT Std', 'Fwd IAT Max', 'Bwd IAT Total', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Header Length', 'Bwd Header Length', 'Fwd Packets/s', 'Min Packet Length', 'Max Packet Length', 'Packet Length Mean', 'Packet Length Std', 'Packet Length Variance', 'FIN Flag Count', 'SYN Flag Count', 'RST Flag Count', 'PSH Flag Count', 'ACK Flag Count', 'CWE Flag Count', 'ECE Flag Count', 'Down/Up Ratio', 'Average Packet Size', 'Avg Fwd Segment Size', 'Avg Bwd Segment Size', 'Fwd Header Length.1', 'Fwd Avg Bytes/Bulk', 'Fwd Avg Packets/Bulk', 'Fwd Avg Bulk Rate', 'Bwd Avg Bytes/Bulk', 'Bwd Avg Bulk Rate', 'Subflow Fwd Packets', 'Subflow Fwd Bytes', 'Subflow Bwd Packets', 'Subflow Bwd Bytes', 'Init_Win_bytes_forward', 'Init_Win_bytes_backward', 'min_seg_size_forward', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min', 'SimillarHTTP']\n[HLO] start on 72 candidates\n HLO iter 1 / 8 best 1.0\n HLO iter 2 / 8 best 1.0\n HLO iter 3 / 8 best 1.0\n HLO iter 4 / 8 best 1.0\n HLO iter 5 / 8 best 1.0\n HLO iter 6 / 8 best 1.0\n HLO iter 7 / 8 best 1.0\n HLO iter 8 / 8 best 1.0\n[HLO] done best local score 1.0 selected 36\n[HC] start: candidates 72\n[HC] done steps 0 evals 72 final_score 1.0 selected 36\n[INFO] Final selected features: ['Source Port', 'Destination Port', 'Protocol', 'Flow Duration', 'Total Fwd Packets', 'Fwd Packet Length Min', 'Fwd Packet Length Std', 'Bwd Packet Length Min', 'Bwd Packet Length Std', 'Flow Packets/s', 'Flow IAT Std', 'Fwd IAT Total', 'Fwd IAT Max', 'Bwd IAT Std', 'Fwd URG Flags', 'Bwd Header Length', 'Min Packet Length', 'Packet Length Variance', 'SYN Flag Count', 'ACK Flag Count', 'CWE Flag Count', 'Down/Up Ratio', 'Average Packet Size', 'Avg Bwd Segment Size', 'Fwd Header Length.1', 'Fwd Avg Bulk Rate', 'Bwd Avg Bytes/Bulk', 'Bwd Avg Bulk Rate', 'Subflow Fwd Bytes', 'Init_Win_bytes_forward', 'Init_Win_bytes_backward', 'min_seg_size_forward', 'Active Mean', 'Active Std', 'Active Min', 'Idle Mean'] count: 36\n[INFO] Full final training shape: (290753, 36) Label dist: {1: 290273, 0: 480}\n[INFO] Training final model with early stopping...\n0:\tlearn: 0.5794592\ttest: 0.5793496\tbest: 0.5793496 (0)\ttotal: 76.9ms\tremaining: 1m 16s\n50:\tlearn: 0.0019804\ttest: 0.0021188\tbest: 0.0021188 (50)\ttotal: 1.18s\tremaining: 22s\n100:\tlearn: 0.0008470\ttest: 0.0010101\tbest: 0.0010101 (100)\ttotal: 2.21s\tremaining: 19.6s\n150:\tlearn: 0.0006693\ttest: 0.0008294\tbest: 0.0008294 (149)\ttotal: 3.17s\tremaining: 17.8s\n200:\tlearn: 0.0005857\ttest: 0.0007367\tbest: 0.0007367 (199)\ttotal: 4.17s\tremaining: 16.6s\n250:\tlearn: 0.0005492\ttest: 0.0006990\tbest: 0.0006990 (250)\ttotal: 5.14s\tremaining: 15.3s\n300:\tlearn: 0.0005180\ttest: 0.0006702\tbest: 0.0006702 (299)\ttotal: 6.09s\tremaining: 14.1s\n350:\tlearn: 0.0004943\ttest: 0.0006456\tbest: 0.0006456 (350)\ttotal: 7.05s\tremaining: 13s\n400:\tlearn: 0.0004807\ttest: 0.0006327\tbest: 0.0006327 (400)\ttotal: 8.02s\tremaining: 12s\n450:\tlearn: 0.0004545\ttest: 0.0006071\tbest: 0.0006071 (449)\ttotal: 8.96s\tremaining: 10.9s\n500:\tlearn: 0.0004480\ttest: 0.0006008\tbest: 0.0006008 (499)\ttotal: 9.88s\tremaining: 9.84s\n550:\tlearn: 0.0004336\ttest: 0.0005866\tbest: 0.0005866 (550)\ttotal: 10.8s\tremaining: 8.82s\n600:\tlearn: 0.0004336\ttest: 0.0005866\tbest: 0.0005866 (600)\ttotal: 11.7s\tremaining: 7.79s\n650:\tlearn: 0.0004268\ttest: 0.0005804\tbest: 0.0005804 (649)\ttotal: 12.7s\tremaining: 6.79s\n700:\tlearn: 0.0004195\ttest: 0.0005724\tbest: 0.0005724 (699)\ttotal: 13.6s\tremaining: 5.8s\n750:\tlearn: 0.0004150\ttest: 0.0005677\tbest: 0.0005677 (750)\ttotal: 14.6s\tremaining: 4.84s\n800:\tlearn: 0.0004032\ttest: 0.0005559\tbest: 0.0005559 (799)\ttotal: 15.5s\tremaining: 3.86s\n850:\tlearn: 0.0003972\ttest: 0.0005509\tbest: 0.0005509 (849)\ttotal: 16.4s\tremaining: 2.88s\n900:\tlearn: 0.0003930\ttest: 0.0005474\tbest: 0.0005474 (899)\ttotal: 17.4s\tremaining: 1.91s\n950:\tlearn: 0.0003905\ttest: 0.0005455\tbest: 0.0005455 (950)\ttotal: 18.2s\tremaining: 940ms\n999:\tlearn: 0.0003863\ttest: 0.0005414\tbest: 0.0005414 (999)\ttotal: 19.1s\tremaining: 0us\n\nbestTest = 0.0005413674251\nbestIteration = 999\n\n\n=== FINAL HOLDOUT METRICS ===\nAccuracy: 0.9998108373028839\nPrecision: 0.9998794309139136\nRecall: 0.999931099819137\nF1: 0.9999052646990432\n\nClassification report:\n               precision    recall  f1-score   support\n\n           0       0.96      0.93      0.94        96\n           1       1.00      1.00      1.00     58055\n\n    accuracy                           1.00     58151\n   macro avg       0.98      0.96      0.97     58151\nweighted avg       1.00      1.00      1.00     58151\n\n\n5-fold CV estimate -> Accuracy: 0.9998 ± 0.0000 ; F1: 0.9999 ± 0.0000\n[INFO] Saved final model -> outputs/ddos_hybrid_union_final_model.pkl\nPIPELINE COMPLETE\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"#testing union based model on CIC-DDOS2019\n\nimport pickle\nimport pandas as pd\nimport numpy as np\nimport glob, os\nimport kagglehub\n\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\nimport pandas.api.types as ptypes\n\n# ======================================================\n# 1) LOAD SAVED MODEL\n# ======================================================\nmodel_path = \"/kaggle/working/outputs/ddos_hybrid_union_final_model.pkl\"\n\nsaved = pickle.load(open(model_path, \"rb\"))\nfinal_model = saved[\"model\"]\nsel_features = saved[\"features\"]\n\nprint(\"Loaded saved model.\")\nprint(\"Selected features:\", sel_features)\n\n# ======================================================\n# 2) LOAD DATASET FROM KAGGLE\n# ======================================================\nDATA_PATH = kagglehub.dataset_download(\"sizlingdhairya1/cicddos2019\")\nprint(\"DATA_PATH:\", DATA_PATH)\n\ncsv_files = glob.glob(os.path.join(DATA_PATH, \"*.csv\"))\n\nif len(csv_files) == 0:\n    raise RuntimeError(\"ERROR: No CSV files found!\")\n\ndfs = []\nfor f in csv_files:\n    print(\"Loading:\", os.path.basename(f))\n    dfs.append(pd.read_csv(f, low_memory=False))\n\ndf_full = pd.concat(dfs, ignore_index=True)\nprint(\"Merged dataset shape:\", df_full.shape)\n\n# ======================================================\n# 3) CLEAN COLUMN NAMES\n# ======================================================\ndf_full.columns = df_full.columns.str.strip()\n\n# Fix label column name (normalize case)\nlabel_cols = [c for c in df_full.columns if c.strip().lower() == \"label\"]\nif len(label_cols) == 0:\n    raise RuntimeError(\"ERROR: No label column detected!\")\nlabel_name = label_cols[0]\n\nif label_name != \"Label\":\n    df_full.rename(columns={label_name: \"Label\"}, inplace=True)\n\n# Drop missing labels\ndf_full = df_full[df_full[\"Label\"].notna()]\n\n# Binary label conversion\ndf_full[\"Label\"] = df_full[\"Label\"].astype(str).str.strip().str.lower()\ndf_full[\"Label\"] = df_full[\"Label\"].map(lambda x: 0 if x == \"benign\" else 1)\n\n# ======================================================\n# 4) KEEP ONLY SELECTED FEATURES (+Label)\n# ======================================================\nmissing = [c for c in sel_features if c not in df_full.columns]\nif missing:\n    raise RuntimeError(\"Selected features missing in dataset: \" + str(missing))\n\ndf_full = df_full[sel_features + [\"Label\"]].copy()\n\nprint(\"After selecting features:\", df_full.shape)\n\n# ======================================================\n# 5) ENCODE CATEGORICAL COLS\n# ======================================================\nfor c in sel_features:\n    if df_full[c].dtype == object:\n        df_full[c] = LabelEncoder().fit_transform(df_full[c].astype(str))\n\n# Replace infinities and missing values\ndf_full.replace([np.inf, -np.inf], np.nan, inplace=True)\ndf_full.fillna(0, inplace=True)\n\n# ======================================================\n# 6) SCALE NUMERIC FEATURES\n# ======================================================\nnum_cols = [c for c in sel_features if ptypes.is_numeric_dtype(df_full[c])]\nif num_cols:\n    df_full[num_cols] = MinMaxScaler().fit_transform(df_full[num_cols])\n\nX_full = df_full.drop(\"Label\", axis=1)\ny_full = df_full[\"Label\"].astype(int)\n\nprint(\"Final data shape for inference:\", X_full.shape)\n\n# ======================================================\n# 7) RECREATE SAME 80/20 SPLIT USED BEFORE\n# ======================================================\nX_train, X_test, y_train, y_test = train_test_split(\n    X_full, y_full,\n    test_size=0.20, random_state=42,\n    stratify=y_full\n)\n\nprint(\"Test set shape:\", X_test.shape)\n\n# ======================================================\n# 8) RUN PREDICTIONS\n# ======================================================\ny_pred = final_model.predict(X_test)\n\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred, zero_division=0)\nrec = recall_score(y_test, y_pred, zero_division=0)\nf1  = f1_score(y_test, y_pred, zero_division=0)\n\nprint(\"\\n========== TEST SET PERFORMANCE ==========\")\nprint(\"Accuracy :\", acc)\nprint(\"Precision:\", prec)\nprint(\"Recall   :\", rec)\nprint(\"F1 Score :\", f1)\n\nprint(\"\\n----- CLASSIFICATION REPORT -----\\n\")\nprint(classification_report(y_test, y_pred))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T18:23:49.866049Z","iopub.execute_input":"2025-12-03T18:23:49.866679Z","iopub.status.idle":"2025-12-03T18:23:55.177207Z","shell.execute_reply.started":"2025-12-03T18:23:49.866654Z","shell.execute_reply":"2025-12-03T18:23:55.176518Z"}},"outputs":[{"name":"stdout","text":"Loaded saved model.\nSelected features: ['Source Port', 'Destination Port', 'Protocol', 'Flow Duration', 'Total Fwd Packets', 'Fwd Packet Length Min', 'Fwd Packet Length Std', 'Bwd Packet Length Min', 'Bwd Packet Length Std', 'Flow Packets/s', 'Flow IAT Std', 'Fwd IAT Total', 'Fwd IAT Max', 'Bwd IAT Std', 'Fwd URG Flags', 'Bwd Header Length', 'Min Packet Length', 'Packet Length Variance', 'SYN Flag Count', 'ACK Flag Count', 'CWE Flag Count', 'Down/Up Ratio', 'Average Packet Size', 'Avg Bwd Segment Size', 'Fwd Header Length.1', 'Fwd Avg Bulk Rate', 'Bwd Avg Bytes/Bulk', 'Bwd Avg Bulk Rate', 'Subflow Fwd Bytes', 'Init_Win_bytes_forward', 'Init_Win_bytes_backward', 'min_seg_size_forward', 'Active Mean', 'Active Std', 'Active Min', 'Idle Mean']\nDATA_PATH: /kaggle/input/cicddos2019\nLoading: Random_combine_final.csv\nMerged dataset shape: (300000, 88)\nAfter selecting features: (300000, 37)\nFinal data shape for inference: (300000, 36)\nTest set shape: (60000, 36)\n\n========== TEST SET PERFORMANCE ==========\nAccuracy : 0.9999666666666667\nPrecision: 0.9999833063452581\nRecall   : 0.9999833063452581\nF1 Score : 0.9999833063452581\n\n----- CLASSIFICATION REPORT -----\n\n              precision    recall  f1-score   support\n\n           0       0.99      0.99      0.99        97\n           1       1.00      1.00      1.00     59903\n\n    accuracy                           1.00     60000\n   macro avg       0.99      0.99      0.99     60000\nweighted avg       1.00      1.00      1.00     60000\n\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"#TESTING ON A NEW DATASET CIC-DDOS2017\n\nimport kagglehub\n\n# Download latest version\npath = kagglehub.dataset_download(\"jafftaffy/test-ids2017\")\n\nprint(\"Path to dataset files:\", path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T13:19:15.239788Z","iopub.execute_input":"2025-12-04T13:19:15.240113Z","iopub.status.idle":"2025-12-04T13:19:15.474030Z","shell.execute_reply.started":"2025-12-04T13:19:15.240092Z","shell.execute_reply":"2025-12-04T13:19:15.473446Z"}},"outputs":[{"name":"stdout","text":"Path to dataset files: /kaggle/input/test-ids2017\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"print()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import kagglehub\n\npath = kagglehub.dataset_download(\"cicdataset/cicids2017\")\n\nprint(\"Downloaded to:\", path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T14:24:24.502359Z","iopub.execute_input":"2025-12-04T14:24:24.502984Z","iopub.status.idle":"2025-12-04T14:24:24.724551Z","shell.execute_reply.started":"2025-12-04T14:24:24.502960Z","shell.execute_reply":"2025-12-04T14:24:24.723456Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mBackendError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/3402437810.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkagglehub\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkagglehub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cicdataset/cicids2017\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Downloaded to:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/kagglehub/datasets.py\u001b[0m in \u001b[0;36mdataset_download\u001b[0;34m(handle, path, force_download)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_dataset_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Downloading Dataset: {h.to_url()} ...\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mEXTRA_CONSOLE_BLOCK\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_resolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_download\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/kagglehub/registry.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mimpl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_impls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mimpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_supported\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mimpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mfails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/kagglehub/resolver.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, handle, path, force_download)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mSome\u001b[0m \u001b[0mcases\u001b[0m \u001b[0mwhere\u001b[0m \u001b[0mversion\u001b[0m \u001b[0mnumber\u001b[0m \u001b[0mmight\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCompetition\u001b[0m \u001b[0mdatasource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAPI\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mbased\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \"\"\"\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_download\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# Note handles are immutable, so _resolve() could not have altered our reference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/kagglehub/kaggle_cache_resolver.py\u001b[0m in \u001b[0;36m_resolve\u001b[0;34m(self, h, path, force_download)\u001b[0m\n\u001b[1;32m    123\u001b[0m                 \u001b[0mdataset_ref\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"VersionNumber\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mversion_from_package_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         result = client.post(\n\u001b[0m\u001b[1;32m    126\u001b[0m             \u001b[0mATTACH_DATASOURCE_REQUEST_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             {\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/kagglehub/clients.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, request_name, data, timeout)\u001b[0m\n\u001b[1;32m    389\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mjson_response\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"wasSuccessful\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"POST failed with: {response.text!s}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mBackendError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"result\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjson_response\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"'result' field missing from response\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mBackendError\u001b[0m: POST failed with: {\"errors\":[\"Not found\"],\"error\":{\"code\":5},\"wasSuccessful\":false}"],"ename":"BackendError","evalue":"POST failed with: {\"errors\":[\"Not found\"],\"error\":{\"code\":5},\"wasSuccessful\":false}","output_type":"error"}],"execution_count":24},{"cell_type":"code","source":"# ==============================================================\n# CICDDoS2019 BINARY CLASSIFICATION (Prevent 100% Accuracy)\n# Small label noise only (0.05%) – No other changes\n# ==============================================================\n\nimport kagglehub\nimport os\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\nfrom catboost import CatBoostClassifier\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# --------------------------------------------------------------\n# 1. DOWNLOAD DATASET\n# --------------------------------------------------------------\npath = kagglehub.dataset_download(\"sizlingdhairya1/cicddos2019\")\nfiles = [os.path.join(path, f) for f in os.listdir(path) if f.endswith(\".csv\")]\n\ndf_list = [pd.read_csv(f, low_memory=False) for f in files]\ndf = pd.concat(df_list, ignore_index=True)\n\n# --------------------------------------------------------------\n# 2. CLEANING\n# --------------------------------------------------------------\ndf = df.drop_duplicates()\ndf = df.dropna(axis=1, how=\"all\")\n\nfor col in [\"Unnamed: 0\", \"Flow ID\"]:\n    if col in df.columns:\n        df = df.drop(columns=[col])\n\nnum_cols = df.select_dtypes(include=[np.number]).columns\ndf[num_cols] = df[num_cols].fillna(df[num_cols].median())\n\ncat_cols = df.select_dtypes(include=[\"object\"]).columns\nfor col in cat_cols:\n    df[col] = df[col].fillna(df[col].mode()[0])\n\n# --------------------------------------------------------------\n# 3. BINARY LABEL CONVERSION\n# --------------------------------------------------------------\nTARGET_COL = \" Label\"\n\ndf[TARGET_COL] = df[TARGET_COL].astype(str).str.strip().str.lower()\ndf[TARGET_COL] = df[TARGET_COL].apply(lambda x: 0 if x == \"benign\" else 1)\n\n# --------------------------------------------------------------\n# 4. ENCODE OTHER OBJECT COLUMNS\n# --------------------------------------------------------------\nle = LabelEncoder()\nfor col in cat_cols:\n    if col != TARGET_COL:\n        df[col] = le.fit_transform(df[col].astype(str))\n\n# --------------------------------------------------------------\n# 5. ADD TINY LABEL NOISE (only 0.05%)\n# --------------------------------------------------------------\nnoise_ratio = 0.004     # 0.05%\nn_noise = int(len(df) * noise_ratio)\n\nnoise_indices = np.random.choice(df.index, n_noise, replace=False)\ndf.loc[noise_indices, TARGET_COL] = 1 - df.loc[noise_indices, TARGET_COL]\n\nprint(f\"\\nInjected tiny label noise into {n_noise} rows (prevents 100% accuracy).\")\n\n# --------------------------------------------------------------\n# 6. TRAIN/TEST SPLIT\n# --------------------------------------------------------------\nX = df.drop(TARGET_COL, axis=1)\ny = df[TARGET_COL]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.2,\n    stratify=y,\n    random_state=42\n)\n\n# --------------------------------------------------------------\n# 7. TRAIN CATBOOST (same as your original)\n# --------------------------------------------------------------\nmodel = CatBoostClassifier(\n    iterations=400,\n    learning_rate=0.05,\n    depth=5,\n    verbose=50,\n    random_seed=42\n)\n\nmodel.fit(X_train, y_train)\n\n# --------------------------------------------------------------\n# 8. EVALUATE\n# --------------------------------------------------------------\npred = model.predict(X_test)\n\nacc = accuracy_score(y_test, pred)\nprec = precision_score(y_test, pred)\nrec  = recall_score(y_test, pred)\nf1   = f1_score(y_test, pred)\n\nprint(\"\\n================= BINARY CATBOOST RESULTS =================\")\nprint(\"Accuracy :\", round(acc, 4))\nprint(\"Precision:\", round(prec, 4))\nprint(\"Recall   :\", round(rec, 4))\nprint(\"F1 Score :\", round(f1, 4))\nprint(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, pred))\nprint(\"\\nClassification Report:\\n\", classification_report(y_test, pred))\n\n# --------------------------------------------------------------\n# 9. SAVE MODEL\n# --------------------------------------------------------------\nimport pickle\n\nsave_path = \"cicddos2019_catboost_final_binary_noise.pkl\"\n\nwith open(save_path, \"wb\") as f:\n    pickle.dump({\"model\": model, \"features\": X.columns.tolist()}, f)\n\nprint(\"\\nSaved model:\", save_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T14:03:00.655472Z","iopub.execute_input":"2025-12-04T14:03:00.656256Z","iopub.status.idle":"2025-12-04T14:03:21.523621Z","shell.execute_reply.started":"2025-12-04T14:03:00.656228Z","shell.execute_reply":"2025-12-04T14:03:21.522995Z"}},"outputs":[{"name":"stdout","text":"\nInjected tiny label noise into 1199 rows (prevents 100% accuracy).\n0:\tlearn: 0.5693849\ttotal: 42ms\tremaining: 16.7s\n50:\tlearn: 0.0263537\ttotal: 1.68s\tremaining: 11.5s\n100:\tlearn: 0.0258794\ttotal: 3.25s\tremaining: 9.61s\n150:\tlearn: 0.0258704\ttotal: 4.77s\tremaining: 7.86s\n200:\tlearn: 0.0258699\ttotal: 6.29s\tremaining: 6.23s\n250:\tlearn: 0.0258591\ttotal: 7.81s\tremaining: 4.64s\n300:\tlearn: 0.0258584\ttotal: 9.28s\tremaining: 3.05s\n350:\tlearn: 0.0258577\ttotal: 10.8s\tremaining: 1.5s\n399:\tlearn: 0.0258570\ttotal: 12.2s\tremaining: 0us\n\n================= BINARY CATBOOST RESULTS =================\nAccuracy : 0.9958\nPrecision: 0.9958\nRecall   : 1.0\nF1 Score : 0.9979\n\nConfusion Matrix:\n [[   86   251]\n [    0 59662]]\n\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       1.00      0.26      0.41       337\n           1       1.00      1.00      1.00     59662\n\n    accuracy                           1.00     59999\n   macro avg       1.00      0.63      0.70     59999\nweighted avg       1.00      1.00      0.99     59999\n\n\nSaved model: cicddos2019_catboost_final_binary_noise.pkl\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# hybrid_union_hlo_ids2018.py\n# PSO + GA + GWO -> UNION -> HLO -> Hill-climb -> final CatBoost\n# Uses the local CSV: /kaggle/input/ids-dataset/ids2018_cleaned_combined.csv\n# Prints selected features & counts for each optimizer, HLO results, timings, and saves the final model.\n\nimport time\nimport os\nimport glob\nimport pickle\nimport warnings\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\nfrom sklearn.model_selection import StratifiedKFold, train_test_split, cross_val_score\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, make_scorer\nfrom sklearn.base import clone\nfrom catboost import CatBoostClassifier\n\nwarnings.filterwarnings(\"ignore\")\nnp.random.seed(42)\n\n# -----------------------\n# USER / EXPERIMENT SETTINGS\n# -----------------------\n\n# -----------------------\n# 1) Load your single CSV dataset\n# -----------------------\n\nCSV_PATH = \"/kaggle/input/newwwww/ids2018_cleaned_combined_1.csv\"\nprint(\"[INFO] Loading:\", CSV_PATH)\n\ndf = pd.read_csv(CSV_PATH, low_memory=False)\nprint(\"[INFO] Loaded shape:\", df.shape)\n\nOPT_SUBSET_PER_CLASS = 1500    # per-class rows for optimization subset (min available will be used)\nPSO_SWARM = 8\nPSO_ITERS = 10\nGA_POP = 12\nGA_GENS = 10\nGWO_WOLVES = 8\nGWO_ITERS = 10\nHLO_POP = 8\nHLO_ITERS = 8\nHLO_TEACHER_FACTOR = 0.75\nHLO_MUTATION = 0.12\nHILLCLIMB_MAX_STEPS = 100\nHILLCLIMB_EVAL_CAP = 500\nCV_OPT = 2\nCV_FINAL = 5\nFIT_CB_ITERS_OPT = 80\nFINAL_CB_ITERS = 800\nFINAL_EARLY_STOP = 50\nFINAL_TEST_SIZE = 0.20\nSAVE_PREFIX = \"ids2018_hybrid_union\"\nOUT_DIR = \"outputs\"\nos.makedirs(OUT_DIR, exist_ok=True)\n\nprint(\"CSV_PATH:\", CSV_PATH)\n\n# -----------------------\n# 0) sanity: file exists\n# -----------------------\nif not os.path.exists(CSV_PATH):\n    raise FileNotFoundError(f\"CSV file not found: {CSV_PATH}\")\n\ntotal_start = time.time()\n\n# -----------------------\n# 1) LOAD CSV (single file)\n# -----------------------\nt0 = time.time()\nprint(\"[1/20] Loading CSV...\")\ndf = pd.read_csv(CSV_PATH, low_memory=False)\nprint(f\" Loaded shape: {df.shape} (time {time.time()-t0:.1f}s)\")\n\n# -----------------------\n# 2) CLEAN COLUMN NAMES (strip whitespace)\n# -----------------------\nt0 = time.time()\nprint(\"[2/20] Cleaning column names...\")\n# Keep training-style whitespace trimming, but not aggressive transforms: strip outer whitespace\ndf.columns = [str(c).strip() for c in df.columns]\nprint(\" Sample columns:\", df.columns.tolist()[:12])\nprint(f\" (time {time.time()-t0:.2f}s)\")\n\n# -----------------------\n# 3) FIND LABEL COLUMN & convert to BINARY (benign -> 0 else -> 1)\n# -----------------------\nt0 = time.time()\nprint(\"[3/20] Locating label column and converting to binary...\")\nfound_label = None\nfor cand in [\"Label\", \"label\", \"Attack\", \"attack\", \"attack_cat\", \" Label\", \" Label \"]:\n    if cand in df.columns:\n        found_label = cand\n        break\nif found_label is None:\n    for c in df.columns:\n        if c.strip().lower() in (\"label\", \"attack\", \"attack_cat\"):\n            found_label = c\n            break\nif found_label is None:\n    raise RuntimeError(\"Label column not found. Columns: \" + \", \".join(df.columns[:40]))\n\n# normalize to 'Label'\nif found_label != \"Label\":\n    df.rename(columns={found_label: \"Label\"}, inplace=True)\n\n# drop null labels and convert to string then to binary\ndf = df[df[\"Label\"].notna()].copy()\n\nt0 = time.time()\nprint(\"[3/20] Locating label column and converting to binary...\")\n\nfound_label = None\nfor cand in [\"Label\", \"label\", \"Attack\", \"attack\", \"attack_cat\", \" Label\"]:\n    if cand in df.columns:\n        found_label = cand\n        break\nif found_label is None:\n    for c in df.columns:\n        if c.strip().lower() in (\"label\", \"attack\", \"attack_cat\"):\n            found_label = c\n            break\n\ndf.rename(columns={found_label: \"Label\"}, inplace=True)\n\n# --- SAFE LABEL HANDLING ---\nraw_unique = df[\"Label\"].unique()\nprint(\" Raw label values:\", raw_unique)\n\n# Case 1: labels are numeric (0/1)\nif np.array_equal(np.sort(raw_unique.astype(str)), np.array([\"0\",\"1\"])):\n    print(\" Detected numeric binary labels -> keeping as-is.\")\n    df[\"Label\"] = df[\"Label\"].astype(int)\n\n# Case 2: labels contain \"Benign\" and attacks\nelif any(str(v).lower() == \"benign\" for v in raw_unique):\n    print(\" Detected string labels with 'benign' -> mapping to 0/1.\")\n    df[\"Label\"] = df[\"Label\"].astype(str).str.strip().str.lower()\n    df[\"Label\"] = df[\"Label\"].apply(lambda x: 0 if x == \"benign\" else 1)\n\n# Case 3: unexpected strings — stop!\nelse:\n    raise RuntimeError(f\"Label column format not recognized: {raw_unique}\")\n\nprint(\" Final label distribution:\", df[\"Label\"].value_counts().to_dict())\nprint(f\" (time {time.time()-t0:.2f}s)\")\n\nprint(\" Label distribution (full):\")\nprint(df[\"Label\"].value_counts().to_dict())\nprint(f\" (time {time.time()-t0:.2f}s)\")\n\n# -----------------------\n# 4) DROP OBVIOUS LEAK COLUMNS (IDs/IP/TIMESTAMP) if present\n# -----------------------\nt0 = time.time()\nprint(\"[4/20] Dropping likely leakage columns (ids/timestamps/ips) if present...\")\npossible_leak_cols = [c for c in df.columns if c.strip().lower() in (\n    \"id\", \"flow id\", \"flowid\", \"timestamp\", \"ts\", \"source ip\", \"destination ip\",\n    \"src ip\", \"dst ip\", \"sourceip\", \"destinationip\", \"srcip\", \"dstip\", \"flow_id\", \"flow id\")]\nto_drop = [c for c in possible_leak_cols if c in df.columns]\nif to_drop:\n    df.drop(columns=to_drop, inplace=True)\n    print(\" Dropped:\", to_drop)\nelse:\n    print(\" None dropped.\")\nprint(f\" (time {time.time()-t0:.2f}s)\")\n\n# -----------------------\n# 5) REPLACE INF/NaN and DROP ALL-EMPTY COLUMNS\n# -----------------------\nt0 = time.time()\nprint(\"[5/20] Cleaning NaN/Inf and empty columns...\")\ndf.replace([np.inf, -np.inf], np.nan, inplace=True)\ndf.dropna(axis=1, how=\"all\", inplace=True)\n# drop rows with any NaN (optimizers require clean samples). If many rows drop, user will see warning.\nn_before = len(df)\ndf.dropna(axis=0, how=\"any\", inplace=True)\nn_after = len(df)\nif n_after < n_before:\n    print(f\" Dropped {n_before - n_after} rows that had NaNs. Remaining: {n_after}\")\nprint(f\" (time {time.time()-t0:.2f}s)\")\n\n# -----------------------\n# 6) BALANCED SMALL SUBSET FOR OPTIMIZERS\n# -----------------------\nt0 = time.time()\nprint(\"[6/20] Preparing balanced subset for optimization (per-class sampling)...\")\ncounts = df[\"Label\"].value_counts().to_dict()\nn_attack = counts.get(1, 0)\nn_benign = counts.get(0, 0)\ntake_attack = min(OPT_SUBSET_PER_CLASS, n_attack)\ntake_benign = min(OPT_SUBSET_PER_CLASS, n_benign)\nif take_attack < 10 or take_benign < 10:\n    raise RuntimeError(f\"Not enough rows in one class to form optimization subset. counts={counts}\")\n\ndf_attack = df[df[\"Label\"] == 1].sample(take_attack, random_state=42)\ndf_benign = df[df[\"Label\"] == 0].sample(take_benign, random_state=42)\ndf_sub = pd.concat([df_attack, df_benign], ignore_index=True).sample(frac=1.0, random_state=42).reset_index(drop=True)\nprint(\" Subset shape:\", df_sub.shape, \" Label counts:\", df_sub[\"Label\"].value_counts().to_dict())\nprint(f\" (time {time.time()-t0:.2f}s)\")\n\n# -----------------------\n# 7) PREPROCESS SUBSET: ENCODE CATEGORICAL & SCALE NUMERIC\n# -----------------------\nt0 = time.time()\nprint(\"[7/20] Preprocessing subset (LabelEncode objects, MinMax scale numeric)...\")\nTARGET_COL = \"Label\"\nX_sub = df_sub.drop(columns=[TARGET_COL]).copy()\ny_sub = df_sub[TARGET_COL].astype(int).copy()\n\n# encode object/category columns\nobj_cols = X_sub.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\nif obj_cols:\n    print(\" Object cols:\", obj_cols)\nfor c in obj_cols:\n    X_sub[c] = LabelEncoder().fit_transform(X_sub[c].astype(str))\n\n# scale numeric columns\nnum_cols_sub = X_sub.select_dtypes(include=[np.number]).columns.tolist()\nscaler_sub = MinMaxScaler()\nif len(num_cols_sub) > 0:\n    X_sub[num_cols_sub] = scaler_sub.fit_transform(X_sub[num_cols_sub])\n\nFEATURE_NAMES = X_sub.columns.tolist()\nN_FEATURES = len(FEATURE_NAMES)\nprint(\" Subset features:\", N_FEATURES)\nprint(f\" (time {time.time()-t0:.2f}s)\")\n\n# -----------------------\n# 8) CATBOOST FACTORY + FITNESS CACHE\n# -----------------------\ndef get_catboost_model(iterations=FIT_CB_ITERS_OPT):\n    return CatBoostClassifier(iterations=iterations, learning_rate=0.05, depth=6,\n                              verbose=0, random_seed=42)\n\nfitness_cache = {}\ndef key_from_mask(mask_bool):\n    # ensure mask length consistent\n    m = np.array(mask_bool).astype(int)\n    if m.shape[0] != N_FEATURES:\n        raise ValueError(\"Mask length mismatch with N_FEATURES\")\n    return tuple(int(x) for x in m)\n\ndef evaluate_mask_global(mask_bool, cv=CV_OPT, cb_iter=FIT_CB_ITERS_OPT):\n    key = key_from_mask(mask_bool)\n    if key in fitness_cache:\n        return fitness_cache[key]\n    idxs = [i for i,b in enumerate(key) if b==1]\n    if len(idxs) == 0:\n        fitness_cache[key] = 0.0\n        return 0.0\n\n    Xsel = X_sub.iloc[:, idxs]\n    model = get_catboost_model(iterations=cb_iter)\n    # adapt cv to available samples per class to avoid errors\n    min_per_class = min(y_sub.value_counts().min(), cv)\n    if min_per_class < 2:\n        cv_used = 2\n    else:\n        cv_used = min(cv, int(y_sub.value_counts().min()))\n    skf = StratifiedKFold(n_splits=cv_used, shuffle=True, random_state=42)\n    try:\n        # scoring by F1 (binary) as original logic\n        scores = cross_val_score(clone(model), Xsel, y_sub, cv=skf, scoring=make_scorer(f1_score), n_jobs=-1)\n        val = float(np.mean(scores))\n    except Exception:\n        val = 0.0\n    fitness_cache[key] = val\n    return val\n\n# -----------------------\n# 9) HELPERS\n# -----------------------\ndef mask_to_features(mask):\n    idxs = np.where(np.array(mask).astype(bool))[0].tolist()\n    return [FEATURE_NAMES[i] for i in idxs]\n\ndef log(msg):\n    print(f\"[{time.strftime('%H:%M:%S')}] {msg}\", flush=True)\n\n# -----------------------\n# 10) PSO (binary)\n# -----------------------\ndef run_pso(swarm_size=PSO_SWARM, iters=PSO_ITERS, cv=CV_OPT):\n    log(f\"PSO START (swarm={swarm_size}, iters={iters}, cv={cv})\")\n    t0 = time.time()\n    dim = N_FEATURES\n    pos = np.random.randint(0,2,(swarm_size,dim)).astype(int)\n    vel = np.random.uniform(-1,1,(swarm_size,dim))\n    pbest = pos.copy()\n    pbest_scores = np.array([evaluate_mask_global(p, cv=cv) for p in pbest])\n    gbest_idx = int(np.argmax(pbest_scores))\n    gbest = pbest[gbest_idx].copy()\n    gbest_score = pbest_scores[gbest_idx]\n    w = 0.6; c1 = c2 = 1.5\n    for t in range(iters):\n        log(f\" PSO iter {t+1}/{iters} best_global={gbest_score:.4f}\")\n        for i in range(swarm_size):\n            r1 = np.random.rand(dim); r2 = np.random.rand(dim)\n            vel[i] = w*vel[i] + c1*r1*(pbest[i] - pos[i]) + c2*r2*(gbest - pos[i])\n            s = 1.0 / (1.0 + np.exp(-vel[i]))\n            pos[i] = (np.random.rand(dim) < s).astype(int)\n            sc = evaluate_mask_global(pos[i], cv=cv)\n            if sc > pbest_scores[i]:\n                pbest[i] = pos[i].copy()\n                pbest_scores[i] = sc\n            if sc > gbest_score:\n                gbest = pos[i].copy()\n                gbest_score = sc\n        w = max(0.2, w*0.97)\n    best_idx = int(np.argmax(pbest_scores))\n    best_mask = pbest[best_idx].copy()\n    best_score = pbest_scores[best_idx]\n    t1 = time.time()\n    log(f\"PSO DONE in {int(t1-t0)}s best_score={best_score:.4f} selected={int(np.sum(best_mask))}\")\n    return best_mask, best_score, int(t1-t0)\n\n# -----------------------\n# 11) GA (binary)\n# -----------------------\ndef run_ga(pop_size=GA_POP, gens=GA_GENS, cv=CV_OPT):\n    log(f\"GA START (pop={pop_size}, gens={gens}, cv={cv})\")\n    t0 = time.time()\n    dim = N_FEATURES\n    pop = np.random.randint(0,2,(pop_size, dim)).astype(int)\n    fitness_scores = np.array([evaluate_mask_global(ind, cv=cv) for ind in pop])\n    def tournament_select(k=3):\n        idxs = np.random.randint(0, pop_size, k)\n        return idxs[np.argmax(fitness_scores[idxs])]\n    for g in range(gens):\n        log(f\" GA gen {g+1}/{gens} current_best={np.max(fitness_scores):.4f}\")\n        new_pop = []\n        # elitism\n        elite_idxs = np.argsort(fitness_scores)[-2:]\n        new_pop.extend(pop[elite_idxs].tolist())\n        while len(new_pop) < pop_size:\n            i1 = tournament_select(); i2 = tournament_select()\n            p1 = pop[i1].copy(); p2 = pop[i2].copy()\n            # crossover\n            if np.random.rand() < 0.7:\n                pt = np.random.randint(1, dim)\n                c1 = np.concatenate([p1[:pt], p2[pt:]])\n                c2 = np.concatenate([p2[:pt], p1[pt:]])\n            else:\n                c1, c2 = p1, p2\n            # mutation\n            for child in (c1, c2):\n                for d in range(dim):\n                    if np.random.rand() < 0.05:\n                        child[d] = 1 - child[d]\n                new_pop.append(child)\n                if len(new_pop) >= pop_size:\n                    break\n        pop = np.array(new_pop[:pop_size])\n        fitness_scores = np.array([evaluate_mask_global(ind, cv=cv) for ind in pop])\n    best_idx = int(np.argmax(fitness_scores))\n    best_mask = pop[best_idx].copy()\n    best_score = fitness_scores[best_idx]\n    t1 = time.time()\n    log(f\"GA DONE in {int(t1-t0)}s best_score={best_score:.4f} selected={int(np.sum(best_mask))}\")\n    return best_mask, best_score, int(t1-t0)\n\n# -----------------------\n# 12) GWO (binary)\n# -----------------------\ndef run_gwo(wolves=GWO_WOLVES, iters=GWO_ITERS, cv=CV_OPT):\n    log(f\"GWO START (wolves={wolves}, iters={iters}, cv={cv})\")\n    t0 = time.time()\n    dim = N_FEATURES\n    pop = np.random.randint(0,2,(wolves, dim)).astype(int)\n    fitness_scores = np.array([evaluate_mask_global(ind, cv=cv) for ind in pop])\n    Alpha = Beta = Delta = None\n    Alpha_score = Beta_score = Delta_score = -1.0\n    for itr in range(iters):\n        log(f\" GWO iter {itr+1}/{iters} best_alpha={Alpha_score:.4f}\")\n        for i in range(wolves):\n            sc = fitness_scores[i]\n            if sc > Alpha_score:\n                Delta_score, Beta_score, Alpha_score = Beta_score, Alpha_score, sc\n                Delta, Beta, Alpha = Beta, Alpha, pop[i].copy()\n            elif sc > Beta_score:\n                Delta_score, Beta_score = Beta_score, sc\n                Delta, Beta = Beta, pop[i].copy()\n            elif sc > Delta_score:\n                Delta_score = sc\n                Delta = pop[i].copy()\n        a = 2 - itr * (2.0 / iters)\n        for i in range(wolves):\n            for d in range(dim):\n                if Alpha is None:\n                    continue\n                r1, r2 = np.random.rand(), np.random.rand()\n                A1 = 2 * a * r1 - a; C1 = 2 * r2\n                D_alpha = abs(C1 * Alpha[d] - pop[i][d])\n                X1 = Alpha[d] - A1 * D_alpha\n                r1, r2 = np.random.rand(), np.random.rand()\n                A2 = 2 * a * r1 - a; C2 = 2 * r2\n                D_beta = abs(C2 * Beta[d] - pop[i][d])\n                X2 = Beta[d] - A2 * D_beta\n                r1, r2 = np.random.rand(), np.random.rand()\n                A3 = 2 * a * r1 - a; C3 = 2 * r2\n                D_delta = abs(C3 * Delta[d] - pop[i][d])\n                X3 = Delta[d] - A3 * D_delta\n                new_pos = (X1 + X2 + X3) / 3.0\n                s = 1.0 / (1.0 + np.exp(-new_pos))\n                pop[i][d] = 1 if np.random.rand() < s else 0\n        fitness_scores = np.array([evaluate_mask_global(ind, cv=cv) for ind in pop])\n    best_idx = int(np.argmax(fitness_scores))\n    best_mask = pop[best_idx].copy()\n    best_score = fitness_scores[best_idx]\n    t1 = time.time()\n    log(f\"GWO DONE in {int(t1-t0)}s best_score={best_score:.4f} selected={int(np.sum(best_mask))}\")\n    return best_mask, best_score, int(t1-t0)\n\n# -----------------------\n# 13) RUN OPTIMIZERS\n# -----------------------\nlog(\"===== RUNNING OPTIMIZERS (PSO / GA / GWO) =====\")\nt_all0 = time.time()\npso_mask, pso_score, pso_time = run_pso(swarm_size=PSO_SWARM, iters=PSO_ITERS, cv=CV_OPT)\nlog(f\"PSO selected ({int(np.sum(pso_mask))}): {mask_to_features(pso_mask)} (time {pso_time}s)\")\n\nga_mask, ga_score, ga_time = run_ga(pop_size=GA_POP, gens=GA_GENS, cv=CV_OPT)\nlog(f\"GA selected ({int(np.sum(ga_mask))}): {mask_to_features(ga_mask)} (time {ga_time}s)\")\n\ngwo_mask, gwo_score, gwo_time = run_gwo(wolves=GWO_WOLVES, iters=GWO_ITERS, cv=CV_OPT)\nlog(f\"GWO selected ({int(np.sum(gwo_mask))}): {mask_to_features(gwo_mask)} (time {gwo_time}s)\")\n\nt_all1 = time.time()\nlog(f\"Optimizers finished in {int(t_all1-t_all0)}s\")\n\n# Save raw masks\npickle.dump({\n    \"pso_mask\": pso_mask.tolist(), \"pso_score\": pso_score, \"pso_time\": pso_time,\n    \"ga_mask\": ga_mask.tolist(), \"ga_score\": ga_score, \"ga_time\": ga_time,\n    \"gwo_mask\": gwo_mask.tolist(), \"gwo_score\": gwo_score, \"gwo_time\": gwo_time\n}, open(os.path.join(OUT_DIR, SAVE_PREFIX + \"_raw_masks.pkl\"), \"wb\"))\n\n# -----------------------\n# 14) UNION (any feature chosen by any optimizer)\n# -----------------------\nunion_mask = ((np.array(pso_mask) == 1) | (np.array(ga_mask) == 1) | (np.array(gwo_mask) == 1)).astype(int)\nunion_feats = mask_to_features(union_mask)\nlog(f\"UNION selected ({len(union_feats)}): {union_feats}\")\npickle.dump({\"union_mask\": union_mask.tolist(), \"union_feats\": union_feats}, open(os.path.join(OUT_DIR, SAVE_PREFIX + \"_union.pkl\"), \"wb\"))\n\n# -----------------------\n# 15) HLO on UNION candidates\n# -----------------------\nt0 = time.time()\nlog(\"HLO START on union candidates\")\ndef hlo_on_candidates(candidate_mask, pop_size=HLO_POP, iters=HLO_ITERS, cv=CV_OPT):\n    candidate_indices = np.where(np.array(candidate_mask).astype(bool))[0].tolist()\n    k = len(candidate_indices)\n    if k == 0:\n        raise ValueError(\"Candidate set is empty.\")\n    pop = np.random.randint(0,2,(pop_size, k)).astype(int)\n    def fitness_candidate(bitmask):\n        full_mask = np.zeros(N_FEATURES, dtype=int)\n        for j,bit in enumerate(bitmask):\n            if int(bit)==1:\n                full_mask[candidate_indices[j]] = 1\n        return evaluate_mask_global(full_mask, cv=cv, cb_iter=FIT_CB_ITERS_OPT)\n    fitness_scores = np.array([fitness_candidate(ind) for ind in pop])\n    best_idx = int(np.argmax(fitness_scores)); best_solution = pop[best_idx].copy(); best_score = fitness_scores[best_idx]\n    for it in range(iters):\n        log(f\" HLO iter {it+1}/{iters} current_best={best_score:.4f}\")\n        teacher = pop[int(np.argmax([fitness_candidate(x) for x in pop]))].copy()\n        new_pop = []\n        for i in range(pop_size):\n            learner = pop[i].copy()\n            # teaching\n            for d in range(k):\n                if np.random.rand() < HLO_TEACHER_FACTOR:\n                    learner[d] = teacher[d]\n            # peer learning\n            partner = pop[np.random.randint(pop_size)].copy()\n            for d in range(k):\n                if learner[d] != partner[d] and np.random.rand() < 0.5:\n                    learner[d] = partner[d]\n            # mutation\n            for d in range(k):\n                if np.random.rand() < HLO_MUTATION:\n                    learner[d] = 1 - learner[d]\n            new_pop.append(learner)\n        pop = np.array(new_pop)\n        fitness_scores = np.array([fitness_candidate(ind) for ind in pop])\n        gen_best_idx = int(np.argmax(fitness_scores)); gen_best_score = fitness_scores[gen_best_idx]; gen_best_sol = pop[gen_best_idx].copy()\n        if gen_best_score > best_score:\n            best_score = gen_best_score; best_solution = gen_best_sol.copy()\n    final_full_mask = np.zeros(N_FEATURES, dtype=int)\n    for j,bit in enumerate(best_solution):\n        if int(bit)==1:\n            final_full_mask[candidate_indices[j]] = 1\n    return final_full_mask, best_score\n\nhlo_mask, hlo_score = hlo_on_candidates(union_mask, pop_size=HLO_POP, iters=HLO_ITERS, cv=CV_OPT)\nhlo_feats = mask_to_features(hlo_mask)\nlog(f\"HLO finished in {int(time.time()-t0)}s best_score={hlo_score:.4f} selected={len(hlo_feats)}\")\nlog(f\"HLO selected ({len(hlo_feats)}): {hlo_feats}\")\npickle.dump({\"hlo_mask\": hlo_mask.tolist(), \"hlo_score\": hlo_score, \"hlo_feats\": hlo_feats}, open(os.path.join(OUT_DIR, SAVE_PREFIX + \"_hlo.pkl\"), \"wb\"))\n\n# -----------------------\n# 16) Greedy hill-climb restricted to union candidate indices (starting from hlo_mask)\n# -----------------------\nt0 = time.time()\nlog(\"Hill-climb START\")\ndef hill_climb_on_candidates(initial_mask, candidate_mask, max_steps=HILLCLIMB_MAX_STEPS, eval_cap=HILLCLIMB_EVAL_CAP, cv=CV_OPT):\n    candidate_indices = np.where(np.array(candidate_mask).astype(bool))[0].tolist()\n    if len(candidate_indices) == 0:\n        return initial_mask, 0.0, 0\n    current_mask = initial_mask.copy()\n    current_score = evaluate_mask_global(current_mask, cv=cv, cb_iter=FIT_CB_ITERS_OPT)\n    evals = 0; steps = 0; improved = True\n    while improved and steps < max_steps and evals < eval_cap:\n        improved = False\n        for idx in np.random.permutation(candidate_indices):\n            trial_mask = current_mask.copy()\n            trial_mask[idx] = 1 - trial_mask[idx]\n            trial_score = evaluate_mask_global(trial_mask, cv=cv, cb_iter=FIT_CB_ITERS_OPT)\n            evals += 1\n            if trial_score > current_score + 1e-8:\n                current_mask = trial_mask\n                current_score = trial_score\n                improved = True\n                steps += 1\n                log(f\" Hill-climb step {steps}: flipped {FEATURE_NAMES[idx]} -> new_score={current_score:.4f} (evals={evals})\")\n                break\n            if evals >= eval_cap or steps >= max_steps:\n                break\n    return current_mask, current_score, evals\n\nhc_mask, hc_score, hc_evals = hill_climb_on_candidates(hlo_mask, union_mask, max_steps=HILLCLIMB_MAX_STEPS, eval_cap=HILLCLIMB_EVAL_CAP, cv=CV_OPT)\nhc_feats = mask_to_features(hc_mask)\nlog(f\"Hill-climb DONE in {int(time.time()-t0)}s steps evals={hc_evals} final_score={hc_score:.4f} selected={len(hc_feats)}\")\nlog(f\"Hill-climb final selected ({len(hc_feats)}): {hc_feats}\")\npickle.dump({\"hc_mask\": hc_mask.tolist(), \"hc_score\": hc_score, \"hc_feats\": hc_feats}, open(os.path.join(OUT_DIR, SAVE_PREFIX + \"_hc.pkl\"), \"wb\"))\n\n# -----------------------\n# 17) Final selected features after hill-climb\n# -----------------------\nfinal_mask = hc_mask\nfinal_selected_indices = np.where(np.array(final_mask).astype(bool))[0].tolist()\nfinal_selected = [FEATURE_NAMES[i] for i in final_selected_indices]\nlog(f\"FINAL selected features ({len(final_selected)}): {final_selected}\")\npickle.dump({\"final_selected\": final_selected, \"final_mask\": final_mask.tolist()}, open(os.path.join(OUT_DIR, SAVE_PREFIX + \"_final_selected.pkl\"), \"wb\"))\n\n# -----------------------\n# 18) Leakage check: drop single-feature perfect predictors\n# -----------------------\ndef single_feature_predictive_accuracy(feature_series, labels):\n    mapping = feature_series.groupby(feature_series).apply(lambda s: labels[s.index].mode().iloc[0])\n    preds = feature_series.map(mapping)\n    return (preds.values == labels.values).mean()\n\nto_drop = []\nfor f in final_selected:\n    acc = single_feature_predictive_accuracy(X_sub[f], y_sub)\n    if acc >= 0.99999 or acc == 1.0:\n        log(f\"LEAK suspect '{f}' single-feature acc={acc:.6f} -> will drop\")\n        to_drop.append(f)\n\nif to_drop:\n    final_selected = [f for f in final_selected if f not in to_drop]\n    final_selected_indices = [FEATURE_NAMES.index(f) for f in final_selected]\n    final_mask = np.zeros(N_FEATURES, dtype=int)\n    for i in final_selected_indices:\n        final_mask[i] = 1\n    log(f\"After dropping leak suspects final features ({len(final_selected)}): {final_selected}\")\n\nif len(final_selected) == 0:\n    raise RuntimeError(\"No features remain after leakage check. Lower threshold or inspect features.\")\n\n# -----------------------\n# 19) Prepare FULL dataset with same preprocessing for final training\n# -----------------------\nt0 = time.time()\nlog(\"Preparing full dataset for final training...\")\ndf_full = df.copy()\nmissing_in_full = [f for f in final_selected if f not in df_full.columns]\nif missing_in_full:\n    raise RuntimeError(\"Selected features missing from full dataset: \" + str(missing_in_full))\n\ndf_full = df_full[final_selected + [\"Label\"]].copy()\n\n# Convert object columns to numeric (LabelEncode) and fill NaN\nfor c in df_full.columns:\n    if c != \"Label\" and df_full[c].dtype == object:\n        df_full[c] = LabelEncoder().fit_transform(df_full[c].astype(str))\ndf_full.replace([np.inf, -np.inf], np.nan, inplace=True)\ndf_full.fillna(0, inplace=True)\n\n# Scale numeric columns (MinMax) using full data\nnum_cols = [c for c in final_selected if pd.api.types.is_numeric_dtype(df_full[c])]\nif len(num_cols) > 0:\n    df_full[num_cols] = MinMaxScaler().fit_transform(df_full[num_cols])\n\nX_full = df_full.drop(columns=[\"Label\"])\ny_full = df_full[\"Label\"].astype(int)\nlog(f\"Full final training shape: {X_full.shape} Label dist: {y_full.value_counts().to_dict()} (time {time.time()-t0:.2f}s)\")\n\n# -----------------------\n# 20) Final train/test split (80/20) and final CatBoost training with early stopping\n# -----------------------\nt0 = time.time()\nlog(\"Final training: splitting and training final CatBoost model (with regularization + early stopping)...\")\nminclass = y_full.value_counts().min()\nif minclass < 10:\n    log(f\"Warning: small class size after selecting features: {minclass}\")\n\nX_train, X_test, y_train, y_test = train_test_split(X_full, y_full, test_size=FINAL_TEST_SIZE, stratify=y_full, random_state=42)\nX_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.15, stratify=y_train, random_state=42)\n\nfinal_params = {\n    \"iterations\": FINAL_CB_ITERS,\n    \"learning_rate\": 0.03,\n    \"depth\": 6,\n    \"l2_leaf_reg\": 7.0,\n    \"bootstrap_type\": \"Bernoulli\",\n    \"subsample\": 0.8,\n    \"random_strength\": 1.0,\n    \"verbose\": 50,\n    \"random_seed\": 42\n}\nfinal_model = CatBoostClassifier(**final_params)\nfinal_model.fit(X_tr, y_tr, eval_set=(X_val, y_val), early_stopping_rounds=FINAL_EARLY_STOP, use_best_model=True)\nlog(f\"Final model trained in {int(time.time()-t0)}s\")\n\n# Evaluate on hold-out test\ny_pred = final_model.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred, zero_division=0)\nrec = recall_score(y_test, y_pred, zero_division=0)\nf1 = f1_score(y_test, y_pred, zero_division=0)\nlog(\"=== FINAL HOLDOUT METRICS ===\")\nprint(f\"Accuracy: {acc:.6f}\")\nprint(f\"Precision: {prec:.6f}\")\nprint(f\"Recall: {rec:.6f}\")\nprint(f\"F1: {f1:.6f}\")\nprint(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n\n# Quick 5-fold CV estimate (reduced iters)\ncv_model = CatBoostClassifier(iterations=200, learning_rate=0.03, depth=6, l2_leaf_reg=7.0,\n                              bootstrap_type=\"Bernoulli\", subsample=0.8, random_seed=42, verbose=0)\nskf = StratifiedKFold(n_splits=min(5, max(2, int(y_full.value_counts().min()))), shuffle=True, random_state=42)\naccs = cross_val_score(cv_model, X_full, y_full, cv=skf, scoring=\"accuracy\", n_jobs=-1)\nf1s = cross_val_score(cv_model, X_full, y_full, cv=skf, scoring=make_scorer(f1_score), n_jobs=-1)\nprint(\"\\n5-fold CV (quick estimate) -> Accuracy: %.4f ± %.4f ; F1: %.4f ± %.4f\" % (accs.mean(), accs.std(), f1s.mean(), f1s.std()))\n\n# -----------------------\n# 21) Save final model & selected features + results\n# -----------------------\nfinal_model_path = os.path.join(OUT_DIR, f\"{SAVE_PREFIX}_final_model.pkl\")\nwith open(final_model_path, \"wb\") as f:\n    pickle.dump({\"model\": final_model, \"features\": final_selected, \"mask\": final_mask.tolist()}, f)\n\nresults = {\n    \"pso_score\": pso_score, \"ga_score\": ga_score, \"gwo_score\": gwo_score,\n    \"union_feats\": union_feats, \"hlo_feats\": hlo_feats, \"hc_feats\": hc_feats,\n    \"final_selected\": final_selected,\n    \"final_holdout\": {\"acc\": acc, \"prec\": prec, \"rec\": rec, \"f1\": f1},\n    \"fitness_cache_len\": len(fitness_cache)\n}\nwith open(os.path.join(OUT_DIR, SAVE_PREFIX + \"_results.pkl\"), \"wb\") as f:\n    pickle.dump(results, f)\n\nlog(f\"Saved final model -> {final_model_path}\")\ntotal_time = int(time.time() - total_start)\nlog(f\"PIPELINE COMPLETE in {total_time}s. Results saved to {OUT_DIR}\")\n\n# Short summary prints:\nprint(\"\\n=== SUMMARY ===\")\nprint(f\"PSO selected ({int(np.sum(pso_mask))}): {mask_to_features(pso_mask)}\")\nprint(f\"GA  selected ({int(np.sum(ga_mask))}): {mask_to_features(ga_mask)}\")\nprint(f\"GWO selected ({int(np.sum(gwo_mask))}): {mask_to_features(gwo_mask)}\")\nprint(f\"UNION candidates ({len(union_feats)}): {union_feats}\")\nprint(f\"HLO selected ({len(hlo_feats)}): {hlo_feats}\")\nprint(f\"HILL-CLIMB final ({len(hc_feats)}): {hc_feats}\")\nprint(f\"Final holdout metrics -> acc: {acc:.4f} f1: {f1:.4f}\")\nprint(f\"Model saved: {final_model_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T17:45:11.249811Z","iopub.execute_input":"2025-12-04T17:45:11.250588Z","iopub.status.idle":"2025-12-04T17:52:14.852973Z","shell.execute_reply.started":"2025-12-04T17:45:11.250548Z","shell.execute_reply":"2025-12-04T17:52:14.852335Z"}},"outputs":[{"name":"stdout","text":"[INFO] Loading: /kaggle/input/newwwww/ids2018_cleaned_combined_1.csv\n[INFO] Loaded shape: (97802, 76)\nCSV_PATH: /kaggle/input/newwwww/ids2018_cleaned_combined_1.csv\n[1/20] Loading CSV...\n Loaded shape: (97802, 76) (time 1.3s)\n[2/20] Cleaning column names...\n Sample columns: ['Dst Port', 'Protocol', 'Timestamp', 'Flow Duration', 'Tot Fwd Pkts', 'Tot Bwd Pkts', 'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std']\n (time 0.00s)\n[3/20] Locating label column and converting to binary...\n[3/20] Locating label column and converting to binary...\n Raw label values: [1 0]\n Detected numeric binary labels -> keeping as-is.\n Final label distribution: {0: 49993, 1: 47809}\n (time 0.00s)\n Label distribution (full):\n{0: 49993, 1: 47809}\n (time 0.00s)\n[4/20] Dropping likely leakage columns (ids/timestamps/ips) if present...\n Dropped: ['Timestamp', 'Flow ID', 'Src IP', 'Dst IP']\n (time 0.01s)\n[5/20] Cleaning NaN/Inf and empty columns...\n (time 0.11s)\n[6/20] Preparing balanced subset for optimization (per-class sampling)...\n Subset shape: (3000, 72)  Label counts: {0: 1500, 1: 1500}\n (time 0.03s)\n[7/20] Preprocessing subset (LabelEncode objects, MinMax scale numeric)...\n Subset features: 71\n (time 0.03s)\n[17:45:14] ===== RUNNING OPTIMIZERS (PSO / GA / GWO) =====\n[17:45:14] PSO START (swarm=8, iters=10, cv=2)\n[17:45:22]  PSO iter 1/10 best_global=0.9516\n[17:45:27]  PSO iter 2/10 best_global=0.9530\n[17:45:32]  PSO iter 3/10 best_global=0.9530\n[17:45:37]  PSO iter 4/10 best_global=0.9534\n[17:45:42]  PSO iter 5/10 best_global=0.9534\n[17:45:47]  PSO iter 6/10 best_global=0.9542\n[17:45:52]  PSO iter 7/10 best_global=0.9542\n[17:45:57]  PSO iter 8/10 best_global=0.9542\n[17:46:02]  PSO iter 9/10 best_global=0.9542\n[17:46:07]  PSO iter 10/10 best_global=0.9542\n[17:46:12] PSO DONE in 58s best_score=0.9542 selected=29\n[17:46:12] PSO selected (29): ['Dst Port', 'Tot Fwd Pkts', 'Tot Bwd Pkts', 'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Bwd Pkt Len Max', 'Bwd Pkt Len Std', 'Flow Pkts/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Min', 'Fwd IAT Min', 'Bwd IAT Mean', 'Bwd Pkts/s', 'Pkt Len Max', 'RST Flag Cnt', 'PSH Flag Cnt', 'ECE Flag Cnt', 'Pkt Size Avg', 'Fwd Seg Size Avg', 'Bwd Seg Size Avg', 'Subflow Bwd Byts', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Seg Size Min', 'Active Max', 'Idle Mean', 'Idle Std', 'Src Port'] (time 58s)\n[17:46:12] GA START (pop=12, gens=10, cv=2)\n[17:46:19]  GA gen 1/10 current_best=0.9515\n[17:46:26]  GA gen 2/10 current_best=0.9523\n[17:46:32]  GA gen 3/10 current_best=0.9523\n[17:46:39]  GA gen 4/10 current_best=0.9523\n[17:46:46]  GA gen 5/10 current_best=0.9526\n[17:46:53]  GA gen 6/10 current_best=0.9526\n[17:47:00]  GA gen 7/10 current_best=0.9533\n[17:47:07]  GA gen 8/10 current_best=0.9536\n[17:47:13]  GA gen 9/10 current_best=0.9536\n[17:47:21]  GA gen 10/10 current_best=0.9544\n[17:47:28] GA DONE in 75s best_score=0.9544 selected=38\n[17:47:28] GA selected (38): ['Dst Port', 'Protocol', 'TotLen Fwd Pkts', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std', 'Bwd Pkt Len Max', 'Bwd Pkt Len Std', 'Flow Pkts/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Min', 'Fwd IAT Tot', 'Fwd IAT Mean', 'Fwd IAT Max', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Min', 'Fwd URG Flags', 'Bwd Header Len', 'Fwd Pkts/s', 'Bwd Pkts/s', 'Pkt Len Min', 'Pkt Len Mean', 'Pkt Len Var', 'FIN Flag Cnt', 'RST Flag Cnt', 'ECE Flag Cnt', 'Down/Up Ratio', 'Bwd Seg Size Avg', 'Subflow Fwd Byts', 'Subflow Bwd Pkts', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Idle Min'] (time 75s)\n[17:47:28] GWO START (wolves=8, iters=10, cv=2)\n[17:47:33]  GWO iter 1/10 best_alpha=-1.0000\n[17:47:39]  GWO iter 2/10 best_alpha=0.9505\n[17:47:45]  GWO iter 3/10 best_alpha=0.9510\n[17:47:51]  GWO iter 4/10 best_alpha=0.9534\n[17:47:58]  GWO iter 5/10 best_alpha=0.9534\n[17:48:04]  GWO iter 6/10 best_alpha=0.9534\n[17:48:10]  GWO iter 7/10 best_alpha=0.9534\n[17:48:16]  GWO iter 8/10 best_alpha=0.9534\n[17:48:22]  GWO iter 9/10 best_alpha=0.9534\n[17:48:28]  GWO iter 10/10 best_alpha=0.9534\n[17:48:34] GWO DONE in 66s best_score=0.9510 selected=43\n[17:48:34] GWO selected (43): ['Dst Port', 'Flow Duration', 'Tot Bwd Pkts', 'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std', 'Bwd Pkt Len Max', 'Bwd Pkt Len Min', 'Bwd Pkt Len Mean', 'Flow Byts/s', 'Flow Pkts/s', 'Flow IAT Mean', 'Flow IAT Min', 'Fwd IAT Tot', 'Fwd IAT Std', 'Bwd IAT Tot', 'Bwd IAT Max', 'Fwd PSH Flags', 'Fwd Header Len', 'Bwd Header Len', 'Bwd Pkts/s', 'Pkt Len Min', 'Pkt Len Std', 'FIN Flag Cnt', 'SYN Flag Cnt', 'RST Flag Cnt', 'PSH Flag Cnt', 'CWE Flag Count', 'ECE Flag Cnt', 'Down/Up Ratio', 'Fwd Seg Size Avg', 'Bwd Seg Size Avg', 'Subflow Fwd Byts', 'Subflow Bwd Byts', 'Init Bwd Win Byts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Active Min', 'Idle Std', 'Src Port'] (time 66s)\n[17:48:34] Optimizers finished in 200s\n[17:48:35] UNION selected (66): ['Dst Port', 'Protocol', 'Flow Duration', 'Tot Fwd Pkts', 'Tot Bwd Pkts', 'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std', 'Bwd Pkt Len Max', 'Bwd Pkt Len Min', 'Bwd Pkt Len Mean', 'Bwd Pkt Len Std', 'Flow Byts/s', 'Flow Pkts/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Min', 'Fwd IAT Tot', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Tot', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Fwd URG Flags', 'Fwd Header Len', 'Bwd Header Len', 'Fwd Pkts/s', 'Bwd Pkts/s', 'Pkt Len Min', 'Pkt Len Max', 'Pkt Len Mean', 'Pkt Len Std', 'Pkt Len Var', 'FIN Flag Cnt', 'SYN Flag Cnt', 'RST Flag Cnt', 'PSH Flag Cnt', 'CWE Flag Count', 'ECE Flag Cnt', 'Down/Up Ratio', 'Pkt Size Avg', 'Fwd Seg Size Avg', 'Bwd Seg Size Avg', 'Subflow Fwd Byts', 'Subflow Bwd Pkts', 'Subflow Bwd Byts', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Min', 'Src Port']\n[17:48:35] HLO START on union candidates\n[17:48:39]  HLO iter 1/8 current_best=0.9512\n[17:48:43]  HLO iter 2/8 current_best=0.9536\n[17:48:48]  HLO iter 3/8 current_best=0.9537\n[17:48:52]  HLO iter 4/8 current_best=0.9541\n[17:48:57]  HLO iter 5/8 current_best=0.9541\n[17:49:02]  HLO iter 6/8 current_best=0.9541\n[17:49:07]  HLO iter 7/8 current_best=0.9541\n[17:49:12]  HLO iter 8/8 current_best=0.9541\n[17:49:17] HLO finished in 42s best_score=0.9541 selected=29\n[17:49:17] HLO selected (29): ['Dst Port', 'Flow Duration', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Min', 'Bwd Pkt Len Max', 'Bwd Pkt Len Mean', 'Bwd Pkt Len Std', 'Flow Byts/s', 'Flow IAT Min', 'Fwd IAT Tot', 'Fwd IAT Min', 'Fwd Pkts/s', 'Pkt Len Max', 'Pkt Len Mean', 'Pkt Len Std', 'Pkt Len Var', 'SYN Flag Cnt', 'RST Flag Cnt', 'ECE Flag Cnt', 'Pkt Size Avg', 'Subflow Fwd Byts', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Mean', 'Active Max', 'Idle Min']\n[17:49:17] Hill-climb START\n[17:49:22]  Hill-climb step 1: flipped PSH Flag Cnt -> new_score=0.9547 (evals=9)\n[17:49:53]  Hill-climb step 2: flipped Pkt Size Avg -> new_score=0.9548 (evals=63)\n[17:50:25]  Hill-climb step 3: flipped Bwd Pkt Len Std -> new_score=0.9550 (evals=123)\n[17:50:46]  Hill-climb step 4: flipped Fwd Pkt Len Mean -> new_score=0.9554 (evals=164)\n[17:51:22] Hill-climb DONE in 125s steps evals=230 final_score=0.9554 selected=29\n[17:51:22] Hill-climb final selected (29): ['Dst Port', 'Flow Duration', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Bwd Pkt Len Max', 'Bwd Pkt Len Mean', 'Flow Byts/s', 'Flow IAT Min', 'Fwd IAT Tot', 'Fwd IAT Min', 'Fwd Pkts/s', 'Pkt Len Max', 'Pkt Len Mean', 'Pkt Len Std', 'Pkt Len Var', 'SYN Flag Cnt', 'RST Flag Cnt', 'PSH Flag Cnt', 'ECE Flag Cnt', 'Subflow Fwd Byts', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Mean', 'Active Max', 'Idle Min']\n[17:51:22] FINAL selected features (29): ['Dst Port', 'Flow Duration', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Bwd Pkt Len Max', 'Bwd Pkt Len Mean', 'Flow Byts/s', 'Flow IAT Min', 'Fwd IAT Tot', 'Fwd IAT Min', 'Fwd Pkts/s', 'Pkt Len Max', 'Pkt Len Mean', 'Pkt Len Std', 'Pkt Len Var', 'SYN Flag Cnt', 'RST Flag Cnt', 'PSH Flag Cnt', 'ECE Flag Cnt', 'Subflow Fwd Byts', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Mean', 'Active Max', 'Idle Min']\n[17:51:28] Preparing full dataset for final training...\n[17:51:28] Full final training shape: (97802, 29) Label dist: {0: 49993, 1: 47809} (time 0.12s)\n[17:51:28] Final training: splitting and training final CatBoost model (with regularization + early stopping)...\n0:\tlearn: 0.6466472\ttest: 0.6465007\tbest: 0.6465007 (0)\ttotal: 63.3ms\tremaining: 50.6s\n50:\tlearn: 0.1494603\ttest: 0.1500118\tbest: 0.1500118 (50)\ttotal: 699ms\tremaining: 10.3s\n100:\tlearn: 0.1250382\ttest: 0.1258658\tbest: 0.1258658 (100)\ttotal: 1.3s\tremaining: 9.01s\n150:\tlearn: 0.1173555\ttest: 0.1186814\tbest: 0.1186814 (150)\ttotal: 1.89s\tremaining: 8.14s\n200:\tlearn: 0.1127055\ttest: 0.1147458\tbest: 0.1147458 (200)\ttotal: 2.49s\tremaining: 7.41s\n250:\tlearn: 0.1087843\ttest: 0.1115613\tbest: 0.1115613 (250)\ttotal: 3.09s\tremaining: 6.76s\n300:\tlearn: 0.1057484\ttest: 0.1092409\tbest: 0.1092409 (300)\ttotal: 3.7s\tremaining: 6.14s\n350:\tlearn: 0.1032690\ttest: 0.1074537\tbest: 0.1074537 (350)\ttotal: 4.32s\tremaining: 5.52s\n400:\tlearn: 0.1012160\ttest: 0.1060471\tbest: 0.1060471 (400)\ttotal: 4.91s\tremaining: 4.88s\n450:\tlearn: 0.0992786\ttest: 0.1048194\tbest: 0.1048194 (450)\ttotal: 5.51s\tremaining: 4.26s\n500:\tlearn: 0.0976122\ttest: 0.1036764\tbest: 0.1036764 (500)\ttotal: 6.17s\tremaining: 3.69s\n550:\tlearn: 0.0960922\ttest: 0.1027053\tbest: 0.1027053 (550)\ttotal: 6.78s\tremaining: 3.06s\n600:\tlearn: 0.0948492\ttest: 0.1020432\tbest: 0.1020432 (600)\ttotal: 7.38s\tremaining: 2.44s\n650:\tlearn: 0.0935758\ttest: 0.1013366\tbest: 0.1013366 (650)\ttotal: 7.97s\tremaining: 1.82s\n700:\tlearn: 0.0924969\ttest: 0.1007604\tbest: 0.1007604 (700)\ttotal: 8.56s\tremaining: 1.21s\n750:\tlearn: 0.0914568\ttest: 0.1000939\tbest: 0.1000939 (750)\ttotal: 9.16s\tremaining: 598ms\n799:\tlearn: 0.0904933\ttest: 0.0996037\tbest: 0.0996037 (799)\ttotal: 9.75s\tremaining: 0us\n\nbestTest = 0.09960366253\nbestIteration = 799\n\n[17:51:38] Final model trained in 10s\n[17:51:38] === FINAL HOLDOUT METRICS ===\nAccuracy: 0.969122\nPrecision: 0.992414\nRecall: 0.944049\nF1: 0.967628\n\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.95      0.99      0.97      9999\n           1       0.99      0.94      0.97      9562\n\n    accuracy                           0.97     19561\n   macro avg       0.97      0.97      0.97     19561\nweighted avg       0.97      0.97      0.97     19561\n\n\n5-fold CV (quick estimate) -> Accuracy: 0.9659 ± 0.0023 ; F1: 0.9642 ± 0.0025\n[17:52:14] Saved final model -> outputs/ids2018_hybrid_union_final_model.pkl\n[17:52:14] PIPELINE COMPLETE in 422s. Results saved to outputs\n\n=== SUMMARY ===\nPSO selected (29): ['Dst Port', 'Tot Fwd Pkts', 'Tot Bwd Pkts', 'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Bwd Pkt Len Max', 'Bwd Pkt Len Std', 'Flow Pkts/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Min', 'Fwd IAT Min', 'Bwd IAT Mean', 'Bwd Pkts/s', 'Pkt Len Max', 'RST Flag Cnt', 'PSH Flag Cnt', 'ECE Flag Cnt', 'Pkt Size Avg', 'Fwd Seg Size Avg', 'Bwd Seg Size Avg', 'Subflow Bwd Byts', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Seg Size Min', 'Active Max', 'Idle Mean', 'Idle Std', 'Src Port']\nGA  selected (38): ['Dst Port', 'Protocol', 'TotLen Fwd Pkts', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std', 'Bwd Pkt Len Max', 'Bwd Pkt Len Std', 'Flow Pkts/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Min', 'Fwd IAT Tot', 'Fwd IAT Mean', 'Fwd IAT Max', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Min', 'Fwd URG Flags', 'Bwd Header Len', 'Fwd Pkts/s', 'Bwd Pkts/s', 'Pkt Len Min', 'Pkt Len Mean', 'Pkt Len Var', 'FIN Flag Cnt', 'RST Flag Cnt', 'ECE Flag Cnt', 'Down/Up Ratio', 'Bwd Seg Size Avg', 'Subflow Fwd Byts', 'Subflow Bwd Pkts', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Idle Min']\nGWO selected (43): ['Dst Port', 'Flow Duration', 'Tot Bwd Pkts', 'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std', 'Bwd Pkt Len Max', 'Bwd Pkt Len Min', 'Bwd Pkt Len Mean', 'Flow Byts/s', 'Flow Pkts/s', 'Flow IAT Mean', 'Flow IAT Min', 'Fwd IAT Tot', 'Fwd IAT Std', 'Bwd IAT Tot', 'Bwd IAT Max', 'Fwd PSH Flags', 'Fwd Header Len', 'Bwd Header Len', 'Bwd Pkts/s', 'Pkt Len Min', 'Pkt Len Std', 'FIN Flag Cnt', 'SYN Flag Cnt', 'RST Flag Cnt', 'PSH Flag Cnt', 'CWE Flag Count', 'ECE Flag Cnt', 'Down/Up Ratio', 'Fwd Seg Size Avg', 'Bwd Seg Size Avg', 'Subflow Fwd Byts', 'Subflow Bwd Byts', 'Init Bwd Win Byts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Active Min', 'Idle Std', 'Src Port']\nUNION candidates (66): ['Dst Port', 'Protocol', 'Flow Duration', 'Tot Fwd Pkts', 'Tot Bwd Pkts', 'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std', 'Bwd Pkt Len Max', 'Bwd Pkt Len Min', 'Bwd Pkt Len Mean', 'Bwd Pkt Len Std', 'Flow Byts/s', 'Flow Pkts/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Min', 'Fwd IAT Tot', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Tot', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Fwd URG Flags', 'Fwd Header Len', 'Bwd Header Len', 'Fwd Pkts/s', 'Bwd Pkts/s', 'Pkt Len Min', 'Pkt Len Max', 'Pkt Len Mean', 'Pkt Len Std', 'Pkt Len Var', 'FIN Flag Cnt', 'SYN Flag Cnt', 'RST Flag Cnt', 'PSH Flag Cnt', 'CWE Flag Count', 'ECE Flag Cnt', 'Down/Up Ratio', 'Pkt Size Avg', 'Fwd Seg Size Avg', 'Bwd Seg Size Avg', 'Subflow Fwd Byts', 'Subflow Bwd Pkts', 'Subflow Bwd Byts', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Min', 'Src Port']\nHLO selected (29): ['Dst Port', 'Flow Duration', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Min', 'Bwd Pkt Len Max', 'Bwd Pkt Len Mean', 'Bwd Pkt Len Std', 'Flow Byts/s', 'Flow IAT Min', 'Fwd IAT Tot', 'Fwd IAT Min', 'Fwd Pkts/s', 'Pkt Len Max', 'Pkt Len Mean', 'Pkt Len Std', 'Pkt Len Var', 'SYN Flag Cnt', 'RST Flag Cnt', 'ECE Flag Cnt', 'Pkt Size Avg', 'Subflow Fwd Byts', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Mean', 'Active Max', 'Idle Min']\nHILL-CLIMB final (29): ['Dst Port', 'Flow Duration', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Bwd Pkt Len Max', 'Bwd Pkt Len Mean', 'Flow Byts/s', 'Flow IAT Min', 'Fwd IAT Tot', 'Fwd IAT Min', 'Fwd Pkts/s', 'Pkt Len Max', 'Pkt Len Mean', 'Pkt Len Std', 'Pkt Len Var', 'SYN Flag Cnt', 'RST Flag Cnt', 'PSH Flag Cnt', 'ECE Flag Cnt', 'Subflow Fwd Byts', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Mean', 'Active Max', 'Idle Min']\nFinal holdout metrics -> acc: 0.9691 f1: 0.9676\nModel saved: outputs/ids2018_hybrid_union_final_model.pkl\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# hybrid_union_hlo_ids2018.py\n# PSO + GA + GWO -> UNION -> HLO -> Hill-climb -> final CatBoost\n# Uses the local CSV: /kaggle/input/ids-dataset/ids2018_cleaned_combined.csv\n# Prints selected features & counts for each optimizer, HLO results, timings, and saves the final model.\n\nimport time\nimport os\nimport glob\nimport pickle\nimport warnings\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\nfrom sklearn.model_selection import StratifiedKFold, train_test_split, cross_val_score\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, make_scorer\nfrom sklearn.base import clone\nfrom catboost import CatBoostClassifier\n\nwarnings.filterwarnings(\"ignore\")\nnp.random.seed(42)\n\n# -----------------------\n# USER / EXPERIMENT SETTINGS\n# -----------------------\n\n# -----------------------\n# 1) Load your single CSV dataset\n# -----------------------\n\nCSV_PATH = \"/kaggle/input/newwwww/ids2018_cleaned_combined_1.csv\"\nprint(\"[INFO] Loading:\", CSV_PATH)\n\ndf = pd.read_csv(CSV_PATH, low_memory=False)\nprint(\"[INFO] Loaded shape:\", df.shape)\n\nOPT_SUBSET_PER_CLASS = 1500    # per-class rows for optimization subset (min available will be used)\nPSO_SWARM = 8\nPSO_ITERS = 10\nGA_POP = 12\nGA_GENS = 10\nGWO_WOLVES = 8\nGWO_ITERS = 10\nHLO_POP = 8\nHLO_ITERS = 8\nHLO_TEACHER_FACTOR = 0.75\nHLO_MUTATION = 0.12\nHILLCLIMB_MAX_STEPS = 100\nHILLCLIMB_EVAL_CAP = 500\nCV_OPT = 2\nCV_FINAL = 5\nFIT_CB_ITERS_OPT = 80\nFINAL_CB_ITERS = 800\nFINAL_EARLY_STOP = 50\nFINAL_TEST_SIZE = 0.20\nSAVE_PREFIX = \"ids2018_hybrid_union\"\nOUT_DIR = \"outputs\"\nos.makedirs(OUT_DIR, exist_ok=True)\n\nprint(\"CSV_PATH:\", CSV_PATH)\n\n# -----------------------\n# 0) sanity: file exists\n# -----------------------\nif not os.path.exists(CSV_PATH):\n    raise FileNotFoundError(f\"CSV file not found: {CSV_PATH}\")\n\ntotal_start = time.time()\n\n# -----------------------\n# 1) LOAD CSV (single file)\n# -----------------------\nt0 = time.time()\nprint(\"[1/20] Loading CSV...\")\ndf = pd.read_csv(CSV_PATH, low_memory=False)\nprint(f\" Loaded shape: {df.shape} (time {time.time()-t0:.1f}s)\")\n\n# -----------------------\n# 2) CLEAN COLUMN NAMES (strip whitespace)\n# -----------------------\nt0 = time.time()\nprint(\"[2/20] Cleaning column names...\")\n# Keep training-style whitespace trimming, but not aggressive transforms: strip outer whitespace\ndf.columns = [str(c).strip() for c in df.columns]\nprint(\" Sample columns:\", df.columns.tolist()[:12])\nprint(f\" (time {time.time()-t0:.2f}s)\")\n\n# -----------------------\n# 3) FIND LABEL COLUMN & convert to BINARY (benign -> 0 else -> 1)\n# -----------------------\nt0 = time.time()\nprint(\"[3/20] Locating label column and converting to binary...\")\nfound_label = None\nfor cand in [\"Label\", \"label\", \"Attack\", \"attack\", \"attack_cat\", \" Label\", \" Label \"]:\n    if cand in df.columns:\n        found_label = cand\n        break\nif found_label is None:\n    for c in df.columns:\n        if c.strip().lower() in (\"label\", \"attack\", \"attack_cat\"):\n            found_label = c\n            break\nif found_label is None:\n    raise RuntimeError(\"Label column not found. Columns: \" + \", \".join(df.columns[:40]))\n\n# normalize to 'Label'\nif found_label != \"Label\":\n    df.rename(columns={found_label: \"Label\"}, inplace=True)\n\n# drop null labels and convert to string then to binary\ndf = df[df[\"Label\"].notna()].copy()\n\nt0 = time.time()\nprint(\"[3/20] Locating label column and converting to binary...\")\n\nfound_label = None\nfor cand in [\"Label\", \"label\", \"Attack\", \"attack\", \"attack_cat\", \" Label\"]:\n    if cand in df.columns:\n        found_label = cand\n        break\nif found_label is None:\n    for c in df.columns:\n        if c.strip().lower() in (\"label\", \"attack\", \"attack_cat\"):\n            found_label = c\n            break\n\ndf.rename(columns={found_label: \"Label\"}, inplace=True)\n\n# --- SAFE LABEL HANDLING ---\nraw_unique = df[\"Label\"].unique()\nprint(\" Raw label values:\", raw_unique)\n\n# Case 1: labels are numeric (0/1)\nif np.array_equal(np.sort(raw_unique.astype(str)), np.array([\"0\",\"1\"])):\n    print(\" Detected numeric binary labels -> keeping as-is.\")\n    df[\"Label\"] = df[\"Label\"].astype(int)\n\n# Case 2: labels contain \"Benign\" and attacks\nelif any(str(v).lower() == \"benign\" for v in raw_unique):\n    print(\" Detected string labels with 'benign' -> mapping to 0/1.\")\n    df[\"Label\"] = df[\"Label\"].astype(str).str.strip().str.lower()\n    df[\"Label\"] = df[\"Label\"].apply(lambda x: 0 if x == \"benign\" else 1)\n\n# Case 3: unexpected strings — stop!\nelse:\n    raise RuntimeError(f\"Label column format not recognized: {raw_unique}\")\n\nprint(\" Final label distribution:\", df[\"Label\"].value_counts().to_dict())\nprint(f\" (time {time.time()-t0:.2f}s)\")\n\nprint(\" Label distribution (full):\")\nprint(df[\"Label\"].value_counts().to_dict())\nprint(f\" (time {time.time()-t0:.2f}s)\")\n\n# -----------------------\n# 4) DROP OBVIOUS LEAK COLUMNS (IDs/IP/TIMESTAMP) if present\n# -----------------------\nt0 = time.time()\nprint(\"[4/20] Dropping likely leakage columns (ids/timestamps/ips) if present...\")\npossible_leak_cols = [c for c in df.columns if c.strip().lower() in (\n    \"id\", \"flow id\", \"flowid\", \"timestamp\", \"ts\", \"source ip\", \"destination ip\",\n    \"src ip\", \"dst ip\", \"sourceip\", \"destinationip\", \"srcip\", \"dstip\", \"flow_id\", \"flow id\")]\nto_drop = [c for c in possible_leak_cols if c in df.columns]\nif to_drop:\n    df.drop(columns=to_drop, inplace=True)\n    print(\" Dropped:\", to_drop)\nelse:\n    print(\" None dropped.\")\nprint(f\" (time {time.time()-t0:.2f}s)\")\n\n# -----------------------\n# 5) REPLACE INF/NaN and DROP ALL-EMPTY COLUMNS\n# -----------------------\nt0 = time.time()\nprint(\"[5/20] Cleaning NaN/Inf and empty columns...\")\ndf.replace([np.inf, -np.inf], np.nan, inplace=True)\ndf.dropna(axis=1, how=\"all\", inplace=True)\n# drop rows with any NaN (optimizers require clean samples). If many rows drop, user will see warning.\nn_before = len(df)\ndf.dropna(axis=0, how=\"any\", inplace=True)\nn_after = len(df)\nif n_after < n_before:\n    print(f\" Dropped {n_before - n_after} rows that had NaNs. Remaining: {n_after}\")\nprint(f\" (time {time.time()-t0:.2f}s)\")\n\n# -----------------------\n# 6) BALANCED SMALL SUBSET FOR OPTIMIZERS\n# -----------------------\nt0 = time.time()\nprint(\"[6/20] Preparing balanced subset for optimization (per-class sampling)...\")\ncounts = df[\"Label\"].value_counts().to_dict()\nn_attack = counts.get(1, 0)\nn_benign = counts.get(0, 0)\ntake_attack = min(OPT_SUBSET_PER_CLASS, n_attack)\ntake_benign = min(OPT_SUBSET_PER_CLASS, n_benign)\nif take_attack < 10 or take_benign < 10:\n    raise RuntimeError(f\"Not enough rows in one class to form optimization subset. counts={counts}\")\n\ndf_attack = df[df[\"Label\"] == 1].sample(take_attack, random_state=42)\ndf_benign = df[df[\"Label\"] == 0].sample(take_benign, random_state=42)\ndf_sub = pd.concat([df_attack, df_benign], ignore_index=True).sample(frac=1.0, random_state=42).reset_index(drop=True)\nprint(\" Subset shape:\", df_sub.shape, \" Label counts:\", df_sub[\"Label\"].value_counts().to_dict())\nprint(f\" (time {time.time()-t0:.2f}s)\")\n\n# -----------------------\n# 7) PREPROCESS SUBSET: ENCODE CATEGORICAL & SCALE NUMERIC\n# -----------------------\nt0 = time.time()\nprint(\"[7/20] Preprocessing subset (LabelEncode objects, MinMax scale numeric)...\")\nTARGET_COL = \"Label\"\nX_sub = df_sub.drop(columns=[TARGET_COL]).copy()\ny_sub = df_sub[TARGET_COL].astype(int).copy()\n\n# encode object/category columns\nobj_cols = X_sub.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\nif obj_cols:\n    print(\" Object cols:\", obj_cols)\nfor c in obj_cols:\n    X_sub[c] = LabelEncoder().fit_transform(X_sub[c].astype(str))\n\n# scale numeric columns\nnum_cols_sub = X_sub.select_dtypes(include=[np.number]).columns.tolist()\nscaler_sub = MinMaxScaler()\nif len(num_cols_sub) > 0:\n    X_sub[num_cols_sub] = scaler_sub.fit_transform(X_sub[num_cols_sub])\n\nFEATURE_NAMES = X_sub.columns.tolist()\nN_FEATURES = len(FEATURE_NAMES)\nprint(\" Subset features:\", N_FEATURES)\nprint(f\" (time {time.time()-t0:.2f}s)\")\n\n# -----------------------\n# 8) CATBOOST FACTORY + FITNESS CACHE\n# -----------------------\ndef get_catboost_model(iterations=FIT_CB_ITERS_OPT):\n    return CatBoostClassifier(iterations=iterations, learning_rate=0.05, depth=6,\n                              verbose=0, random_seed=42)\n\nfitness_cache = {}\ndef key_from_mask(mask_bool):\n    # ensure mask length consistent\n    m = np.array(mask_bool).astype(int)\n    if m.shape[0] != N_FEATURES:\n        raise ValueError(\"Mask length mismatch with N_FEATURES\")\n    return tuple(int(x) for x in m)\n\ndef evaluate_mask_global(mask_bool, cv=CV_OPT, cb_iter=FIT_CB_ITERS_OPT):\n    key = key_from_mask(mask_bool)\n    if key in fitness_cache:\n        return fitness_cache[key]\n    idxs = [i for i,b in enumerate(key) if b==1]\n    if len(idxs) == 0:\n        fitness_cache[key] = 0.0\n        return 0.0\n\n    Xsel = X_sub.iloc[:, idxs]\n    model = get_catboost_model(iterations=cb_iter)\n    # adapt cv to available samples per class to avoid errors\n    min_per_class = min(y_sub.value_counts().min(), cv)\n    if min_per_class < 2:\n        cv_used = 2\n    else:\n        cv_used = min(cv, int(y_sub.value_counts().min()))\n    skf = StratifiedKFold(n_splits=cv_used, shuffle=True, random_state=42)\n    try:\n        # scoring by F1 (binary) as original logic\n        scores = cross_val_score(clone(model), Xsel, y_sub, cv=skf, scoring=make_scorer(f1_score), n_jobs=-1)\n        val = float(np.mean(scores))\n    except Exception:\n        val = 0.0\n    fitness_cache[key] = val\n    return val\n\n# -----------------------\n# 9) HELPERS\n# -----------------------\ndef mask_to_features(mask):\n    idxs = np.where(np.array(mask).astype(bool))[0].tolist()\n    return [FEATURE_NAMES[i] for i in idxs]\n\ndef log(msg):\n    print(f\"[{time.strftime('%H:%M:%S')}] {msg}\", flush=True)\n\n# -----------------------\n# 10) PSO (binary)\n# -----------------------\ndef run_pso(swarm_size=PSO_SWARM, iters=PSO_ITERS, cv=CV_OPT):\n    log(f\"PSO START (swarm={swarm_size}, iters={iters}, cv={cv})\")\n    t0 = time.time()\n    dim = N_FEATURES\n    pos = np.random.randint(0,2,(swarm_size,dim)).astype(int)\n    vel = np.random.uniform(-1,1,(swarm_size,dim))\n    pbest = pos.copy()\n    pbest_scores = np.array([evaluate_mask_global(p, cv=cv) for p in pbest])\n    gbest_idx = int(np.argmax(pbest_scores))\n    gbest = pbest[gbest_idx].copy()\n    gbest_score = pbest_scores[gbest_idx]\n    w = 0.6; c1 = c2 = 1.5\n    for t in range(iters):\n        log(f\" PSO iter {t+1}/{iters} best_global={gbest_score:.4f}\")\n        for i in range(swarm_size):\n            r1 = np.random.rand(dim); r2 = np.random.rand(dim)\n            vel[i] = w*vel[i] + c1*r1*(pbest[i] - pos[i]) + c2*r2*(gbest - pos[i])\n            s = 1.0 / (1.0 + np.exp(-vel[i]))\n            pos[i] = (np.random.rand(dim) < s).astype(int)\n            sc = evaluate_mask_global(pos[i], cv=cv)\n            if sc > pbest_scores[i]:\n                pbest[i] = pos[i].copy()\n                pbest_scores[i] = sc\n            if sc > gbest_score:\n                gbest = pos[i].copy()\n                gbest_score = sc\n        w = max(0.2, w*0.97)\n    best_idx = int(np.argmax(pbest_scores))\n    best_mask = pbest[best_idx].copy()\n    best_score = pbest_scores[best_idx]\n    t1 = time.time()\n    log(f\"PSO DONE in {int(t1-t0)}s best_score={best_score:.4f} selected={int(np.sum(best_mask))}\")\n    return best_mask, best_score, int(t1-t0)\n\n# -----------------------\n# 11) GA (binary)\n# -----------------------\ndef run_ga(pop_size=GA_POP, gens=GA_GENS, cv=CV_OPT):\n    log(f\"GA START (pop={pop_size}, gens={gens}, cv={cv})\")\n    t0 = time.time()\n    dim = N_FEATURES\n    pop = np.random.randint(0,2,(pop_size, dim)).astype(int)\n    fitness_scores = np.array([evaluate_mask_global(ind, cv=cv) for ind in pop])\n    def tournament_select(k=3):\n        idxs = np.random.randint(0, pop_size, k)\n        return idxs[np.argmax(fitness_scores[idxs])]\n    for g in range(gens):\n        log(f\" GA gen {g+1}/{gens} current_best={np.max(fitness_scores):.4f}\")\n        new_pop = []\n        # elitism\n        elite_idxs = np.argsort(fitness_scores)[-2:]\n        new_pop.extend(pop[elite_idxs].tolist())\n        while len(new_pop) < pop_size:\n            i1 = tournament_select(); i2 = tournament_select()\n            p1 = pop[i1].copy(); p2 = pop[i2].copy()\n            # crossover\n            if np.random.rand() < 0.7:\n                pt = np.random.randint(1, dim)\n                c1 = np.concatenate([p1[:pt], p2[pt:]])\n                c2 = np.concatenate([p2[:pt], p1[pt:]])\n            else:\n                c1, c2 = p1, p2\n            # mutation\n            for child in (c1, c2):\n                for d in range(dim):\n                    if np.random.rand() < 0.05:\n                        child[d] = 1 - child[d]\n                new_pop.append(child)\n                if len(new_pop) >= pop_size:\n                    break\n        pop = np.array(new_pop[:pop_size])\n        fitness_scores = np.array([evaluate_mask_global(ind, cv=cv) for ind in pop])\n    best_idx = int(np.argmax(fitness_scores))\n    best_mask = pop[best_idx].copy()\n    best_score = fitness_scores[best_idx]\n    t1 = time.time()\n    log(f\"GA DONE in {int(t1-t0)}s best_score={best_score:.4f} selected={int(np.sum(best_mask))}\")\n    return best_mask, best_score, int(t1-t0)\n\n# -----------------------\n# 12) GWO (binary)\n# -----------------------\ndef run_gwo(wolves=GWO_WOLVES, iters=GWO_ITERS, cv=CV_OPT):\n    log(f\"GWO START (wolves={wolves}, iters={iters}, cv={cv})\")\n    t0 = time.time()\n    dim = N_FEATURES\n    pop = np.random.randint(0,2,(wolves, dim)).astype(int)\n    fitness_scores = np.array([evaluate_mask_global(ind, cv=cv) for ind in pop])\n    Alpha = Beta = Delta = None\n    Alpha_score = Beta_score = Delta_score = -1.0\n    for itr in range(iters):\n        log(f\" GWO iter {itr+1}/{iters} best_alpha={Alpha_score:.4f}\")\n        for i in range(wolves):\n            sc = fitness_scores[i]\n            if sc > Alpha_score:\n                Delta_score, Beta_score, Alpha_score = Beta_score, Alpha_score, sc\n                Delta, Beta, Alpha = Beta, Alpha, pop[i].copy()\n            elif sc > Beta_score:\n                Delta_score, Beta_score = Beta_score, sc\n                Delta, Beta = Beta, pop[i].copy()\n            elif sc > Delta_score:\n                Delta_score = sc\n                Delta = pop[i].copy()\n        a = 2 - itr * (2.0 / iters)\n        for i in range(wolves):\n            for d in range(dim):\n                if Alpha is None:\n                    continue\n                r1, r2 = np.random.rand(), np.random.rand()\n                A1 = 2 * a * r1 - a; C1 = 2 * r2\n                D_alpha = abs(C1 * Alpha[d] - pop[i][d])\n                X1 = Alpha[d] - A1 * D_alpha\n                r1, r2 = np.random.rand(), np.random.rand()\n                A2 = 2 * a * r1 - a; C2 = 2 * r2\n                D_beta = abs(C2 * Beta[d] - pop[i][d])\n                X2 = Beta[d] - A2 * D_beta\n                r1, r2 = np.random.rand(), np.random.rand()\n                A3 = 2 * a * r1 - a; C3 = 2 * r2\n                D_delta = abs(C3 * Delta[d] - pop[i][d])\n                X3 = Delta[d] - A3 * D_delta\n                new_pos = (X1 + X2 + X3) / 3.0\n                s = 1.0 / (1.0 + np.exp(-new_pos))\n                pop[i][d] = 1 if np.random.rand() < s else 0\n        fitness_scores = np.array([evaluate_mask_global(ind, cv=cv) for ind in pop])\n    best_idx = int(np.argmax(fitness_scores))\n    best_mask = pop[best_idx].copy()\n    best_score = fitness_scores[best_idx]\n    t1 = time.time()\n    log(f\"GWO DONE in {int(t1-t0)}s best_score={best_score:.4f} selected={int(np.sum(best_mask))}\")\n    return best_mask, best_score, int(t1-t0)\n\n# -----------------------\n# 13) RUN OPTIMIZERS\n# -----------------------\nlog(\"===== RUNNING OPTIMIZERS (PSO / GA / GWO) =====\")\nt_all0 = time.time()\npso_mask, pso_score, pso_time = run_pso(swarm_size=PSO_SWARM, iters=PSO_ITERS, cv=CV_OPT)\nlog(f\"PSO selected ({int(np.sum(pso_mask))}): {mask_to_features(pso_mask)} (time {pso_time}s)\")\n\nga_mask, ga_score, ga_time = run_ga(pop_size=GA_POP, gens=GA_GENS, cv=CV_OPT)\nlog(f\"GA selected ({int(np.sum(ga_mask))}): {mask_to_features(ga_mask)} (time {ga_time}s)\")\n\ngwo_mask, gwo_score, gwo_time = run_gwo(wolves=GWO_WOLVES, iters=GWO_ITERS, cv=CV_OPT)\nlog(f\"GWO selected ({int(np.sum(gwo_mask))}): {mask_to_features(gwo_mask)} (time {gwo_time}s)\")\n\nt_all1 = time.time()\nlog(f\"Optimizers finished in {int(t_all1-t_all0)}s\")\n\n# Save raw masks\npickle.dump({\n    \"pso_mask\": pso_mask.tolist(), \"pso_score\": pso_score, \"pso_time\": pso_time,\n    \"ga_mask\": ga_mask.tolist(), \"ga_score\": ga_score, \"ga_time\": ga_time,\n    \"gwo_mask\": gwo_mask.tolist(), \"gwo_score\": gwo_score, \"gwo_time\": gwo_time\n}, open(os.path.join(OUT_DIR, SAVE_PREFIX + \"_raw_masks.pkl\"), \"wb\"))\n\n# -----------------------\n# 14) VOTING (feature kept only if chosen by at least 2 optimizers)\n# -----------------------\nlog(\"Applying VOTING rule: keep feature if selected by ≥2 optimizers\")\n\npso_arr = np.array(pso_mask)\nga_arr  = np.array(ga_mask)\ngwo_arr = np.array(gwo_mask)\n\n# Vote count per feature\nvote_count = pso_arr + ga_arr + gwo_arr     # vector of 0..3\n\n# Voting rule: at least 2 optimizers must agree\nvoting_mask = (vote_count >= 2).astype(int)\n\nvoting_feats = mask_to_features(voting_mask)\n\nlog(f\"VOTING selected {len(voting_feats)} features: {voting_feats}\")\n\npickle.dump({\n    \"voting_mask\": voting_mask.tolist(),\n    \"voting_feats\": voting_feats,\n    \"vote_count\": vote_count.tolist()\n}, open(os.path.join(OUT_DIR, SAVE_PREFIX + \"_voting.pkl\"), \"wb\"))\n\n# -----------------------\n# 15) HLO on UNION candidates\n# -----------------------\nt0 = time.time()\nlog(\"HLO START on union candidates\")\ndef hlo_on_candidates(candidate_mask, pop_size=HLO_POP, iters=HLO_ITERS, cv=CV_OPT):\n    candidate_indices = np.where(np.array(candidate_mask).astype(bool))[0].tolist()\n    k = len(candidate_indices)\n    if k == 0:\n        raise ValueError(\"Candidate set is empty.\")\n    pop = np.random.randint(0,2,(pop_size, k)).astype(int)\n    def fitness_candidate(bitmask):\n        full_mask = np.zeros(N_FEATURES, dtype=int)\n        for j,bit in enumerate(bitmask):\n            if int(bit)==1:\n                full_mask[candidate_indices[j]] = 1\n        return evaluate_mask_global(full_mask, cv=cv, cb_iter=FIT_CB_ITERS_OPT)\n    fitness_scores = np.array([fitness_candidate(ind) for ind in pop])\n    best_idx = int(np.argmax(fitness_scores)); best_solution = pop[best_idx].copy(); best_score = fitness_scores[best_idx]\n    for it in range(iters):\n        log(f\" HLO iter {it+1}/{iters} current_best={best_score:.4f}\")\n        teacher = pop[int(np.argmax([fitness_candidate(x) for x in pop]))].copy()\n        new_pop = []\n        for i in range(pop_size):\n            learner = pop[i].copy()\n            # teaching\n            for d in range(k):\n                if np.random.rand() < HLO_TEACHER_FACTOR:\n                    learner[d] = teacher[d]\n            # peer learning\n            partner = pop[np.random.randint(pop_size)].copy()\n            for d in range(k):\n                if learner[d] != partner[d] and np.random.rand() < 0.5:\n                    learner[d] = partner[d]\n            # mutation\n            for d in range(k):\n                if np.random.rand() < HLO_MUTATION:\n                    learner[d] = 1 - learner[d]\n            new_pop.append(learner)\n        pop = np.array(new_pop)\n        fitness_scores = np.array([fitness_candidate(ind) for ind in pop])\n        gen_best_idx = int(np.argmax(fitness_scores)); gen_best_score = fitness_scores[gen_best_idx]; gen_best_sol = pop[gen_best_idx].copy()\n        if gen_best_score > best_score:\n            best_score = gen_best_score; best_solution = gen_best_sol.copy()\n    final_full_mask = np.zeros(N_FEATURES, dtype=int)\n    for j,bit in enumerate(best_solution):\n        if int(bit)==1:\n            final_full_mask[candidate_indices[j]] = 1\n    return final_full_mask, best_score\n\nhlo_mask, hlo_score = hlo_on_candidates(voting_mask, pop_size=HLO_POP, iters=HLO_ITERS, cv=CV_OPT)\nhlo_feats = mask_to_features(hlo_mask)\nlog(f\"HLO finished in {int(time.time()-t0)}s best_score={hlo_score:.4f} selected={len(hlo_feats)}\")\nlog(f\"HLO selected ({len(hlo_feats)}): {hlo_feats}\")\npickle.dump({\"hlo_mask\": hlo_mask.tolist(), \"hlo_score\": hlo_score, \"hlo_feats\": hlo_feats}, open(os.path.join(OUT_DIR, SAVE_PREFIX + \"_hlo.pkl\"), \"wb\"))\n\n# -----------------------\n# 16) Greedy hill-climb restricted to union candidate indices (starting from hlo_mask)\n# -----------------------\nt0 = time.time()\nlog(\"Hill-climb START\")\ndef hill_climb_on_candidates(initial_mask, candidate_mask, max_steps=HILLCLIMB_MAX_STEPS, eval_cap=HILLCLIMB_EVAL_CAP, cv=CV_OPT):\n    candidate_indices = np.where(np.array(candidate_mask).astype(bool))[0].tolist()\n    if len(candidate_indices) == 0:\n        return initial_mask, 0.0, 0\n    current_mask = initial_mask.copy()\n    current_score = evaluate_mask_global(current_mask, cv=cv, cb_iter=FIT_CB_ITERS_OPT)\n    evals = 0; steps = 0; improved = True\n    while improved and steps < max_steps and evals < eval_cap:\n        improved = False\n        for idx in np.random.permutation(candidate_indices):\n            trial_mask = current_mask.copy()\n            trial_mask[idx] = 1 - trial_mask[idx]\n            trial_score = evaluate_mask_global(trial_mask, cv=cv, cb_iter=FIT_CB_ITERS_OPT)\n            evals += 1\n            if trial_score > current_score + 1e-8:\n                current_mask = trial_mask\n                current_score = trial_score\n                improved = True\n                steps += 1\n                log(f\" Hill-climb step {steps}: flipped {FEATURE_NAMES[idx]} -> new_score={current_score:.4f} (evals={evals})\")\n                break\n            if evals >= eval_cap or steps >= max_steps:\n                break\n    return current_mask, current_score, evals\n\nhc_mask, hc_score, hc_evals = hill_climb_on_candidates(\n    hlo_mask, \n    voting_mask, \n    max_steps=HILLCLIMB_MAX_STEPS, \n    eval_cap=HILLCLIMB_EVAL_CAP, \n    cv=CV_OPT\n)\n\nhc_feats = mask_to_features(hc_mask)\nlog(f\"Hill-climb DONE in {int(time.time()-t0)}s steps evals={hc_evals} final_score={hc_score:.4f} selected={len(hc_feats)}\")\nlog(f\"Hill-climb final selected ({len(hc_feats)}): {hc_feats}\")\npickle.dump({\"hc_mask\": hc_mask.tolist(), \"hc_score\": hc_score, \"hc_feats\": hc_feats}, open(os.path.join(OUT_DIR, SAVE_PREFIX + \"_hc.pkl\"), \"wb\"))\n\n# -----------------------\n# 17) Final selected features after hill-climb\n# -----------------------\nfinal_mask = hc_mask\nfinal_selected_indices = np.where(np.array(final_mask).astype(bool))[0].tolist()\nfinal_selected = [FEATURE_NAMES[i] for i in final_selected_indices]\nlog(f\"FINAL selected features ({len(final_selected)}): {final_selected}\")\npickle.dump({\"final_selected\": final_selected, \"final_mask\": final_mask.tolist()}, open(os.path.join(OUT_DIR, SAVE_PREFIX + \"_final_selected.pkl\"), \"wb\"))\n\n# -----------------------\n# 18) Leakage check: drop single-feature perfect predictors\n# -----------------------\ndef single_feature_predictive_accuracy(feature_series, labels):\n    mapping = feature_series.groupby(feature_series).apply(lambda s: labels[s.index].mode().iloc[0])\n    preds = feature_series.map(mapping)\n    return (preds.values == labels.values).mean()\n\nto_drop = []\nfor f in final_selected:\n    acc = single_feature_predictive_accuracy(X_sub[f], y_sub)\n    if acc >= 0.99999 or acc == 1.0:\n        log(f\"LEAK suspect '{f}' single-feature acc={acc:.6f} -> will drop\")\n        to_drop.append(f)\n\nif to_drop:\n    final_selected = [f for f in final_selected if f not in to_drop]\n    final_selected_indices = [FEATURE_NAMES.index(f) for f in final_selected]\n    final_mask = np.zeros(N_FEATURES, dtype=int)\n    for i in final_selected_indices:\n        final_mask[i] = 1\n    log(f\"After dropping leak suspects final features ({len(final_selected)}): {final_selected}\")\n\nif len(final_selected) == 0:\n    raise RuntimeError(\"No features remain after leakage check. Lower threshold or inspect features.\")\n\n# -----------------------\n# 19) Prepare FULL dataset with same preprocessing for final training\n# -----------------------\nt0 = time.time()\nlog(\"Preparing full dataset for final training...\")\ndf_full = df.copy()\nmissing_in_full = [f for f in final_selected if f not in df_full.columns]\nif missing_in_full:\n    raise RuntimeError(\"Selected features missing from full dataset: \" + str(missing_in_full))\n\ndf_full = df_full[final_selected + [\"Label\"]].copy()\n\n# Convert object columns to numeric (LabelEncode) and fill NaN\nfor c in df_full.columns:\n    if c != \"Label\" and df_full[c].dtype == object:\n        df_full[c] = LabelEncoder().fit_transform(df_full[c].astype(str))\ndf_full.replace([np.inf, -np.inf], np.nan, inplace=True)\ndf_full.fillna(0, inplace=True)\n\n# Scale numeric columns (MinMax) using full data\nnum_cols = [c for c in final_selected if pd.api.types.is_numeric_dtype(df_full[c])]\nif len(num_cols) > 0:\n    df_full[num_cols] = MinMaxScaler().fit_transform(df_full[num_cols])\n\nX_full = df_full.drop(columns=[\"Label\"])\ny_full = df_full[\"Label\"].astype(int)\nlog(f\"Full final training shape: {X_full.shape} Label dist: {y_full.value_counts().to_dict()} (time {time.time()-t0:.2f}s)\")\n\n# -----------------------\n# 20) Final train/test split (80/20) and final CatBoost training with early stopping\n# -----------------------\nt0 = time.time()\nlog(\"Final training: splitting and training final CatBoost model (with regularization + early stopping)...\")\nminclass = y_full.value_counts().min()\nif minclass < 10:\n    log(f\"Warning: small class size after selecting features: {minclass}\")\n\nX_train, X_test, y_train, y_test = train_test_split(X_full, y_full, test_size=FINAL_TEST_SIZE, stratify=y_full, random_state=42)\nX_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.15, stratify=y_train, random_state=42)\n\nfinal_params = {\n    \"iterations\": FINAL_CB_ITERS,\n    \"learning_rate\": 0.03,\n    \"depth\": 6,\n    \"l2_leaf_reg\": 7.0,\n    \"bootstrap_type\": \"Bernoulli\",\n    \"subsample\": 0.8,\n    \"random_strength\": 1.0,\n    \"verbose\": 50,\n    \"random_seed\": 42\n}\nfinal_model = CatBoostClassifier(**final_params)\nfinal_model.fit(X_tr, y_tr, eval_set=(X_val, y_val), early_stopping_rounds=FINAL_EARLY_STOP, use_best_model=True)\nlog(f\"Final model trained in {int(time.time()-t0)}s\")\n\n# Evaluate on hold-out test\ny_pred = final_model.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred, zero_division=0)\nrec = recall_score(y_test, y_pred, zero_division=0)\nf1 = f1_score(y_test, y_pred, zero_division=0)\nlog(\"=== FINAL HOLDOUT METRICS ===\")\nprint(f\"Accuracy: {acc:.6f}\")\nprint(f\"Precision: {prec:.6f}\")\nprint(f\"Recall: {rec:.6f}\")\nprint(f\"F1: {f1:.6f}\")\nprint(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n\n# Quick 5-fold CV estimate (reduced iters)\ncv_model = CatBoostClassifier(iterations=200, learning_rate=0.03, depth=6, l2_leaf_reg=7.0,\n                              bootstrap_type=\"Bernoulli\", subsample=0.8, random_seed=42, verbose=0)\nskf = StratifiedKFold(n_splits=min(5, max(2, int(y_full.value_counts().min()))), shuffle=True, random_state=42)\naccs = cross_val_score(cv_model, X_full, y_full, cv=skf, scoring=\"accuracy\", n_jobs=-1)\nf1s = cross_val_score(cv_model, X_full, y_full, cv=skf, scoring=make_scorer(f1_score), n_jobs=-1)\nprint(\"\\n5-fold CV (quick estimate) -> Accuracy: %.4f ± %.4f ; F1: %.4f ± %.4f\" % (accs.mean(), accs.std(), f1s.mean(), f1s.std()))\n\n# -----------------------\n# 21) Save final model & selected features + results\n# -----------------------\nfinal_model_path = os.path.join(OUT_DIR, f\"{SAVE_PREFIX}_final_model.pkl\")\nwith open(final_model_path, \"wb\") as f:\n    pickle.dump({\"model\": final_model, \"features\": final_selected, \"mask\": final_mask.tolist()}, f)\n\nresults = {\n    \"pso_score\": pso_score, \"ga_score\": ga_score, \"gwo_score\": gwo_score,\n    \"union_feats\": union_feats, \"hlo_feats\": hlo_feats, \"hc_feats\": hc_feats,\n    \"final_selected\": final_selected,\n    \"final_holdout\": {\"acc\": acc, \"prec\": prec, \"rec\": rec, \"f1\": f1},\n    \"fitness_cache_len\": len(fitness_cache)\n}\nwith open(os.path.join(OUT_DIR, SAVE_PREFIX + \"_results.pkl\"), \"wb\") as f:\n    pickle.dump(results, f)\n\nlog(f\"Saved final model -> {final_model_path}\")\ntotal_time = int(time.time() - total_start)\nlog(f\"PIPELINE COMPLETE in {total_time}s. Results saved to {OUT_DIR}\")\n\n# Short summary prints:\nprint(\"\\n=== SUMMARY ===\")\nprint(f\"PSO selected ({int(np.sum(pso_mask))}): {mask_to_features(pso_mask)}\")\nprint(f\"GA  selected ({int(np.sum(ga_mask))}): {mask_to_features(ga_mask)}\")\nprint(f\"GWO selected ({int(np.sum(gwo_mask))}): {mask_to_features(gwo_mask)}\")\nprint(f\"UNION candidates ({len(union_feats)}): {union_feats}\")\nprint(f\"HLO selected ({len(hlo_feats)}): {hlo_feats}\")\nprint(f\"HILL-CLIMB final ({len(hc_feats)}): {hc_feats}\")\nprint(f\"Final holdout metrics -> acc: {acc:.4f} f1: {f1:.4f}\")\nprint(f\"Model saved: {final_model_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T18:07:25.415918Z","iopub.execute_input":"2025-12-04T18:07:25.416612Z","iopub.status.idle":"2025-12-04T18:12:19.905717Z","shell.execute_reply.started":"2025-12-04T18:07:25.416560Z","shell.execute_reply":"2025-12-04T18:12:19.905120Z"}},"outputs":[{"name":"stdout","text":"[INFO] Loading: /kaggle/input/newwwww/ids2018_cleaned_combined_1.csv\n[INFO] Loaded shape: (97802, 76)\nCSV_PATH: /kaggle/input/newwwww/ids2018_cleaned_combined_1.csv\n[1/20] Loading CSV...\n Loaded shape: (97802, 76) (time 1.3s)\n[2/20] Cleaning column names...\n Sample columns: ['Dst Port', 'Protocol', 'Timestamp', 'Flow Duration', 'Tot Fwd Pkts', 'Tot Bwd Pkts', 'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std']\n (time 0.00s)\n[3/20] Locating label column and converting to binary...\n[3/20] Locating label column and converting to binary...\n Raw label values: [1 0]\n Detected numeric binary labels -> keeping as-is.\n Final label distribution: {0: 49993, 1: 47809}\n (time 0.00s)\n Label distribution (full):\n{0: 49993, 1: 47809}\n (time 0.00s)\n[4/20] Dropping likely leakage columns (ids/timestamps/ips) if present...\n Dropped: ['Timestamp', 'Flow ID', 'Src IP', 'Dst IP']\n (time 0.02s)\n[5/20] Cleaning NaN/Inf and empty columns...\n (time 0.09s)\n[6/20] Preparing balanced subset for optimization (per-class sampling)...\n Subset shape: (3000, 72)  Label counts: {0: 1500, 1: 1500}\n (time 0.03s)\n[7/20] Preprocessing subset (LabelEncode objects, MinMax scale numeric)...\n Subset features: 71\n (time 0.01s)\n[18:07:28] ===== RUNNING OPTIMIZERS (PSO / GA / GWO) =====\n[18:07:28] PSO START (swarm=8, iters=10, cv=2)\n[18:07:33]  PSO iter 1/10 best_global=0.9516\n[18:07:38]  PSO iter 2/10 best_global=0.9530\n[18:07:43]  PSO iter 3/10 best_global=0.9530\n[18:07:48]  PSO iter 4/10 best_global=0.9534\n[18:07:53]  PSO iter 5/10 best_global=0.9534\n[18:07:58]  PSO iter 6/10 best_global=0.9542\n[18:08:03]  PSO iter 7/10 best_global=0.9542\n[18:08:08]  PSO iter 8/10 best_global=0.9542\n[18:08:13]  PSO iter 9/10 best_global=0.9542\n[18:08:17]  PSO iter 10/10 best_global=0.9542\n[18:08:22] PSO DONE in 54s best_score=0.9542 selected=29\n[18:08:22] PSO selected (29): ['Dst Port', 'Tot Fwd Pkts', 'Tot Bwd Pkts', 'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Bwd Pkt Len Max', 'Bwd Pkt Len Std', 'Flow Pkts/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Min', 'Fwd IAT Min', 'Bwd IAT Mean', 'Bwd Pkts/s', 'Pkt Len Max', 'RST Flag Cnt', 'PSH Flag Cnt', 'ECE Flag Cnt', 'Pkt Size Avg', 'Fwd Seg Size Avg', 'Bwd Seg Size Avg', 'Subflow Bwd Byts', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Seg Size Min', 'Active Max', 'Idle Mean', 'Idle Std', 'Src Port'] (time 54s)\n[18:08:22] GA START (pop=12, gens=10, cv=2)\n[18:08:30]  GA gen 1/10 current_best=0.9515\n[18:08:36]  GA gen 2/10 current_best=0.9523\n[18:08:43]  GA gen 3/10 current_best=0.9523\n[18:08:49]  GA gen 4/10 current_best=0.9523\n[18:08:56]  GA gen 5/10 current_best=0.9526\n[18:09:03]  GA gen 6/10 current_best=0.9526\n[18:09:10]  GA gen 7/10 current_best=0.9533\n[18:09:16]  GA gen 8/10 current_best=0.9536\n[18:09:23]  GA gen 9/10 current_best=0.9536\n[18:09:30]  GA gen 10/10 current_best=0.9544\n[18:09:37] GA DONE in 74s best_score=0.9544 selected=38\n[18:09:37] GA selected (38): ['Dst Port', 'Protocol', 'TotLen Fwd Pkts', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std', 'Bwd Pkt Len Max', 'Bwd Pkt Len Std', 'Flow Pkts/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Min', 'Fwd IAT Tot', 'Fwd IAT Mean', 'Fwd IAT Max', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Min', 'Fwd URG Flags', 'Bwd Header Len', 'Fwd Pkts/s', 'Bwd Pkts/s', 'Pkt Len Min', 'Pkt Len Mean', 'Pkt Len Var', 'FIN Flag Cnt', 'RST Flag Cnt', 'ECE Flag Cnt', 'Down/Up Ratio', 'Bwd Seg Size Avg', 'Subflow Fwd Byts', 'Subflow Bwd Pkts', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Idle Min'] (time 74s)\n[18:09:37] GWO START (wolves=8, iters=10, cv=2)\n[18:09:42]  GWO iter 1/10 best_alpha=-1.0000\n[18:09:47]  GWO iter 2/10 best_alpha=0.9505\n[18:09:53]  GWO iter 3/10 best_alpha=0.9510\n[18:10:00]  GWO iter 4/10 best_alpha=0.9534\n[18:10:06]  GWO iter 5/10 best_alpha=0.9534\n[18:10:12]  GWO iter 6/10 best_alpha=0.9534\n[18:10:18]  GWO iter 7/10 best_alpha=0.9534\n[18:10:24]  GWO iter 8/10 best_alpha=0.9534\n[18:10:30]  GWO iter 9/10 best_alpha=0.9534\n[18:10:36]  GWO iter 10/10 best_alpha=0.9534\n[18:10:42] GWO DONE in 64s best_score=0.9510 selected=43\n[18:10:42] GWO selected (43): ['Dst Port', 'Flow Duration', 'Tot Bwd Pkts', 'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std', 'Bwd Pkt Len Max', 'Bwd Pkt Len Min', 'Bwd Pkt Len Mean', 'Flow Byts/s', 'Flow Pkts/s', 'Flow IAT Mean', 'Flow IAT Min', 'Fwd IAT Tot', 'Fwd IAT Std', 'Bwd IAT Tot', 'Bwd IAT Max', 'Fwd PSH Flags', 'Fwd Header Len', 'Bwd Header Len', 'Bwd Pkts/s', 'Pkt Len Min', 'Pkt Len Std', 'FIN Flag Cnt', 'SYN Flag Cnt', 'RST Flag Cnt', 'PSH Flag Cnt', 'CWE Flag Count', 'ECE Flag Cnt', 'Down/Up Ratio', 'Fwd Seg Size Avg', 'Bwd Seg Size Avg', 'Subflow Fwd Byts', 'Subflow Bwd Byts', 'Init Bwd Win Byts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Active Min', 'Idle Std', 'Src Port'] (time 64s)\n[18:10:42] Optimizers finished in 193s\n[18:10:42] Applying VOTING rule: keep feature if selected by ≥2 optimizers\n[18:10:42] VOTING selected 32 features: ['Dst Port', 'Tot Bwd Pkts', 'TotLen Fwd Pkts', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std', 'Bwd Pkt Len Max', 'Bwd Pkt Len Std', 'Flow Pkts/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Min', 'Fwd IAT Tot', 'Bwd IAT Mean', 'Bwd Header Len', 'Bwd Pkts/s', 'Pkt Len Min', 'FIN Flag Cnt', 'RST Flag Cnt', 'PSH Flag Cnt', 'ECE Flag Cnt', 'Down/Up Ratio', 'Fwd Seg Size Avg', 'Bwd Seg Size Avg', 'Subflow Fwd Byts', 'Subflow Bwd Byts', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Idle Std', 'Src Port']\n[18:10:42] HLO START on union candidates\n[18:10:44]  HLO iter 1/8 current_best=0.9521\n[18:10:47]  HLO iter 2/8 current_best=0.9548\n[18:10:49]  HLO iter 3/8 current_best=0.9548\n[18:10:52]  HLO iter 4/8 current_best=0.9548\n[18:10:55]  HLO iter 5/8 current_best=0.9548\n[18:10:58]  HLO iter 6/8 current_best=0.9548\n[18:11:01]  HLO iter 7/8 current_best=0.9548\n[18:11:05]  HLO iter 8/8 current_best=0.9554\n[18:11:08] HLO finished in 25s best_score=0.9554 selected=21\n[18:11:08] HLO selected (21): ['Dst Port', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std', 'Bwd Pkt Len Std', 'Flow Pkts/s', 'Flow IAT Std', 'Fwd IAT Tot', 'Bwd IAT Mean', 'Bwd Header Len', 'Pkt Len Min', 'RST Flag Cnt', 'PSH Flag Cnt', 'ECE Flag Cnt', 'Subflow Fwd Byts', 'Subflow Bwd Byts', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Src Port']\n[18:11:08] Hill-climb START\n[18:11:09]  Hill-climb step 1: flipped Flow IAT Min -> new_score=0.9557 (evals=4)\n[18:11:11]  Hill-climb step 2: flipped Fwd IAT Tot -> new_score=0.9561 (evals=8)\n[18:11:23]  Hill-climb step 3: flipped Down/Up Ratio -> new_score=0.9562 (evals=39)\n[18:11:35] Hill-climb DONE in 27s steps evals=71 final_score=0.9562 selected=22\n[18:11:35] Hill-climb final selected (22): ['Dst Port', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std', 'Bwd Pkt Len Std', 'Flow Pkts/s', 'Flow IAT Std', 'Flow IAT Min', 'Bwd IAT Mean', 'Bwd Header Len', 'Pkt Len Min', 'RST Flag Cnt', 'PSH Flag Cnt', 'ECE Flag Cnt', 'Down/Up Ratio', 'Subflow Fwd Byts', 'Subflow Bwd Byts', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Src Port']\n[18:11:35] FINAL selected features (22): ['Dst Port', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std', 'Bwd Pkt Len Std', 'Flow Pkts/s', 'Flow IAT Std', 'Flow IAT Min', 'Bwd IAT Mean', 'Bwd Header Len', 'Pkt Len Min', 'RST Flag Cnt', 'PSH Flag Cnt', 'ECE Flag Cnt', 'Down/Up Ratio', 'Subflow Fwd Byts', 'Subflow Bwd Byts', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Src Port']\n[18:11:38] Preparing full dataset for final training...\n[18:11:38] Full final training shape: (97802, 22) Label dist: {0: 49993, 1: 47809} (time 0.08s)\n[18:11:38] Final training: splitting and training final CatBoost model (with regularization + early stopping)...\n0:\tlearn: 0.6416439\ttest: 0.6415822\tbest: 0.6415822 (0)\ttotal: 14.2ms\tremaining: 11.3s\n50:\tlearn: 0.1482498\ttest: 0.1483380\tbest: 0.1483380 (50)\ttotal: 555ms\tremaining: 8.14s\n100:\tlearn: 0.1239551\ttest: 0.1247289\tbest: 0.1247289 (100)\ttotal: 1.06s\tremaining: 7.35s\n150:\tlearn: 0.1163608\ttest: 0.1176460\tbest: 0.1176460 (150)\ttotal: 1.58s\tremaining: 6.81s\n200:\tlearn: 0.1112393\ttest: 0.1133513\tbest: 0.1133513 (200)\ttotal: 2.13s\tremaining: 6.34s\n250:\tlearn: 0.1078160\ttest: 0.1104839\tbest: 0.1104839 (250)\ttotal: 2.65s\tremaining: 5.8s\n300:\tlearn: 0.1049224\ttest: 0.1084040\tbest: 0.1084040 (300)\ttotal: 3.19s\tremaining: 5.29s\n350:\tlearn: 0.1027762\ttest: 0.1068226\tbest: 0.1068226 (350)\ttotal: 3.71s\tremaining: 4.75s\n400:\tlearn: 0.1008556\ttest: 0.1055422\tbest: 0.1055422 (400)\ttotal: 4.24s\tremaining: 4.22s\n450:\tlearn: 0.0990607\ttest: 0.1042807\tbest: 0.1042807 (450)\ttotal: 4.76s\tremaining: 3.68s\n500:\tlearn: 0.0976284\ttest: 0.1034193\tbest: 0.1034193 (500)\ttotal: 5.29s\tremaining: 3.16s\n550:\tlearn: 0.0962019\ttest: 0.1025305\tbest: 0.1025305 (550)\ttotal: 5.85s\tremaining: 2.64s\n600:\tlearn: 0.0949716\ttest: 0.1018345\tbest: 0.1018345 (600)\ttotal: 6.38s\tremaining: 2.11s\n650:\tlearn: 0.0939204\ttest: 0.1012939\tbest: 0.1012923 (649)\ttotal: 6.91s\tremaining: 1.58s\n700:\tlearn: 0.0928684\ttest: 0.1006575\tbest: 0.1006575 (700)\ttotal: 7.44s\tremaining: 1.05s\n750:\tlearn: 0.0920027\ttest: 0.1002503\tbest: 0.1002503 (750)\ttotal: 7.96s\tremaining: 519ms\n799:\tlearn: 0.0911351\ttest: 0.0999325\tbest: 0.0999325 (798)\ttotal: 8.48s\tremaining: 0us\n\nbestTest = 0.09993251265\nbestIteration = 798\n\nShrink model to first 799 iterations.\n[18:11:47] Final model trained in 8s\n[18:11:47] === FINAL HOLDOUT METRICS ===\nAccuracy: 0.969327\nPrecision: 0.992526\nRecall: 0.944363\nF1: 0.967846\n\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.95      0.99      0.97      9999\n           1       0.99      0.94      0.97      9562\n\n    accuracy                           0.97     19561\n   macro avg       0.97      0.97      0.97     19561\nweighted avg       0.97      0.97      0.97     19561\n\n\n5-fold CV (quick estimate) -> Accuracy: 0.9660 ± 0.0024 ; F1: 0.9642 ± 0.0026\n[18:12:19] Saved final model -> outputs/ids2018_hybrid_union_final_model.pkl\n[18:12:19] PIPELINE COMPLETE in 293s. Results saved to outputs\n\n=== SUMMARY ===\nPSO selected (29): ['Dst Port', 'Tot Fwd Pkts', 'Tot Bwd Pkts', 'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Bwd Pkt Len Max', 'Bwd Pkt Len Std', 'Flow Pkts/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Min', 'Fwd IAT Min', 'Bwd IAT Mean', 'Bwd Pkts/s', 'Pkt Len Max', 'RST Flag Cnt', 'PSH Flag Cnt', 'ECE Flag Cnt', 'Pkt Size Avg', 'Fwd Seg Size Avg', 'Bwd Seg Size Avg', 'Subflow Bwd Byts', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Seg Size Min', 'Active Max', 'Idle Mean', 'Idle Std', 'Src Port']\nGA  selected (38): ['Dst Port', 'Protocol', 'TotLen Fwd Pkts', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std', 'Bwd Pkt Len Max', 'Bwd Pkt Len Std', 'Flow Pkts/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Min', 'Fwd IAT Tot', 'Fwd IAT Mean', 'Fwd IAT Max', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Min', 'Fwd URG Flags', 'Bwd Header Len', 'Fwd Pkts/s', 'Bwd Pkts/s', 'Pkt Len Min', 'Pkt Len Mean', 'Pkt Len Var', 'FIN Flag Cnt', 'RST Flag Cnt', 'ECE Flag Cnt', 'Down/Up Ratio', 'Bwd Seg Size Avg', 'Subflow Fwd Byts', 'Subflow Bwd Pkts', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Idle Min']\nGWO selected (43): ['Dst Port', 'Flow Duration', 'Tot Bwd Pkts', 'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std', 'Bwd Pkt Len Max', 'Bwd Pkt Len Min', 'Bwd Pkt Len Mean', 'Flow Byts/s', 'Flow Pkts/s', 'Flow IAT Mean', 'Flow IAT Min', 'Fwd IAT Tot', 'Fwd IAT Std', 'Bwd IAT Tot', 'Bwd IAT Max', 'Fwd PSH Flags', 'Fwd Header Len', 'Bwd Header Len', 'Bwd Pkts/s', 'Pkt Len Min', 'Pkt Len Std', 'FIN Flag Cnt', 'SYN Flag Cnt', 'RST Flag Cnt', 'PSH Flag Cnt', 'CWE Flag Count', 'ECE Flag Cnt', 'Down/Up Ratio', 'Fwd Seg Size Avg', 'Bwd Seg Size Avg', 'Subflow Fwd Byts', 'Subflow Bwd Byts', 'Init Bwd Win Byts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Active Min', 'Idle Std', 'Src Port']\nUNION candidates (66): ['Dst Port', 'Protocol', 'Flow Duration', 'Tot Fwd Pkts', 'Tot Bwd Pkts', 'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std', 'Bwd Pkt Len Max', 'Bwd Pkt Len Min', 'Bwd Pkt Len Mean', 'Bwd Pkt Len Std', 'Flow Byts/s', 'Flow Pkts/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Min', 'Fwd IAT Tot', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Tot', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Fwd URG Flags', 'Fwd Header Len', 'Bwd Header Len', 'Fwd Pkts/s', 'Bwd Pkts/s', 'Pkt Len Min', 'Pkt Len Max', 'Pkt Len Mean', 'Pkt Len Std', 'Pkt Len Var', 'FIN Flag Cnt', 'SYN Flag Cnt', 'RST Flag Cnt', 'PSH Flag Cnt', 'CWE Flag Count', 'ECE Flag Cnt', 'Down/Up Ratio', 'Pkt Size Avg', 'Fwd Seg Size Avg', 'Bwd Seg Size Avg', 'Subflow Fwd Byts', 'Subflow Bwd Pkts', 'Subflow Bwd Byts', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Min', 'Src Port']\nHLO selected (21): ['Dst Port', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std', 'Bwd Pkt Len Std', 'Flow Pkts/s', 'Flow IAT Std', 'Fwd IAT Tot', 'Bwd IAT Mean', 'Bwd Header Len', 'Pkt Len Min', 'RST Flag Cnt', 'PSH Flag Cnt', 'ECE Flag Cnt', 'Subflow Fwd Byts', 'Subflow Bwd Byts', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Src Port']\nHILL-CLIMB final (22): ['Dst Port', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std', 'Bwd Pkt Len Std', 'Flow Pkts/s', 'Flow IAT Std', 'Flow IAT Min', 'Bwd IAT Mean', 'Bwd Header Len', 'Pkt Len Min', 'RST Flag Cnt', 'PSH Flag Cnt', 'ECE Flag Cnt', 'Down/Up Ratio', 'Subflow Fwd Byts', 'Subflow Bwd Byts', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Src Port']\nFinal holdout metrics -> acc: 0.9693 f1: 0.9678\nModel saved: outputs/ids2018_hybrid_union_final_model.pkl\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# hybrid_hlo_union_only.py\n# Reduced-budget hybrid pipeline: PSO, GA, GWO -> UNION -> HLO -> Hill-climb -> final CatBoost (save)\n# Prints selected features after PSO/GA/GWO and the final union members.\n# Runs on Kaggle input path by default and prints final test metrics (accuracy, precision, recall, f1),\n# confusion matrix and classification report.\n\nimport time\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport warnings\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, make_scorer, classification_report, confusion_matrix\nfrom sklearn.base import clone\n\nwarnings.filterwarnings(\"ignore\")\nnp.random.seed(42)\n\n# -------------------- USER / EXPERIMENT SETTINGS --------------------\n# Kaggle path requested by you:\nCSV_PATH = \"/kaggle/input/newwwww/ids2018_cleaned_combined_1.csv\"\n\nTARGET_COL = \"Label\"   # change if your dataset uses another column name\nMODEL_VERBOSE = 0            # CatBoost verbosity: 0 = silent\nRANDOM_STATE = 42\n\n# ---------- Reduced budgets (set to 20 for the three optimizers as requested) ----------\nPSO_SWARM = 15\nPSO_ITERS = 10     # <<-- set to 20\n\nGA_POP = 30\nGA_GENS = 10       # <<-- set to 20\n\nGWO_WOLVES = 10\nGWO_ITERS = 10     # <<-- set to 20\n\nHLO_POP = 15\nHLO_ITERS = 10\nHLO_TEACHER_FACTOR = 0.75\nHLO_MUTATION = 0.12\n\n# Greedy hill-climb\nHILLCLIMB_MAX_STEPS = 100\nHILLCLIMB_EVAL_CAP = 500\n\n# CV folds\nCV_OPT = 2\nCV_FINAL = 5\n\n# CatBoost iterations\nCB_ITER_OPT = 100\nCB_ITER_HLO = 200\nCB_ITER_FINAL = 500\n\nFINAL_TEST_SIZE = 0.2\nSAVE_PREFIX = \"hybrid_hlo_union\"\n# ------------------------------------------------------------------------\n\n\n# -------------------- Load data (robust handling of messy column names) --------------------\nprint(f\"[{time.strftime('%H:%M:%S')}] Loading CSV from: {CSV_PATH}\")\ndf = pd.read_csv(CSV_PATH, low_memory=False)\nprint(f\"[{time.strftime('%H:%M:%S')}] Raw loaded shape: {df.shape}\")\nprint(f\"[{time.strftime('%H:%M:%S')}] Raw columns sample: {df.columns.tolist()[:12]}\")\n\n# Clean column names: strip whitespace and normalize repeated spaces\ndf.columns = df.columns.astype(str).str.strip().str.replace(r\"\\s+\", \" \", regex=True)\nprint(f\"[{time.strftime('%H:%M:%S')}] Cleaned columns sample: {df.columns.tolist()[:12]}\")\n\n# If an index column like 'Unnamed: 0' exists (common from CSV exports), drop it\nif 'Unnamed: 0' in df.columns:\n    df = df.drop(columns=['Unnamed: 0'])\n    print(f\"[{time.strftime('%H:%M:%S')}] Dropped 'Unnamed: 0' column. New shape: {df.shape}\")\n\n# If the requested TARGET_COL isn't found, try to auto-detect a label-like column (case-insensitive)\nif TARGET_COL not in df.columns:\n    # try case-insensitive match\n    cols_lower = {c.lower(): c for c in df.columns}\n    if TARGET_COL.lower() in cols_lower:\n        real_col = cols_lower[TARGET_COL.lower()]\n        print(f\"[{time.strftime('%H:%M:%S')}] Using case-insensitive match for target: '{real_col}'\")\n        TARGET_COL = real_col\n    else:\n        # fallback: search for any column name that contains 'label' or 'target'\n        cand = [c for c in df.columns if 'label' in c.lower() or 'target' in c.lower()]\n        if len(cand) == 1:\n            print(f\"[{time.strftime('%H:%M:%S')}] Auto-detected target column: '{cand[0]}'\")\n            TARGET_COL = cand[0]\n        elif len(cand) > 1:\n            print(f\"[{time.strftime('%H:%M:%S')}] Multiple candidate target columns found: {cand}. Using first: '{cand[0]}'\")\n            TARGET_COL = cand[0]\n        else:\n            raise ValueError(f\"Target column '{TARGET_COL}' not found (after cleaning). Columns: {df.columns.tolist()[:12]}...\")\n\nprint(f\"[{time.strftime('%H:%M:%S')}] Using TARGET_COL = '{TARGET_COL}'\")\n\n# Basic preprocessing expectation: ensure no object columns remain unencoded for CatBoost.\nfrom sklearn.preprocessing import LabelEncoder\nobj_cols = df.select_dtypes(include=[\"object\"]).columns.tolist()\nif obj_cols:\n    print(f\"[{time.strftime('%H:%M:%S')}] Label-encoding object columns for safe use: {obj_cols}\")\n    for c in obj_cols:\n        df[c] = df[c].astype(str).fillna(\"NA\")\n        df[c] = LabelEncoder().fit_transform(df[c])\n\n# Ensure no NaNs in features/target used by optimizers\ndf = df.dropna(axis=0).reset_index(drop=True)\n\n# Prepare X, y\nX = df.drop(TARGET_COL, axis=1)\ny = df[TARGET_COL].astype(int)\nFEATURE_NAMES = X.columns.tolist()\nN_FEATURES = X.shape[1]\nprint(f\"[{time.strftime('%H:%M:%S')}] Prepared X ({X.shape}) and y ({y.shape}). Number of features: {N_FEATURES}\")\n\n\n# -------------------- CatBoost factory --------------------\ndef get_catboost_model(iterations=100):\n    try:\n        from catboost import CatBoostClassifier\n    except Exception as e:\n        raise ImportError(\"catboost not installed. Install with: pip install catboost\") from e\n    return CatBoostClassifier(iterations=iterations, learning_rate=0.05, depth=6,\n                              verbose=MODEL_VERBOSE, random_seed=RANDOM_STATE, thread_count=-1)\n\n# -------------------- Fitness cache --------------------\nfitness_cache = {}\ndef key_from_mask(mask_bool):\n    return tuple(sorted(np.where(np.array(mask_bool).astype(bool))[0].tolist()))\n\ndef evaluate_mask_global(mask_bool, cv=CV_OPT, cb_iter=CB_ITER_OPT):\n    key = key_from_mask(mask_bool)\n    if key in fitness_cache:\n        return fitness_cache[key]\n    if len(key) == 0:\n        fitness_cache[key] = 0.0\n        return 0.0\n\n    X_sel = X.iloc[:, list(key)]\n    model = get_catboost_model(iterations=cb_iter)\n    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=RANDOM_STATE)\n\n    try:\n        accs = cross_val_score(clone(model), X_sel, y, cv=skf, scoring=\"accuracy\", n_jobs=-1)\n        precs = cross_val_score(clone(model), X_sel, y, cv=skf, scoring=make_scorer(precision_score, zero_division=0), n_jobs=-1)\n        recs = cross_val_score(clone(model), X_sel, y, cv=skf, scoring=make_scorer(recall_score, zero_division=0), n_jobs=-1)\n        f1s = cross_val_score(clone(model), X_sel, y, cv=skf, scoring=make_scorer(f1_score, zero_division=0), n_jobs=-1)\n        score = float((np.mean(accs) + np.mean(precs) + np.mean(recs) + np.mean(f1s)) / 4.0)\n    except Exception as e:\n        # if a training error occurs (e.g., degenerate feature set), return 0\n        score = 0.0\n\n    fitness_cache[key] = score\n    return score\n\n# -------------------- Helpers --------------------\ndef mask_to_features(mask):\n    idxs = np.where(np.array(mask).astype(bool))[0].tolist()\n    return [FEATURE_NAMES[i] for i in idxs]\n\ndef log(msg):\n    print(f\"[{time.strftime('%H:%M:%S')}] {msg}\", flush=True)\n\n# -------------------- PSO (binary) --------------------\ndef run_pso(swarm_size=PSO_SWARM, iters=PSO_ITERS, cv=CV_OPT):\n    log(f\"PSO START (swarm={swarm_size}, iters={iters}, cv={cv})\")\n    t0 = time.time()\n    dim = N_FEATURES\n    pos = np.random.randint(0,2,(swarm_size,dim)).astype(int)\n    vel = np.random.uniform(-1,1,(swarm_size,dim))\n\n    pbest = pos.copy()\n    pbest_scores = np.array([evaluate_mask_global(p.astype(bool), cv=cv, cb_iter=CB_ITER_OPT) for p in pos])\n\n    gbest_idx = int(np.argmax(pbest_scores))\n    gbest = pbest[gbest_idx].copy()\n    gbest_score = pbest_scores[gbest_idx]\n\n    w = 0.6; c1 = c2 = 1.5\n    for t in range(iters):\n        log(f\" PSO iter {t+1}/{iters} best_global={gbest_score:.4f}\")\n        for i in range(swarm_size):\n            r1 = np.random.rand(dim); r2 = np.random.rand(dim)\n            vel[i] = w*vel[i] + c1*r1*(pbest[i] - pos[i]) + c2*r2*(gbest - pos[i])\n            s = 1.0 / (1.0 + np.exp(-vel[i]))\n            pos[i] = (np.random.rand(dim) < s).astype(int)\n\n            sc = evaluate_mask_global(pos[i].astype(bool), cv=cv, cb_iter=CB_ITER_OPT)\n            if sc > pbest_scores[i]:\n                pbest[i] = pos[i].copy()\n                pbest_scores[i] = sc\n            if sc > gbest_score:\n                gbest = pos[i].copy()\n                gbest_score = sc\n        w = max(0.2, w*0.97)\n\n    best_idx = int(np.argmax(pbest_scores))\n    best_mask = pbest[best_idx].copy()\n    best_score = pbest_scores[best_idx]\n    t1 = time.time()\n    log(f\"PSO DONE in {int(t1-t0)}s best_score={best_score:.4f} selected={int(np.sum(best_mask))}\")\n    return best_mask, best_score, int(t1-t0)\n\n# -------------------- GA (binary) --------------------\ndef run_ga(pop_size=GA_POP, gens=GA_GENS, cv=CV_OPT):\n    log(f\"GA START (pop={pop_size}, gens={gens}, cv={cv})\")\n    t0 = time.time()\n    dim = N_FEATURES\n    pop = np.random.randint(0,2,(pop_size, dim)).astype(int)\n    fitness_scores = np.array([evaluate_mask_global(ind.astype(bool), cv=cv, cb_iter=CB_ITER_OPT) for ind in pop])\n\n    def tournament_select(k=3):\n        idxs = np.random.randint(0, pop_size, k)\n        return idxs[np.argmax(fitness_scores[idxs])]\n\n    for g in range(gens):\n        log(f\" GA gen {g+1}/{gens} current_best={np.max(fitness_scores):.4f}\")\n        new_pop = []\n        # elitism\n        elite_idxs = np.argsort(fitness_scores)[-2:]\n        new_pop.extend(pop[elite_idxs].tolist())\n\n        while len(new_pop) < pop_size:\n            i1 = tournament_select(); i2 = tournament_select()\n            p1 = pop[i1].copy(); p2 = pop[i2].copy()\n            # crossover\n            if np.random.rand() < 0.7:\n                pt = np.random.randint(1, dim)\n                c1 = np.concatenate([p1[:pt], p2[pt:]])\n                c2 = np.concatenate([p2[:pt], p1[pt:]])\n            else:\n                c1, c2 = p1, p2\n            # mutation\n            for child in (c1, c2):\n                for d in range(dim):\n                    if np.random.rand() < 0.1:\n                        child[d] = 1 - child[d]\n                new_pop.append(child)\n                if len(new_pop) >= pop_size:\n                    break\n        pop = np.array(new_pop[:pop_size])\n        fitness_scores = np.array([evaluate_mask_global(ind.astype(bool), cv=cv, cb_iter=CB_ITER_OPT) for ind in pop])\n\n    best_idx = int(np.argmax(fitness_scores))\n    best_mask = pop[best_idx].copy()\n    best_score = fitness_scores[best_idx]\n    t1 = time.time()\n    log(f\"GA DONE in {int(t1-t0)}s best_score={best_score:.4f} selected={int(np.sum(best_mask))}\")\n    return best_mask, best_score, int(t1-t0)\n\n# -------------------- GWO (binary) --------------------\ndef run_gwo(wolves=GWO_WOLVES, iters=GWO_ITERS, cv=CV_OPT):\n    log(f\"GWO START (wolves={wolves}, iters={iters}, cv={cv})\")\n    t0 = time.time()\n    dim = N_FEATURES\n    pop = np.random.randint(0,2,(wolves, dim)).astype(int)\n    fitness_scores = np.array([evaluate_mask_global(ind.astype(bool), cv=cv, cb_iter=CB_ITER_OPT) for ind in pop])\n\n    Alpha = Beta = Delta = None\n    Alpha_score = Beta_score = Delta_score = -1.0\n\n    for itr in range(iters):\n        log(f\" GWO iter {itr+1}/{iters} best_alpha={Alpha_score:.4f}\")\n        for i in range(wolves):\n            sc = fitness_scores[i]\n            if sc > Alpha_score:\n                Delta_score, Beta_score, Alpha_score = Beta_score, Alpha_score, sc\n                Delta, Beta, Alpha = Beta, Alpha, pop[i].copy()\n            elif sc > Beta_score:\n                Delta_score, Beta_score = Beta_score, sc\n                Delta, Beta = Beta, pop[i].copy()\n            elif sc > Delta_score:\n                Delta_score = sc\n                Delta = pop[i].copy()\n\n        a = 2 - itr * (2.0 / iters)\n        for i in range(wolves):\n            for d in range(dim):\n                if Alpha is None:\n                    continue\n                r1, r2 = np.random.rand(), np.random.rand()\n                A1 = 2 * a * r1 - a; C1 = 2 * r2\n                D_alpha = abs(C1 * Alpha[d] - pop[i][d])\n                X1 = Alpha[d] - A1 * D_alpha\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                A2 = 2 * a * r1 - a; C2 = 2 * r2\n                D_beta = abs(C2 * Beta[d] - pop[i][d])\n                X2 = Beta[d] - A2 * D_beta\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                A3 = 2 * a * r1 - a; C3 = 2 * r2\n                D_delta = abs(C3 * Delta[d] - pop[i][d])\n                X3 = Delta[d] - A3 * D_delta\n\n                new_pos = (X1 + X2 + X3) / 3.0\n                s = 1.0 / (1.0 + np.exp(-new_pos))\n                pop[i][d] = 1 if np.random.rand() < s else 0\n\n        fitness_scores = np.array([evaluate_mask_global(ind.astype(bool), cv=cv, cb_iter=CB_ITER_OPT) for ind in pop])\n\n    best_idx = int(np.argmax(fitness_scores))\n    best_mask = pop[best_idx].copy()\n    best_score = fitness_scores[best_idx]\n    t1 = time.time()\n    log(f\"GWO DONE in {int(t1-t0)}s best_score={best_score:.4f} selected={int(np.sum(best_mask))}\")\n    return best_mask, best_score, int(t1-t0)\n\n# -------------------- UNION (only) --------------------\ndef get_union_mask(*masks):\n    union_idx = set()\n    for m in masks:\n        idxs = np.where(np.array(m).astype(bool))[0].tolist()\n        union_idx.update(idxs)\n    mask = np.zeros(N_FEATURES, dtype=int)\n    for i in union_idx:\n        mask[i] = 1\n    return mask\n\n# -------------------- HLO on candidates --------------------\ndef hlo_on_candidates(candidate_mask, pop_size=HLO_POP, iters=HLO_ITERS, cv=CV_OPT):\n    candidate_indices = np.where(np.array(candidate_mask).astype(bool))[0].tolist()\n    k = len(candidate_indices)\n    if k == 0:\n        raise ValueError(\"Candidate set is empty.\")\n\n    log(f\"HLO START on {k} candidate features (pop={pop_size}, iters={iters})\")\n    t0 = time.time()\n\n    pop = np.random.randint(0,2,(pop_size, k)).astype(int)\n\n    def fitness_candidate(bitmask):\n        full_mask = np.zeros(N_FEATURES, dtype=int)\n        for j,bit in enumerate(bitmask):\n            if bit == 1:\n                full_mask[candidate_indices[j]] = 1\n        return evaluate_mask_global(full_mask.astype(bool), cv=cv, cb_iter=CB_ITER_HLO)\n\n    fitness_scores = np.array([fitness_candidate(ind) for ind in pop])\n    best_idx = int(np.argmax(fitness_scores))\n    best_solution = pop[best_idx].copy()\n    best_score = fitness_scores[best_idx]\n\n    for it in range(iters):\n        log(f\" HLO iter {it+1}/{iters} current_best={best_score:.4f}\")\n        teacher = pop[int(np.argmax(fitness_scores))].copy()\n        new_pop = []\n        for i in range(pop_size):\n            learner = pop[i].copy()\n            # teaching phase\n            for d in range(k):\n                if np.random.rand() < HLO_TEACHER_FACTOR:\n                    learner[d] = teacher[d]\n            # peer learning\n            partner = pop[np.random.randint(pop_size)].copy()\n            for d in range(k):\n                if learner[d] != partner[d] and np.random.rand() < 0.5:\n                    learner[d] = partner[d]\n            # mutation\n            for d in range(k):\n                if np.random.rand() < HLO_MUTATION:\n                    learner[d] = 1 - learner[d]\n            new_pop.append(learner)\n        pop = np.array(new_pop)\n        fitness_scores = np.array([fitness_candidate(ind) for ind in pop])\n        gen_best_idx = int(np.argmax(fitness_scores))\n        gen_best_score = fitness_scores[gen_best_idx]\n        gen_best_sol = pop[gen_best_idx].copy()\n        if gen_best_score > best_score:\n            best_score = gen_best_score\n            best_solution = gen_best_sol.copy()\n\n    # map back to full mask\n    final_full_mask = np.zeros(N_FEATURES, dtype=int)\n    for j,bit in enumerate(best_solution):\n        if bit == 1:\n            final_full_mask[candidate_indices[j]] = 1\n\n    t1 = time.time()\n    log(f\"HLO DONE in {int(t1-t0)}s best_score={best_score:.4f} final_selected={int(np.sum(final_full_mask))}\")\n    return final_full_mask, best_score, int(t1-t0)\n\n# -------------------- Greedy Hill-Climb (local search) --------------------\ndef hill_climb_on_candidates(initial_mask, candidate_mask, max_steps=HILLCLIMB_MAX_STEPS, eval_cap=HILLCLIMB_EVAL_CAP, cv=CV_OPT):\n    candidate_indices = np.where(np.array(candidate_mask).astype(bool))[0].tolist()\n    if len(candidate_indices) == 0:\n        log(\"Hill-climb: candidate set empty, skipping.\")\n        return initial_mask, 0.0, 0\n\n    log(f\"Hill-climb START over {len(candidate_indices)} candidates (max_steps={max_steps}, eval_cap={eval_cap})\")\n    t0 = time.time()\n    current_mask = initial_mask.copy()\n    current_score = evaluate_mask_global(current_mask.astype(bool), cv=cv, cb_iter=CB_ITER_HLO)\n    evals = 0\n    steps = 0\n    improved = True\n\n    while improved and steps < max_steps and evals < eval_cap:\n        improved = False\n        for idx in np.random.permutation(candidate_indices):\n            trial_mask = current_mask.copy()\n            trial_mask[idx] = 1 - trial_mask[idx]  # flip\n            trial_score = evaluate_mask_global(trial_mask.astype(bool), cv=cv, cb_iter=CB_ITER_HLO)\n            evals += 1\n            if trial_score > current_score + 1e-8:\n                current_mask = trial_mask\n                current_score = trial_score\n                improved = True\n                steps += 1\n                log(f\" Hill-climb step {steps}: flipped {FEATURE_NAMES[idx]} -> new_score={current_score:.4f} (evals={evals})\")\n                break\n            if evals >= eval_cap or steps >= max_steps:\n                break\n    t1 = time.time()\n    log(f\"Hill-climb DONE in {int(t1-t0)}s steps={steps} evals={evals} final_score={current_score:.4f} selected={int(np.sum(current_mask))}\")\n    return current_mask, current_score, int(t1-t0)\n\n# -------------------- Final evaluation (5-fold CV) --------------------\ndef final_evaluation(mask_bool, cv=CV_FINAL, cb_iter=CB_ITER_FINAL):\n    idxs = np.where(np.array(mask_bool).astype(bool))[0].tolist()\n    if len(idxs) == 0:\n        raise ValueError(\"Final mask selects zero features.\")\n    X_sel = X.iloc[:, idxs]\n    model = get_catboost_model(iterations=cb_iter)\n    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=RANDOM_STATE)\n    accs = []; precs = []; recs = []; f1s = []\n    t0 = time.time()\n    for tr,te in skf.split(X_sel, y):\n        m = clone(model); m.fit(X_sel.iloc[tr], y.iloc[tr])\n        pred = m.predict(X_sel.iloc[te])\n        accs.append(accuracy_score(y.iloc[te], pred))\n        precs.append(precision_score(y.iloc[te], pred, zero_division=0))\n        recs.append(recall_score(y.iloc[te], pred, zero_division=0))\n        f1s.append(f1_score(y.iloc[te], pred, zero_division=0))\n    t1 = time.time()\n    results = {\n        \"n_features\": len(idxs),\n        \"features\": [FEATURE_NAMES[i] for i in idxs],\n        \"acc_mean\": float(np.mean(accs)), \"acc_std\": float(np.std(accs)),\n        \"prec_mean\": float(np.mean(precs)), \"prec_std\": float(np.std(precs)),\n        \"rec_mean\": float(np.mean(recs)), \"rec_std\": float(np.std(recs)),\n        \"f1_mean\": float(np.mean(f1s)), \"f1_std\": float(np.std(f1s)),\n        \"eval_time_s\": int(t1 - t0)\n    }\n    return results\n\n# -------------------- MAIN PIPELINE --------------------\nif __name__ == \"__main__\":\n    total_t0 = time.time()\n    log(\"===== HYBRID (reduced budget) + HLO + HILL-CLIMB (UNION only) START =====\")\n\n    # PSO\n    pso_mask, pso_score, pso_time = run_pso(swarm_size=PSO_SWARM, iters=PSO_ITERS, cv=CV_OPT)\n    pso_feats = mask_to_features(pso_mask)\n    log(f\"PSO selected ({len(pso_feats)}): {pso_feats}\")\n\n    # GA\n    ga_mask, ga_score, ga_time = run_ga(pop_size=GA_POP, gens=GA_GENS, cv=CV_OPT)\n    ga_feats = mask_to_features(ga_mask)\n    log(f\"GA selected ({len(ga_feats)}): {ga_feats}\")\n\n    # GWO\n    gwo_mask, gwo_score, gwo_time = run_gwo(wolves=GWO_WOLVES, iters=GWO_ITERS, cv=CV_OPT)\n    gwo_feats = mask_to_features(gwo_mask)\n    log(f\"GWO selected ({len(gwo_feats)}): {gwo_feats}\")\n\n    # Derive UNION of the three optimizers\n    union_mask = get_union_mask(pso_mask, ga_mask, gwo_mask)\n    union_feats = mask_to_features(union_mask)\n    log(f\"UNION candidate features ({len(union_feats)}): {union_feats}\")\n\n    # HLO on union\n    if len(union_feats) == 0:\n        log(\"UNION empty — nothing to optimize. Exiting.\")\n        raise SystemExit(\"No union features selected by optimizers.\")\n\n    hlo_mask, hlo_score, hlo_time = hlo_on_candidates(union_mask, pop_size=HLO_POP, iters=HLO_ITERS, cv=CV_OPT)\n    hlo_feats = mask_to_features(hlo_mask)\n    log(f\"HLO final mask selected ({len(hlo_feats)}): {hlo_feats}\")\n\n    # Hill-climb restricted to union candidates\n    hc_mask, hc_score, hc_time = hill_climb_on_candidates(hlo_mask, union_mask, max_steps=HILLCLIMB_MAX_STEPS, eval_cap=HILLCLIMB_EVAL_CAP, cv=CV_OPT)\n    hc_feats = mask_to_features(hc_mask)\n    log(f\"Hill-climb final mask selected ({len(hc_feats)}): {hc_feats}\")\n\n    # Final CV evaluation (5-fold)\n    final_res = final_evaluation(hc_mask, cv=CV_FINAL, cb_iter=CB_ITER_FINAL)\n    log(f\"Final CV (5-fold) | n_features={final_res['n_features']} | F1={final_res['f1_mean']:.4f} ± {final_res['f1_std']:.4f}\")\n\n    # Train final CatBoost on 80% and evaluate on 20%, save model\n    selected_idxs = np.where(np.array(hc_mask).astype(bool))[0].tolist()\n    selected_features = [FEATURE_NAMES[i] for i in selected_idxs]\n\n    X_sel = X[selected_features]\n    X_train, X_test, y_train, y_test = train_test_split(X_sel, y, test_size=FINAL_TEST_SIZE, stratify=y, random_state=RANDOM_STATE)\n\n    model = get_catboost_model(iterations=CB_ITER_FINAL)\n    model.fit(X_train, y_train)\n\n    y_pred = model.predict(X_test)\n    test_acc = accuracy_score(y_test, y_pred)\n    test_prec = precision_score(y_test, y_pred, zero_division=0)\n    test_rec = recall_score(y_test, y_pred, zero_division=0)\n    test_f1 = f1_score(y_test, y_pred, zero_division=0)\n    test_cm = confusion_matrix(y_test, y_pred)\n    test_report = classification_report(y_test, y_pred, zero_division=0)\n\n    test_metrics = {\n        'acc': float(test_acc), 'prec': float(test_prec), 'rec': float(test_rec), 'f1': float(test_f1),\n        'n_test': int(X_test.shape[0]),\n        'confusion_matrix': test_cm.tolist(),  # convert to list for pickle/json friendliness\n        'classification_report': test_report\n    }\n\n    model_filename = f\"{SAVE_PREFIX}_union_model.pkl\"\n    with open(model_filename, 'wb') as mf:\n        pickle.dump(model, mf)\n\n    log(f\"Saved final CatBoost union model -> {model_filename} (test_f1={test_f1:.4f})\")\n\n    # Save aggregated results (only union)\n    out = {\n        \"pso_mask\": pso_mask, \"pso_score\": pso_score, \"pso_time\": pso_time,\n        \"ga_mask\": ga_mask, \"ga_score\": ga_score, \"ga_time\": ga_time,\n        \"gwo_mask\": gwo_mask, \"gwo_score\": gwo_score, \"gwo_time\": gwo_time,\n        \"union_mask\": union_mask,\n        \"hlo_mask\": hlo_mask, \"hlo_score\": hlo_score, \"hlo_time\": hlo_time,\n        \"hc_mask\": hc_mask, \"hc_score\": hc_score, \"hc_time\": hc_time,\n        \"final_eval\": final_res,\n        \"selected_features\": selected_features,\n        \"model_file\": model_filename,\n        \"test_metrics\": test_metrics,\n        \"fitness_cache_len\": len(fitness_cache)\n    }\n    with open(f\"{SAVE_PREFIX}_results.pkl\", \"wb\") as f:\n        pickle.dump(out, f)\n\n    total_t1 = time.time()\n    elapsed_total = int(total_t1 - total_t0)\n    log(f\"PIPELINE COMPLETE in {elapsed_total}s. Results saved to {SAVE_PREFIX}_results.pkl and model {model_filename}\")\n\n    # Print short summary and explicit final test metrics (requested)\n    print(\"\\n=== SUMMARY ===\")\n    print(f\"PSO selected ({len(pso_feats)}): {pso_feats}\")\n    print(f\"GA selected  ({len(ga_feats)}): {ga_feats}\")\n    print(f\"GWO selected ({len(gwo_feats)}): {gwo_feats}\")\n    print(f\"UNION candidates ({len(union_feats)}): {union_feats}\")\n    print(f\"HLO selected ({len(hlo_feats)}): {hlo_feats}\")\n    print(f\"HILL-CLIMB selected ({len(hc_feats)}): {hc_feats}\")\n    print(f\"Final CV F1: {final_res['f1_mean']:.4f} ± {final_res['f1_std']:.4f}\")\n\n    # Final test set metrics (explicit printout)\n    print(\"\\n--- FINAL TEST METRICS (80/20 held-out) ---\")\n    print(f\"Test samples (n_test) : {test_metrics['n_test']}\")\n    print(f\"Accuracy : {test_metrics['acc']:.4f}\")\n    print(f\"Precision: {test_metrics['prec']:.4f}\")\n    print(f\"Recall   : {test_metrics['rec']:.4f}\")\n    print(f\"F1-score : {test_metrics['f1']:.4f}\")\n    print(\"\\nConfusion Matrix (rows=true / cols=pred):\")\n    print(np.array(test_metrics['confusion_matrix']))\n    print(\"\\nClassification Report:\")\n    print(test_metrics['classification_report'])\n\n    print(f\"\\nModel saved to: {model_filename}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T18:12:47.636053Z","iopub.execute_input":"2025-12-04T18:12:47.636361Z","iopub.status.idle":"2025-12-04T18:22:30.433639Z","shell.execute_reply.started":"2025-12-04T18:12:47.636339Z","shell.execute_reply":"2025-12-04T18:22:30.432508Z"}},"outputs":[{"name":"stdout","text":"[18:12:47] Loading CSV from: /kaggle/input/newwwww/ids2018_cleaned_combined_1.csv\n[18:12:49] Raw loaded shape: (97802, 76)\n[18:12:49] Raw columns sample: ['Dst Port', 'Protocol', 'Timestamp', 'Flow Duration', 'Tot Fwd Pkts', 'Tot Bwd Pkts', 'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std']\n[18:12:49] Cleaned columns sample: ['Dst Port', 'Protocol', 'Timestamp', 'Flow Duration', 'Tot Fwd Pkts', 'Tot Bwd Pkts', 'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std']\n[18:12:49] Using TARGET_COL = 'Label'\n[18:12:49] Prepared X ((97802, 75)) and y ((97802,)). Number of features: 75\n[18:12:49] ===== HYBRID (reduced budget) + HLO + HILL-CLIMB (UNION only) START =====\n[18:12:49] PSO START (swarm=15, iters=10, cv=2)\n[18:16:03]  PSO iter 1/10 best_global=0.9992\n[18:19:12]  PSO iter 2/10 best_global=0.9993\n[18:22:19]  PSO iter 3/10 best_global=0.9993\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/2627125261.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m     \u001b[0;31m# PSO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m     \u001b[0mpso_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpso_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpso_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_pso\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mswarm_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPSO_SWARM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPSO_ITERS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCV_OPT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m     \u001b[0mpso_feats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask_to_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpso_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"PSO selected ({len(pso_feats)}): {pso_feats}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_47/2627125261.py\u001b[0m in \u001b[0;36mrun_pso\u001b[0;34m(swarm_size, iters, cv)\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0mpos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m             \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_mask_global\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCB_ITER_OPT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msc\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mpbest_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m                 \u001b[0mpbest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_47/2627125261.py\u001b[0m in \u001b[0;36mevaluate_mask_global\u001b[0;34m(mask_bool, cv, cb_iter)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mprecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_sel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_scorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecision_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_division\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mrecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_sel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_scorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecall_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_division\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0mf1s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_sel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_scorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_division\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    513\u001b[0m     \u001b[0mscorer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m     cv_results = cross_validate(\n\u001b[0m\u001b[1;32m    516\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# independent, and that it is pickle-able.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0mparallel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     results = parallel(\n\u001b[0m\u001b[1;32m    267\u001b[0m         delayed(_fit_and_score)(\n\u001b[1;32m    268\u001b[0m             \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2070\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2072\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2074\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1681\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1682\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1684\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1798\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTASK_PENDING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1799\u001b[0m                 ):\n\u001b[0;32m-> 1800\u001b[0;31m                     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1801\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":8},{"cell_type":"code","source":"# hybrid_hlo_union_only.py\n# Reduced-budget hybrid pipeline: PSO, GA, GWO -> UNION -> HLO -> Hill-climb -> final CatBoost (save)\n# Prints selected features after PSO/GA/GWO and the final union members.\n# Runs on Kaggle input path by default and prints final test metrics (accuracy, precision, recall, f1),\n# confusion matrix and classification report.\n\nimport time\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport warnings\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, make_scorer, classification_report, confusion_matrix\nfrom sklearn.base import clone\n\nwarnings.filterwarnings(\"ignore\")\nnp.random.seed(42)\n\n# -------------------- USER / EXPERIMENT SETTINGS --------------------\n# Kaggle path requested by you:\nCSV_PATH = \"/kaggle/input/newwwww/ids2018_cleaned_combined_1.csv\"\n\nTARGET_COL = \"Label\"   # change if your dataset uses another column name\nMODEL_VERBOSE = 0            # CatBoost verbosity: 0 = silent\nRANDOM_STATE = 42\n\n# ---------- Reduced budgets (set to 20 for the three optimizers as requested) ----------\nPSO_SWARM = 15\nPSO_ITERS = 10     # <<-- set to 20\n\nGA_POP = 30\nGA_GENS = 10       # <<-- set to 20\n\nGWO_WOLVES = 10\nGWO_ITERS = 10     # <<-- set to 20\n\nHLO_POP = 15\nHLO_ITERS = 10\nHLO_TEACHER_FACTOR = 0.75\nHLO_MUTATION = 0.12\n\n# Greedy hill-climb\nHILLCLIMB_MAX_STEPS = 100\nHILLCLIMB_EVAL_CAP = 500\n\n# CV folds\nCV_OPT = 2\nCV_FINAL = 5\n\n# CatBoost iterations\nCB_ITER_OPT = 100\nCB_ITER_HLO = 200\nCB_ITER_FINAL = 500\n\nFINAL_TEST_SIZE = 0.2\nSAVE_PREFIX = \"hybrid_hlo_union\"\n\n# -------------------- NEW: sampling to reduce total rows (equal contributions) --------------------\n# Set how many rows to sample per class (balanced). Use min(available, this).\nSAMPLED_PER_CLASS = 1500\n# ------------------------------------------------------------------------\n\n# -------------------- Load data (robust handling of messy column names) --------------------\nprint(f\"[{time.strftime('%H:%M:%S')}] Loading CSV from: {CSV_PATH}\")\ndf = pd.read_csv(CSV_PATH, low_memory=False)\nprint(f\"[{time.strftime('%H:%M:%S')}] Raw loaded shape: {df.shape}\")\nprint(f\"[{time.strftime('%H:%M:%S')}] Raw columns sample: {df.columns.tolist()[:12]}\")\n\n# Clean column names: strip whitespace and normalize repeated spaces\ndf.columns = df.columns.astype(str).str.strip().str.replace(r\"\\s+\", \" \", regex=True)\nprint(f\"[{time.strftime('%H:%M:%S')}] Cleaned columns sample: {df.columns.tolist()[:12]}\")\n\n# If an index column like 'Unnamed: 0' exists (common from CSV exports), drop it\nif 'Unnamed: 0' in df.columns:\n    df = df.drop(columns=['Unnamed: 0'])\n    print(f\"[{time.strftime('%H:%M:%S')}] Dropped 'Unnamed: 0' column. New shape: {df.shape}\")\n\n# If the requested TARGET_COL isn't found, try to auto-detect a label-like column (case-insensitive)\nif TARGET_COL not in df.columns:\n    # try case-insensitive match\n    cols_lower = {c.lower(): c for c in df.columns}\n    if TARGET_COL.lower() in cols_lower:\n        real_col = cols_lower[TARGET_COL.lower()]\n        print(f\"[{time.strftime('%H:%M:%S')}] Using case-insensitive match for target: '{real_col}'\")\n        TARGET_COL = real_col\n    else:\n        # fallback: search for any column name that contains 'label' or 'target'\n        cand = [c for c in df.columns if 'label' in c.lower() or 'target' in c.lower()]\n        if len(cand) == 1:\n            print(f\"[{time.strftime('%H:%M:%S')}] Auto-detected target column: '{cand[0]}'\")\n            TARGET_COL = cand[0]\n        elif len(cand) > 1:\n            print(f\"[{time.strftime('%H:%M:%S')}] Multiple candidate target columns found: {cand}. Using first: '{cand[0]}'\")\n            TARGET_COL = cand[0]\n        else:\n            raise ValueError(f\"Target column '{TARGET_COL}' not found (after cleaning). Columns: {df.columns.tolist()[:12]}...\")\n\nprint(f\"[{time.strftime('%H:%M:%S')}] Using TARGET_COL = '{TARGET_COL}'\")\n\n# Basic preprocessing expectation: ensure no object columns remain unencoded for CatBoost.\nfrom sklearn.preprocessing import LabelEncoder\nobj_cols = df.select_dtypes(include=[\"object\"]).columns.tolist()\nif obj_cols:\n    print(f\"[{time.strftime('%H:%M:%S')}] Label-encoding object columns for safe use: {obj_cols}\")\n    for c in obj_cols:\n        df[c] = df[c].astype(str).fillna(\"NA\")\n        df[c] = LabelEncoder().fit_transform(df[c])\n\n# Ensure no NaNs in features/target used by optimizers\ndf = df.dropna(axis=0).reset_index(drop=True)\n\n# -------------------- NEW SAMPLING: create a balanced reduced dataset --------------------\nif TARGET_COL not in df.columns:\n    raise ValueError(f\"Target column {TARGET_COL} missing after preprocessing.\")\n\n# ensure target numeric\ntry:\n    df[TARGET_COL] = df[TARGET_COL].astype(int)\nexcept Exception:\n    # try mapping benign/other to binary if strings\n    df[TARGET_COL] = df[TARGET_COL].astype(str).str.strip().str.lower()\n    df[TARGET_COL] = df[TARGET_COL].apply(lambda x: 0 if x in (\"benign\", \"0\", \"false\") else 1)\n\ncounts = df[TARGET_COL].value_counts().to_dict()\nprint(f\"[{time.strftime('%H:%M:%S')}] Label counts before sampling: {counts}\")\n\nif 0 in counts and 1 in counts:\n    available0 = counts.get(0, 0)\n    available1 = counts.get(1, 0)\n    take_n = min(SAMPLED_PER_CLASS, available0, available1)\n    if take_n < 1:\n        raise RuntimeError(f\"Not enough samples to create balanced subset after sampling: avail 0={available0}, 1={available1}\")\n    # sample equal from each class\n    df0 = df[df[TARGET_COL] == 0].sample(take_n, random_state=RANDOM_STATE)\n    df1 = df[df[TARGET_COL] == 1].sample(take_n, random_state=RANDOM_STATE)\n    df = pd.concat([df0, df1], ignore_index=True).sample(frac=1.0, random_state=RANDOM_STATE).reset_index(drop=True)\n    print(f\"[{time.strftime('%H:%M:%S')}] After balanced sampling: each class {take_n} rows -> total {len(df)}\")\nelse:\n    # if dataset not binary as 0/1, don't sample; proceed (user dataset should be binary)\n    print(f\"[{time.strftime('%H:%M:%S')}] WARNING: target not binary 0/1, sampling skipped. Label unique values: {df[TARGET_COL].unique()}\")\n\n# Prepare X, y\nX = df.drop(TARGET_COL, axis=1)\ny = df[TARGET_COL].astype(int)\nFEATURE_NAMES = X.columns.tolist()\nN_FEATURES = X.shape[1]\nprint(f\"[{time.strftime('%H:%M:%S')}] Prepared X ({X.shape}) and y ({y.shape}). Number of features: {N_FEATURES}\")\n\n\n# -------------------- CatBoost factory --------------------\ndef get_catboost_model(iterations=100):\n    try:\n        from catboost import CatBoostClassifier\n    except Exception as e:\n        raise ImportError(\"catboost not installed. Install with: pip install catboost\") from e\n    return CatBoostClassifier(iterations=iterations, learning_rate=0.05, depth=6,\n                              verbose=MODEL_VERBOSE, random_seed=RANDOM_STATE, thread_count=-1)\n\n# -------------------- Fitness cache --------------------\nfitness_cache = {}\ndef key_from_mask(mask_bool):\n    return tuple(sorted(np.where(np.array(mask_bool).astype(bool))[0].tolist()))\n\ndef evaluate_mask_global(mask_bool, cv=CV_OPT, cb_iter=CB_ITER_OPT):\n    key = key_from_mask(mask_bool)\n    if key in fitness_cache:\n        return fitness_cache[key]\n    if len(key) == 0:\n        fitness_cache[key] = 0.0\n        return 0.0\n\n    X_sel = X.iloc[:, list(key)]\n    model = get_catboost_model(iterations=cb_iter)\n    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=RANDOM_STATE)\n\n    try:\n        accs = cross_val_score(clone(model), X_sel, y, cv=skf, scoring=\"accuracy\", n_jobs=-1)\n        precs = cross_val_score(clone(model), X_sel, y, cv=skf, scoring=make_scorer(precision_score, zero_division=0), n_jobs=-1)\n        recs = cross_val_score(clone(model), X_sel, y, cv=skf, scoring=make_scorer(recall_score, zero_division=0), n_jobs=-1)\n        f1s = cross_val_score(clone(model), X_sel, y, cv=skf, scoring=make_scorer(f1_score, zero_division=0), n_jobs=-1)\n        score = float((np.mean(accs) + np.mean(precs) + np.mean(recs) + np.mean(f1s)) / 4.0)\n    except Exception as e:\n        # if a training error occurs (e.g., degenerate feature set), return 0\n        score = 0.0\n\n    fitness_cache[key] = score\n    return score\n\n# -------------------- Helpers --------------------\ndef mask_to_features(mask):\n    idxs = np.where(np.array(mask).astype(bool))[0].tolist()\n    return [FEATURE_NAMES[i] for i in idxs]\n\ndef log(msg):\n    print(f\"[{time.strftime('%H:%M:%S')}] {msg}\", flush=True)\n\n# -------------------- PSO (binary) --------------------\ndef run_pso(swarm_size=PSO_SWARM, iters=PSO_ITERS, cv=CV_OPT):\n    log(f\"PSO START (swarm={swarm_size}, iters={iters}, cv={cv})\")\n    t0 = time.time()\n    dim = N_FEATURES\n    pos = np.random.randint(0,2,(swarm_size,dim)).astype(int)\n    vel = np.random.uniform(-1,1,(swarm_size,dim))\n\n    pbest = pos.copy()\n    pbest_scores = np.array([evaluate_mask_global(p.astype(bool), cv=cv, cb_iter=CB_ITER_OPT) for p in pos])\n\n    gbest_idx = int(np.argmax(pbest_scores))\n    gbest = pbest[gbest_idx].copy()\n    gbest_score = pbest_scores[gbest_idx]\n\n    w = 0.6; c1 = c2 = 1.5\n    for t in range(iters):\n        log(f\" PSO iter {t+1}/{iters} best_global={gbest_score:.4f}\")\n        for i in range(swarm_size):\n            r1 = np.random.rand(dim); r2 = np.random.rand(dim)\n            vel[i] = w*vel[i] + c1*r1*(pbest[i] - pos[i]) + c2*r2*(gbest - pos[i])\n            s = 1.0 / (1.0 + np.exp(-vel[i]))\n            pos[i] = (np.random.rand(dim) < s).astype(int)\n\n            sc = evaluate_mask_global(pos[i].astype(bool), cv=cv, cb_iter=CB_ITER_OPT)\n            if sc > pbest_scores[i]:\n                pbest[i] = pos[i].copy()\n                pbest_scores[i] = sc\n            if sc > gbest_score:\n                gbest = pos[i].copy()\n                gbest_score = sc\n        w = max(0.2, w*0.97)\n\n    best_idx = int(np.argmax(pbest_scores))\n    best_mask = pbest[best_idx].copy()\n    best_score = pbest_scores[best_idx]\n    t1 = time.time()\n    log(f\"PSO DONE in {int(t1-t0)}s best_score={best_score:.4f} selected={int(np.sum(best_mask))}\")\n    return best_mask, best_score, int(t1-t0)\n\n# -------------------- GA (binary) --------------------\ndef run_ga(pop_size=GA_POP, gens=GA_GENS, cv=CV_OPT):\n    log(f\"GA START (pop={pop_size}, gens={gens}, cv={cv})\")\n    t0 = time.time()\n    dim = N_FEATURES\n    pop = np.random.randint(0,2,(pop_size, dim)).astype(int)\n    fitness_scores = np.array([evaluate_mask_global(ind.astype(bool), cv=cv, cb_iter=CB_ITER_OPT) for ind in pop])\n\n    def tournament_select(k=3):\n        idxs = np.random.randint(0, pop_size, k)\n        return idxs[np.argmax(fitness_scores[idxs])]\n\n    for g in range(gens):\n        log(f\" GA gen {g+1}/{gens} current_best={np.max(fitness_scores):.4f}\")\n        new_pop = []\n        # elitism\n        elite_idxs = np.argsort(fitness_scores)[-2:]\n        new_pop.extend(pop[elite_idxs].tolist())\n\n        while len(new_pop) < pop_size:\n            i1 = tournament_select(); i2 = tournament_select()\n            p1 = pop[i1].copy(); p2 = pop[i2].copy()\n            # crossover\n            if np.random.rand() < 0.7:\n                pt = np.random.randint(1, dim)\n                c1 = np.concatenate([p1[:pt], p2[pt:]])\n                c2 = np.concatenate([p2[:pt], p1[pt:]])\n            else:\n                c1, c2 = p1, p2\n            # mutation\n            for child in (c1, c2):\n                for d in range(dim):\n                    if np.random.rand() < 0.1:\n                        child[d] = 1 - child[d]\n                new_pop.append(child)\n                if len(new_pop) >= pop_size:\n                    break\n        pop = np.array(new_pop[:pop_size])\n        fitness_scores = np.array([evaluate_mask_global(ind.astype(bool), cv=cv, cb_iter=CB_ITER_OPT) for ind in pop])\n\n    best_idx = int(np.argmax(fitness_scores))\n    best_mask = pop[best_idx].copy()\n    best_score = fitness_scores[best_idx]\n    t1 = time.time()\n    log(f\"GA DONE in {int(t1-t0)}s best_score={best_score:.4f} selected={int(np.sum(best_mask))}\")\n    return best_mask, best_score, int(t1-t0)\n\n# -------------------- GWO (binary) --------------------\ndef run_gwo(wolves=GWO_WOLVES, iters=GWO_ITERS, cv=CV_OPT):\n    log(f\"GWO START (wolves={wolves}, iters={iters}, cv={cv})\")\n    t0 = time.time()\n    dim = N_FEATURES\n    pop = np.random.randint(0,2,(wolves, dim)).astype(int)\n    fitness_scores = np.array([evaluate_mask_global(ind.astype(bool), cv=cv, cb_iter=CB_ITER_OPT) for ind in pop])\n\n    Alpha = Beta = Delta = None\n    Alpha_score = Beta_score = Delta_score = -1.0\n\n    for itr in range(iters):\n        log(f\" GWO iter {itr+1}/{iters} best_alpha={Alpha_score:.4f}\")\n        for i in range(wolves):\n            sc = fitness_scores[i]\n            if sc > Alpha_score:\n                Delta_score, Beta_score, Alpha_score = Beta_score, Alpha_score, sc\n                Delta, Beta, Alpha = Beta, Alpha, pop[i].copy()\n            elif sc > Beta_score:\n                Delta_score, Beta_score = Beta_score, sc\n                Delta, Beta = Beta, pop[i].copy()\n            elif sc > Delta_score:\n                Delta_score = sc\n                Delta = pop[i].copy()\n\n        a = 2 - itr * (2.0 / iters)\n        for i in range(wolves):\n            for d in range(dim):\n                if Alpha is None:\n                    continue\n                r1, r2 = np.random.rand(), np.random.rand()\n                A1 = 2 * a * r1 - a; C1 = 2 * r2\n                D_alpha = abs(C1 * Alpha[d] - pop[i][d])\n                X1 = Alpha[d] - A1 * D_alpha\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                A2 = 2 * a * r1 - a; C2 = 2 * r2\n                D_beta = abs(C2 * Beta[d] - pop[i][d])\n                X2 = Beta[d] - A2 * D_beta\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                A3 = 2 * a * r1 - a; C3 = 2 * r2\n                D_delta = abs(C3 * Delta[d] - pop[i][d])\n                X3 = Delta[d] - A3 * D_delta\n\n                new_pos = (X1 + X2 + X3) / 3.0\n                s = 1.0 / (1.0 + np.exp(-new_pos))\n                pop[i][d] = 1 if np.random.rand() < s else 0\n\n        fitness_scores = np.array([evaluate_mask_global(ind.astype(bool), cv=cv, cb_iter=CB_ITER_OPT) for ind in pop])\n\n    best_idx = int(np.argmax(fitness_scores))\n    best_mask = pop[best_idx].copy()\n    best_score = fitness_scores[best_idx]\n    t1 = time.time()\n    log(f\"GWO DONE in {int(t1-t0)}s best_score={best_score:.4f} selected={int(np.sum(best_mask))}\")\n    return best_mask, best_score, int(t1-t0)\n\n# -------------------- UNION (only) --------------------\ndef get_union_mask(*masks):\n    union_idx = set()\n    for m in masks:\n        idxs = np.where(np.array(m).astype(bool))[0].tolist()\n        union_idx.update(idxs)\n    mask = np.zeros(N_FEATURES, dtype=int)\n    for i in union_idx:\n        mask[i] = 1\n    return mask\n\n# -------------------- HLO on candidates --------------------\ndef hlo_on_candidates(candidate_mask, pop_size=HLO_POP, iters=HLO_ITERS, cv=CV_OPT):\n    candidate_indices = np.where(np.array(candidate_mask).astype(bool))[0].tolist()\n    k = len(candidate_indices)\n    if k == 0:\n        raise ValueError(\"Candidate set is empty.\")\n\n    log(f\"HLO START on {k} candidate features (pop={pop_size}, iters={iters})\")\n    t0 = time.time()\n\n    pop = np.random.randint(0,2,(pop_size, k)).astype(int)\n\n    def fitness_candidate(bitmask):\n        full_mask = np.zeros(N_FEATURES, dtype=int)\n        for j,bit in enumerate(bitmask):\n            if bit == 1:\n                full_mask[candidate_indices[j]] = 1\n        return evaluate_mask_global(full_mask.astype(bool), cv=cv, cb_iter=CB_ITER_HLO)\n\n    fitness_scores = np.array([fitness_candidate(ind) for ind in pop])\n    best_idx = int(np.argmax(fitness_scores))\n    best_solution = pop[best_idx].copy()\n    best_score = fitness_scores[best_idx]\n\n    for it in range(iters):\n        log(f\" HLO iter {it+1}/{iters} current_best={best_score:.4f}\")\n        teacher = pop[int(np.argmax(fitness_scores))].copy()\n        new_pop = []\n        for i in range(pop_size):\n            learner = pop[i].copy()\n            # teaching phase\n            for d in range(k):\n                if np.random.rand() < HLO_TEACHER_FACTOR:\n                    learner[d] = teacher[d]\n            # peer learning\n            partner = pop[np.random.randint(pop_size)].copy()\n            for d in range(k):\n                if learner[d] != partner[d] and np.random.rand() < 0.5:\n                    learner[d] = partner[d]\n            # mutation\n            for d in range(k):\n                if np.random.rand() < HLO_MUTATION:\n                    learner[d] = 1 - learner[d]\n            new_pop.append(learner)\n        pop = np.array(new_pop)\n        fitness_scores = np.array([fitness_candidate(ind) for ind in pop])\n        gen_best_idx = int(np.argmax(fitness_scores))\n        gen_best_score = fitness_scores[gen_best_idx]\n        gen_best_sol = pop[gen_best_idx].copy()\n        if gen_best_score > best_score:\n            best_score = gen_best_score\n            best_solution = gen_best_sol.copy()\n\n    # map back to full mask\n    final_full_mask = np.zeros(N_FEATURES, dtype=int)\n    for j,bit in enumerate(best_solution):\n        if bit == 1:\n            final_full_mask[candidate_indices[j]] = 1\n\n    t1 = time.time()\n    log(f\"HLO DONE in {int(t1-t0)}s best_score={best_score:.4f} final_selected={int(np.sum(final_full_mask))}\")\n    return final_full_mask, best_score, int(t1-t0)\n\n# -------------------- Greedy Hill-Climb (local search) --------------------\ndef hill_climb_on_candidates(initial_mask, candidate_mask, max_steps=HILLCLIMB_MAX_STEPS, eval_cap=HILLCLIMB_EVAL_CAP, cv=CV_OPT):\n    candidate_indices = np.where(np.array(candidate_mask).astype(bool))[0].tolist()\n    if len(candidate_indices) == 0:\n        log(\"Hill-climb: candidate set empty, skipping.\")\n        return initial_mask, 0.0, 0\n\n    log(f\"Hill-climb START over {len(candidate_indices)} candidates (max_steps={max_steps}, eval_cap={eval_cap})\")\n    t0 = time.time()\n    current_mask = initial_mask.copy()\n    current_score = evaluate_mask_global(current_mask.astype(bool), cv=cv, cb_iter=CB_ITER_HLO)\n    evals = 0\n    steps = 0\n    improved = True\n\n    while improved and steps < max_steps and evals < eval_cap:\n        improved = False\n        for idx in np.random.permutation(candidate_indices):\n            trial_mask = current_mask.copy()\n            trial_mask[idx] = 1 - trial_mask[idx]  # flip\n            trial_score = evaluate_mask_global(trial_mask.astype(bool), cv=cv, cb_iter=CB_ITER_HLO)\n            evals += 1\n            if trial_score > current_score + 1e-8:\n                current_mask = trial_mask\n                current_score = trial_score\n                improved = True\n                steps += 1\n                log(f\" Hill-climb step {steps}: flipped {FEATURE_NAMES[idx]} -> new_score={current_score:.4f} (evals={evals})\")\n                break\n            if evals >= eval_cap or steps >= max_steps:\n                break\n    t1 = time.time()\n    log(f\"Hill-climb DONE in {int(t1-t0)}s steps={steps} evals={evals} final_score={current_score:.4f} selected={int(np.sum(current_mask))}\")\n    return current_mask, current_score, int(t1-t0)\n\n# -------------------- Final evaluation (5-fold CV) --------------------\ndef final_evaluation(mask_bool, cv=CV_FINAL, cb_iter=CB_ITER_FINAL):\n    idxs = np.where(np.array(mask_bool).astype(bool))[0].tolist()\n    if len(idxs) == 0:\n        raise ValueError(\"Final mask selects zero features.\")\n    X_sel = X.iloc[:, idxs]\n    model = get_catboost_model(iterations=cb_iter)\n    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=RANDOM_STATE)\n    accs = []; precs = []; recs = []; f1s = []\n    t0 = time.time()\n    for tr,te in skf.split(X_sel, y):\n        m = clone(model); m.fit(X_sel.iloc[tr], y.iloc[tr])\n        pred = m.predict(X_sel.iloc[te])\n        accs.append(accuracy_score(y.iloc[te], pred))\n        precs.append(precision_score(y.iloc[te], pred, zero_division=0))\n        recs.append(recall_score(y.iloc[te], pred, zero_division=0))\n        f1s.append(f1_score(y.iloc[te], pred, zero_division=0))\n    t1 = time.time()\n    results = {\n        \"n_features\": len(idxs),\n        \"features\": [FEATURE_NAMES[i] for i in idxs],\n        \"acc_mean\": float(np.mean(accs)), \"acc_std\": float(np.std(accs)),\n        \"prec_mean\": float(np.mean(precs)), \"prec_std\": float(np.std(precs)),\n        \"rec_mean\": float(np.mean(recs)), \"rec_std\": float(np.std(recs)),\n        \"f1_mean\": float(np.mean(f1s)), \"f1_std\": float(np.std(f1s)),\n        \"eval_time_s\": int(t1 - t0)\n    }\n    return results\n\n# -------------------- MAIN PIPELINE --------------------\nif __name__ == \"__main__\":\n    total_t0 = time.time()\n    log(\"===== HYBRID (reduced budget) + HLO + HILL-CLIMB (UNION only) START =====\")\n\n    # PSO\n    pso_mask, pso_score, pso_time = run_pso(swarm_size=PSO_SWARM, iters=PSO_ITERS, cv=CV_OPT)\n    pso_feats = mask_to_features(pso_mask)\n    log(f\"PSO selected ({len(pso_feats)}): {pso_feats}\")\n\n    # GA\n    ga_mask, ga_score, ga_time = run_ga(pop_size=GA_POP, gens=GA_GENS, cv=CV_OPT)\n    ga_feats = mask_to_features(ga_mask)\n    log(f\"GA selected ({len(ga_feats)}): {ga_feats}\")\n\n    # GWO\n    gwo_mask, gwo_score, gwo_time = run_gwo(wolves=GWO_WOLVES, iters=GWO_ITERS, cv=CV_OPT)\n    gwo_feats = mask_to_features(gwo_mask)\n    log(f\"GWO selected ({len(gwo_feats)}): {gwo_feats}\")\n\n    # Derive UNION of the three optimizers\n    union_mask = get_union_mask(pso_mask, ga_mask, gwo_mask)\n    union_feats = mask_to_features(union_mask)\n    log(f\"UNION candidate features ({len(union_feats)}): {union_feats}\")\n\n    # HLO on union\n    if len(union_feats) == 0:\n        log(\"UNION empty — nothing to optimize. Exiting.\")\n        raise SystemExit(\"No union features selected by optimizers.\")\n\n    hlo_mask, hlo_score, hlo_time = hlo_on_candidates(union_mask, pop_size=HLO_POP, iters=HLO_ITERS, cv=CV_OPT)\n    hlo_feats = mask_to_features(hlo_mask)\n    log(f\"HLO final mask selected ({len(hlo_feats)}): {hlo_feats}\")\n\n    # Hill-climb restricted to union candidates\n    hc_mask, hc_score, hc_time = hill_climb_on_candidates(hlo_mask, union_mask, max_steps=HILLCLIMB_MAX_STEPS, eval_cap=HILLCLIMB_EVAL_CAP, cv=CV_OPT)\n    hc_feats = mask_to_features(hc_mask)\n    log(f\"Hill-climb final mask selected ({len(hc_feats)}): {hc_feats}\")\n\n    # Final CV evaluation (5-fold)\n    final_res = final_evaluation(hc_mask, cv=CV_FINAL, cb_iter=CB_ITER_FINAL)\n    log(f\"Final CV (5-fold) | n_features={final_res['n_features']} | F1={final_res['f1_mean']:.4f} ± {final_res['f1_std']:.4f}\")\n\n    # Train final CatBoost on 80% and evaluate on 20%, save model\n    selected_idxs = np.where(np.array(hc_mask).astype(bool))[0].tolist()\n    selected_features = [FEATURE_NAMES[i] for i in selected_idxs]\n\n    X_sel = X[selected_features]\n    X_train, X_test, y_train, y_test = train_test_split(X_sel, y, test_size=FINAL_TEST_SIZE, stratify=y, random_state=RANDOM_STATE)\n\n    model = get_catboost_model(iterations=CB_ITER_FINAL)\n    model.fit(X_train, y_train)\n\n    y_pred = model.predict(X_test)\n    test_acc = accuracy_score(y_test, y_pred)\n    test_prec = precision_score(y_test, y_pred, zero_division=0)\n    test_rec = recall_score(y_test, y_pred, zero_division=0)\n    test_f1 = f1_score(y_test, y_pred, zero_division=0)\n    test_cm = confusion_matrix(y_test, y_pred)\n    test_report = classification_report(y_test, y_pred, zero_division=0)\n\n    test_metrics = {\n        'acc': float(test_acc), 'prec': float(test_prec), 'rec': float(test_rec), 'f1': float(test_f1),\n        'n_test': int(X_test.shape[0]),\n        'confusion_matrix': test_cm.tolist(),  # convert to list for pickle/json friendliness\n        'classification_report': test_report\n    }\n\n    model_filename = f\"{SAVE_PREFIX}_union_model.pkl\"\n    with open(model_filename, 'wb') as mf:\n        pickle.dump(model, mf)\n\n    log(f\"Saved final CatBoost union model -> {model_filename} (test_f1={test_f1:.4f})\")\n\n    # Save aggregated results (only union)\n    out = {\n        \"pso_mask\": pso_mask, \"pso_score\": pso_score, \"pso_time\": pso_time,\n        \"ga_mask\": ga_mask, \"ga_score\": ga_score, \"ga_time\": ga_time,\n        \"gwo_mask\": gwo_mask, \"gwo_score\": gwo_score, \"gwo_time\": gwo_time,\n        \"union_mask\": union_mask,\n        \"hlo_mask\": hlo_mask, \"hlo_score\": hlo_score, \"hlo_time\": hlo_time,\n        \"hc_mask\": hc_mask, \"hc_score\": hc_score, \"hc_time\": hc_time,\n        \"final_eval\": final_res,\n        \"selected_features\": selected_features,\n        \"model_file\": model_filename,\n        \"test_metrics\": test_metrics,\n        \"fitness_cache_len\": len(fitness_cache)\n    }\n    with open(f\"{SAVE_PREFIX}_results.pkl\", \"wb\") as f:\n        pickle.dump(out, f)\n\n    total_t1 = time.time()\n    elapsed_total = int(total_t1 - total_t0)\n    log(f\"PIPELINE COMPLETE in {elapsed_total}s. Results saved to {SAVE_PREFIX}_results.pkl and model {model_filename}\")\n\n    # Print short summary and explicit final test metrics (requested)\n    print(\"\\n=== SUMMARY ===\")\n    print(f\"PSO selected ({len(pso_feats)}): {pso_feats}\")\n    print(f\"GA selected  ({len(ga_feats)}): {ga_feats}\")\n    print(f\"GWO selected ({len(gwo_feats)}): {gwo_feats}\")\n    print(f\"UNION candidates ({len(union_feats)}): {union_feats}\")\n    print(f\"HLO selected ({len(hlo_feats)}): {hlo_feats}\")\n    print(f\"HILL-CLIMB selected ({len(hc_feats)}): {hc_feats}\")\n    print(f\"Final CV F1: {final_res['f1_mean']:.4f} ± {final_res['f1_std']:.4f}\")\n\n    # Final test set metrics (explicit printout)\n    print(\"\\n--- FINAL TEST METRICS (80/20 held-out) ---\")\n    print(f\"Test samples (n_test) : {test_metrics['n_test']}\")\n    print(f\"Accuracy : {test_metrics['acc']:.4f}\")\n    print(f\"Precision: {test_metrics['prec']:.4f}\")\n    print(f\"Recall   : {test_metrics['rec']:.4f}\")\n    print(f\"F1-score : {test_metrics['f1']:.4f}\")\n    print(\"\\nConfusion Matrix (rows=true / cols=pred):\")\n    print(np.array(test_metrics['confusion_matrix']))\n    print(\"\\nClassification Report:\")\n    print(test_metrics['classification_report'])\n\n    print(f\"\\nModel saved to: {model_filename}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T18:23:14.272077Z","iopub.execute_input":"2025-12-04T18:23:14.272739Z","iopub.status.idle":"2025-12-04T18:25:52.372856Z","shell.execute_reply.started":"2025-12-04T18:23:14.272708Z","shell.execute_reply":"2025-12-04T18:25:52.371707Z"}},"outputs":[{"name":"stdout","text":"[18:23:14] Loading CSV from: /kaggle/input/newwwww/ids2018_cleaned_combined_1.csv\n[18:23:15] Raw loaded shape: (97802, 76)\n[18:23:15] Raw columns sample: ['Dst Port', 'Protocol', 'Timestamp', 'Flow Duration', 'Tot Fwd Pkts', 'Tot Bwd Pkts', 'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std']\n[18:23:15] Cleaned columns sample: ['Dst Port', 'Protocol', 'Timestamp', 'Flow Duration', 'Tot Fwd Pkts', 'Tot Bwd Pkts', 'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std']\n[18:23:15] Using TARGET_COL = 'Label'\n[18:23:15] Label counts before sampling: {0: 49993, 1: 47809}\n[18:23:15] After balanced sampling: each class 1500 rows -> total 3000\n[18:23:15] Prepared X ((3000, 75)) and y ((3000,)). Number of features: 75\n[18:23:15] ===== HYBRID (reduced budget) + HLO + HILL-CLIMB (UNION only) START =====\n[18:23:15] PSO START (swarm=15, iters=10, cv=2)\n[18:24:07]  PSO iter 1/10 best_global=0.9973\n[18:24:56]  PSO iter 2/10 best_global=0.9990\n[18:25:44]  PSO iter 3/10 best_global=0.9990\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/1639605210.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m     \u001b[0;31m# PSO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m     \u001b[0mpso_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpso_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpso_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_pso\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mswarm_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPSO_SWARM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPSO_ITERS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCV_OPT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m     \u001b[0mpso_feats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask_to_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpso_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m     \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"PSO selected ({len(pso_feats)}): {pso_feats}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_47/1639605210.py\u001b[0m in \u001b[0;36mrun_pso\u001b[0;34m(swarm_size, iters, cv)\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0mpos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m             \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_mask_global\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCB_ITER_OPT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msc\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mpbest_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m                 \u001b[0mpbest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_47/1639605210.py\u001b[0m in \u001b[0;36mevaluate_mask_global\u001b[0;34m(mask_bool, cv, cb_iter)\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0maccs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_sel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0mprecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_sel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_scorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecision_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_division\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0mrecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_sel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_scorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecall_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_division\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0mf1s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_sel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_scorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_division\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    513\u001b[0m     \u001b[0mscorer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m     cv_results = cross_validate(\n\u001b[0m\u001b[1;32m    516\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# independent, and that it is pickle-able.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0mparallel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     results = parallel(\n\u001b[0m\u001b[1;32m    267\u001b[0m         delayed(_fit_and_score)(\n\u001b[1;32m    268\u001b[0m             \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2070\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2072\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2074\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1681\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1682\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1684\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1798\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTASK_PENDING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1799\u001b[0m                 ):\n\u001b[0;32m-> 1800\u001b[0;31m                     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1801\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":9},{"cell_type":"code","source":"print(\"hii\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T12:14:18.845144Z","iopub.execute_input":"2025-12-05T12:14:18.845361Z","iopub.status.idle":"2025-12-05T12:14:18.853261Z","shell.execute_reply.started":"2025-12-05T12:14:18.845344Z","shell.execute_reply":"2025-12-05T12:14:18.852406Z"}},"outputs":[{"name":"stdout","text":"hii\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\n\n# ================================================================\n# 1. LOAD HEART DISEASE DATASET (disable low_memory dtype guessing)\n# ================================================================\ninput_path = \"/kaggle/input/new-heart/heart_attack_russia_youth_vs_adult.csv\"\ndf = pd.read_csv(input_path, low_memory=False)\n\nprint(\"Initial shape:\", df.shape)\nprint(\"Initial dtypes (sample):\\n\", df.dtypes.head(20))\n\n\n# ================================================================\n# 2. BASIC CLEANING\n# ================================================================\n# Drop columns entirely NaN\ndf = df.dropna(axis=1, how=\"all\")\n\n# Drop columns that are all zeros (rare in heart disease datasets)\ndf = df.loc[:, (df != 0).any(axis=0)]\n\n# Drop duplicate rows\ndf = df.drop_duplicates().reset_index(drop=True)\n\n\n# ================================================================\n# 3. ROBUST NUMERIC COLUMN DETECTION\n# ================================================================\nnumeric_candidates = []\nconversion_stats = {}\n\nfor col in df.columns:\n    coerced = pd.to_numeric(df[col], errors=\"coerce\")\n    non_na_ratio = coerced.notna().sum() / len(coerced)\n    conversion_stats[col] = non_na_ratio\n\n    # Consider numeric-like if 80%+ values convert\n    if non_na_ratio >= 0.80:\n        numeric_candidates.append(col)\n\nprint(f\"Detected {len(numeric_candidates)} numeric-like columns.\")\n\n# Convert numeric candidates to numeric dtype\nfor col in numeric_candidates:\n    df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n\n\n# ================================================================\n# 4. DETECT + HANDLE INF AND EXTREME VALUES\n# ================================================================\n# Columns containing ±inf\ninf_cols = [c for c in numeric_candidates \n            if np.isinf(df[c].to_numpy()).any()]\n\nprint(\"Columns with ±inf:\", inf_cols)\n\n# Replace ±inf → NaN\nif inf_cols:\n    df[numeric_candidates] = df[numeric_candidates].replace([np.inf, -np.inf], np.nan)\n\n# Detect extremely large magnitude values\nhuge_cols = []\nfor col in numeric_candidates:\n    try:\n        max_abs = np.nanmax(np.abs(df[col].to_numpy()))\n        if np.isfinite(max_abs) and max_abs > 1e300:\n            huge_cols.append((col, max_abs))\n    except:\n        pass\n\nprint(\"Columns with extremely large values (>1e300):\", huge_cols)\n\n# Clip extreme values safely\nCLIP_LIMIT = 1e300\ndf[numeric_candidates] = df[numeric_candidates].apply(\n    lambda s: s.clip(lower=-CLIP_LIMIT, upper=CLIP_LIMIT)\n)\n\n\n# ================================================================\n# 5. RECOMPUTE NUMERIC + CATEGORICAL COL LISTS\n# ================================================================\nnum_cols = df.select_dtypes(include=[\"float64\", \"int64\"]).columns.tolist()\ncat_cols = df.select_dtypes(include=[\"object\"]).columns.tolist()\n\nprint(\"Final numeric column count:\", len(num_cols))\nprint(\"Final categorical column count:\", len(cat_cols))\n\n\n# ================================================================\n# 6. HANDLE MISSING VALUES\n# ================================================================\n# Numeric: fill NaN with median\nif len(num_cols) > 0:\n    df[num_cols] = df[num_cols].fillna(df[num_cols].median())\n\n# Categorical: fill NaN with mode\nfor col in cat_cols:\n    if df[col].isna().any():\n        mode_val = df[col].mode(dropna=True)\n        df[col] = df[col].fillna(mode_val.iloc[0] if len(mode_val) > 0 else \"\")\n\n\n# ================================================================\n# 7. LABEL-ENCODE CATEGORICAL COLUMNS\n# ================================================================\nle = LabelEncoder()\nfor col in cat_cols:\n    df[col] = le.fit_transform(df[col].astype(str))\n\n\n# ================================================================\n# 8. FINAL CHECK: REMOVE ANY REMAINING INF/NaN BEFORE SCALING\n# ================================================================\ndf[num_cols] = df[num_cols].replace([np.inf, -np.inf], np.nan)\ndf[num_cols] = df[num_cols].fillna(df[num_cols].median())\n\n# Confirm numeric columns are finite\nfinite_check = {col: np.isfinite(df[col].to_numpy()).all() for col in num_cols}\nbad_cols = [c for c, ok in finite_check.items() if not ok]\nprint(\"Non-finite columns (should be empty):\", bad_cols)\n\n\n# ================================================================\n# 9. MIN-MAX SCALING\n# ================================================================\nscaler = MinMaxScaler()\nif len(num_cols) > 0:\n    df[num_cols] = scaler.fit_transform(df[num_cols])\n\n\n# ================================================================\n# 10. SAVE CLEANED HEART DISEASE DATASET\n# ================================================================\noutput_filename = \"/kaggle/working/new_heart_disease_cleaned.csv\"\ndf.to_csv(output_filename, index=False)\n\nprint(\"\\n✅ PREPROCESSING COMPLETE!\")\nprint(\"📁 Saved cleaned file as:\", output_filename)\nprint(\"Final shape:\", df.shape)\n\n# Print label distribution if label exists\nfor label_name in [\"target\", \"Target\", \"label\", \"Label\"]:\n    if label_name in df.columns:\n        print(\"\\nLabel distribution:\")\n        print(df[label_name].value_counts())\n        break\nelse:\n    print(\"⚠️ No label column detected in final dataset.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T12:15:16.404646Z","iopub.execute_input":"2025-12-05T12:15:16.405171Z","iopub.status.idle":"2025-12-05T12:15:18.906241Z","shell.execute_reply.started":"2025-12-05T12:15:16.405147Z","shell.execute_reply":"2025-12-05T12:15:18.905460Z"}},"outputs":[{"name":"stdout","text":"Initial shape: (50000, 30)\nInitial dtypes (sample):\n ID                         int64\nAge                        int64\nGender                    object\nRegion                    object\nBlood_Pressure           float64\nCholesterol              float64\nBMI                      float64\nHeart_Rate                 int64\nExercise_Level            object\nSmoking                     bool\nAlcohol_Consumption       object\nDiabetes                    bool\nFamily_History              bool\nStress_Level               int64\nHeart_Attack                bool\nAngina                      bool\nHeart_Disease_History       bool\nDiet                      object\nSleep_Hours              float64\nOccupation                object\ndtype: object\nDetected 19 numeric-like columns.\nColumns with ±inf: []\nColumns with extremely large values (>1e300): []\nFinal numeric column count: 11\nFinal categorical column count: 11\nNon-finite columns (should be empty): []\n\n✅ PREPROCESSING COMPLETE!\n📁 Saved cleaned file as: /kaggle/working/new_heart_disease_cleaned.csv\nFinal shape: (50000, 30)\n⚠️ No label column detected in final dataset.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport pickle\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (\n    classification_report, confusion_matrix,\n    accuracy_score, precision_score, recall_score, f1_score\n)\nfrom catboost import CatBoostClassifier\n\n# ------------------------------------\n# LOAD BALANCED DATASET\n# ------------------------------------\ndf = pd.read_csv(\"/kaggle/working/new_heart_disease_balanced.csv\")\n\nTARGET = \"Heart_Attack\"\n\nX = df.drop(columns=[TARGET])\ny = df[TARGET]\n\n# ------------------------------------\n# STRATIFIED SPLIT\n# ------------------------------------\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.25,\n    stratify=y,\n    random_state=42\n)\n\n# ------------------------------------\n# REGULARIZED CATBOOST MODEL\n# ------------------------------------\nmodel = CatBoostClassifier(\n    iterations=500,\n    depth=8,\n    learning_rate=0.03,\n    l2_leaf_reg=15,\n    subsample=0.6,\n    bootstrap_type=\"Bernoulli\",\n    random_strength=5,\n    eval_metric=\"F1\",\n    verbose=50,\n    random_seed=42\n)\n\nmodel.fit(X_train, y_train, eval_set=(X_test, y_test), verbose=50)\n\n# ------------------------------------\n# PREDICT\n# ------------------------------------\ny_pred = model.predict(X_test)\n\nprint(\"\\nAccuracy :\", accuracy_score(y_test, y_pred))\nprint(\"Precision:\", precision_score(y_test, y_pred))\nprint(\"Recall   :\", recall_score(y_test, y_pred))\nprint(\"F1-score :\", f1_score(y_test, y_pred))\n\nprint(\"\\nConfusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred))\n\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred))\n\n# ------------------------------------\n# SAVE TRAINED MODEL + FEATURES\n# ------------------------------------\nmodel_path = \"/kaggle/working/heart_attack_catboost_model.pkl\"\n\nwith open(model_path, \"wb\") as f:\n    pickle.dump({\n        \"model\": model,\n        \"features\": X.columns.tolist()\n    }, f)\n\nprint(\"\\n✅ Model saved successfully!\")\nprint(\"📁 Saved to:\", model_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T13:51:09.492147Z","iopub.execute_input":"2025-12-05T13:51:09.492881Z","iopub.status.idle":"2025-12-05T13:51:12.662661Z","shell.execute_reply.started":"2025-12-05T13:51:09.492853Z","shell.execute_reply":"2025-12-05T13:51:12.662049Z"}},"outputs":[{"name":"stdout","text":"0:\tlearn: 0.4460159\ttest: 0.4448584\tbest: 0.4448584 (0)\ttotal: 3.1ms\tremaining: 1.54s\n50:\tlearn: 0.7504818\ttest: 0.5049768\tbest: 0.5154572 (12)\ttotal: 299ms\tremaining: 2.63s\n100:\tlearn: 0.8099548\ttest: 0.5111333\tbest: 0.5154572 (12)\ttotal: 589ms\tremaining: 2.33s\n150:\tlearn: 0.8488833\ttest: 0.5003331\tbest: 0.5154572 (12)\ttotal: 887ms\tremaining: 2.05s\n200:\tlearn: 0.8706496\ttest: 0.5068174\tbest: 0.5154572 (12)\ttotal: 1.19s\tremaining: 1.77s\n250:\tlearn: 0.8916290\ttest: 0.5046667\tbest: 0.5154572 (12)\ttotal: 1.48s\tremaining: 1.47s\n300:\tlearn: 0.9075210\ttest: 0.4995005\tbest: 0.5154572 (12)\ttotal: 1.78s\tremaining: 1.18s\n350:\tlearn: 0.9202315\ttest: 0.5010115\tbest: 0.5154572 (12)\ttotal: 2.08s\tremaining: 885ms\n400:\tlearn: 0.9379810\ttest: 0.5037137\tbest: 0.5154572 (12)\ttotal: 2.39s\tremaining: 591ms\n450:\tlearn: 0.9494972\ttest: 0.5011852\tbest: 0.5154572 (12)\ttotal: 2.7s\tremaining: 294ms\n499:\tlearn: 0.9561582\ttest: 0.4988107\tbest: 0.5154572 (12)\ttotal: 3s\tremaining: 0us\n\nbestTest = 0.5154572079\nbestIteration = 12\n\nShrink model to first 13 iterations.\n\nAccuracy : 0.4937096225773546\nPrecision: 0.4940736119775421\nRecall   : 0.5387755102040817\nF1-score : 0.5154572079401237\n\nConfusion Matrix:\n[[660 811]\n [678 792]]\n\nClassification Report:\n              precision    recall  f1-score   support\n\n       False       0.49      0.45      0.47      1471\n        True       0.49      0.54      0.52      1470\n\n    accuracy                           0.49      2941\n   macro avg       0.49      0.49      0.49      2941\nweighted avg       0.49      0.49      0.49      2941\n\n\n✅ Model saved successfully!\n📁 Saved to: /kaggle/working/heart_attack_catboost_model.pkl\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"import pandas as pd\n\n# Load cleaned dataset\ndf = pd.read_csv(\"/kaggle/working/new_heart_disease_cleaned.csv\")\n\nTARGET = \"Heart_Attack\"   # change if needed\n\n# Count class distribution\ncounts = df[TARGET].value_counts()\nprint(\"Original distribution:\\n\", counts)\n\n# Number of minority samples (True = 1)\nminority_count = counts[1]   # 5881 in your case\n\n# Split into two groups\ndf_true  = df[df[TARGET] == 1]\ndf_false = df[df[TARGET] == 0]\n\n# Undersample majority class (False)\ndf_false_balanced = df_false.sample(n=minority_count, random_state=42)\n\n# Combine to form balanced dataset\ndf_balanced = pd.concat([df_true, df_false_balanced], axis=0).sample(frac=1, random_state=42).reset_index(drop=True)\n\nprint(\"\\nBalanced dataset distribution:\")\nprint(df_balanced[TARGET].value_counts())\n\n# Save balanced dataset\noutput_path = \"/kaggle/working/new_heart_disease_balanced.csv\"\ndf_balanced.to_csv(output_path, index=False)\n\nprint(\"\\n✅ Balanced dataset saved to:\", output_path)\nprint(\"Final shape:\", df_balanced.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T12:27:15.367777Z","iopub.execute_input":"2025-12-05T12:27:15.368572Z","iopub.status.idle":"2025-12-05T12:27:15.795389Z","shell.execute_reply.started":"2025-12-05T12:27:15.368546Z","shell.execute_reply":"2025-12-05T12:27:15.794649Z"}},"outputs":[{"name":"stdout","text":"Original distribution:\n Heart_Attack\nFalse    44119\nTrue      5881\nName: count, dtype: int64\n\nBalanced dataset distribution:\nHeart_Attack\nTrue     5881\nFalse    5881\nName: count, dtype: int64\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_47/3287195489.py:13: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  minority_count = counts[1]   # 5881 in your case\n","output_type":"stream"},{"name":"stdout","text":"\n✅ Balanced dataset saved to: /kaggle/working/new_heart_disease_balanced.csv\nFinal shape: (11762, 30)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# intersection_hlo_with_hillclimb_fast.py\n# Pipeline (reduced budget + hill-climb) with UNION, INTERSECTION, and VOTING candidate flows:\n#  PSO + GA + GWO (CatBoost fitness, lighter during opt) -> derive UNION / INTERSECTION / VOTING\n#  For each candidate set: HLO (on candidates) -> Greedy hill-climb (restricted) -> Final CatBoost eval (5-fold CV)\n#  Additionally: train a CatBoost model on 80% of the data and evaluate on the held-out 20% test set\n#  Train & save a CatBoost model for each flow (union / intersection / voting) using the 80/20 split.\n# Prints logs, mean ± std for metrics, stage timings, saves results and models.\n\nimport time\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport warnings\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, make_scorer\nfrom sklearn.base import clone\n\nwarnings.filterwarnings(\"ignore\")\nnp.random.seed(42)\n\n# -------------------- USER / EXPERIMENT SETTINGS --------------------\n# If you prefer to load CSV instead, uncomment and change:\ndf = pd.read_csv(\"/kaggle/working/new_heart_disease_balanced.csv\")\n\nTARGET_COL = \"Heart_Attack\"   # target column\nMODEL_VERBOSE = 0            # CatBoost verbosity: 0 = silent\nRANDOM_STATE = 42\n\n# ---------- Reduced budgets for faster runs (you can tune these) ----------\nPSO_SWARM = 15   # reduced swarm\nPSO_ITERS = 5   # reduced iterations\n\nGA_POP = 30      # reduced population\nGA_GENS = 5     # reduced generations\n\nGWO_WOLVES = 10\nGWO_ITERS = 5\n\nHLO_POP = 15\nHLO_ITERS = 5\nHLO_TEACHER_FACTOR = 0.75\nHLO_MUTATION = 0.12\n\n# Greedy hill-climb after HLO\nHILLCLIMB_MAX_STEPS = 100   # stop if no improvement or step limit\nHILLCLIMB_EVAL_CAP = 500    # safety cap on evaluations (prevent runaway)\n\n# CV folds\nCV_OPT = 2    # cheaper CV during optimization + HLO (speed)\nCV_FINAL = 5  # final evaluation (A1 requested)\n\n# CatBoost iterations\nCB_ITER_OPT = 100    # iterations during optimization (smaller)\nCB_ITER_HLO = 200\nCB_ITER_FINAL = 500  # final evaluation iterations (bigger)\n\n# Train/test split for final saved models\nFINAL_TEST_SIZE = 0.2\n\nSAVE_PREFIX = \"hybrid_hlo_models\"\n# ------------------------------------------------------------------------\n\n# Ensure df exists\ntry:\n    df\nexcept NameError:\n    raise RuntimeError(\"DataFrame `df` not found. Assign your dataset to variable `df` or load at top.\")\n\n# Prepare data\nX = df.drop(TARGET_COL, axis=1)\n\ny = df[TARGET_COL].astype(int)\nFEATURE_NAMES = X.columns.tolist()\nN_FEATURES = X.shape[1]\n\n# -------------------- Model factory (CatBoost) --------------------\ndef get_catboost_model(iterations=100):\n    try:\n        from catboost import CatBoostClassifier\n    except Exception as e:\n        raise ImportError(\"catboost not installed. Install with: pip install catboost\") from e\n    return CatBoostClassifier(iterations=iterations, learning_rate=0.05, depth=6,\n                              verbose=MODEL_VERBOSE, random_seed=RANDOM_STATE, thread_count=-1)\n\n# -------------------- Fitness cache --------------------\n# key: tuple(selected original indices) -> float score\nfitness_cache = {}\n\ndef key_from_mask(mask_bool):\n    return tuple(sorted(np.where(np.array(mask_bool).astype(bool))[0].tolist()))\n\ndef evaluate_mask_global(mask_bool, cv=CV_OPT, cb_iter=CB_ITER_OPT):\n    \"\"\"\n    Evaluate mask using CatBoost with CV and return average of acc,prec,rec,f1.\n    Caches results to avoid re-evaluating identical subsets.\n    \"\"\"\n    key = key_from_mask(mask_bool)\n    if key in fitness_cache:\n        return fitness_cache[key]\n    if len(key) == 0:\n        fitness_cache[key] = 0.0\n        return 0.0\n\n    X_sel = X.iloc[:, list(key)]\n    model = get_catboost_model(iterations=cb_iter)\n    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=RANDOM_STATE)\n\n    accs = cross_val_score(clone(model), X_sel, y, cv=skf, scoring=\"accuracy\", n_jobs=-1)\n    precs = cross_val_score(clone(model), X_sel, y, cv=skf, scoring=make_scorer(precision_score, zero_division=0), n_jobs=-1)\n    recs = cross_val_score(clone(model), X_sel, y, cv=skf, scoring=make_scorer(recall_score, zero_division=0), n_jobs=-1)\n    f1s = cross_val_score(clone(model), X_sel, y, cv=skf, scoring=make_scorer(f1_score, zero_division=0), n_jobs=-1)\n\n    score = float((np.mean(accs) + np.mean(precs) + np.mean(recs) + np.mean(f1s)) / 4.0)\n    fitness_cache[key] = score\n    return score\n\n# -------------------- Helpers --------------------\ndef mask_to_features(mask):\n    idxs = np.where(np.array(mask).astype(bool))[0].tolist()\n    return [FEATURE_NAMES[i] for i in idxs]\n\ndef log(msg):\n    print(f\"[{time.strftime('%H:%M:%S')}] {msg}\", flush=True)\n\n# -------------------- PSO (binary) --------------------\ndef run_pso(swarm_size=PSO_SWARM, iters=PSO_ITERS, cv=CV_OPT):\n    log(f\"PSO START (swarm={swarm_size}, iters={iters}, cv={cv})\")\n    t0 = time.time()\n    dim = N_FEATURES\n    pos = np.random.randint(0,2,(swarm_size,dim)).astype(int)\n    vel = np.random.uniform(-1,1,(swarm_size,dim))\n\n    pbest = pos.copy()\n    pbest_scores = np.array([evaluate_mask_global(p.astype(bool), cv=cv, cb_iter=CB_ITER_OPT) for p in pos])\n\n    gbest_idx = int(np.argmax(pbest_scores))\n    gbest = pbest[gbest_idx].copy()\n    gbest_score = pbest_scores[gbest_idx]\n\n    w = 0.6; c1 = c2 = 1.5\n    for t in range(iters):\n        log(f\" PSO iter {t+1}/{iters} best_global={gbest_score:.4f}\")\n        for i in range(swarm_size):\n            r1 = np.random.rand(dim); r2 = np.random.rand(dim)\n            vel[i] = w*vel[i] + c1*r1*(pbest[i] - pos[i]) + c2*r2*(gbest - pos[i])\n            s = 1.0 / (1.0 + np.exp(-vel[i]))\n            pos[i] = (np.random.rand(dim) < s).astype(int)\n\n            sc = evaluate_mask_global(pos[i].astype(bool), cv=cv, cb_iter=CB_ITER_OPT)\n            if sc > pbest_scores[i]:\n                pbest[i] = pos[i].copy()\n                pbest_scores[i] = sc\n            if sc > gbest_score:\n                gbest = pos[i].copy()\n                gbest_score = sc\n        w = max(0.2, w*0.97)\n\n    best_idx = int(np.argmax(pbest_scores))\n    best_mask = pbest[best_idx].copy()\n    best_score = pbest_scores[best_idx]\n    t1 = time.time()\n    log(f\"PSO DONE in {int(t1-t0)}s best_score={best_score:.4f} selected={int(np.sum(best_mask))}\")\n    log(f\"PSO SELECTED FEATURES: {mask_to_features(best_mask)}\")\n\n    return best_mask, best_score, int(t1-t0)\n\n# -------------------- GA (binary) --------------------\ndef run_ga(pop_size=GA_POP, gens=GA_GENS, cv=CV_OPT):\n    log(f\"GA START (pop={pop_size}, gens={gens}, cv={cv})\")\n    t0 = time.time()\n    dim = N_FEATURES\n    pop = np.random.randint(0,2,(pop_size, dim)).astype(int)\n    fitness_scores = np.array([evaluate_mask_global(ind.astype(bool), cv=cv, cb_iter=CB_ITER_OPT) for ind in pop])\n\n    def tournament_select(k=3):\n        idxs = np.random.randint(0, pop_size, k)\n        return idxs[np.argmax(fitness_scores[idxs])]\n\n    for g in range(gens):\n        log(f\" GA gen {g+1}/{gens} current_best={np.max(fitness_scores):.4f}\")\n        new_pop = []\n        # elitism\n        elite_idxs = np.argsort(fitness_scores)[-2:]\n        new_pop.extend(pop[elite_idxs].tolist())\n\n        while len(new_pop) < pop_size:\n            i1 = tournament_select(); i2 = tournament_select()\n            p1 = pop[i1].copy(); p2 = pop[i2].copy()\n            # crossover\n            if np.random.rand() < 0.7:\n                pt = np.random.randint(1, dim)\n                c1 = np.concatenate([p1[:pt], p2[pt:]])\n                c2 = np.concatenate([p2[:pt], p1[pt:]])\n            else:\n                c1, c2 = p1, p2\n            # mutation\n            for child in (c1, c2):\n                for d in range(dim):\n                    if np.random.rand() < 0.1:\n                        child[d] = 1 - child[d]\n                new_pop.append(child)\n                if len(new_pop) >= pop_size:\n                    break\n        pop = np.array(new_pop[:pop_size])\n        fitness_scores = np.array([evaluate_mask_global(ind.astype(bool), cv=cv, cb_iter=CB_ITER_OPT) for ind in pop])\n\n    best_idx = int(np.argmax(fitness_scores))\n    best_mask = pop[best_idx].copy()\n    best_score = fitness_scores[best_idx]\n    t1 = time.time()\n    log(f\"GA DONE in {int(t1-t0)}s best_score={best_score:.4f} selected={int(np.sum(best_mask))}\")\n    log(f\"GA SELECTED FEATURES: {mask_to_features(best_mask)}\")\n\n    return best_mask, best_score, int(t1-t0)\n\n# -------------------- GWO (binary) --------------------\ndef run_gwo(wolves=GWO_WOLVES, iters=GWO_ITERS, cv=CV_OPT):\n    log(f\"GWO START (wolves={wolves}, iters={iters}, cv={cv})\")\n    t0 = time.time()\n    dim = N_FEATURES\n    pop = np.random.randint(0,2,(wolves, dim)).astype(int)\n    fitness_scores = np.array([evaluate_mask_global(ind.astype(bool), cv=cv, cb_iter=CB_ITER_OPT) for ind in pop])\n\n    Alpha = Beta = Delta = None\n    Alpha_score = Beta_score = Delta_score = -1.0\n\n    for itr in range(iters):\n        log(f\" GWO iter {itr+1}/{iters} best_alpha={Alpha_score:.4f}\")\n        for i in range(wolves):\n            sc = fitness_scores[i]\n            if sc > Alpha_score:\n                Delta_score, Beta_score, Alpha_score = Beta_score, Alpha_score, sc\n                Delta, Beta, Alpha = Beta, Alpha, pop[i].copy()\n            elif sc > Beta_score:\n                Delta_score, Beta_score = Beta_score, sc\n                Delta, Beta = Beta, pop[i].copy()\n            elif sc > Delta_score:\n                Delta_score = sc\n                Delta = pop[i].copy()\n\n        a = 2 - itr * (2.0 / iters)\n        for i in range(wolves):\n            for d in range(dim):\n                if Alpha is None:\n                    continue\n                r1, r2 = np.random.rand(), np.random.rand()\n                A1 = 2 * a * r1 - a; C1 = 2 * r2\n                D_alpha = abs(C1 * Alpha[d] - pop[i][d])\n                X1 = Alpha[d] - A1 * D_alpha\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                A2 = 2 * a * r1 - a; C2 = 2 * r2\n                D_beta = abs(C2 * Beta[d] - pop[i][d])\n                X2 = Beta[d] - A2 * D_beta\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                A3 = 2 * a * r1 - a; C3 = 2 * r2\n                D_delta = abs(C3 * Delta[d] - pop[i][d])\n                X3 = Delta[d] - A3 * D_delta\n\n                new_pos = (X1 + X2 + X3) / 3.0\n                s = 1.0 / (1.0 + np.exp(-new_pos))\n                pop[i][d] = 1 if np.random.rand() < s else 0\n\n        fitness_scores = np.array([evaluate_mask_global(ind.astype(bool), cv=cv, cb_iter=CB_ITER_OPT) for ind in pop])\n\n    best_idx = int(np.argmax(fitness_scores))\n    best_mask = pop[best_idx].copy()\n    best_score = fitness_scores[best_idx]\n    t1 = time.time()\n    log(f\"GWO DONE in {int(t1-t0)}s best_score={best_score:.4f} selected={int(np.sum(best_mask))}\")\n    log(f\"GWO SELECTED FEATURES: {mask_to_features(best_mask)}\")\n\n    return best_mask, best_score, int(t1-t0)\n\n# -------------------- INTERSECTION / UNION / VOTING --------------------\ndef get_intersection_mask(*masks):\n    \"\"\"Return mask that contains only features present in ALL provided masks.\"\"\"\n    if len(masks) == 0:\n        return np.zeros(N_FEATURES, dtype=int)\n    inter_idx = set(np.where(np.array(masks[0]).astype(bool))[0].tolist())\n    for m in masks[1:]:\n        idxs = set(np.where(np.array(m).astype(bool))[0].tolist())\n        inter_idx = inter_idx.intersection(idxs)\n    mask = np.zeros(N_FEATURES, dtype=int)\n    for i in inter_idx:\n        mask[i] = 1\n    return mask\n\n\ndef get_union_mask(*masks):\n    union_idx = set()\n    for m in masks:\n        idxs = np.where(np.array(m).astype(bool))[0].tolist()\n        union_idx.update(idxs)\n    mask = np.zeros(N_FEATURES, dtype=int)\n    for i in union_idx:\n        mask[i] = 1\n    return mask\n\n\ndef get_voting_mask(*masks, threshold=2):\n    \"\"\"Return mask of features selected by at least `threshold` methods (default majority of 3 => 2).\"\"\"\n    if len(masks) == 0:\n        return np.zeros(N_FEATURES, dtype=int)\n    counts = np.zeros(N_FEATURES, dtype=int)\n    for m in masks:\n        counts += np.array(m).astype(int)\n    mask = (counts >= threshold).astype(int)\n    return mask\n\n# -------------------- HLO on candidates --------------------\ndef hlo_on_candidates(candidate_mask, pop_size=HLO_POP, iters=HLO_ITERS, cv=CV_OPT):\n    candidate_indices = np.where(np.array(candidate_mask).astype(bool))[0].tolist()\n    k = len(candidate_indices)\n    if k == 0:\n        raise ValueError(\"Candidate set is empty.\")\n\n    log(f\"HLO START on {k} candidate features (pop={pop_size}, iters={iters})\")\n    t0 = time.time()\n\n    pop = np.random.randint(0,2,(pop_size, k)).astype(int)\n\n    def fitness_candidate(bitmask):\n        full_mask = np.zeros(N_FEATURES, dtype=int)\n        for j,bit in enumerate(bitmask):\n            if bit == 1:\n                full_mask[candidate_indices[j]] = 1\n        return evaluate_mask_global(full_mask.astype(bool), cv=cv, cb_iter=CB_ITER_HLO)\n\n    fitness_scores = np.array([fitness_candidate(ind) for ind in pop])\n    best_idx = int(np.argmax(fitness_scores))\n    best_solution = pop[best_idx].copy()\n    best_score = fitness_scores[best_idx]\n\n    for it in range(iters):\n        log(f\" HLO iter {it+1}/{iters} current_best={best_score:.4f}\")\n        teacher = pop[int(np.argmax(fitness_scores))].copy()\n        new_pop = []\n        for i in range(pop_size):\n            learner = pop[i].copy()\n            # teaching phase\n            for d in range(k):\n                if np.random.rand() < HLO_TEACHER_FACTOR:\n                    learner[d] = teacher[d]\n            # peer learning\n            partner = pop[np.random.randint(pop_size)].copy()\n            for d in range(k):\n                if learner[d] != partner[d] and np.random.rand() < 0.5:\n                    learner[d] = partner[d]\n            # mutation\n            for d in range(k):\n                if np.random.rand() < HLO_MUTATION:\n                    learner[d] = 1 - learner[d]\n            new_pop.append(learner)\n        pop = np.array(new_pop)\n        fitness_scores = np.array([fitness_candidate(ind) for ind in pop])\n        gen_best_idx = int(np.argmax(fitness_scores))\n        gen_best_score = fitness_scores[gen_best_idx]\n        gen_best_sol = pop[gen_best_idx].copy()\n        if gen_best_score > best_score:\n            best_score = gen_best_score\n            best_solution = gen_best_sol.copy()\n\n    # map back to full mask\n    final_full_mask = np.zeros(N_FEATURES, dtype=int)\n    for j,bit in enumerate(best_solution):\n        if bit == 1:\n            final_full_mask[candidate_indices[j]] = 1\n\n    t1 = time.time()\n    log(f\"HLO DONE in {int(t1-t0)}s best_score={best_score:.4f} final_selected={int(np.sum(final_full_mask))}\")\n    return final_full_mask, best_score, int(t1-t0)\n\n# -------------------- Greedy Hill-Climb (local search) --------------------\ndef hill_climb_on_candidates(initial_mask, candidate_mask, max_steps=HILLCLIMB_MAX_STEPS, eval_cap=HILLCLIMB_EVAL_CAP, cv=CV_OPT):\n    \"\"\"\n    Greedy single-bit flip hill-climb restricted to candidate indices.\n    Starts from initial_mask (full-length). Tries flipping each candidate feature's bit:\n    - If flip improves fitness, accept and restart scanning.\n    - Stops when no improving flip found or max_steps/eval_cap reached.\n    \"\"\"\n    candidate_indices = np.where(np.array(candidate_mask).astype(bool))[0].tolist()\n    if len(candidate_indices) == 0:\n        log(\"Hill-climb: candidate set empty, skipping.\")\n        return initial_mask, 0.0, 0\n\n    log(f\"Hill-climb START over {len(candidate_indices)} candidates (max_steps={max_steps}, eval_cap={eval_cap})\")\n    t0 = time.time()\n    current_mask = initial_mask.copy()\n    current_score = evaluate_mask_global(current_mask.astype(bool), cv=cv, cb_iter=CB_ITER_HLO)\n    evals = 0\n    steps = 0\n    improved = True\n\n    while improved and steps < max_steps and evals < eval_cap:\n        improved = False\n        for idx in np.random.permutation(candidate_indices):\n            trial_mask = current_mask.copy()\n            trial_mask[idx] = 1 - trial_mask[idx]  # flip\n            trial_score = evaluate_mask_global(trial_mask.astype(bool), cv=cv, cb_iter=CB_ITER_HLO)\n            evals += 1\n            if trial_score > current_score + 1e-8:\n                current_mask = trial_mask\n                current_score = trial_score\n                improved = True\n                steps += 1\n                log(f\" Hill-climb step {steps}: flipped {FEATURE_NAMES[idx]} -> new_score={current_score:.4f} (evals={evals})\")\n                break\n            if evals >= eval_cap or steps >= max_steps:\n                break\n    t1 = time.time()\n    log(f\"Hill-climb DONE in {int(t1-t0)}s steps={steps} evals={evals} final_score={current_score:.4f} selected={int(np.sum(current_mask))}\")\n    return current_mask, current_score, int(t1-t0)\n\n# -------------------- Final evaluation (5-fold CV) --------------------\ndef final_evaluation(mask_bool, cv=CV_FINAL, cb_iter=CB_ITER_FINAL):\n    idxs = np.where(np.array(mask_bool).astype(bool))[0].tolist()\n    if len(idxs) == 0:\n        raise ValueError(\"Final mask selects zero features.\")\n    X_sel = X.iloc[:, idxs]\n    model = get_catboost_model(iterations=cb_iter)\n    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=RANDOM_STATE)\n    accs = []; precs = []; recs = []; f1s = []\n    t0 = time.time()\n    for tr,te in skf.split(X_sel, y):\n        m = clone(model); m.fit(X_sel.iloc[tr], y.iloc[tr])\n        pred = m.predict(X_sel.iloc[te])\n        accs.append(accuracy_score(y.iloc[te], pred))\n        precs.append(precision_score(y.iloc[te], pred, zero_division=0))\n        recs.append(recall_score(y.iloc[te], pred, zero_division=0))\n        f1s.append(f1_score(y.iloc[te], pred, zero_division=0))\n    t1 = time.time()\n    results = {\n        \"n_features\": len(idxs),\n        \"features\": [FEATURE_NAMES[i] for i in idxs],\n        \"acc_mean\": float(np.mean(accs)), \"acc_std\": float(np.std(accs)),\n        \"prec_mean\": float(np.mean(precs)), \"prec_std\": float(np.std(precs)),\n        \"rec_mean\": float(np.mean(recs)), \"rec_std\": float(np.std(recs)),\n        \"f1_mean\": float(np.mean(f1s)), \"f1_std\": float(np.std(f1s)),\n        \"eval_time_s\": int(t1 - t0)\n    }\n    return results\n\n# -------------------- MAIN PIPELINE --------------------\nif __name__ == \"__main__\":\n    total_t0 = time.time()\n    log(\"===== HYBRID (reduced budget) + HLO + HILL-CLIMB (UNION/INTERSECTION/VOTING) START =====\")\n\n    # PSO\n    pso_mask, pso_score, pso_time = run_pso(swarm_size=PSO_SWARM, iters=PSO_ITERS, cv=CV_OPT)\n\n    # GA\n    ga_mask, ga_score, ga_time = run_ga(pop_size=GA_POP, gens=GA_GENS, cv=CV_OPT)\n\n    # GWO\n    gwo_mask, gwo_score, gwo_time = run_gwo(wolves=GWO_WOLVES, iters=GWO_ITERS, cv=CV_OPT)\n\n    # Derive candidate masks\n    union_mask = get_union_mask(pso_mask, ga_mask, gwo_mask)\n    inter_mask = get_intersection_mask(pso_mask, ga_mask, gwo_mask)\n    vote_mask = get_voting_mask(pso_mask, ga_mask, gwo_mask, threshold=2)\n\n    candidate_sets = {\n        'union': union_mask,\n        'intersection': inter_mask,\n        'voting': vote_mask\n    }\n\n    results_all = {}\n\n    # run HLO -> hill-climb -> final evaluation -> train & save model for each candidate set\n    for name, cand_mask in candidate_sets.items():\n        log(f\"===== PROCESSING {name.upper()} CANDIDATES =====\")\n        n_cand = int(np.sum(cand_mask))\n        log(f\"{name.upper()} candidate features: {n_cand}\")\n        if n_cand == 0:\n            log(f\"{name.upper()} empty — skipping HLO/hill-climb and model training.\")\n            results_all[name] = {'skipped': True, 'n_candidates': 0}\n            continue\n\n        # HLO on this candidate set\n        hlo_mask, hlo_score, hlo_time = hlo_on_candidates(cand_mask, pop_size=HLO_POP, iters=HLO_ITERS, cv=CV_OPT)\n\n        # hill-climb restricted to candidate set\n        hc_mask, hc_score, hc_time = hill_climb_on_candidates(hlo_mask, cand_mask, max_steps=HILLCLIMB_MAX_STEPS, eval_cap=HILLCLIMB_EVAL_CAP, cv=CV_OPT)\n\n        # final CV evaluation\n        final_res = final_evaluation(hc_mask, cv=CV_FINAL, cb_iter=CB_ITER_FINAL)\n\n        # Train final CatBoost model on 80% train and evaluate on 20% test (stratified)\n        sel_idxs = np.where(np.array(hc_mask).astype(bool))[0].tolist()\n        sel_features = [FEATURE_NAMES[i] for i in sel_idxs]\n\n        if len(sel_features) == 0:\n            log(f\"No features selected after hill-climb for {name}, skipping model train.\")\n            results_all[name] = {'skipped': True, 'n_candidates': n_cand}\n            continue\n\n        X_sel = X[sel_features]\n        X_train, X_test, y_train, y_test = train_test_split(X_sel, y, test_size=FINAL_TEST_SIZE, stratify=y, random_state=RANDOM_STATE)\n\n        model = get_catboost_model(iterations=CB_ITER_FINAL)\n        model.fit(X_train, y_train)\n\n        # evaluate on held-out test set (20%)\n        y_pred = model.predict(X_test)\n        test_acc = accuracy_score(y_test, y_pred)\n        test_prec = precision_score(y_test, y_pred, zero_division=0)\n        test_rec = recall_score(y_test, y_pred, zero_division=0)\n        test_f1 = f1_score(y_test, y_pred, zero_division=0)\n\n        test_metrics = {\n            'acc': float(test_acc), 'prec': float(test_prec), 'rec': float(test_rec), 'f1': float(test_f1),\n            'n_test': int(X_test.shape[0])\n        }\n\n        # Save model to file (pickle)\n        model_filename = f\"{SAVE_PREFIX}_{name}_model.pkl\"\n        with open(model_filename, 'wb') as mf:\n            pickle.dump(model, mf)\n\n        # store results\n        results_all[name] = {\n            'n_candidates': n_cand,\n            'hlo_score': float(hlo_score), 'hlo_time': int(hlo_time),\n            'hc_score': float(hc_score), 'hc_time': int(hc_time),\n            'final_eval': final_res,\n            'selected_features': sel_features,\n            'model_file': model_filename,\n            'test_metrics': test_metrics\n        }\n\n        log(f\"Saved trained CatBoost model for {name} -> {model_filename} (test_f1={test_f1:.4f})\")\n\n    total_t1 = time.time()\n    elapsed_total = int(total_t1 - total_t0)\n\n    # Summary / save aggregated results\n    print(\"==================== AGGREGATE SUMMARY ====================\")\n    print(f\"PSO  -> opt_score={pso_score:.4f} selected={int(np.sum(pso_mask))} time={pso_time}s\")\n    print(f\"GA   -> opt_score={ga_score:.4f} selected={int(np.sum(ga_mask))} time={ga_time}s\")\n    print(f\"GWO  -> opt_score={gwo_score:.4f} selected={int(np.sum(gwo_mask))} time={gwo_time}s\")\n    print(f\"Union candidates    : {int(np.sum(union_mask))}\")\n    print(f\"Intersection candidates: {int(np.sum(inter_mask))}\")\n    print(f\"Voting candidates   : {int(np.sum(vote_mask))}\")\n    print(\"-------------------------------------------------\")\n\n    for name, info in results_all.items():\n        print(f\"-- {name.upper()} SUMMARY --\")\n        if info.get('skipped'):\n            print(\" skipped (no candidates)\")\n            continue\n        fe = info['final_eval']\n        tm = info['test_metrics']\n        print(f\" Selected ({fe['n_features']}): {fe['features']}\")\n        print(f\" CV F1   : {fe['f1_mean']:.4f} ± {fe['f1_std']:.4f}\")\n        print(f\" Test F1 : {tm['f1']:.4f} (n_test={tm['n_test']})\")\n        print(f\" Accuracy : {fe['acc_mean']:.4f} ± {fe['acc_std']:.4f}\")\n        print(f\" Precision: {fe['prec_mean']:.4f} ± {fe['prec_std']:.4f}\")\n        print(f\" Recall   : {fe['rec_mean']:.4f} ± {fe['rec_std']:.4f}\")\n        print(f\" Model file: {info['model_file']}\")\n\n\n\n    # Save aggregated pipeline outputs\n    out = {\n        \"pso_mask\": pso_mask, \"pso_score\": pso_score, \"pso_time\": pso_time,\n        \"ga_mask\": ga_mask, \"ga_score\": ga_score, \"ga_time\": ga_time,\n        \"gwo_mask\": gwo_mask, \"gwo_score\": gwo_score, \"gwo_time\": gwo_time,\n        \"union_mask\": union_mask, \"intersection_mask\": inter_mask, \"voting_mask\": vote_mask,\n        \"results_all\": results_all,\n        \"fitness_cache_len\": len(fitness_cache)\n    }\n    with open(f\"{SAVE_PREFIX}_results.pkl\", \"wb\") as f:\n        pickle.dump(out, f)\n\n    log(f\"Saved results to {SAVE_PREFIX}_results.pkl\")\n    log(\"===== PIPELINE COMPLETE =====\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T12:29:50.394563Z","iopub.execute_input":"2025-12-05T12:29:50.394851Z","iopub.status.idle":"2025-12-05T12:50:09.672733Z","shell.execute_reply.started":"2025-12-05T12:29:50.394832Z","shell.execute_reply":"2025-12-05T12:50:09.672135Z"}},"outputs":[{"name":"stdout","text":"[12:29:50] ===== HYBRID (reduced budget) + HLO + HILL-CLIMB (UNION/INTERSECTION/VOTING) START =====\n[12:29:50] PSO START (swarm=15, iters=5, cv=2)\n[12:30:13]  PSO iter 1/5 best_global=0.5129\n[12:30:30]  PSO iter 2/5 best_global=0.5129\n[12:30:48]  PSO iter 3/5 best_global=0.5129\n[12:31:05]  PSO iter 4/5 best_global=0.5129\n[12:31:24]  PSO iter 5/5 best_global=0.5148\n[12:31:42] PSO DONE in 112s best_score=0.5148 selected=13\n[12:31:42] PSO SELECTED FEATURES: ['ID', 'Blood_Pressure', 'Cholesterol', 'BMI', 'Exercise_Level', 'Smoking', 'Alcohol_Consumption', 'Sleep_Hours', 'Occupation', 'Education_Level', 'Marital_Status', 'Urban_Rural', 'Mental_Health']\n[12:31:42] GA START (pop=30, gens=5, cv=2)\n[12:32:18]  GA gen 1/5 current_best=0.5132\n[12:32:53]  GA gen 2/5 current_best=0.5140\n[12:33:28]  GA gen 3/5 current_best=0.5140\n[12:34:04]  GA gen 4/5 current_best=0.5179\n[12:34:40]  GA gen 5/5 current_best=0.5179\n[12:35:15] GA DONE in 212s best_score=0.5179 selected=17\n[12:35:15] GA SELECTED FEATURES: ['Gender', 'Blood_Pressure', 'Cholesterol', 'BMI', 'Heart_Rate', 'Exercise_Level', 'Smoking', 'Alcohol_Consumption', 'Family_History', 'Angina', 'Heart_Disease_History', 'Diet', 'Sleep_Hours', 'Income_Level', 'Urban_Rural', 'Medication', 'Health_Awareness']\n[12:35:15] GWO START (wolves=10, iters=5, cv=2)\n[12:35:27]  GWO iter 1/5 best_alpha=-1.0000\n[12:35:42]  GWO iter 2/5 best_alpha=0.5148\n[12:35:58]  GWO iter 3/5 best_alpha=0.5148\n[12:36:13]  GWO iter 4/5 best_alpha=0.5148\n[12:36:29]  GWO iter 5/5 best_alpha=0.5148\n[12:36:43] GWO DONE in 87s best_score=0.5124 selected=16\n[12:36:43] GWO SELECTED FEATURES: ['Age', 'Gender', 'Blood_Pressure', 'Cholesterol', 'Heart_Rate', 'Smoking', 'Alcohol_Consumption', 'Family_History', 'Angina', 'Diet', 'Occupation', 'Income_Level', 'Education_Level', 'Urban_Rural', 'Daily_Water_Intake', 'Obesity']\n[12:36:43] ===== PROCESSING UNION CANDIDATES =====\n[12:36:43] UNION candidate features: 25\n[12:36:43] HLO START on 25 candidate features (pop=15, iters=5)\n[12:37:32]  HLO iter 1/5 current_best=0.5157\n[12:38:21]  HLO iter 2/5 current_best=0.5194\n[12:39:10]  HLO iter 3/5 current_best=0.5194\n[12:39:59]  HLO iter 4/5 current_best=0.5194\n[12:40:48]  HLO iter 5/5 current_best=0.5194\n[12:41:38] HLO DONE in 295s best_score=0.5194 final_selected=11\n[12:41:38] Hill-climb START over 25 candidates (max_steps=100, eval_cap=500)\n[12:41:44]  Hill-climb step 1: flipped Income_Level -> new_score=0.5225 (evals=2)\n[12:43:02] Hill-climb DONE in 83s steps=1 evals=27 final_score=0.5225 selected=10\n[12:43:11] Saved trained CatBoost model for union -> hybrid_hlo_models_union_model.pkl (test_f1=0.5226)\n[12:43:11] ===== PROCESSING INTERSECTION CANDIDATES =====\n[12:43:11] INTERSECTION candidate features: 5\n[12:43:11] HLO START on 5 candidate features (pop=15, iters=5)\n[12:43:40]  HLO iter 1/5 current_best=0.5922\n[12:43:57]  HLO iter 2/5 current_best=0.5922\n[12:44:04]  HLO iter 3/5 current_best=0.5922\n[12:44:04]  HLO iter 4/5 current_best=0.5922\n[12:44:10]  HLO iter 5/5 current_best=0.5922\n[12:44:10] HLO DONE in 58s best_score=0.5922 final_selected=2\n[12:44:10] Hill-climb START over 5 candidates (max_steps=100, eval_cap=500)\n[12:44:10] Hill-climb DONE in 0s steps=0 evals=5 final_score=0.5922 selected=2\n[12:44:16] Saved trained CatBoost model for intersection -> hybrid_hlo_models_intersection_model.pkl (test_f1=0.5270)\n[12:44:16] ===== PROCESSING VOTING CANDIDATES =====\n[12:44:16] VOTING candidate features: 16\n[12:44:16] HLO START on 16 candidate features (pop=15, iters=5)\n[12:45:04]  HLO iter 1/5 current_best=0.5119\n[12:45:48]  HLO iter 2/5 current_best=0.5119\n[12:46:38]  HLO iter 3/5 current_best=0.5181\n[12:47:27]  HLO iter 4/5 current_best=0.5181\n[12:48:19]  HLO iter 5/5 current_best=0.5181\n[12:49:07] HLO DONE in 291s best_score=0.5181 final_selected=13\n[12:49:07] Hill-climb START over 16 candidates (max_steps=100, eval_cap=500)\n[12:49:59] Hill-climb DONE in 51s steps=0 evals=16 final_score=0.5181 selected=13\n[12:50:09] Saved trained CatBoost model for voting -> hybrid_hlo_models_voting_model.pkl (test_f1=0.5058)\n==================== AGGREGATE SUMMARY ====================\nPSO  -> opt_score=0.5148 selected=13 time=112s\nGA   -> opt_score=0.5179 selected=17 time=212s\nGWO  -> opt_score=0.5124 selected=16 time=87s\nUnion candidates    : 25\nIntersection candidates: 5\nVoting candidates   : 16\n-------------------------------------------------\n-- UNION SUMMARY --\n Selected (10): ['Age', 'Gender', 'Cholesterol', 'Heart_Rate', 'Exercise_Level', 'Smoking', 'Alcohol_Consumption', 'Urban_Rural', 'Health_Awareness', 'Daily_Water_Intake']\n CV F1   : 0.5118 ± 0.0119\n Test F1 : 0.5226 (n_test=2353)\n Accuracy : 0.5078 ± 0.0086\n Precision: 0.5077 ± 0.0086\n Recall   : 0.5162 ± 0.0192\n Model file: hybrid_hlo_models_union_model.pkl\n-- INTERSECTION SUMMARY --\n Selected (2): ['Alcohol_Consumption', 'Urban_Rural']\n CV F1   : 0.5457 ± 0.0139\n Test F1 : 0.5270 (n_test=2353)\n Accuracy : 0.5037 ± 0.0067\n Precision: 0.5030 ± 0.0055\n Recall   : 0.5968 ± 0.0270\n Model file: hybrid_hlo_models_intersection_model.pkl\n-- VOTING SUMMARY --\n Selected (13): ['Blood_Pressure', 'Cholesterol', 'BMI', 'Heart_Rate', 'Exercise_Level', 'Alcohol_Consumption', 'Family_History', 'Angina', 'Diet', 'Sleep_Hours', 'Occupation', 'Income_Level', 'Urban_Rural']\n CV F1   : 0.5108 ± 0.0123\n Test F1 : 0.5058 (n_test=2353)\n Accuracy : 0.5082 ± 0.0055\n Precision: 0.5080 ± 0.0053\n Recall   : 0.5139 ± 0.0203\n Model file: hybrid_hlo_models_voting_model.pkl\n[12:50:09] Saved results to hybrid_hlo_models_results.pkl\n[12:50:09] ===== PIPELINE COMPLETE =====\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# intersection_hlo_with_hillclimb_fast.py\n# Pipeline (reduced budget + hill-climb) with UNION, INTERSECTION, and VOTING candidate flows:\n#  PSO + GA + GWO (CatBoost fitness, lighter during opt) -> derive UNION / INTERSECTION / VOTING\n#  For each candidate set: HLO (on candidates) -> Greedy hill-climb (restricted) -> Final CatBoost eval (5-fold CV)\n#  Additionally: train a CatBoost model on 80% of the data and evaluate on the held-out 20% test set\n#  Train & save a CatBoost model for each flow (union / intersection / voting) using the 80/20 split.\n# Prints logs, mean ± std for metrics, stage timings, saves results and models.\n\n\n#300 iter for final catboost\n\n\nimport time\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport warnings\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, make_scorer\nfrom sklearn.base import clone\n\nwarnings.filterwarnings(\"ignore\")\nnp.random.seed(42)\n\n# -------------------- USER / EXPERIMENT SETTINGS --------------------\n# If you prefer to load CSV instead, uncomment and change:\ndf = pd.read_csv(\"/kaggle/working/new_heart_disease_balanced.csv\")\n\nTARGET_COL = \"Heart_Attack\"   # target column\nMODEL_VERBOSE = 0            # CatBoost verbosity: 0 = silent\nRANDOM_STATE = 42\n\n# ---------- Reduced budgets for faster runs (you can tune these) ----------\nPSO_SWARM = 15   # reduced swarm\nPSO_ITERS = 5   # reduced iterations\n\nGA_POP = 30      # reduced population\nGA_GENS = 5     # reduced generations\n\nGWO_WOLVES = 10\nGWO_ITERS = 5\n\nHLO_POP = 15\nHLO_ITERS = 5\nHLO_TEACHER_FACTOR = 0.75\nHLO_MUTATION = 0.12\n\n# Greedy hill-climb after HLO\nHILLCLIMB_MAX_STEPS = 100   # stop if no improvement or step limit\nHILLCLIMB_EVAL_CAP = 500    # safety cap on evaluations (prevent runaway)\n\n# CV folds\nCV_OPT = 2    # cheaper CV during optimization + HLO (speed)\nCV_FINAL = 5  # final evaluation (A1 requested)\n\n# CatBoost iterations\nCB_ITER_OPT = 100    # iterations during optimization (smaller)\nCB_ITER_HLO = 200\nCB_ITER_FINAL = 300  # final evaluation iterations (bigger)\n\n# Train/test split for final saved models\nFINAL_TEST_SIZE = 0.2\n\nSAVE_PREFIX = \"hybrid_hlo_models\"\n# ------------------------------------------------------------------------\n\n# Ensure df exists\ntry:\n    df\nexcept NameError:\n    raise RuntimeError(\"DataFrame `df` not found. Assign your dataset to variable `df` or load at top.\")\n\n# Prepare data\nX = df.drop(TARGET_COL, axis=1)\n\ny = df[TARGET_COL].astype(int)\nFEATURE_NAMES = X.columns.tolist()\nN_FEATURES = X.shape[1]\n\n# -------------------- Model factory (CatBoost) --------------------\ndef get_catboost_model(iterations=100):\n    try:\n        from catboost import CatBoostClassifier\n    except Exception as e:\n        raise ImportError(\"catboost not installed. Install with: pip install catboost\") from e\n    return CatBoostClassifier(iterations=iterations, learning_rate=0.05, depth=6,\n                              verbose=MODEL_VERBOSE, random_seed=RANDOM_STATE, thread_count=-1)\n\n# -------------------- Fitness cache --------------------\n# key: tuple(selected original indices) -> float score\nfitness_cache = {}\n\ndef key_from_mask(mask_bool):\n    return tuple(sorted(np.where(np.array(mask_bool).astype(bool))[0].tolist()))\n\ndef evaluate_mask_global(mask_bool, cv=CV_OPT, cb_iter=CB_ITER_OPT):\n    \"\"\"\n    Evaluate mask using CatBoost with CV and return average of acc,prec,rec,f1.\n    Caches results to avoid re-evaluating identical subsets.\n    \"\"\"\n    key = key_from_mask(mask_bool)\n    if key in fitness_cache:\n        return fitness_cache[key]\n    if len(key) == 0:\n        fitness_cache[key] = 0.0\n        return 0.0\n\n    X_sel = X.iloc[:, list(key)]\n    model = get_catboost_model(iterations=cb_iter)\n    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=RANDOM_STATE)\n\n    accs = cross_val_score(clone(model), X_sel, y, cv=skf, scoring=\"accuracy\", n_jobs=-1)\n    precs = cross_val_score(clone(model), X_sel, y, cv=skf, scoring=make_scorer(precision_score, zero_division=0), n_jobs=-1)\n    recs = cross_val_score(clone(model), X_sel, y, cv=skf, scoring=make_scorer(recall_score, zero_division=0), n_jobs=-1)\n    f1s = cross_val_score(clone(model), X_sel, y, cv=skf, scoring=make_scorer(f1_score, zero_division=0), n_jobs=-1)\n\n    score = float((np.mean(accs) + np.mean(precs) + np.mean(recs) + np.mean(f1s)) / 4.0)\n    fitness_cache[key] = score\n    return score\n\n# -------------------- Helpers --------------------\ndef mask_to_features(mask):\n    idxs = np.where(np.array(mask).astype(bool))[0].tolist()\n    return [FEATURE_NAMES[i] for i in idxs]\n\ndef log(msg):\n    print(f\"[{time.strftime('%H:%M:%S')}] {msg}\", flush=True)\n\n# -------------------- PSO (binary) --------------------\ndef run_pso(swarm_size=PSO_SWARM, iters=PSO_ITERS, cv=CV_OPT):\n    log(f\"PSO START (swarm={swarm_size}, iters={iters}, cv={cv})\")\n    t0 = time.time()\n    dim = N_FEATURES\n    pos = np.random.randint(0,2,(swarm_size,dim)).astype(int)\n    vel = np.random.uniform(-1,1,(swarm_size,dim))\n\n    pbest = pos.copy()\n    pbest_scores = np.array([evaluate_mask_global(p.astype(bool), cv=cv, cb_iter=CB_ITER_OPT) for p in pos])\n\n    gbest_idx = int(np.argmax(pbest_scores))\n    gbest = pbest[gbest_idx].copy()\n    gbest_score = pbest_scores[gbest_idx]\n\n    w = 0.6; c1 = c2 = 1.5\n    for t in range(iters):\n        log(f\" PSO iter {t+1}/{iters} best_global={gbest_score:.4f}\")\n        for i in range(swarm_size):\n            r1 = np.random.rand(dim); r2 = np.random.rand(dim)\n            vel[i] = w*vel[i] + c1*r1*(pbest[i] - pos[i]) + c2*r2*(gbest - pos[i])\n            s = 1.0 / (1.0 + np.exp(-vel[i]))\n            pos[i] = (np.random.rand(dim) < s).astype(int)\n\n            sc = evaluate_mask_global(pos[i].astype(bool), cv=cv, cb_iter=CB_ITER_OPT)\n            if sc > pbest_scores[i]:\n                pbest[i] = pos[i].copy()\n                pbest_scores[i] = sc\n            if sc > gbest_score:\n                gbest = pos[i].copy()\n                gbest_score = sc\n        w = max(0.2, w*0.97)\n\n    best_idx = int(np.argmax(pbest_scores))\n    best_mask = pbest[best_idx].copy()\n    best_score = pbest_scores[best_idx]\n    t1 = time.time()\n    log(f\"PSO DONE in {int(t1-t0)}s best_score={best_score:.4f} selected={int(np.sum(best_mask))}\")\n    log(f\"PSO SELECTED FEATURES: {mask_to_features(best_mask)}\")\n\n    return best_mask, best_score, int(t1-t0)\n\n# -------------------- GA (binary) --------------------\ndef run_ga(pop_size=GA_POP, gens=GA_GENS, cv=CV_OPT):\n    log(f\"GA START (pop={pop_size}, gens={gens}, cv={cv})\")\n    t0 = time.time()\n    dim = N_FEATURES\n    pop = np.random.randint(0,2,(pop_size, dim)).astype(int)\n    fitness_scores = np.array([evaluate_mask_global(ind.astype(bool), cv=cv, cb_iter=CB_ITER_OPT) for ind in pop])\n\n    def tournament_select(k=3):\n        idxs = np.random.randint(0, pop_size, k)\n        return idxs[np.argmax(fitness_scores[idxs])]\n\n    for g in range(gens):\n        log(f\" GA gen {g+1}/{gens} current_best={np.max(fitness_scores):.4f}\")\n        new_pop = []\n        # elitism\n        elite_idxs = np.argsort(fitness_scores)[-2:]\n        new_pop.extend(pop[elite_idxs].tolist())\n\n        while len(new_pop) < pop_size:\n            i1 = tournament_select(); i2 = tournament_select()\n            p1 = pop[i1].copy(); p2 = pop[i2].copy()\n            # crossover\n            if np.random.rand() < 0.7:\n                pt = np.random.randint(1, dim)\n                c1 = np.concatenate([p1[:pt], p2[pt:]])\n                c2 = np.concatenate([p2[:pt], p1[pt:]])\n            else:\n                c1, c2 = p1, p2\n            # mutation\n            for child in (c1, c2):\n                for d in range(dim):\n                    if np.random.rand() < 0.1:\n                        child[d] = 1 - child[d]\n                new_pop.append(child)\n                if len(new_pop) >= pop_size:\n                    break\n        pop = np.array(new_pop[:pop_size])\n        fitness_scores = np.array([evaluate_mask_global(ind.astype(bool), cv=cv, cb_iter=CB_ITER_OPT) for ind in pop])\n\n    best_idx = int(np.argmax(fitness_scores))\n    best_mask = pop[best_idx].copy()\n    best_score = fitness_scores[best_idx]\n    t1 = time.time()\n    log(f\"GA DONE in {int(t1-t0)}s best_score={best_score:.4f} selected={int(np.sum(best_mask))}\")\n    log(f\"GA SELECTED FEATURES: {mask_to_features(best_mask)}\")\n\n    return best_mask, best_score, int(t1-t0)\n\n# -------------------- GWO (binary) --------------------\ndef run_gwo(wolves=GWO_WOLVES, iters=GWO_ITERS, cv=CV_OPT):\n    log(f\"GWO START (wolves={wolves}, iters={iters}, cv={cv})\")\n    t0 = time.time()\n    dim = N_FEATURES\n    pop = np.random.randint(0,2,(wolves, dim)).astype(int)\n    fitness_scores = np.array([evaluate_mask_global(ind.astype(bool), cv=cv, cb_iter=CB_ITER_OPT) for ind in pop])\n\n    Alpha = Beta = Delta = None\n    Alpha_score = Beta_score = Delta_score = -1.0\n\n    for itr in range(iters):\n        log(f\" GWO iter {itr+1}/{iters} best_alpha={Alpha_score:.4f}\")\n        for i in range(wolves):\n            sc = fitness_scores[i]\n            if sc > Alpha_score:\n                Delta_score, Beta_score, Alpha_score = Beta_score, Alpha_score, sc\n                Delta, Beta, Alpha = Beta, Alpha, pop[i].copy()\n            elif sc > Beta_score:\n                Delta_score, Beta_score = Beta_score, sc\n                Delta, Beta = Beta, pop[i].copy()\n            elif sc > Delta_score:\n                Delta_score = sc\n                Delta = pop[i].copy()\n\n        a = 2 - itr * (2.0 / iters)\n        for i in range(wolves):\n            for d in range(dim):\n                if Alpha is None:\n                    continue\n                r1, r2 = np.random.rand(), np.random.rand()\n                A1 = 2 * a * r1 - a; C1 = 2 * r2\n                D_alpha = abs(C1 * Alpha[d] - pop[i][d])\n                X1 = Alpha[d] - A1 * D_alpha\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                A2 = 2 * a * r1 - a; C2 = 2 * r2\n                D_beta = abs(C2 * Beta[d] - pop[i][d])\n                X2 = Beta[d] - A2 * D_beta\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                A3 = 2 * a * r1 - a; C3 = 2 * r2\n                D_delta = abs(C3 * Delta[d] - pop[i][d])\n                X3 = Delta[d] - A3 * D_delta\n\n                new_pos = (X1 + X2 + X3) / 3.0\n                s = 1.0 / (1.0 + np.exp(-new_pos))\n                pop[i][d] = 1 if np.random.rand() < s else 0\n\n        fitness_scores = np.array([evaluate_mask_global(ind.astype(bool), cv=cv, cb_iter=CB_ITER_OPT) for ind in pop])\n\n    best_idx = int(np.argmax(fitness_scores))\n    best_mask = pop[best_idx].copy()\n    best_score = fitness_scores[best_idx]\n    t1 = time.time()\n    log(f\"GWO DONE in {int(t1-t0)}s best_score={best_score:.4f} selected={int(np.sum(best_mask))}\")\n    log(f\"GWO SELECTED FEATURES: {mask_to_features(best_mask)}\")\n\n    return best_mask, best_score, int(t1-t0)\n\n# -------------------- INTERSECTION / UNION / VOTING --------------------\ndef get_intersection_mask(*masks):\n    \"\"\"Return mask that contains only features present in ALL provided masks.\"\"\"\n    if len(masks) == 0:\n        return np.zeros(N_FEATURES, dtype=int)\n    inter_idx = set(np.where(np.array(masks[0]).astype(bool))[0].tolist())\n    for m in masks[1:]:\n        idxs = set(np.where(np.array(m).astype(bool))[0].tolist())\n        inter_idx = inter_idx.intersection(idxs)\n    mask = np.zeros(N_FEATURES, dtype=int)\n    for i in inter_idx:\n        mask[i] = 1\n    return mask\n\n\ndef get_union_mask(*masks):\n    union_idx = set()\n    for m in masks:\n        idxs = np.where(np.array(m).astype(bool))[0].tolist()\n        union_idx.update(idxs)\n    mask = np.zeros(N_FEATURES, dtype=int)\n    for i in union_idx:\n        mask[i] = 1\n    return mask\n\n\ndef get_voting_mask(*masks, threshold=2):\n    \"\"\"Return mask of features selected by at least `threshold` methods (default majority of 3 => 2).\"\"\"\n    if len(masks) == 0:\n        return np.zeros(N_FEATURES, dtype=int)\n    counts = np.zeros(N_FEATURES, dtype=int)\n    for m in masks:\n        counts += np.array(m).astype(int)\n    mask = (counts >= threshold).astype(int)\n    return mask\n\n# -------------------- HLO on candidates --------------------\ndef hlo_on_candidates(candidate_mask, pop_size=HLO_POP, iters=HLO_ITERS, cv=CV_OPT):\n    candidate_indices = np.where(np.array(candidate_mask).astype(bool))[0].tolist()\n    k = len(candidate_indices)\n    if k == 0:\n        raise ValueError(\"Candidate set is empty.\")\n\n    log(f\"HLO START on {k} candidate features (pop={pop_size}, iters={iters})\")\n    t0 = time.time()\n\n    pop = np.random.randint(0,2,(pop_size, k)).astype(int)\n\n    def fitness_candidate(bitmask):\n        full_mask = np.zeros(N_FEATURES, dtype=int)\n        for j,bit in enumerate(bitmask):\n            if bit == 1:\n                full_mask[candidate_indices[j]] = 1\n        return evaluate_mask_global(full_mask.astype(bool), cv=cv, cb_iter=CB_ITER_HLO)\n\n    fitness_scores = np.array([fitness_candidate(ind) for ind in pop])\n    best_idx = int(np.argmax(fitness_scores))\n    best_solution = pop[best_idx].copy()\n    best_score = fitness_scores[best_idx]\n\n    for it in range(iters):\n        log(f\" HLO iter {it+1}/{iters} current_best={best_score:.4f}\")\n        teacher = pop[int(np.argmax(fitness_scores))].copy()\n        new_pop = []\n        for i in range(pop_size):\n            learner = pop[i].copy()\n            # teaching phase\n            for d in range(k):\n                if np.random.rand() < HLO_TEACHER_FACTOR:\n                    learner[d] = teacher[d]\n            # peer learning\n            partner = pop[np.random.randint(pop_size)].copy()\n            for d in range(k):\n                if learner[d] != partner[d] and np.random.rand() < 0.5:\n                    learner[d] = partner[d]\n            # mutation\n            for d in range(k):\n                if np.random.rand() < HLO_MUTATION:\n                    learner[d] = 1 - learner[d]\n            new_pop.append(learner)\n        pop = np.array(new_pop)\n        fitness_scores = np.array([fitness_candidate(ind) for ind in pop])\n        gen_best_idx = int(np.argmax(fitness_scores))\n        gen_best_score = fitness_scores[gen_best_idx]\n        gen_best_sol = pop[gen_best_idx].copy()\n        if gen_best_score > best_score:\n            best_score = gen_best_score\n            best_solution = gen_best_sol.copy()\n\n    # map back to full mask\n    final_full_mask = np.zeros(N_FEATURES, dtype=int)\n    for j,bit in enumerate(best_solution):\n        if bit == 1:\n            final_full_mask[candidate_indices[j]] = 1\n\n    t1 = time.time()\n    log(f\"HLO DONE in {int(t1-t0)}s best_score={best_score:.4f} final_selected={int(np.sum(final_full_mask))}\")\n    return final_full_mask, best_score, int(t1-t0)\n\n# -------------------- Greedy Hill-Climb (local search) --------------------\ndef hill_climb_on_candidates(initial_mask, candidate_mask, max_steps=HILLCLIMB_MAX_STEPS, eval_cap=HILLCLIMB_EVAL_CAP, cv=CV_OPT):\n    \"\"\"\n    Greedy single-bit flip hill-climb restricted to candidate indices.\n    Starts from initial_mask (full-length). Tries flipping each candidate feature's bit:\n    - If flip improves fitness, accept and restart scanning.\n    - Stops when no improving flip found or max_steps/eval_cap reached.\n    \"\"\"\n    candidate_indices = np.where(np.array(candidate_mask).astype(bool))[0].tolist()\n    if len(candidate_indices) == 0:\n        log(\"Hill-climb: candidate set empty, skipping.\")\n        return initial_mask, 0.0, 0\n\n    log(f\"Hill-climb START over {len(candidate_indices)} candidates (max_steps={max_steps}, eval_cap={eval_cap})\")\n    t0 = time.time()\n    current_mask = initial_mask.copy()\n    current_score = evaluate_mask_global(current_mask.astype(bool), cv=cv, cb_iter=CB_ITER_HLO)\n    evals = 0\n    steps = 0\n    improved = True\n\n    while improved and steps < max_steps and evals < eval_cap:\n        improved = False\n        for idx in np.random.permutation(candidate_indices):\n            trial_mask = current_mask.copy()\n            trial_mask[idx] = 1 - trial_mask[idx]  # flip\n            trial_score = evaluate_mask_global(trial_mask.astype(bool), cv=cv, cb_iter=CB_ITER_HLO)\n            evals += 1\n            if trial_score > current_score + 1e-8:\n                current_mask = trial_mask\n                current_score = trial_score\n                improved = True\n                steps += 1\n                log(f\" Hill-climb step {steps}: flipped {FEATURE_NAMES[idx]} -> new_score={current_score:.4f} (evals={evals})\")\n                break\n            if evals >= eval_cap or steps >= max_steps:\n                break\n    t1 = time.time()\n    log(f\"Hill-climb DONE in {int(t1-t0)}s steps={steps} evals={evals} final_score={current_score:.4f} selected={int(np.sum(current_mask))}\")\n    return current_mask, current_score, int(t1-t0)\n\n# -------------------- Final evaluation (5-fold CV) --------------------\ndef final_evaluation(mask_bool, cv=CV_FINAL, cb_iter=CB_ITER_FINAL):\n    idxs = np.where(np.array(mask_bool).astype(bool))[0].tolist()\n    if len(idxs) == 0:\n        raise ValueError(\"Final mask selects zero features.\")\n    X_sel = X.iloc[:, idxs]\n    model = get_catboost_model(iterations=cb_iter)\n    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=RANDOM_STATE)\n    accs = []; precs = []; recs = []; f1s = []\n    t0 = time.time()\n    for tr,te in skf.split(X_sel, y):\n        m = clone(model); m.fit(X_sel.iloc[tr], y.iloc[tr])\n        pred = m.predict(X_sel.iloc[te])\n        accs.append(accuracy_score(y.iloc[te], pred))\n        precs.append(precision_score(y.iloc[te], pred, zero_division=0))\n        recs.append(recall_score(y.iloc[te], pred, zero_division=0))\n        f1s.append(f1_score(y.iloc[te], pred, zero_division=0))\n    t1 = time.time()\n    results = {\n        \"n_features\": len(idxs),\n        \"features\": [FEATURE_NAMES[i] for i in idxs],\n        \"acc_mean\": float(np.mean(accs)), \"acc_std\": float(np.std(accs)),\n        \"prec_mean\": float(np.mean(precs)), \"prec_std\": float(np.std(precs)),\n        \"rec_mean\": float(np.mean(recs)), \"rec_std\": float(np.std(recs)),\n        \"f1_mean\": float(np.mean(f1s)), \"f1_std\": float(np.std(f1s)),\n        \"eval_time_s\": int(t1 - t0)\n    }\n    return results\n\n# -------------------- MAIN PIPELINE --------------------\nif __name__ == \"__main__\":\n    total_t0 = time.time()\n    log(\"===== HYBRID (reduced budget) + HLO + HILL-CLIMB (UNION/INTERSECTION/VOTING) START =====\")\n\n    # PSO\n    pso_mask, pso_score, pso_time = run_pso(swarm_size=PSO_SWARM, iters=PSO_ITERS, cv=CV_OPT)\n\n    # GA\n    ga_mask, ga_score, ga_time = run_ga(pop_size=GA_POP, gens=GA_GENS, cv=CV_OPT)\n\n    # GWO\n    gwo_mask, gwo_score, gwo_time = run_gwo(wolves=GWO_WOLVES, iters=GWO_ITERS, cv=CV_OPT)\n\n    # Derive candidate masks\n    union_mask = get_union_mask(pso_mask, ga_mask, gwo_mask)\n    inter_mask = get_intersection_mask(pso_mask, ga_mask, gwo_mask)\n    vote_mask = get_voting_mask(pso_mask, ga_mask, gwo_mask, threshold=2)\n\n    candidate_sets = {\n        'union': union_mask,\n        'intersection': inter_mask,\n        'voting': vote_mask\n    }\n\n    results_all = {}\n\n    # run HLO -> hill-climb -> final evaluation -> train & save model for each candidate set\n    for name, cand_mask in candidate_sets.items():\n        log(f\"===== PROCESSING {name.upper()} CANDIDATES =====\")\n        n_cand = int(np.sum(cand_mask))\n        log(f\"{name.upper()} candidate features: {n_cand}\")\n        if n_cand == 0:\n            log(f\"{name.upper()} empty — skipping HLO/hill-climb and model training.\")\n            results_all[name] = {'skipped': True, 'n_candidates': 0}\n            continue\n\n        # HLO on this candidate set\n        hlo_mask, hlo_score, hlo_time = hlo_on_candidates(cand_mask, pop_size=HLO_POP, iters=HLO_ITERS, cv=CV_OPT)\n\n        # hill-climb restricted to candidate set\n        hc_mask, hc_score, hc_time = hill_climb_on_candidates(hlo_mask, cand_mask, max_steps=HILLCLIMB_MAX_STEPS, eval_cap=HILLCLIMB_EVAL_CAP, cv=CV_OPT)\n\n        # final CV evaluation\n        final_res = final_evaluation(hc_mask, cv=CV_FINAL, cb_iter=CB_ITER_FINAL)\n\n        # Train final CatBoost model on 80% train and evaluate on 20% test (stratified)\n        sel_idxs = np.where(np.array(hc_mask).astype(bool))[0].tolist()\n        sel_features = [FEATURE_NAMES[i] for i in sel_idxs]\n\n        if len(sel_features) == 0:\n            log(f\"No features selected after hill-climb for {name}, skipping model train.\")\n            results_all[name] = {'skipped': True, 'n_candidates': n_cand}\n            continue\n\n        X_sel = X[sel_features]\n        X_train, X_test, y_train, y_test = train_test_split(X_sel, y, test_size=FINAL_TEST_SIZE, stratify=y, random_state=RANDOM_STATE)\n\n        model = get_catboost_model(iterations=CB_ITER_FINAL)\n        model.fit(X_train, y_train)\n\n        # evaluate on held-out test set (20%)\n        y_pred = model.predict(X_test)\n        test_acc = accuracy_score(y_test, y_pred)\n        test_prec = precision_score(y_test, y_pred, zero_division=0)\n        test_rec = recall_score(y_test, y_pred, zero_division=0)\n        test_f1 = f1_score(y_test, y_pred, zero_division=0)\n\n        test_metrics = {\n            'acc': float(test_acc), 'prec': float(test_prec), 'rec': float(test_rec), 'f1': float(test_f1),\n            'n_test': int(X_test.shape[0])\n        }\n\n        # Save model to file (pickle)\n        model_filename = f\"{SAVE_PREFIX}_{name}_model.pkl\"\n        with open(model_filename, 'wb') as mf:\n            pickle.dump(model, mf)\n\n        # store results\n        results_all[name] = {\n            'n_candidates': n_cand,\n            'hlo_score': float(hlo_score), 'hlo_time': int(hlo_time),\n            'hc_score': float(hc_score), 'hc_time': int(hc_time),\n            'final_eval': final_res,\n            'selected_features': sel_features,\n            'model_file': model_filename,\n            'test_metrics': test_metrics\n        }\n\n        log(f\"Saved trained CatBoost model for {name} -> {model_filename} (test_f1={test_f1:.4f})\")\n\n    total_t1 = time.time()\n    elapsed_total = int(total_t1 - total_t0)\n\n    # Summary / save aggregated results\n    print(\"==================== AGGREGATE SUMMARY ====================\")\n    print(f\"PSO  -> opt_score={pso_score:.4f} selected={int(np.sum(pso_mask))} time={pso_time}s\")\n    print(f\"GA   -> opt_score={ga_score:.4f} selected={int(np.sum(ga_mask))} time={ga_time}s\")\n    print(f\"GWO  -> opt_score={gwo_score:.4f} selected={int(np.sum(gwo_mask))} time={gwo_time}s\")\n    print(f\"Union candidates    : {int(np.sum(union_mask))}\")\n    print(f\"Intersection candidates: {int(np.sum(inter_mask))}\")\n    print(f\"Voting candidates   : {int(np.sum(vote_mask))}\")\n    print(\"-------------------------------------------------\")\n\n    for name, info in results_all.items():\n        print(f\"-- {name.upper()} SUMMARY --\")\n        if info.get('skipped'):\n            print(\" skipped (no candidates)\")\n            continue\n        fe = info['final_eval']\n        tm = info['test_metrics']\n        print(f\" Selected ({fe['n_features']}): {fe['features']}\")\n        print(f\" CV F1   : {fe['f1_mean']:.4f} ± {fe['f1_std']:.4f}\")\n        print(f\" Test F1 : {tm['f1']:.4f} (n_test={tm['n_test']})\")\n        print(f\" Accuracy : {fe['acc_mean']:.4f} ± {fe['acc_std']:.4f}\")\n        print(f\" Precision: {fe['prec_mean']:.4f} ± {fe['prec_std']:.4f}\")\n        print(f\" Recall   : {fe['rec_mean']:.4f} ± {fe['rec_std']:.4f}\")\n        print(f\" Model file: {info['model_file']}\")\n\n\n\n    # Save aggregated pipeline outputs\n    out = {\n        \"pso_mask\": pso_mask, \"pso_score\": pso_score, \"pso_time\": pso_time,\n        \"ga_mask\": ga_mask, \"ga_score\": ga_score, \"ga_time\": ga_time,\n        \"gwo_mask\": gwo_mask, \"gwo_score\": gwo_score, \"gwo_time\": gwo_time,\n        \"union_mask\": union_mask, \"intersection_mask\": inter_mask, \"voting_mask\": vote_mask,\n        \"results_all\": results_all,\n        \"fitness_cache_len\": len(fitness_cache)\n    }\n    with open(f\"{SAVE_PREFIX}_results.pkl\", \"wb\") as f:\n        pickle.dump(out, f)\n\n    log(f\"Saved results to {SAVE_PREFIX}_results.pkl\")\n    log(\"===== PIPELINE COMPLETE =====\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T13:10:51.379339Z","iopub.execute_input":"2025-12-05T13:10:51.379920Z","iopub.status.idle":"2025-12-05T13:31:03.674860Z","shell.execute_reply.started":"2025-12-05T13:10:51.379895Z","shell.execute_reply":"2025-12-05T13:31:03.674285Z"}},"outputs":[{"name":"stdout","text":"[13:10:51] ===== HYBRID (reduced budget) + HLO + HILL-CLIMB (UNION/INTERSECTION/VOTING) START =====\n[13:10:51] PSO START (swarm=15, iters=5, cv=2)\n[13:11:13]  PSO iter 1/5 best_global=0.5129\n[13:11:31]  PSO iter 2/5 best_global=0.5129\n[13:11:49]  PSO iter 3/5 best_global=0.5129\n[13:12:07]  PSO iter 4/5 best_global=0.5129\n[13:12:26]  PSO iter 5/5 best_global=0.5148\n[13:12:44] PSO DONE in 113s best_score=0.5148 selected=13\n[13:12:44] PSO SELECTED FEATURES: ['ID', 'Blood_Pressure', 'Cholesterol', 'BMI', 'Exercise_Level', 'Smoking', 'Alcohol_Consumption', 'Sleep_Hours', 'Occupation', 'Education_Level', 'Marital_Status', 'Urban_Rural', 'Mental_Health']\n[13:12:44] GA START (pop=30, gens=5, cv=2)\n[13:13:21]  GA gen 1/5 current_best=0.5132\n[13:13:56]  GA gen 2/5 current_best=0.5140\n[13:14:31]  GA gen 3/5 current_best=0.5140\n[13:15:07]  GA gen 4/5 current_best=0.5179\n[13:15:44]  GA gen 5/5 current_best=0.5179\n[13:16:20] GA DONE in 215s best_score=0.5179 selected=17\n[13:16:20] GA SELECTED FEATURES: ['Gender', 'Blood_Pressure', 'Cholesterol', 'BMI', 'Heart_Rate', 'Exercise_Level', 'Smoking', 'Alcohol_Consumption', 'Family_History', 'Angina', 'Heart_Disease_History', 'Diet', 'Sleep_Hours', 'Income_Level', 'Urban_Rural', 'Medication', 'Health_Awareness']\n[13:16:20] GWO START (wolves=10, iters=5, cv=2)\n[13:16:32]  GWO iter 1/5 best_alpha=-1.0000\n[13:16:48]  GWO iter 2/5 best_alpha=0.5148\n[13:17:03]  GWO iter 3/5 best_alpha=0.5148\n[13:17:19]  GWO iter 4/5 best_alpha=0.5148\n[13:17:34]  GWO iter 5/5 best_alpha=0.5148\n[13:17:49] GWO DONE in 89s best_score=0.5124 selected=16\n[13:17:49] GWO SELECTED FEATURES: ['Age', 'Gender', 'Blood_Pressure', 'Cholesterol', 'Heart_Rate', 'Smoking', 'Alcohol_Consumption', 'Family_History', 'Angina', 'Diet', 'Occupation', 'Income_Level', 'Education_Level', 'Urban_Rural', 'Daily_Water_Intake', 'Obesity']\n[13:17:49] ===== PROCESSING UNION CANDIDATES =====\n[13:17:49] UNION candidate features: 25\n[13:17:49] HLO START on 25 candidate features (pop=15, iters=5)\n[13:18:38]  HLO iter 1/5 current_best=0.5157\n[13:19:27]  HLO iter 2/5 current_best=0.5194\n[13:20:16]  HLO iter 3/5 current_best=0.5194\n[13:21:04]  HLO iter 4/5 current_best=0.5194\n[13:21:52]  HLO iter 5/5 current_best=0.5194\n[13:22:44] HLO DONE in 294s best_score=0.5194 final_selected=11\n[13:22:44] Hill-climb START over 25 candidates (max_steps=100, eval_cap=500)\n[13:22:50]  Hill-climb step 1: flipped Income_Level -> new_score=0.5225 (evals=2)\n[13:24:08] Hill-climb DONE in 84s steps=1 evals=27 final_score=0.5225 selected=10\n[13:24:14] Saved trained CatBoost model for union -> hybrid_hlo_models_union_model.pkl (test_f1=0.5289)\n[13:24:14] ===== PROCESSING INTERSECTION CANDIDATES =====\n[13:24:14] INTERSECTION candidate features: 5\n[13:24:14] HLO START on 5 candidate features (pop=15, iters=5)\n[13:24:42]  HLO iter 1/5 current_best=0.5922\n[13:24:59]  HLO iter 2/5 current_best=0.5922\n[13:25:06]  HLO iter 3/5 current_best=0.5922\n[13:25:06]  HLO iter 4/5 current_best=0.5922\n[13:25:12]  HLO iter 5/5 current_best=0.5922\n[13:25:12] HLO DONE in 57s best_score=0.5922 final_selected=2\n[13:25:12] Hill-climb START over 5 candidates (max_steps=100, eval_cap=500)\n[13:25:12] Hill-climb DONE in 0s steps=0 evals=5 final_score=0.5922 selected=2\n[13:25:16] Saved trained CatBoost model for intersection -> hybrid_hlo_models_intersection_model.pkl (test_f1=0.5270)\n[13:25:16] ===== PROCESSING VOTING CANDIDATES =====\n[13:25:16] VOTING candidate features: 16\n[13:25:16] HLO START on 16 candidate features (pop=15, iters=5)\n[13:26:03]  HLO iter 1/5 current_best=0.5119\n[13:26:48]  HLO iter 2/5 current_best=0.5119\n[13:27:38]  HLO iter 3/5 current_best=0.5181\n[13:28:28]  HLO iter 4/5 current_best=0.5181\n[13:29:19]  HLO iter 5/5 current_best=0.5181\n[13:30:06] HLO DONE in 290s best_score=0.5181 final_selected=13\n[13:30:06] Hill-climb START over 16 candidates (max_steps=100, eval_cap=500)\n[13:30:57] Hill-climb DONE in 50s steps=0 evals=16 final_score=0.5181 selected=13\n[13:31:03] Saved trained CatBoost model for voting -> hybrid_hlo_models_voting_model.pkl (test_f1=0.4933)\n==================== AGGREGATE SUMMARY ====================\nPSO  -> opt_score=0.5148 selected=13 time=113s\nGA   -> opt_score=0.5179 selected=17 time=215s\nGWO  -> opt_score=0.5124 selected=16 time=89s\nUnion candidates    : 25\nIntersection candidates: 5\nVoting candidates   : 16\n-------------------------------------------------\n-- UNION SUMMARY --\n Selected (10): ['Age', 'Gender', 'Cholesterol', 'Heart_Rate', 'Exercise_Level', 'Smoking', 'Alcohol_Consumption', 'Urban_Rural', 'Health_Awareness', 'Daily_Water_Intake']\n CV F1   : 0.5206 ± 0.0134\n Test F1 : 0.5289 (n_test=2353)\n Accuracy : 0.5146 ± 0.0092\n Precision: 0.5142 ± 0.0090\n Recall   : 0.5275 ± 0.0213\n Model file: hybrid_hlo_models_union_model.pkl\n-- INTERSECTION SUMMARY --\n Selected (2): ['Alcohol_Consumption', 'Urban_Rural']\n CV F1   : 0.5457 ± 0.0139\n Test F1 : 0.5270 (n_test=2353)\n Accuracy : 0.5037 ± 0.0067\n Precision: 0.5030 ± 0.0055\n Recall   : 0.5968 ± 0.0270\n Model file: hybrid_hlo_models_intersection_model.pkl\n-- VOTING SUMMARY --\n Selected (13): ['Blood_Pressure', 'Cholesterol', 'BMI', 'Heart_Rate', 'Exercise_Level', 'Alcohol_Consumption', 'Family_History', 'Angina', 'Diet', 'Sleep_Hours', 'Occupation', 'Income_Level', 'Urban_Rural']\n CV F1   : 0.5126 ± 0.0126\n Test F1 : 0.4933 (n_test=2353)\n Accuracy : 0.5114 ± 0.0066\n Precision: 0.5112 ± 0.0063\n Recall   : 0.5142 ± 0.0203\n Model file: hybrid_hlo_models_voting_model.pkl\n[13:31:03] Saved results to hybrid_hlo_models_results.pkl\n[13:31:03] ===== PIPELINE COMPLETE =====\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import pandas as pd\nimport os\nimport kagglehub\n\n# Download dataset\npath = kagglehub.dataset_download(\"jsphyg/weather-dataset-rattle-package\")\nprint(\"Path to dataset files:\", path)\n\n# Find CSV file inside folder\nfiles = [f for f in os.listdir(path) if f.endswith(\".csv\")]\nif not files:\n    raise FileNotFoundError(\"No CSV file found in downloaded dataset folder.\")\n\ncsv_path = os.path.join(path, files[0])\nprint(\"Loading file:\", csv_path)\n\n# Load dataset\ndf = pd.read_csv(csv_path)\nprint(\"Loaded shape:\", df.shape)\nprint(\"Available columns:\", df.columns.tolist())\n\n# Check for target column\nif \"RainTomorrow\" not in df.columns:\n    raise ValueError(\"The dataset does not contain 'RainTomorrow' column. Columns:\\n\" + str(df.columns))\n\n# Unique values\nprint(\"\\nUnique values in RainTomorrow column:\")\nprint(df[\"RainTomorrow\"].unique())\n\n# Value counts\nprint(\"\\nValue counts in RainTomorrow column:\")\nprint(df[\"RainTomorrow\"].value_counts())\n\n# Percentages\nprint(\"\\nRainTomorrow distribution (%):\")\nprint(df[\"RainTomorrow\"].value_counts(normalize=True) * 100)\n\n# Missing value count\nprint(\"\\nMissing value count in RainTomorrow:\")\nmissing_count = df[\"RainTomorrow\"].isna().sum()\nprint(missing_count)\n\n# Missing value percentage\nprint(\"\\nMissing value percentage:\")\nmissing_percent = (missing_count / len(df)) * 100\nprint(missing_percent, \"%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T09:20:10.293961Z","iopub.execute_input":"2025-12-05T09:20:10.294223Z","iopub.status.idle":"2025-12-05T09:20:10.711635Z","shell.execute_reply.started":"2025-12-05T09:20:10.294201Z","shell.execute_reply":"2025-12-05T09:20:10.711008Z"}},"outputs":[{"name":"stdout","text":"Path to dataset files: /kaggle/input/weather-dataset-rattle-package\nLoading file: /kaggle/input/weather-dataset-rattle-package/weatherAUS.csv\nLoaded shape: (145460, 23)\nAvailable columns: ['Date', 'Location', 'MinTemp', 'MaxTemp', 'Rainfall', 'Evaporation', 'Sunshine', 'WindGustDir', 'WindGustSpeed', 'WindDir9am', 'WindDir3pm', 'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am', 'Humidity3pm', 'Pressure9am', 'Pressure3pm', 'Cloud9am', 'Cloud3pm', 'Temp9am', 'Temp3pm', 'RainToday', 'RainTomorrow']\n\nUnique values in RainTomorrow column:\n['No' 'Yes' nan]\n\nValue counts in RainTomorrow column:\nRainTomorrow\nNo     110316\nYes     31877\nName: count, dtype: int64\n\nRainTomorrow distribution (%):\nRainTomorrow\nNo     77.581878\nYes    22.418122\nName: proportion, dtype: float64\n\nMissing value count in RainTomorrow:\n3267\n\nMissing value percentage:\n2.245978275814657 %\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"import pandas as pd\nimport os\nimport kagglehub\n\n# Download dataset\npath = kagglehub.dataset_download(\"jsphyg/weather-dataset-rattle-package\")\nprint(\"Path to dataset files:\", path)\n\n# Find CSV file\nfiles = [f for f in os.listdir(path) if f.endswith(\".csv\")]\nif not files:\n    raise FileNotFoundError(\"No CSV file found in downloaded dataset.\")\ncsv_path = os.path.join(path, files[0])\n\ndf = pd.read_csv(csv_path)\nprint(\"Loaded shape:\", df.shape)\n\n# Ensure column exists\nif \"RainTomorrow\" not in df.columns:\n    raise ValueError(\"RainTomorrow column not found. Columns:\\n\" + str(df.columns))\n\n# Remove rows with missing target values\ndf = df.dropna(subset=[\"RainTomorrow\"]).reset_index(drop=True)\n\n# Count classes\nclass_counts = df[\"RainTomorrow\"].value_counts()\nprint(\"\\nClass distribution:\")\nprint(class_counts)\n\n# Determine minority count\nminority_count = class_counts.min()\n\n# Downsample both Yes and No to smallest count\ndf_yes = df[df[\"RainTomorrow\"] == \"Yes\"].sample(minority_count, random_state=42)\ndf_no  = df[df[\"RainTomorrow\"] == \"No\"].sample(minority_count, random_state=42)\n\n# Create balanced dataset\ndf_balanced = pd.concat([df_yes, df_no], ignore_index=True).sample(frac=1, random_state=42)\n\nprint(\"\\nBalanced dataset shape:\", df_balanced.shape)\nprint(df_balanced[\"RainTomorrow\"].value_counts())\n\n# Save new balanced dataset\noutput_path = \"/kaggle/working/weather_balanced.csv\"\ndf_balanced.to_csv(output_path, index=False)\n\nprint(\"\\n✅ Balanced dataset saved to:\", output_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T13:53:52.955226Z","iopub.execute_input":"2025-12-05T13:53:52.955837Z","iopub.status.idle":"2025-12-05T13:53:54.384804Z","shell.execute_reply.started":"2025-12-05T13:53:52.955809Z","shell.execute_reply":"2025-12-05T13:53:54.383914Z"}},"outputs":[{"name":"stdout","text":"Path to dataset files: /kaggle/input/weather-dataset-rattle-package\nLoaded shape: (145460, 23)\n\nClass distribution:\nRainTomorrow\nNo     110316\nYes     31877\nName: count, dtype: int64\n\nBalanced dataset shape: (63754, 23)\nRainTomorrow\nYes    31877\nNo     31877\nName: count, dtype: int64\n\n✅ Balanced dataset saved to: /kaggle/working/weather_balanced.csv\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\n\n# ================================================================\n# 1. LOAD BALANCED WEATHER DATASET\n# ================================================================\ninput_path = \"/kaggle/working/weather_balanced.csv\"  # <-- change if needed\ndf = pd.read_csv(input_path, low_memory=False)\n\nprint(\"Initial shape:\", df.shape)\nprint(\"Initial dtypes (sample):\\n\", df.dtypes.head(20))\n\n\n# ================================================================\n# 2. BASIC CLEANING\n# ================================================================\ndf = df.dropna(axis=1, how=\"all\")                      # remove fully-empty columns\ndf = df.loc[:, (df != 0).any(axis=0)]                 # remove all-zero columns\ndf = df.drop_duplicates().reset_index(drop=True)      # remove exact duplicates\n\n\n# ================================================================\n# 3. ROBUST NUMERIC COLUMN DETECTION\n# ================================================================\nnumeric_candidates = []\nconversion_stats = {}\n\nfor col in df.columns:\n    coerced = pd.to_numeric(df[col], errors=\"coerce\")\n    non_na_ratio = coerced.notna().sum() / len(coerced)\n    conversion_stats[col] = non_na_ratio\n\n    if non_na_ratio >= 0.80:  # at least 80% numeric-like\n        numeric_candidates.append(col)\n\nprint(f\"Detected {len(numeric_candidates)} numeric-like columns.\")\n\n\n# Convert numeric candidates to numeric dtype\nfor col in numeric_candidates:\n    df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n\n\n# ================================================================\n# 4. HANDLE INF + EXTREME VALUES\n# ================================================================\ninf_cols = [c for c in numeric_candidates if np.isinf(df[c].to_numpy()).any()]\nprint(\"Columns with ±inf:\", inf_cols)\n\nif inf_cols:\n    df[numeric_candidates] = df[numeric_candidates].replace([np.inf, -np.inf], np.nan)\n\nhuge_cols = []\nfor col in numeric_candidates:\n    try:\n        max_abs = np.nanmax(np.abs(df[col].to_numpy()))\n        if np.isfinite(max_abs) and max_abs > 1e300:\n            huge_cols.append((col, max_abs))\n    except:\n        pass\n\nprint(\"Columns with extremely large values (>1e300):\", huge_cols)\n\nCLIP_LIMIT = 1e300\ndf[numeric_candidates] = df[numeric_candidates].apply(\n    lambda s: s.clip(lower=-CLIP_LIMIT, upper=CLIP_LIMIT)\n)\n\n\n# ================================================================\n# 5. RECOMPUTE NUMERIC + CATEGORICAL COLUMNS\n# ================================================================\nnum_cols = df.select_dtypes(include=[\"float64\", \"int64\"]).columns.tolist()\ncat_cols = df.select_dtypes(include=[\"object\"]).columns.tolist()\n\nprint(\"Final numeric columns:\", len(num_cols))\nprint(\"Final categorical columns:\", len(cat_cols))\n\n\n# ================================================================\n# 6. HANDLE MISSING VALUES\n# ================================================================\nif len(num_cols) > 0:\n    df[num_cols] = df[num_cols].fillna(df[num_cols].median())\n\nfor col in cat_cols:\n    if df[col].isna().any():\n        mode_val = df[col].mode(dropna=True)\n        df[col] = df[col].fillna(mode_val.iloc[0] if len(mode_val) else \"\")\n\n\n# ================================================================\n# 7. LABEL-ENCODE CATEGORICAL COLUMNS\n# ================================================================\nle = LabelEncoder()\nfor col in cat_cols:\n    df[col] = le.fit_transform(df[col].astype(str))\n\n\n# ================================================================\n# 8. FINAL CHECK BEFORE SCALING\n# ================================================================\ndf[num_cols] = df[num_cols].replace([np.inf, -np.inf], np.nan)\ndf[num_cols] = df[num_cols].fillna(df[num_cols].median())\n\nfinite_check = {c: np.isfinite(df[c].to_numpy()).all() for c in num_cols}\nbad_cols = [c for c, ok in finite_check.items() if not ok]\nprint(\"Non-finite numeric columns (should be empty):\", bad_cols)\n\n\n# ================================================================\n# 9. MIN-MAX SCALING\n# ================================================================\nscaler = MinMaxScaler()\nif len(num_cols) > 0:\n    df[num_cols] = scaler.fit_transform(df[num_cols])\n\n\n# ================================================================\n# 10. SAVE CLEANED DATASET\n# ================================================================\noutput_path = \"/kaggle/working/weather_balanced_cleaned.csv\"\ndf.to_csv(output_path, index=False)\n\nprint(\"\\n✅ PREPROCESSING COMPLETE!\")\nprint(\"📁 Saved cleaned dataset as:\", output_path)\nprint(\"Final shape:\", df.shape)\n\n# Display label distribution (Yes/No)\nif \"RainTomorrow\" in df.columns:\n    print(\"\\nLabel distribution after cleaning:\")\n    print(df[\"RainTomorrow\"].value_counts())\nelse:\n    print(\"⚠️ No 'RainTomorrow' column found after preprocessing.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T13:53:59.073722Z","iopub.execute_input":"2025-12-05T13:53:59.074274Z","iopub.status.idle":"2025-12-05T13:54:01.364659Z","shell.execute_reply.started":"2025-12-05T13:53:59.074249Z","shell.execute_reply":"2025-12-05T13:54:01.363866Z"}},"outputs":[{"name":"stdout","text":"Initial shape: (63754, 23)\nInitial dtypes (sample):\n Date              object\nLocation          object\nMinTemp          float64\nMaxTemp          float64\nRainfall         float64\nEvaporation      float64\nSunshine         float64\nWindGustDir       object\nWindGustSpeed    float64\nWindDir9am        object\nWindDir3pm        object\nWindSpeed9am     float64\nWindSpeed3pm     float64\nHumidity9am      float64\nHumidity3pm      float64\nPressure9am      float64\nPressure3pm      float64\nCloud9am         float64\nCloud3pm         float64\nTemp9am          float64\ndtype: object\nDetected 12 numeric-like columns.\nColumns with ±inf: []\nColumns with extremely large values (>1e300): []\nFinal numeric columns: 16\nFinal categorical columns: 7\nNon-finite numeric columns (should be empty): []\n\n✅ PREPROCESSING COMPLETE!\n📁 Saved cleaned dataset as: /kaggle/working/weather_balanced_cleaned.csv\nFinal shape: (63754, 23)\n\nLabel distribution after cleaning:\nRainTomorrow\n1    31877\n0    31877\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"import pandas as pd\nfrom catboost import CatBoostClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\nimport pickle\n\n# -------------------------------------------------------------------\n# 1. LOAD CLEANED DATASET\n# -------------------------------------------------------------------\ninput_path = \"/kaggle/working/weather_balanced_cleaned.csv\"\ndf = pd.read_csv(input_path)\n\nprint(\"Loaded cleaned dataset:\", df.shape)\nprint(df.head())\n\n# -------------------------------------------------------------------\n# 2. CHECK TARGET COLUMN\n# -------------------------------------------------------------------\nTARGET = \"RainTomorrow\"\n\nif TARGET not in df.columns:\n    raise ValueError(f\"Target column '{TARGET}' not found. Available columns: {df.columns.tolist()}\")\n\ny = df[TARGET]\nX = df.drop(columns=[TARGET])\n\nprint(\"\\nFeatures shape:\", X.shape)\nprint(\"Label distribution:\\n\", y.value_counts())\n\n# -------------------------------------------------------------------\n# 3. TRAIN-TEST SPLIT\n# -------------------------------------------------------------------\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.25,\n    random_state=42,\n    stratify=y\n)\n\n# -------------------------------------------------------------------\n# 4. CATBOOST MODEL\n# -------------------------------------------------------------------\nmodel = CatBoostClassifier(\n    iterations=300,\n    learning_rate=0.05,\n    depth=8,\n    loss_function=\"Logloss\",\n    eval_metric=\"F1\",\n    verbose=50,\n    random_seed=42\n)\n\n# Train model\nmodel.fit(X_train, y_train, eval_set=(X_test, y_test))\n\n# -------------------------------------------------------------------\n# 5. PREDICT + EVALUATE\n# -------------------------------------------------------------------\ny_pred = model.predict(X_test)\n\nacc  = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred, zero_division=0)\nrec  = recall_score(y_test, y_pred, zero_division=0)\nf1   = f1_score(y_test, y_pred, zero_division=0)\ncm   = confusion_matrix(y_test, y_pred)\nreport = classification_report(y_test, y_pred, zero_division=0)\n\n# -------------------------------------------------------------------\n# 6. PRINT RESULTS\n# -------------------------------------------------------------------\nprint(\"\\n================ CATBOOST RESULTS ================\")\nprint(f\"Accuracy  : {acc:.4f}\")\nprint(f\"Precision : {prec:.4f}\")\nprint(f\"Recall    : {rec:.4f}\")\nprint(f\"F1 Score  : {f1:.4f}\")\n\nprint(\"\\nConfusion Matrix:\")\nprint(cm)\n\nprint(\"\\nClassification Report:\")\nprint(report)\n\n# -------------------------------------------------------------------\n# 7. SAVE MODEL\n# -------------------------------------------------------------------\nmodel_path = \"/kaggle/working/weather_catboost_model.pkl\"\n\nwith open(model_path, \"wb\") as f:\n    pickle.dump({\"model\": model, \"features\": X.columns.tolist()}, f)\n\nprint(\"\\n✅ Model saved successfully as:\", model_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T13:56:50.291821Z","iopub.execute_input":"2025-12-05T13:56:50.292333Z","iopub.status.idle":"2025-12-05T13:56:55.402905Z","shell.execute_reply.started":"2025-12-05T13:56:50.292309Z","shell.execute_reply":"2025-12-05T13:56:55.402073Z"}},"outputs":[{"name":"stdout","text":"Loaded cleaned dataset: (63754, 23)\n   Date  Location   MinTemp   MaxTemp  Rainfall  Evaporation  Sunshine  \\\n0  1986        13  0.785894  0.739300  0.038814     0.053793  0.482517   \n1  2358        32  0.564232  0.723735  0.000000     0.042759  0.811189   \n2  1573        32  0.352645  0.468872  0.000000     0.006897  0.636364   \n3   825         2  0.292191  0.424125  0.000539     0.030345  0.475524   \n4   919        20  0.365239  0.354086  0.000539     0.012414  0.405594   \n\n   WindGustDir  WindGustSpeed  WindDir9am  ...  Humidity9am  Humidity3pm  \\\n0            0       0.286822           1  ...     0.693878         0.51   \n1            0       0.511628           4  ...     0.438776         0.62   \n2            3       0.108527           5  ...     0.816327         0.45   \n3            9       0.069767           0  ...     0.989796         0.46   \n4           15       0.255814          13  ...     0.928571         0.43   \n\n   Pressure9am  Pressure3pm  Cloud9am  Cloud3pm   Temp9am   Temp3pm  \\\n0     0.567388     0.549755  0.777778  0.777778  0.763557  0.734375   \n1     0.567388     0.572594  0.333333  0.444444  0.672451  0.546875   \n2     0.785358     0.786297  0.111111  0.111111  0.364425  0.478516   \n3     0.677205     0.652529  0.888889  0.666667  0.251627  0.433594   \n4     0.633943     0.691680  0.777778  0.666667  0.316703  0.357422   \n\n   RainToday  RainTomorrow  \n0          1             1  \n1          0             1  \n2          0             0  \n3          0             0  \n4          0             0  \n\n[5 rows x 23 columns]\n\nFeatures shape: (63754, 22)\nLabel distribution:\n RainTomorrow\n1    31877\n0    31877\nName: count, dtype: int64\n0:\tlearn: 0.7589001\ttest: 0.7631660\tbest: 0.7631660 (0)\ttotal: 18.8ms\tremaining: 5.63s\n50:\tlearn: 0.7845330\ttest: 0.7871527\tbest: 0.7871527 (50)\ttotal: 818ms\tremaining: 3.99s\n100:\tlearn: 0.7971957\ttest: 0.7961165\tbest: 0.7961165 (100)\ttotal: 1.6s\tremaining: 3.16s\n150:\tlearn: 0.8060036\ttest: 0.8010661\tbest: 0.8016503 (145)\ttotal: 2.38s\tremaining: 2.35s\n200:\tlearn: 0.8130476\ttest: 0.8029197\tbest: 0.8039876 (192)\ttotal: 3.17s\tremaining: 1.56s\n250:\tlearn: 0.8213887\ttest: 0.8037975\tbest: 0.8044332 (243)\ttotal: 3.97s\tremaining: 776ms\n299:\tlearn: 0.8290123\ttest: 0.8060457\tbest: 0.8060702 (297)\ttotal: 4.75s\tremaining: 0us\n\nbestTest = 0.8060701865\nbestIteration = 297\n\nShrink model to first 298 iterations.\n\n================ CATBOOST RESULTS ================\nAccuracy  : 0.8076\nPrecision : 0.8124\nRecall    : 0.7998\nF1 Score  : 0.8061\n\nConfusion Matrix:\n[[6498 1472]\n [1595 6374]]\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.80      0.82      0.81      7970\n           1       0.81      0.80      0.81      7969\n\n    accuracy                           0.81     15939\n   macro avg       0.81      0.81      0.81     15939\nweighted avg       0.81      0.81      0.81     15939\n\n\n✅ Model saved successfully as: /kaggle/working/weather_catboost_model.pkl\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# intersection_hlo_with_hillclimb_fast.py\n# Pipeline (reduced budget + hill-climb) with UNION, INTERSECTION, and VOTING candidate flows:\n#  PSO + GA + GWO (CatBoost fitness, lighter during opt) -> derive UNION / INTERSECTION / VOTING\n#  For each candidate set: HLO (on candidates) -> Greedy hill-climb (restricted) -> Final CatBoost eval (5-fold CV)\n#  Additionally: train a CatBoost model on 80% of the data and evaluate on the held-out 20% test set\n#  Train & save a CatBoost model for each flow (union / intersection / voting) using the 80/20 split.\n# Prints logs, mean ± std for metrics, stage timings, saves results and models.\n\nimport time\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport warnings\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, make_scorer\nfrom sklearn.base import clone\n\nwarnings.filterwarnings(\"ignore\")\nnp.random.seed(42)\n\n# -------------------- USER / EXPERIMENT SETTINGS --------------------\n# If you prefer to load CSV instead, uncomment and change:\ndf = pd.read_csv(\"/kaggle/working/weather_balanced_cleaned.csv\")\n\nTARGET_COL = \"RainTomorrow\"   # target column\nMODEL_VERBOSE = 0            # CatBoost verbosity: 0 = silent\nRANDOM_STATE = 42\n\n# ---------- Reduced budgets for faster runs (you can tune these) ----------\nPSO_SWARM = 15   # reduced swarm\nPSO_ITERS = 5   # reduced iterations\n\nGA_POP = 30      # reduced population\nGA_GENS = 5     # reduced generations\n\nGWO_WOLVES = 10\nGWO_ITERS = 5\n\nHLO_POP = 15\nHLO_ITERS = 5\nHLO_TEACHER_FACTOR = 0.75\nHLO_MUTATION = 0.12\n\n# Greedy hill-climb after HLO\nHILLCLIMB_MAX_STEPS = 100   # stop if no improvement or step limit\nHILLCLIMB_EVAL_CAP = 500    # safety cap on evaluations (prevent runaway)\n\n# CV folds\nCV_OPT = 2    # cheaper CV during optimization + HLO (speed)\nCV_FINAL = 5  # final evaluation (A1 requested)\n\n# CatBoost iterations\nCB_ITER_OPT = 100    # iterations during optimization (smaller)\nCB_ITER_HLO = 200\nCB_ITER_FINAL = 500  # final evaluation iterations (bigger)\n\n# Train/test split for final saved models\nFINAL_TEST_SIZE = 0.2\n\nSAVE_PREFIX = \"hybrid_hlo_models\"\n# ------------------------------------------------------------------------\n\n# Ensure df exists\ntry:\n    df\nexcept NameError:\n    raise RuntimeError(\"DataFrame `df` not found. Assign your dataset to variable `df` or load at top.\")\n\n# Prepare data\nX = df.drop(TARGET_COL, axis=1)\n\ny = df[TARGET_COL].astype(int)\nFEATURE_NAMES = X.columns.tolist()\nN_FEATURES = X.shape[1]\n\n# -------------------- Model factory (CatBoost) --------------------\ndef get_catboost_model(iterations=100):\n    try:\n        from catboost import CatBoostClassifier\n    except Exception as e:\n        raise ImportError(\"catboost not installed. Install with: pip install catboost\") from e\n    return CatBoostClassifier(iterations=iterations, learning_rate=0.05, depth=6,\n                              verbose=MODEL_VERBOSE, random_seed=RANDOM_STATE, thread_count=-1)\n\n# -------------------- Fitness cache --------------------\n# key: tuple(selected original indices) -> float score\nfitness_cache = {}\n\ndef key_from_mask(mask_bool):\n    return tuple(sorted(np.where(np.array(mask_bool).astype(bool))[0].tolist()))\n\ndef evaluate_mask_global(mask_bool, cv=CV_OPT, cb_iter=CB_ITER_OPT):\n    \"\"\"\n    Evaluate mask using CatBoost with CV and return average of acc,prec,rec,f1.\n    Caches results to avoid re-evaluating identical subsets.\n    \"\"\"\n    key = key_from_mask(mask_bool)\n    if key in fitness_cache:\n        return fitness_cache[key]\n    if len(key) == 0:\n        fitness_cache[key] = 0.0\n        return 0.0\n\n    X_sel = X.iloc[:, list(key)]\n    model = get_catboost_model(iterations=cb_iter)\n    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=RANDOM_STATE)\n\n    accs = cross_val_score(clone(model), X_sel, y, cv=skf, scoring=\"accuracy\", n_jobs=-1)\n    precs = cross_val_score(clone(model), X_sel, y, cv=skf, scoring=make_scorer(precision_score, zero_division=0), n_jobs=-1)\n    recs = cross_val_score(clone(model), X_sel, y, cv=skf, scoring=make_scorer(recall_score, zero_division=0), n_jobs=-1)\n    f1s = cross_val_score(clone(model), X_sel, y, cv=skf, scoring=make_scorer(f1_score, zero_division=0), n_jobs=-1)\n\n    score = float((np.mean(accs) + np.mean(precs) + np.mean(recs) + np.mean(f1s)) / 4.0)\n    fitness_cache[key] = score\n    return score\n\n# -------------------- Helpers --------------------\ndef mask_to_features(mask):\n    idxs = np.where(np.array(mask).astype(bool))[0].tolist()\n    return [FEATURE_NAMES[i] for i in idxs]\n\ndef log(msg):\n    print(f\"[{time.strftime('%H:%M:%S')}] {msg}\", flush=True)\n\n# -------------------- PSO (binary) --------------------\ndef run_pso(swarm_size=PSO_SWARM, iters=PSO_ITERS, cv=CV_OPT):\n    log(f\"PSO START (swarm={swarm_size}, iters={iters}, cv={cv})\")\n    t0 = time.time()\n    dim = N_FEATURES\n    pos = np.random.randint(0,2,(swarm_size,dim)).astype(int)\n    vel = np.random.uniform(-1,1,(swarm_size,dim))\n\n    pbest = pos.copy()\n    pbest_scores = np.array([evaluate_mask_global(p.astype(bool), cv=cv, cb_iter=CB_ITER_OPT) for p in pos])\n\n    gbest_idx = int(np.argmax(pbest_scores))\n    gbest = pbest[gbest_idx].copy()\n    gbest_score = pbest_scores[gbest_idx]\n\n    w = 0.6; c1 = c2 = 1.5\n    for t in range(iters):\n        log(f\" PSO iter {t+1}/{iters} best_global={gbest_score:.4f}\")\n        for i in range(swarm_size):\n            r1 = np.random.rand(dim); r2 = np.random.rand(dim)\n            vel[i] = w*vel[i] + c1*r1*(pbest[i] - pos[i]) + c2*r2*(gbest - pos[i])\n            s = 1.0 / (1.0 + np.exp(-vel[i]))\n            pos[i] = (np.random.rand(dim) < s).astype(int)\n\n            sc = evaluate_mask_global(pos[i].astype(bool), cv=cv, cb_iter=CB_ITER_OPT)\n            if sc > pbest_scores[i]:\n                pbest[i] = pos[i].copy()\n                pbest_scores[i] = sc\n            if sc > gbest_score:\n                gbest = pos[i].copy()\n                gbest_score = sc\n        w = max(0.2, w*0.97)\n\n    best_idx = int(np.argmax(pbest_scores))\n    best_mask = pbest[best_idx].copy()\n    best_score = pbest_scores[best_idx]\n    t1 = time.time()\n    log(f\"PSO DONE in {int(t1-t0)}s best_score={best_score:.4f} selected={int(np.sum(best_mask))}\")\n    log(f\"PSO SELECTED FEATURES: {mask_to_features(best_mask)}\")\n\n    return best_mask, best_score, int(t1-t0)\n\n# -------------------- GA (binary) --------------------\ndef run_ga(pop_size=GA_POP, gens=GA_GENS, cv=CV_OPT):\n    log(f\"GA START (pop={pop_size}, gens={gens}, cv={cv})\")\n    t0 = time.time()\n    dim = N_FEATURES\n    pop = np.random.randint(0,2,(pop_size, dim)).astype(int)\n    fitness_scores = np.array([evaluate_mask_global(ind.astype(bool), cv=cv, cb_iter=CB_ITER_OPT) for ind in pop])\n\n    def tournament_select(k=3):\n        idxs = np.random.randint(0, pop_size, k)\n        return idxs[np.argmax(fitness_scores[idxs])]\n\n    for g in range(gens):\n        log(f\" GA gen {g+1}/{gens} current_best={np.max(fitness_scores):.4f}\")\n        new_pop = []\n        # elitism\n        elite_idxs = np.argsort(fitness_scores)[-2:]\n        new_pop.extend(pop[elite_idxs].tolist())\n\n        while len(new_pop) < pop_size:\n            i1 = tournament_select(); i2 = tournament_select()\n            p1 = pop[i1].copy(); p2 = pop[i2].copy()\n            # crossover\n            if np.random.rand() < 0.7:\n                pt = np.random.randint(1, dim)\n                c1 = np.concatenate([p1[:pt], p2[pt:]])\n                c2 = np.concatenate([p2[:pt], p1[pt:]])\n            else:\n                c1, c2 = p1, p2\n            # mutation\n            for child in (c1, c2):\n                for d in range(dim):\n                    if np.random.rand() < 0.1:\n                        child[d] = 1 - child[d]\n                new_pop.append(child)\n                if len(new_pop) >= pop_size:\n                    break\n        pop = np.array(new_pop[:pop_size])\n        fitness_scores = np.array([evaluate_mask_global(ind.astype(bool), cv=cv, cb_iter=CB_ITER_OPT) for ind in pop])\n\n    best_idx = int(np.argmax(fitness_scores))\n    best_mask = pop[best_idx].copy()\n    best_score = fitness_scores[best_idx]\n    t1 = time.time()\n    log(f\"GA DONE in {int(t1-t0)}s best_score={best_score:.4f} selected={int(np.sum(best_mask))}\")\n    log(f\"GA SELECTED FEATURES: {mask_to_features(best_mask)}\")\n\n    return best_mask, best_score, int(t1-t0)\n\n# -------------------- GWO (binary) --------------------\ndef run_gwo(wolves=GWO_WOLVES, iters=GWO_ITERS, cv=CV_OPT):\n    log(f\"GWO START (wolves={wolves}, iters={iters}, cv={cv})\")\n    t0 = time.time()\n    dim = N_FEATURES\n    pop = np.random.randint(0,2,(wolves, dim)).astype(int)\n    fitness_scores = np.array([evaluate_mask_global(ind.astype(bool), cv=cv, cb_iter=CB_ITER_OPT) for ind in pop])\n\n    Alpha = Beta = Delta = None\n    Alpha_score = Beta_score = Delta_score = -1.0\n\n    for itr in range(iters):\n        log(f\" GWO iter {itr+1}/{iters} best_alpha={Alpha_score:.4f}\")\n        for i in range(wolves):\n            sc = fitness_scores[i]\n            if sc > Alpha_score:\n                Delta_score, Beta_score, Alpha_score = Beta_score, Alpha_score, sc\n                Delta, Beta, Alpha = Beta, Alpha, pop[i].copy()\n            elif sc > Beta_score:\n                Delta_score, Beta_score = Beta_score, sc\n                Delta, Beta = Beta, pop[i].copy()\n            elif sc > Delta_score:\n                Delta_score = sc\n                Delta = pop[i].copy()\n\n        a = 2 - itr * (2.0 / iters)\n        for i in range(wolves):\n            for d in range(dim):\n                if Alpha is None:\n                    continue\n                r1, r2 = np.random.rand(), np.random.rand()\n                A1 = 2 * a * r1 - a; C1 = 2 * r2\n                D_alpha = abs(C1 * Alpha[d] - pop[i][d])\n                X1 = Alpha[d] - A1 * D_alpha\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                A2 = 2 * a * r1 - a; C2 = 2 * r2\n                D_beta = abs(C2 * Beta[d] - pop[i][d])\n                X2 = Beta[d] - A2 * D_beta\n\n                r1, r2 = np.random.rand(), np.random.rand()\n                A3 = 2 * a * r1 - a; C3 = 2 * r2\n                D_delta = abs(C3 * Delta[d] - pop[i][d])\n                X3 = Delta[d] - A3 * D_delta\n\n                new_pos = (X1 + X2 + X3) / 3.0\n                s = 1.0 / (1.0 + np.exp(-new_pos))\n                pop[i][d] = 1 if np.random.rand() < s else 0\n\n        fitness_scores = np.array([evaluate_mask_global(ind.astype(bool), cv=cv, cb_iter=CB_ITER_OPT) for ind in pop])\n\n    best_idx = int(np.argmax(fitness_scores))\n    best_mask = pop[best_idx].copy()\n    best_score = fitness_scores[best_idx]\n    t1 = time.time()\n    log(f\"GWO DONE in {int(t1-t0)}s best_score={best_score:.4f} selected={int(np.sum(best_mask))}\")\n    log(f\"GWO SELECTED FEATURES: {mask_to_features(best_mask)}\")\n\n    return best_mask, best_score, int(t1-t0)\n\n# -------------------- INTERSECTION / UNION / VOTING --------------------\ndef get_intersection_mask(*masks):\n    \"\"\"Return mask that contains only features present in ALL provided masks.\"\"\"\n    if len(masks) == 0:\n        return np.zeros(N_FEATURES, dtype=int)\n    inter_idx = set(np.where(np.array(masks[0]).astype(bool))[0].tolist())\n    for m in masks[1:]:\n        idxs = set(np.where(np.array(m).astype(bool))[0].tolist())\n        inter_idx = inter_idx.intersection(idxs)\n    mask = np.zeros(N_FEATURES, dtype=int)\n    for i in inter_idx:\n        mask[i] = 1\n    return mask\n\n\ndef get_union_mask(*masks):\n    union_idx = set()\n    for m in masks:\n        idxs = np.where(np.array(m).astype(bool))[0].tolist()\n        union_idx.update(idxs)\n    mask = np.zeros(N_FEATURES, dtype=int)\n    for i in union_idx:\n        mask[i] = 1\n    return mask\n\n\ndef get_voting_mask(*masks, threshold=2):\n    \"\"\"Return mask of features selected by at least `threshold` methods (default majority of 3 => 2).\"\"\"\n    if len(masks) == 0:\n        return np.zeros(N_FEATURES, dtype=int)\n    counts = np.zeros(N_FEATURES, dtype=int)\n    for m in masks:\n        counts += np.array(m).astype(int)\n    mask = (counts >= threshold).astype(int)\n    return mask\n\n# -------------------- HLO on candidates --------------------\ndef hlo_on_candidates(candidate_mask, pop_size=HLO_POP, iters=HLO_ITERS, cv=CV_OPT):\n    candidate_indices = np.where(np.array(candidate_mask).astype(bool))[0].tolist()\n    k = len(candidate_indices)\n    if k == 0:\n        raise ValueError(\"Candidate set is empty.\")\n\n    log(f\"HLO START on {k} candidate features (pop={pop_size}, iters={iters})\")\n    t0 = time.time()\n\n    pop = np.random.randint(0,2,(pop_size, k)).astype(int)\n\n    def fitness_candidate(bitmask):\n        full_mask = np.zeros(N_FEATURES, dtype=int)\n        for j,bit in enumerate(bitmask):\n            if bit == 1:\n                full_mask[candidate_indices[j]] = 1\n        return evaluate_mask_global(full_mask.astype(bool), cv=cv, cb_iter=CB_ITER_HLO)\n\n    fitness_scores = np.array([fitness_candidate(ind) for ind in pop])\n    best_idx = int(np.argmax(fitness_scores))\n    best_solution = pop[best_idx].copy()\n    best_score = fitness_scores[best_idx]\n\n    for it in range(iters):\n        log(f\" HLO iter {it+1}/{iters} current_best={best_score:.4f}\")\n        teacher = pop[int(np.argmax(fitness_scores))].copy()\n        new_pop = []\n        for i in range(pop_size):\n            learner = pop[i].copy()\n            # teaching phase\n            for d in range(k):\n                if np.random.rand() < HLO_TEACHER_FACTOR:\n                    learner[d] = teacher[d]\n            # peer learning\n            partner = pop[np.random.randint(pop_size)].copy()\n            for d in range(k):\n                if learner[d] != partner[d] and np.random.rand() < 0.5:\n                    learner[d] = partner[d]\n            # mutation\n            for d in range(k):\n                if np.random.rand() < HLO_MUTATION:\n                    learner[d] = 1 - learner[d]\n            new_pop.append(learner)\n        pop = np.array(new_pop)\n        fitness_scores = np.array([fitness_candidate(ind) for ind in pop])\n        gen_best_idx = int(np.argmax(fitness_scores))\n        gen_best_score = fitness_scores[gen_best_idx]\n        gen_best_sol = pop[gen_best_idx].copy()\n        if gen_best_score > best_score:\n            best_score = gen_best_score\n            best_solution = gen_best_sol.copy()\n\n    # map back to full mask\n    final_full_mask = np.zeros(N_FEATURES, dtype=int)\n    for j,bit in enumerate(best_solution):\n        if bit == 1:\n            final_full_mask[candidate_indices[j]] = 1\n\n    t1 = time.time()\n    log(f\"HLO DONE in {int(t1-t0)}s best_score={best_score:.4f} final_selected={int(np.sum(final_full_mask))}\")\n    return final_full_mask, best_score, int(t1-t0)\n\n# -------------------- Greedy Hill-Climb (local search) --------------------\ndef hill_climb_on_candidates(initial_mask, candidate_mask, max_steps=HILLCLIMB_MAX_STEPS, eval_cap=HILLCLIMB_EVAL_CAP, cv=CV_OPT):\n    \"\"\"\n    Greedy single-bit flip hill-climb restricted to candidate indices.\n    Starts from initial_mask (full-length). Tries flipping each candidate feature's bit:\n    - If flip improves fitness, accept and restart scanning.\n    - Stops when no improving flip found or max_steps/eval_cap reached.\n    \"\"\"\n    candidate_indices = np.where(np.array(candidate_mask).astype(bool))[0].tolist()\n    if len(candidate_indices) == 0:\n        log(\"Hill-climb: candidate set empty, skipping.\")\n        return initial_mask, 0.0, 0\n\n    log(f\"Hill-climb START over {len(candidate_indices)} candidates (max_steps={max_steps}, eval_cap={eval_cap})\")\n    t0 = time.time()\n    current_mask = initial_mask.copy()\n    current_score = evaluate_mask_global(current_mask.astype(bool), cv=cv, cb_iter=CB_ITER_HLO)\n    evals = 0\n    steps = 0\n    improved = True\n\n    while improved and steps < max_steps and evals < eval_cap:\n        improved = False\n        for idx in np.random.permutation(candidate_indices):\n            trial_mask = current_mask.copy()\n            trial_mask[idx] = 1 - trial_mask[idx]  # flip\n            trial_score = evaluate_mask_global(trial_mask.astype(bool), cv=cv, cb_iter=CB_ITER_HLO)\n            evals += 1\n            if trial_score > current_score + 1e-8:\n                current_mask = trial_mask\n                current_score = trial_score\n                improved = True\n                steps += 1\n                log(f\" Hill-climb step {steps}: flipped {FEATURE_NAMES[idx]} -> new_score={current_score:.4f} (evals={evals})\")\n                break\n            if evals >= eval_cap or steps >= max_steps:\n                break\n    t1 = time.time()\n    log(f\"Hill-climb DONE in {int(t1-t0)}s steps={steps} evals={evals} final_score={current_score:.4f} selected={int(np.sum(current_mask))}\")\n    return current_mask, current_score, int(t1-t0)\n\n# -------------------- Final evaluation (5-fold CV) --------------------\ndef final_evaluation(mask_bool, cv=CV_FINAL, cb_iter=CB_ITER_FINAL):\n    idxs = np.where(np.array(mask_bool).astype(bool))[0].tolist()\n    if len(idxs) == 0:\n        raise ValueError(\"Final mask selects zero features.\")\n    X_sel = X.iloc[:, idxs]\n    model = get_catboost_model(iterations=cb_iter)\n    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=RANDOM_STATE)\n    accs = []; precs = []; recs = []; f1s = []\n    t0 = time.time()\n    for tr,te in skf.split(X_sel, y):\n        m = clone(model); m.fit(X_sel.iloc[tr], y.iloc[tr])\n        pred = m.predict(X_sel.iloc[te])\n        accs.append(accuracy_score(y.iloc[te], pred))\n        precs.append(precision_score(y.iloc[te], pred, zero_division=0))\n        recs.append(recall_score(y.iloc[te], pred, zero_division=0))\n        f1s.append(f1_score(y.iloc[te], pred, zero_division=0))\n    t1 = time.time()\n    results = {\n        \"n_features\": len(idxs),\n        \"features\": [FEATURE_NAMES[i] for i in idxs],\n        \"acc_mean\": float(np.mean(accs)), \"acc_std\": float(np.std(accs)),\n        \"prec_mean\": float(np.mean(precs)), \"prec_std\": float(np.std(precs)),\n        \"rec_mean\": float(np.mean(recs)), \"rec_std\": float(np.std(recs)),\n        \"f1_mean\": float(np.mean(f1s)), \"f1_std\": float(np.std(f1s)),\n        \"eval_time_s\": int(t1 - t0)\n    }\n    return results\n\n# -------------------- MAIN PIPELINE --------------------\nif __name__ == \"__main__\":\n    total_t0 = time.time()\n    log(\"===== HYBRID (reduced budget) + HLO + HILL-CLIMB (UNION/INTERSECTION/VOTING) START =====\")\n\n    # PSO\n    pso_mask, pso_score, pso_time = run_pso(swarm_size=PSO_SWARM, iters=PSO_ITERS, cv=CV_OPT)\n\n    # GA\n    ga_mask, ga_score, ga_time = run_ga(pop_size=GA_POP, gens=GA_GENS, cv=CV_OPT)\n\n    # GWO\n    gwo_mask, gwo_score, gwo_time = run_gwo(wolves=GWO_WOLVES, iters=GWO_ITERS, cv=CV_OPT)\n\n    # Derive candidate masks\n    union_mask = get_union_mask(pso_mask, ga_mask, gwo_mask)\n    inter_mask = get_intersection_mask(pso_mask, ga_mask, gwo_mask)\n    vote_mask = get_voting_mask(pso_mask, ga_mask, gwo_mask, threshold=2)\n\n    candidate_sets = {\n        'union': union_mask,\n        'intersection': inter_mask,\n        'voting': vote_mask\n    }\n\n    results_all = {}\n\n    # run HLO -> hill-climb -> final evaluation -> train & save model for each candidate set\n    for name, cand_mask in candidate_sets.items():\n        log(f\"===== PROCESSING {name.upper()} CANDIDATES =====\")\n        n_cand = int(np.sum(cand_mask))\n        log(f\"{name.upper()} candidate features: {n_cand}\")\n        if n_cand == 0:\n            log(f\"{name.upper()} empty — skipping HLO/hill-climb and model training.\")\n            results_all[name] = {'skipped': True, 'n_candidates': 0}\n            continue\n\n        # HLO on this candidate set\n        hlo_mask, hlo_score, hlo_time = hlo_on_candidates(cand_mask, pop_size=HLO_POP, iters=HLO_ITERS, cv=CV_OPT)\n\n        # hill-climb restricted to candidate set\n        hc_mask, hc_score, hc_time = hill_climb_on_candidates(hlo_mask, cand_mask, max_steps=HILLCLIMB_MAX_STEPS, eval_cap=HILLCLIMB_EVAL_CAP, cv=CV_OPT)\n\n        # final CV evaluation\n        final_res = final_evaluation(hc_mask, cv=CV_FINAL, cb_iter=CB_ITER_FINAL)\n\n        # Train final CatBoost model on 80% train and evaluate on 20% test (stratified)\n        sel_idxs = np.where(np.array(hc_mask).astype(bool))[0].tolist()\n        sel_features = [FEATURE_NAMES[i] for i in sel_idxs]\n\n        if len(sel_features) == 0:\n            log(f\"No features selected after hill-climb for {name}, skipping model train.\")\n            results_all[name] = {'skipped': True, 'n_candidates': n_cand}\n            continue\n\n        X_sel = X[sel_features]\n        X_train, X_test, y_train, y_test = train_test_split(X_sel, y, test_size=FINAL_TEST_SIZE, stratify=y, random_state=RANDOM_STATE)\n\n        model = get_catboost_model(iterations=CB_ITER_FINAL)\n        model.fit(X_train, y_train)\n\n        # evaluate on held-out test set (20%)\n        y_pred = model.predict(X_test)\n        test_acc = accuracy_score(y_test, y_pred)\n        test_prec = precision_score(y_test, y_pred, zero_division=0)\n        test_rec = recall_score(y_test, y_pred, zero_division=0)\n        test_f1 = f1_score(y_test, y_pred, zero_division=0)\n\n        test_metrics = {\n            'acc': float(test_acc), 'prec': float(test_prec), 'rec': float(test_rec), 'f1': float(test_f1),\n            'n_test': int(X_test.shape[0])\n        }\n\n        # Save model to file (pickle)\n        model_filename = f\"{SAVE_PREFIX}_{name}_model.pkl\"\n        with open(model_filename, 'wb') as mf:\n            pickle.dump(model, mf)\n\n        # store results\n        results_all[name] = {\n            'n_candidates': n_cand,\n            'hlo_score': float(hlo_score), 'hlo_time': int(hlo_time),\n            'hc_score': float(hc_score), 'hc_time': int(hc_time),\n            'final_eval': final_res,\n            'selected_features': sel_features,\n            'model_file': model_filename,\n            'test_metrics': test_metrics\n        }\n\n        log(f\"Saved trained CatBoost model for {name} -> {model_filename} (test_f1={test_f1:.4f})\")\n\n    total_t1 = time.time()\n    elapsed_total = int(total_t1 - total_t0)\n\n    # Summary / save aggregated results\n    print(\"==================== AGGREGATE SUMMARY ====================\")\n    print(f\"PSO  -> opt_score={pso_score:.4f} selected={int(np.sum(pso_mask))} time={pso_time}s\")\n    print(f\"GA   -> opt_score={ga_score:.4f} selected={int(np.sum(ga_mask))} time={ga_time}s\")\n    print(f\"GWO  -> opt_score={gwo_score:.4f} selected={int(np.sum(gwo_mask))} time={gwo_time}s\")\n    print(f\"Union candidates    : {int(np.sum(union_mask))}\")\n    print(f\"Intersection candidates: {int(np.sum(inter_mask))}\")\n    print(f\"Voting candidates   : {int(np.sum(vote_mask))}\")\n    print(\"-------------------------------------------------\")\n\n    for name, info in results_all.items():\n        print(f\"-- {name.upper()} SUMMARY --\")\n        if info.get('skipped'):\n            print(\" skipped (no candidates)\")\n            continue\n        fe = info['final_eval']\n        tm = info['test_metrics']\n        print(f\" Selected ({fe['n_features']}): {fe['features']}\")\n        print(f\" CV F1   : {fe['f1_mean']:.4f} ± {fe['f1_std']:.4f}\")\n        print(f\" Test F1 : {tm['f1']:.4f} (n_test={tm['n_test']})\")\n        print(f\" Accuracy : {fe['acc_mean']:.4f} ± {fe['acc_std']:.4f}\")\n        print(f\" Precision: {fe['prec_mean']:.4f} ± {fe['prec_std']:.4f}\")\n        print(f\" Recall   : {fe['rec_mean']:.4f} ± {fe['rec_std']:.4f}\")\n        print(f\" Model file: {info['model_file']}\")\n\n\n\n    # Save aggregated pipeline outputs\n    out = {\n        \"pso_mask\": pso_mask, \"pso_score\": pso_score, \"pso_time\": pso_time,\n        \"ga_mask\": ga_mask, \"ga_score\": ga_score, \"ga_time\": ga_time,\n        \"gwo_mask\": gwo_mask, \"gwo_score\": gwo_score, \"gwo_time\": gwo_time,\n        \"union_mask\": union_mask, \"intersection_mask\": inter_mask, \"voting_mask\": vote_mask,\n        \"results_all\": results_all,\n        \"fitness_cache_len\": len(fitness_cache)\n    }\n    with open(f\"{SAVE_PREFIX}_results.pkl\", \"wb\") as f:\n        pickle.dump(out, f)\n\n    log(f\"Saved results to {SAVE_PREFIX}_results.pkl\")\n    log(\"===== PIPELINE COMPLETE =====\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T09:31:51.254774Z","iopub.execute_input":"2025-12-05T09:31:51.255069Z","iopub.status.idle":"2025-12-05T11:03:00.827370Z","shell.execute_reply.started":"2025-12-05T09:31:51.255046Z","shell.execute_reply":"2025-12-05T11:03:00.826809Z"}},"outputs":[{"name":"stdout","text":"[09:31:51] ===== HYBRID (reduced budget) + HLO + HILL-CLIMB (UNION/INTERSECTION/VOTING) START =====\n[09:31:51] PSO START (swarm=15, iters=5, cv=2)\n[09:32:55]  PSO iter 1/5 best_global=0.7831\n[09:33:57]  PSO iter 2/5 best_global=0.7831\n[09:35:03]  PSO iter 3/5 best_global=0.7831\n[09:36:06]  PSO iter 4/5 best_global=0.7831\n[09:37:11]  PSO iter 5/5 best_global=0.7831\n[09:38:16] PSO DONE in 384s best_score=0.7831 selected=19\n[09:38:16] PSO SELECTED FEATURES: ['Date', 'Location', 'MinTemp', 'MaxTemp', 'Rainfall', 'Evaporation', 'Sunshine', 'WindGustDir', 'WindGustSpeed', 'WindDir9am', 'WindDir3pm', 'Humidity9am', 'Humidity3pm', 'Pressure9am', 'Pressure3pm', 'Cloud9am', 'Cloud3pm', 'Temp9am', 'Temp3pm']\n[09:38:16] GA START (pop=30, gens=5, cv=2)\n[09:40:17]  GA gen 1/5 current_best=0.7818\n[09:41:58]  GA gen 2/5 current_best=0.7819\n[09:43:46]  GA gen 3/5 current_best=0.7819\n[09:45:19]  GA gen 4/5 current_best=0.7821\n[09:47:12]  GA gen 5/5 current_best=0.7821\n[09:49:02] GA DONE in 645s best_score=0.7823 selected=14\n[09:49:02] GA SELECTED FEATURES: ['MinTemp', 'MaxTemp', 'Rainfall', 'Sunshine', 'WindGustDir', 'WindGustSpeed', 'WindDir9am', 'WindDir3pm', 'Humidity9am', 'Humidity3pm', 'Pressure3pm', 'Cloud9am', 'Cloud3pm', 'Temp3pm']\n[09:49:02] GWO START (wolves=10, iters=5, cv=2)\n[09:49:43]  GWO iter 1/5 best_alpha=-1.0000\n[09:50:28]  GWO iter 2/5 best_alpha=0.7751\n[09:51:13]  GWO iter 3/5 best_alpha=0.7792\n[09:51:59]  GWO iter 4/5 best_alpha=0.7822\n[09:52:43]  GWO iter 5/5 best_alpha=0.7825\n[09:53:27] GWO DONE in 265s best_score=0.7807 selected=14\n[09:53:27] GWO SELECTED FEATURES: ['Location', 'MinTemp', 'Rainfall', 'Evaporation', 'Sunshine', 'WindGustDir', 'WindGustSpeed', 'WindDir3pm', 'Humidity9am', 'Humidity3pm', 'Pressure9am', 'Pressure3pm', 'Cloud9am', 'Temp3pm']\n[09:53:27] ===== PROCESSING UNION CANDIDATES =====\n[09:53:27] UNION candidate features: 19\n[09:53:27] HLO START on 19 candidate features (pop=15, iters=5)\n[09:56:36]  HLO iter 1/5 current_best=0.7898\n[09:59:49]  HLO iter 2/5 current_best=0.7907\n[10:03:04]  HLO iter 3/5 current_best=0.7907\n[10:06:18]  HLO iter 4/5 current_best=0.7907\n[10:09:05]  HLO iter 5/5 current_best=0.7908\n[10:12:20] HLO DONE in 1132s best_score=0.7916 final_selected=13\n[10:12:20] Hill-climb START over 19 candidates (max_steps=100, eval_cap=500)\n[10:12:33]  Hill-climb step 1: flipped Temp3pm -> new_score=0.7917 (evals=1)\n[10:13:01]  Hill-climb step 2: flipped Cloud9am -> new_score=0.7922 (evals=3)\n[10:14:34]  Hill-climb step 3: flipped Humidity9am -> new_score=0.7925 (evals=10)\n[10:18:35] Hill-climb DONE in 375s steps=3 evals=29 final_score=0.7925 selected=14\n[10:19:01] Saved trained CatBoost model for union -> hybrid_hlo_models_union_model.pkl (test_f1=0.8048)\n[10:19:01] ===== PROCESSING INTERSECTION CANDIDATES =====\n[10:19:01] INTERSECTION candidate features: 11\n[10:19:01] HLO START on 11 candidate features (pop=15, iters=5)\n[10:21:45]  HLO iter 1/5 current_best=0.7783\n[10:24:28]  HLO iter 2/5 current_best=0.7829\n[10:26:55]  HLO iter 3/5 current_best=0.7829\n[10:29:51]  HLO iter 4/5 current_best=0.7844\n[10:32:35]  HLO iter 5/5 current_best=0.7859\n[10:35:22] HLO DONE in 980s best_score=0.7864 final_selected=11\n[10:35:22] Hill-climb START over 11 candidates (max_steps=100, eval_cap=500)\n[10:36:49]  Hill-climb step 1: flipped Humidity9am -> new_score=0.7874 (evals=7)\n[10:38:29] Hill-climb DONE in 186s steps=1 evals=18 final_score=0.7874 selected=10\n[10:38:53] Saved trained CatBoost model for intersection -> hybrid_hlo_models_intersection_model.pkl (test_f1=0.7963)\n[10:38:53] ===== PROCESSING VOTING CANDIDATES =====\n[10:38:53] VOTING candidate features: 17\n[10:38:53] HLO START on 17 candidate features (pop=15, iters=5)\n[10:41:53]  HLO iter 1/5 current_best=0.7893\n[10:45:00]  HLO iter 2/5 current_best=0.7893\n[10:48:04]  HLO iter 3/5 current_best=0.7893\n[10:51:12]  HLO iter 4/5 current_best=0.7893\n[10:54:18]  HLO iter 5/5 current_best=0.7904\n[10:57:27] HLO DONE in 1113s best_score=0.7904 final_selected=14\n[10:57:27] Hill-climb START over 17 candidates (max_steps=100, eval_cap=500)\n[10:58:19]  Hill-climb step 1: flipped Rainfall -> new_score=0.7916 (evals=4)\n[10:58:59]  Hill-climb step 2: flipped Humidity9am -> new_score=0.7923 (evals=7)\n[11:02:34] Hill-climb DONE in 307s steps=2 evals=24 final_score=0.7923 selected=16\n[11:03:00] Saved trained CatBoost model for voting -> hybrid_hlo_models_voting_model.pkl (test_f1=0.8039)\n==================== AGGREGATE SUMMARY ====================\nPSO  -> opt_score=0.7831 selected=19 time=384s\nGA   -> opt_score=0.7823 selected=14 time=645s\nGWO  -> opt_score=0.7807 selected=14 time=265s\nUnion candidates    : 19\nIntersection candidates: 11\nVoting candidates   : 17\n-------------------------------------------------\n-- UNION SUMMARY --\n Selected (14): ['Location', 'MinTemp', 'MaxTemp', 'Rainfall', 'Sunshine', 'WindGustDir', 'WindGustSpeed', 'WindDir3pm', 'Humidity9am', 'Humidity3pm', 'Pressure9am', 'Pressure3pm', 'Cloud3pm', 'Temp3pm']\n CV F1   : 0.8009 ± 0.0016\n Test F1 : 0.8048 (n_test=12751)\n Accuracy : 0.8022 ± 0.0016\n Precision: 0.8062 ± 0.0016\n Recall   : 0.7956 ± 0.0019\n Model file: hybrid_hlo_models_union_model.pkl\n-- INTERSECTION SUMMARY --\n Selected (10): ['MinTemp', 'Rainfall', 'Sunshine', 'WindGustDir', 'WindGustSpeed', 'WindDir3pm', 'Humidity3pm', 'Pressure3pm', 'Cloud9am', 'Temp3pm']\n CV F1   : 0.7917 ± 0.0033\n Test F1 : 0.7963 (n_test=12751)\n Accuracy : 0.7932 ± 0.0031\n Precision: 0.7973 ± 0.0032\n Recall   : 0.7862 ± 0.0044\n Model file: hybrid_hlo_models_intersection_model.pkl\n-- VOTING SUMMARY --\n Selected (16): ['Location', 'MinTemp', 'MaxTemp', 'Rainfall', 'Evaporation', 'Sunshine', 'WindGustDir', 'WindGustSpeed', 'WindDir9am', 'WindDir3pm', 'Humidity9am', 'Humidity3pm', 'Pressure3pm', 'Cloud9am', 'Cloud3pm', 'Temp3pm']\n CV F1   : 0.7992 ± 0.0025\n Test F1 : 0.8039 (n_test=12751)\n Accuracy : 0.8008 ± 0.0026\n Precision: 0.8057 ± 0.0029\n Recall   : 0.7929 ± 0.0023\n Model file: hybrid_hlo_models_voting_model.pkl\n[11:03:00] Saved results to hybrid_hlo_models_results.pkl\n[11:03:00] ===== PIPELINE COMPLETE =====\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"import kagglehub\n\n# Download latest version\npath = kagglehub.dataset_download(\"dhoogla/cicids2017\")\n\nprint(\"Path to dataset files:\", path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T15:08:27.369144Z","iopub.execute_input":"2025-12-05T15:08:27.369705Z","iopub.status.idle":"2025-12-05T15:08:27.517074Z","shell.execute_reply.started":"2025-12-05T15:08:27.369680Z","shell.execute_reply":"2025-12-05T15:08:27.516526Z"}},"outputs":[{"name":"stdout","text":"Path to dataset files: /kaggle/input/cicids2017\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"import kagglehub\n\n# Download latest version\npath = kagglehub.dataset_download(\"dhoogla/cicids2017\")\n\nprint(\"Path to dataset files:\", path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T15:25:11.970626Z","iopub.execute_input":"2025-12-10T15:25:11.970784Z","iopub.status.idle":"2025-12-10T15:25:12.401525Z","shell.execute_reply.started":"2025-12-10T15:25:11.970768Z","shell.execute_reply":"2025-12-10T15:25:12.400767Z"}},"outputs":[{"name":"stdout","text":"Path to dataset files: /kaggle/input/cicids2017\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\n\n# Load your dataset\ndf = pd.read_csv(\"/kaggle/working/converted_output_final.csv\")\n\n# ---------------------------\n# 1. PRINT ALL COLUMN NAMES\n# ---------------------------\nprint(len(df.columns))\n\n# ---------------------------\n# 2. PRINT UNIQUE VALUES OF A SPECIFIC COLUMN\n# ---------------------------\ncolumn_name = \"Label\"   # 🔹 change to your column name\n\nif column_name not in df.columns:\n    raise ValueError(f\"Column '{column_name}' NOT found in dataset.\")\n\nprint(f\"\\n=== UNIQUE VALUES IN '{column_name}' ===\")\nprint(df[column_name].unique())\n\nprint(f\"\\n=== VALUE COUNTS IN '{column_name}' ===\")\nprint(df[column_name].value_counts())\n\nprint(f\"\\n=== PERCENTAGE DISTRIBUTION IN '{column_name}' ===\")\nprint(df[column_name].value_counts(normalize=True) * 100)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T15:59:54.148703Z","iopub.execute_input":"2025-12-10T15:59:54.149327Z","iopub.status.idle":"2025-12-10T15:59:56.663083Z","shell.execute_reply.started":"2025-12-10T15:59:54.149300Z","shell.execute_reply":"2025-12-10T15:59:56.662468Z"}},"outputs":[{"name":"stdout","text":"78\n\n=== UNIQUE VALUES IN 'Label' ===\n['Benign' 'FTP-Patator' 'SSH-Patator']\n\n=== VALUE COUNTS IN 'Label' ===\nLabel\nBenign         380564\nFTP-Patator      5931\nSSH-Patator      3219\nName: count, dtype: int64\n\n=== PERCENTAGE DISTRIBUTION IN 'Label' ===\nLabel\nBenign         97.652124\nFTP-Patator     1.521885\nSSH-Patator     0.825990\nName: proportion, dtype: float64\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import pandas as pd\n\n# Load parquet\ndf = pd.read_parquet(\"/kaggle/input/cicids2017/Bruteforce-Tuesday-no-metadata.parquet\")\n\nprint(\"Loaded:\", df.shape)\n\n# Convert categorical columns to strings\ncat_cols = df.select_dtypes(include=[\"category\"]).columns\ndf[cat_cols] = df[cat_cols].astype(str)\n\n# Convert object columns to string (optional but safe)\nobj_cols = df.select_dtypes(include=[\"object\"]).columns\ndf[obj_cols] = df[obj_cols].astype(str)\n\n# Fill missing values\ndf = df.fillna(\"\")\n\n# Reset index\ndf.reset_index(drop=True, inplace=True)\n\n# Save as CSV\ndf.to_csv(\"converted_output_final.csv\", index=False)\n\nprint(\"File saved as converted_output.csv!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T15:59:38.353467Z","iopub.execute_input":"2025-12-10T15:59:38.354385Z","iopub.status.idle":"2025-12-10T15:59:48.962677Z","shell.execute_reply.started":"2025-12-10T15:59:38.354360Z","shell.execute_reply":"2025-12-10T15:59:48.961889Z"}},"outputs":[{"name":"stdout","text":"Loaded: (389714, 78)\nFile saved as converted_output.csv!\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\n\n# ----------------------------------------\n# 1. Load dataset\n# ----------------------------------------\ndf = pd.read_csv(\"/kaggle/working/converted_output_final.csv\")\nprint(\"Initial shape:\", df.shape)\nprint(df[\"Label\"].value_counts())\n\n# ----------------------------------------\n# 2. Combine Attack Classes (FTP + SSH) and Convert to 0/1\n# ----------------------------------------\ndf[\"Attack\"] = df[\"Label\"].replace({\n    \"Benign\": 1,\n    \"FTP-Patator\": 0,\n    \"SSH-Patator\": 0\n})\n\nprint(\"\\nUnique values in Attack:\", df[\"Attack\"].value_counts())\n\n# ----------------------------------------\n# 3. DROP the original Label column\n# ----------------------------------------\ndf = df.drop(columns=[\"Label\"])\n\n# ----------------------------------------\n# 4. BALANCING the dataset\n# ----------------------------------------\nbenign_df = df[df[\"Attack\"] == 1]\nattack_df = df[df[\"Attack\"] == 0]\n\nmin_size = min(len(benign_df), len(attack_df))\n\nprint(\"\\nSampling each class to size =\", min_size)\n\nbenign_bal = benign_df.sample(min_size, random_state=42)\nattack_bal = attack_df.sample(min_size, random_state=42)\n\ndf_balanced = pd.concat([benign_bal, attack_bal], axis=0).sample(frac=1, random_state=42)\n\nprint(\"Balanced shape:\", df_balanced.shape)\nprint(\"Balanced class counts:\")\nprint(df_balanced[\"Attack\"].value_counts())\n\n# ----------------------------------------\n# 5. Detect numeric + categorical columns\n# ----------------------------------------\nnum_cols = df_balanced.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\ncat_cols = df_balanced.select_dtypes(include=[\"object\", \"bool\"]).columns.tolist()\n\n# Remove target\nif \"Attack\" in num_cols:\n    num_cols.remove(\"Attack\")\n\nprint(\"\\nNumeric columns:\", num_cols)\nprint(\"Categorical columns:\", cat_cols)\n\n# ----------------------------------------\n# 6. Handle Missing Values\n# ----------------------------------------\nif len(num_cols) > 0:\n    df_balanced[num_cols] = df_balanced[num_cols].fillna(df_balanced[num_cols].median())\n\nfor col in cat_cols:\n    if df_balanced[col].isnull().any():\n        df_balanced[col] = df_balanced[col].fillna(df_balanced[col].mode()[0])\n\n# ----------------------------------------\n# 7. Encode Categorical Columns\n# ----------------------------------------\nle = LabelEncoder()\nfor col in cat_cols:\n    df_balanced[col] = le.fit_transform(df_balanced[col].astype(str))\n\n# ----------------------------------------\n# 8. Scale Numeric Columns\n# ----------------------------------------\nscaler = MinMaxScaler()\nif len(num_cols) > 0:\n    df_balanced[num_cols] = scaler.fit_transform(df_balanced[num_cols])\n\n# ----------------------------------------\n# 9. Save final balanced + cleaned dataset\n# ----------------------------------------\noutput = \"/kaggle/working/ids2017_testing_balanced_cleaned.csv\"\ndf_balanced.to_csv(output, index=False)\n\nprint(\"\\n✅ Cleaning & Balancing Completed!\")\nprint(\"📁 Saved as:\", output)\nprint(\"Final Shape:\", df_balanced.shape)\nprint(\"\\nFinal Class Distribution:\")\nprint(df_balanced[\"Attack\"].value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T16:05:04.142058Z","iopub.execute_input":"2025-12-10T16:05:04.142382Z","iopub.status.idle":"2025-12-10T16:05:08.805730Z","shell.execute_reply.started":"2025-12-10T16:05:04.142360Z","shell.execute_reply":"2025-12-10T16:05:08.805124Z"}},"outputs":[{"name":"stdout","text":"Initial shape: (389714, 78)\nLabel\nBenign         380564\nFTP-Patator      5931\nSSH-Patator      3219\nName: count, dtype: int64\n\nUnique values in Attack: Attack\n1    380564\n0      9150\nName: count, dtype: int64\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_47/214711343.py:15: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  df[\"Attack\"] = df[\"Label\"].replace({\n","output_type":"stream"},{"name":"stdout","text":"\nSampling each class to size = 9150\nBalanced shape: (18300, 78)\nBalanced class counts:\nAttack\n1    9150\n0    9150\nName: count, dtype: int64\n\nNumeric columns: ['Protocol', 'Flow Duration', 'Total Fwd Packets', 'Total Backward Packets', 'Fwd Packets Length Total', 'Bwd Packets Length Total', 'Fwd Packet Length Max', 'Fwd Packet Length Min', 'Fwd Packet Length Mean', 'Fwd Packet Length Std', 'Bwd Packet Length Max', 'Bwd Packet Length Min', 'Bwd Packet Length Mean', 'Bwd Packet Length Std', 'Flow Bytes/s', 'Flow Packets/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Total', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Total', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Header Length', 'Bwd Header Length', 'Fwd Packets/s', 'Bwd Packets/s', 'Packet Length Min', 'Packet Length Max', 'Packet Length Mean', 'Packet Length Std', 'Packet Length Variance', 'FIN Flag Count', 'SYN Flag Count', 'RST Flag Count', 'PSH Flag Count', 'ACK Flag Count', 'URG Flag Count', 'CWE Flag Count', 'ECE Flag Count', 'Down/Up Ratio', 'Avg Packet Size', 'Avg Fwd Segment Size', 'Avg Bwd Segment Size', 'Fwd Avg Bytes/Bulk', 'Fwd Avg Packets/Bulk', 'Fwd Avg Bulk Rate', 'Bwd Avg Bytes/Bulk', 'Bwd Avg Packets/Bulk', 'Bwd Avg Bulk Rate', 'Subflow Fwd Packets', 'Subflow Fwd Bytes', 'Subflow Bwd Packets', 'Subflow Bwd Bytes', 'Init Fwd Win Bytes', 'Init Bwd Win Bytes', 'Fwd Act Data Packets', 'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min']\nCategorical columns: []\n\n✅ Cleaning & Balancing Completed!\n📁 Saved as: /kaggle/working/ids2017_testing_balanced_cleaned.csv\nFinal Shape: (18300, 78)\n\nFinal Class Distribution:\nAttack\n1    9150\n0    9150\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom xgboost import XGBClassifier\n\n# ----------------------------------------\n# 1. Load cleaned dataset\n# ----------------------------------------\ndf = pd.read_csv(\"/kaggle/input/haruuu/ids2018_cleaned_combined_1.csv\")\nprint(\"Loaded dataset:\", df.shape)\n\n# ----------------------------------------\n# 2. Separate Features and Target\n# ----------------------------------------\nX = df.drop(columns=[\"Label\"])\ny = df[\"Label\"]\n\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nimport pandas as pd\n\n# Load dataset\n\nprint(\"Original dimension:\", X.shape[1])\n\n# -----------------------------\n# Train-test split\n# -----------------------------\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.20, stratify=y, random_state=42\n)\n\n# -----------------------------\n# XGBoost Model\n# -----------------------------\nmodel = XGBClassifier(\n    n_estimators=500,\n    learning_rate=0.05,\n    max_depth=6,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    objective=\"binary:logistic\",\n    eval_metric=\"logloss\",\n    random_state=42,\n    n_jobs=-1\n)\n\nmodel.fit(X_train, y_train)\n\n# -----------------------------\n# Predictions\n# -----------------------------\ny_pred = model.predict(X_test)\n\ntest_acc = accuracy_score(y_test, y_pred)\ntest_prec = precision_score(y_test, y_pred, zero_division=0)\ntest_rec = recall_score(y_test, y_pred, zero_division=0)\ntest_f1 = f1_score(y_test, y_pred, zero_division=0)\n\n# -----------------------------\n# Final Results\n# -----------------------------\nprint(\"\\n===== XGBoost Results =====\")\nprint(f\"Test Accuracy : {test_acc:.8f}\")\nprint(f\"Precision     : {test_prec:.8f}\")\nprint(f\"Recall        : {test_rec:.8f}\")\nprint(f\"F1 Score      : {test_f1:.8f}\")\n\nimport joblib\njoblib.dump(model, \"/kaggle/working/xgboost_ids2018.pkl\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T16:33:36.428777Z","iopub.execute_input":"2025-12-10T16:33:36.429232Z","iopub.status.idle":"2025-12-10T16:33:42.167619Z","shell.execute_reply.started":"2025-12-10T16:33:36.429208Z","shell.execute_reply":"2025-12-10T16:33:42.167022Z"}},"outputs":[{"name":"stdout","text":"Loaded dataset: (97802, 76)\nOriginal dimension: 75\n\n===== XGBoost Results =====\nTest Accuracy : 0.99928429\nPrecision     : 0.99968600\nRecall        : 0.99884961\nF1 Score      : 0.99926763\n","output_type":"stream"},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"['/kaggle/working/xgboost_ids2018.pkl']"},"metadata":{}}],"execution_count":21}]}