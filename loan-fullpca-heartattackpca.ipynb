{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14096535,"sourceType":"datasetVersion","datasetId":8976899},{"sourceId":14096785,"sourceType":"datasetVersion","datasetId":8977116},{"sourceId":14097288,"sourceType":"datasetVersion","datasetId":8977518}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"print(\"hi\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T19:17:26.143400Z","iopub.execute_input":"2025-12-10T19:17:26.144285Z","iopub.status.idle":"2025-12-10T19:17:26.149045Z","shell.execute_reply.started":"2025-12-10T19:17:26.144257Z","shell.execute_reply":"2025-12-10T19:17:26.148052Z"}},"outputs":[{"name":"stdout","text":"hi\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n# -----------------------------\n# 1Ô∏è‚É£ Load your dataset\n# -----------------------------\ndf = pd.read_csv(\"/kaggle/input/loanyyy-pca/loan_final_normalized_done _d.csv\")  # change file if needed\n\nTARGET = \"loan_status\"   # <-- change if your target column is different\n\nX = df.drop(TARGET, axis=1)\ny = df[TARGET].astype(int)\n\nprint(\"Original dimension:\", X.shape[1])\n\n# -----------------------------\n# 2Ô∏è‚É£ Train-test split\n# -----------------------------\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.20, stratify=y, random_state=42\n)\n\n# -----------------------------\n# 3Ô∏è‚É£ XGBoost Model (500 Iterations)\n# -----------------------------\nmodel = XGBClassifier(\n    n_estimators=500,          # <-- 500 boosting rounds\n    learning_rate=0.05,\n    max_depth=6,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    objective=\"binary:logistic\",\n    eval_metric=\"logloss\",\n    random_state=42,\n    n_jobs=-1\n)\n\nmodel.fit(X_train, y_train)\n\n# -----------------------------\n# 4Ô∏è‚É£ Predictions\n# -----------------------------\ny_pred = model.predict(X_test)\n\ntest_acc = accuracy_score(y_test, y_pred)\ntest_prec = precision_score(y_test, y_pred, zero_division=0)\ntest_rec = recall_score(y_test, y_pred, zero_division=0)\ntest_f1 = f1_score(y_test, y_pred, zero_division=0)\n\n# -----------------------------\n# 5Ô∏è‚É£ Results\n# -----------------------------\nprint(\"\\n===== XGBoost Results (500 Iterations) =====\")\nprint(f\"Test Accuracy : {test_acc:.8f}\")\nprint(f\"Precision     : {test_prec:.8f}\")\nprint(f\"Recall        : {test_rec:.8f}\")\nprint(f\"F1 Score      : {test_f1:.8f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T19:18:39.537578Z","iopub.execute_input":"2025-12-10T19:18:39.537973Z","iopub.status.idle":"2025-12-10T19:18:43.531478Z","shell.execute_reply.started":"2025-12-10T19:18:39.537946Z","shell.execute_reply":"2025-12-10T19:18:43.530581Z"}},"outputs":[{"name":"stdout","text":"Original dimension: 19\n\n===== XGBoost Results (500 Iterations) =====\nTest Accuracy : 0.92720000\nPrecision     : 0.92613738\nRecall        : 0.94296094\nF1 Score      : 0.93447345\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T19:19:06.702264Z","iopub.execute_input":"2025-12-10T19:19:06.703111Z","iopub.status.idle":"2025-12-10T19:19:06.709400Z","shell.execute_reply.started":"2025-12-10T19:19:06.703072Z","shell.execute_reply":"2025-12-10T19:19:06.708464Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"(50000, 20)"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"pca loan","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nimport numpy as np\nimport pandas as pd\nfrom xgboost import XGBClassifier   # üî• NEW\n\ndf = pd.read_csv(\"/kaggle/input/loanyyy-pca/loan_final_normalized_done _d.csv\")\n\nX = df.drop(\"loan_status\",axis=1)\ny = df[\"loan_status\"].astype(int)\n\n# 1) Scale\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# 2) PCA to retain 95% variance\npca = PCA(n_components=0.95)  \nX_pca = pca.fit_transform(X_scaled)\n\nprint(\"Original dimension:\", X.shape[1])\nprint(\"PCA dimension:\", X_pca.shape[1])\n\n# 3) Train/Test split\nX_train, X_test, y_train, y_test = train_test_split(\n    X_pca, y, test_size=0.20, stratify=y, random_state=42\n)\n\n# ---------------------------------------------------\n# 4) ‚ùó Replace CatBoost with XGBoost\n# ---------------------------------------------------\nmodel = XGBClassifier(\n    n_estimators=500,\n    learning_rate=0.05,\n    max_depth=6,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    objective=\"binary:logistic\",\n    eval_metric=\"logloss\",\n    tree_method=\"hist\"        # Fast training\n)\n\n# Fit\nmodel.fit(X_train, y_train)\n\n# Test prediction\ny_pred = model.predict(X_test)\n\n# Metrics\ntest_acc = accuracy_score(y_test, y_pred)\ntest_prec = precision_score(y_test, y_pred, zero_division=0)\ntest_rec = recall_score(y_test, y_pred, zero_division=0)\ntest_f1 = f1_score(y_test, y_pred, zero_division=0)\n\nprint(\"\\n=== PCA + XGBOOST MODEL RESULTS ===\")\nprint(\"Test Accuracy :\", test_acc)\nprint(\"Precision     :\", test_prec)\nprint(\"Recall        :\", test_rec)\nprint(\"F1 Score      :\", test_f1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T19:21:26.786625Z","iopub.execute_input":"2025-12-10T19:21:26.787027Z","iopub.status.idle":"2025-12-10T19:21:29.730701Z","shell.execute_reply.started":"2025-12-10T19:21:26.786998Z","shell.execute_reply":"2025-12-10T19:21:29.729897Z"}},"outputs":[{"name":"stdout","text":"Original dimension: 19\nPCA dimension: 14\n\n=== PCA + XGBOOST MODEL RESULTS ===\nTest Accuracy : 0.8905\nPrecision     : 0.8879310344827587\nRecall        : 0.916802906448683\nF1 Score      : 0.9021360264545535\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"chi square","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\nfrom xgboost import XGBClassifier   # ‚Üê changed\n\n# -------------------------\n# 1. Prepare data\n# -------------------------\n\nprint(\"Original dimension:\", X.shape[1])\n\n# Train-test split (same style as your hybrid pipeline)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.20,\n    stratify=y,\n    random_state=42\n)\n\n# -------------------------\n# 2. Scale to non-negative for chi-square\n# -------------------------\nscaler = MinMaxScaler()   # maps features to [0, 1]\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# -------------------------\n# 3. Try different numbers of selected features\n# -------------------------\nk_values = [5, 10, 15,11,13,15]\n\nresults_chi2 = {}\n\nfor k in k_values:\n    print(\"\\n\" + \"=\"*60)\n    print(f\"CHI-SQUARE + XGBoost with top-{k} features\")\n    print(\"=\"*60)\n\n    # 3.1 Chi-Square feature selection\n    selector = SelectKBest(score_func=chi2, k=k)\n    X_train_k = selector.fit_transform(X_train_scaled, y_train)\n    X_test_k = selector.transform(X_test_scaled)\n\n    # Get selected feature names (from original X)\n    selected_mask = selector.get_support()\n    selected_features = X.columns[selected_mask].tolist()\n    print(f\"Selected {k} features:\")\n    print(selected_features)\n\n    # 3.2 Train XGBoost on selected features\n    model = XGBClassifier(\n        n_estimators=500,\n        learning_rate=0.05,\n        max_depth=6,\n        subsample=0.9,\n        colsample_bytree=0.9,\n        objective=\"binary:logistic\",\n        use_label_encoder=False,   # avoid label encoder warning\n        eval_metric=\"logloss\",\n        random_state=42,\n        n_jobs=-1\n    )\n\n    model.fit(X_train_k, y_train)\n\n    # 3.3 Evaluate on test set\n    y_pred = model.predict(X_test_k)\n\n    acc = accuracy_score(y_test, y_pred)\n    prec = precision_score(y_test, y_pred, zero_division=0)\n    rec = recall_score(y_test, y_pred, zero_division=0)\n    f1 = f1_score(y_test, y_pred, zero_division=0)\n\n    print(f\"Test Accuracy : {acc:.6f}\")\n    print(f\"Precision     : {prec:.6f}\")\n    print(f\"Recall        : {rec:.6f}\")\n    print(f\"F1 Score      : {f1:.6f}\")\n\n    # store results for later comparison\n    results_chi2[k] = {\n        \"features\": selected_features,\n        \"acc\": acc,\n        \"prec\": prec,\n        \"rec\": rec,\n        \"f1\": f1\n    }\n\nprint(\"\\n=========== SUMMARY: Chi-Square + XGBoost ===========\")\nfor k, info in results_chi2.items():\n    print(f\"Top-{k} features -> F1={info['f1']:.6f}, Acc={info['acc']:.6f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T19:23:48.847685Z","iopub.execute_input":"2025-12-10T19:23:48.848279Z","iopub.status.idle":"2025-12-10T19:23:57.858065Z","shell.execute_reply.started":"2025-12-10T19:23:48.848238Z","shell.execute_reply":"2025-12-10T19:23:57.857130Z"}},"outputs":[{"name":"stdout","text":"Original dimension: 19\n\n============================================================\nCHI-SQUARE + XGBoost with top-5 features\n============================================================\nSelected 5 features:\n['age', 'credit_history_years', 'defaults_on_file', 'delinquencies_last_2yrs', 'derogatory_marks']\nTest Accuracy : 0.728600\nPrecision     : 0.718765\nRecall        : 0.832879\nF1 Score      : 0.771626\n\n============================================================\nCHI-SQUARE + XGBoost with top-10 features\n============================================================\nSelected 10 features:\n['age', 'years_employed', 'credit_score', 'credit_history_years', 'defaults_on_file', 'delinquencies_last_2yrs', 'derogatory_marks', 'product_type', 'debt_to_income_ratio', 'payment_to_income_ratio']\nTest Accuracy : 0.887700\nPrecision     : 0.889858\nRecall        : 0.908447\nF1 Score      : 0.899056\n\n============================================================\nCHI-SQUARE + XGBoost with top-15 features\n============================================================\nSelected 15 features:\n['age', 'years_employed', 'annual_income', 'credit_score', 'credit_history_years', 'savings_assets', 'defaults_on_file', 'delinquencies_last_2yrs', 'derogatory_marks', 'product_type', 'loan_intent', 'interest_rate', 'debt_to_income_ratio', 'loan_to_income_ratio', 'payment_to_income_ratio']\nTest Accuracy : 0.922800\nPrecision     : 0.922816\nRecall        : 0.938238\nF1 Score      : 0.930463\n\n============================================================\nCHI-SQUARE + XGBoost with top-11 features\n============================================================\nSelected 11 features:\n['age', 'years_employed', 'credit_score', 'credit_history_years', 'defaults_on_file', 'delinquencies_last_2yrs', 'derogatory_marks', 'product_type', 'debt_to_income_ratio', 'loan_to_income_ratio', 'payment_to_income_ratio']\nTest Accuracy : 0.888800\nPrecision     : 0.889519\nRecall        : 0.911172\nF1 Score      : 0.900215\n\n============================================================\nCHI-SQUARE + XGBoost with top-13 features\n============================================================\nSelected 13 features:\n['age', 'years_employed', 'annual_income', 'credit_score', 'credit_history_years', 'defaults_on_file', 'delinquencies_last_2yrs', 'derogatory_marks', 'product_type', 'interest_rate', 'debt_to_income_ratio', 'loan_to_income_ratio', 'payment_to_income_ratio']\nTest Accuracy : 0.887800\nPrecision     : 0.888633\nRecall        : 0.910263\nF1 Score      : 0.899318\n\n============================================================\nCHI-SQUARE + XGBoost with top-15 features\n============================================================\nSelected 15 features:\n['age', 'years_employed', 'annual_income', 'credit_score', 'credit_history_years', 'savings_assets', 'defaults_on_file', 'delinquencies_last_2yrs', 'derogatory_marks', 'product_type', 'loan_intent', 'interest_rate', 'debt_to_income_ratio', 'loan_to_income_ratio', 'payment_to_income_ratio']\nTest Accuracy : 0.922800\nPrecision     : 0.922816\nRecall        : 0.938238\nF1 Score      : 0.930463\n\n=========== SUMMARY: Chi-Square + XGBoost ===========\nTop-5 features -> F1=0.771626, Acc=0.728600\nTop-10 features -> F1=0.899056, Acc=0.887700\nTop-15 features -> F1=0.930463, Acc=0.922800\nTop-11 features -> F1=0.900215, Acc=0.888800\nTop-13 features -> F1=0.899318, Acc=0.887800\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"mutual information","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import SelectKBest, mutual_info_classif\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\nfrom xgboost import XGBClassifier   # <-- changed\n\n# -------------------------\n# 1. Prepare data\n# -------------------------\n\nprint(\"Original dimension:\", X.shape[1])\n\n# Train-test split (keep same style as other experiments)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.20,\n    stratify=y,\n    random_state=42\n)\n\n# -------------------------\n# 2. Try different numbers of selected features\n# -------------------------\nk_values = [5,10,15,11,13]\n\nresults_mi = {}\n\nfor k in k_values:\n    print(\"\\n\" + \"=\"*60)\n    print(f\"MUTUAL INFORMATION + XGBoost with top-{k} features\")\n    print(\"=\"*60)\n\n    # 2.1 Mutual Information feature selection\n    selector = SelectKBest(score_func=mutual_info_classif, k=k)\n    X_train_k = selector.fit_transform(X_train, y_train)\n    X_test_k = selector.transform(X_test)\n\n    # Get selected feature names (from original X)\n    selected_mask = selector.get_support()\n    selected_features = X.columns[selected_mask].tolist()\n    print(f\"Selected {k} features:\")\n    print(selected_features)\n\n    # 2.2 Train XGBoost on selected features\n    model = XGBClassifier(\n        n_estimators=500,\n        learning_rate=0.05,\n        max_depth=6,\n        subsample=0.9,\n        colsample_bytree=0.9,\n        objective=\"binary:logistic\",\n        use_label_encoder=False,   # avoid label-encoder warning in older xgboost\n        eval_metric=\"logloss\",\n        random_state=42,\n        n_jobs=-1\n    )\n\n    model.fit(X_train_k, y_train)\n\n    # 2.3 Evaluate on test set\n    y_pred = model.predict(X_test_k)\n\n    acc = accuracy_score(y_test, y_pred)\n    prec = precision_score(y_test, y_pred, zero_division=0)\n    rec = recall_score(y_test, y_pred, zero_division=0)\n    f1 = f1_score(y_test, y_pred, zero_division=0)\n\n    print(f\"Test Accuracy : {acc:.6f}\")\n    print(f\"Precision     : {prec:.6f}\")\n    print(f\"Recall        : {rec:.6f}\")\n    print(f\"F1 Score      : {f1:.6f}\")\n\n    # store results for later comparison\n    results_mi[k] = {\n        \"features\": selected_features,\n        \"acc\": acc,\n        \"prec\": prec,\n        \"rec\": rec,\n        \"f1\": f1\n    }\n\nprint(\"\\n=========== SUMMARY: Mutual Information + XGBoost ===========\")\nfor k, info in results_mi.items():\n    print(f\"Top-{k} features -> F1={info['f1']:.6f}, Acc={info['acc']:.6f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T19:26:32.416779Z","iopub.execute_input":"2025-12-10T19:26:32.417201Z","iopub.status.idle":"2025-12-10T19:26:59.045028Z","shell.execute_reply.started":"2025-12-10T19:26:32.417176Z","shell.execute_reply":"2025-12-10T19:26:59.044227Z"}},"outputs":[{"name":"stdout","text":"Original dimension: 19\n\n============================================================\nMUTUAL INFORMATION + XGBoost with top-5 features\n============================================================\nSelected 5 features:\n['age', 'credit_score', 'delinquencies_last_2yrs', 'interest_rate', 'debt_to_income_ratio']\nTest Accuracy : 0.843900\nPrecision     : 0.841176\nRecall        : 0.883197\nF1 Score      : 0.861675\n\n============================================================\nMUTUAL INFORMATION + XGBoost with top-10 features\n============================================================\nSelected 10 features:\n['age', 'years_employed', 'credit_score', 'credit_history_years', 'savings_assets', 'defaults_on_file', 'delinquencies_last_2yrs', 'derogatory_marks', 'interest_rate', 'debt_to_income_ratio']\nTest Accuracy : 0.864600\nPrecision     : 0.866891\nRecall        : 0.890827\nF1 Score      : 0.878696\n\n============================================================\nMUTUAL INFORMATION + XGBoost with top-15 features\n============================================================\nSelected 15 features:\n['age', 'years_employed', 'annual_income', 'credit_score', 'credit_history_years', 'savings_assets', 'defaults_on_file', 'delinquencies_last_2yrs', 'derogatory_marks', 'product_type', 'loan_intent', 'interest_rate', 'debt_to_income_ratio', 'loan_to_income_ratio', 'payment_to_income_ratio']\nTest Accuracy : 0.922800\nPrecision     : 0.922816\nRecall        : 0.938238\nF1 Score      : 0.930463\n\n============================================================\nMUTUAL INFORMATION + XGBoost with top-11 features\n============================================================\nSelected 11 features:\n['age', 'years_employed', 'credit_score', 'credit_history_years', 'savings_assets', 'defaults_on_file', 'delinquencies_last_2yrs', 'derogatory_marks', 'interest_rate', 'debt_to_income_ratio', 'loan_to_income_ratio']\nTest Accuracy : 0.887100\nPrecision     : 0.889324\nRecall        : 0.907902\nF1 Score      : 0.898517\n\n============================================================\nMUTUAL INFORMATION + XGBoost with top-13 features\n============================================================\nSelected 13 features:\n['age', 'years_employed', 'credit_score', 'credit_history_years', 'savings_assets', 'defaults_on_file', 'delinquencies_last_2yrs', 'derogatory_marks', 'loan_intent', 'interest_rate', 'debt_to_income_ratio', 'loan_to_income_ratio', 'payment_to_income_ratio']\nTest Accuracy : 0.920300\nPrecision     : 0.922469\nRecall        : 0.933697\nF1 Score      : 0.928049\n\n=========== SUMMARY: Mutual Information + XGBoost ===========\nTop-5 features -> F1=0.861675, Acc=0.843900\nTop-10 features -> F1=0.878696, Acc=0.864600\nTop-15 features -> F1=0.930463, Acc=0.922800\nTop-11 features -> F1=0.898517, Acc=0.887100\nTop-13 features -> F1=0.928049, Acc=0.920300\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"recursive feature elimination","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import RFE\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\nfrom xgboost import XGBClassifier   # <-- changed\n\n# -------------------------\n# 1. Prepare data\n# -------------------------\n\nprint(\"Original dimension:\", X.shape[1])\n\n# Train-test split (same style as other baselines)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.20,\n    stratify=y,\n    random_state=42\n)\n\n# -------------------------\n# 2. Try different numbers of selected features with RFE\n# -------------------------\nk_values = [5,10,15,11,13]\n\nresults_rfe = {}\n\nfor k in k_values:\n    print(\"\\n\" + \"=\"*60)\n    print(f\"RFE (XGBoost) with top-{k} features\")\n    print(\"=\"*60)\n\n    # Base estimator for RFE (reduced iterations to save time)\n    base_model = XGBClassifier(\n        n_estimators=200,            # reduced for speed during RFE\n        learning_rate=0.05,\n        max_depth=6,\n        objective=\"binary:logistic\",\n        use_label_encoder=False,     # suppress older xgboost warning\n        eval_metric=\"logloss\",\n        random_state=42,\n        n_jobs=-1\n    )\n\n    # 2.1 RFE setup\n    selector = RFE(\n        estimator=base_model,\n        n_features_to_select=k,\n        step=1\n    )\n\n    # Fit RFE on training data\n    selector.fit(X_train, y_train)\n\n    # Transform train and test sets\n    X_train_k = selector.transform(X_train)\n    X_test_k = selector.transform(X_test)\n\n    # Get selected feature names\n    selected_mask = selector.get_support()\n    selected_features = X.columns[selected_mask].tolist()\n    print(f\"Selected {k} features:\")\n    print(selected_features)\n\n    # 2.2 Train a fresh XGBoost on the selected features (for fair comparison)\n    model = XGBClassifier(\n        n_estimators=500,\n        learning_rate=0.05,\n        max_depth=6,\n        subsample=0.9,\n        colsample_bytree=0.9,\n        objective=\"binary:logistic\",\n        use_label_encoder=False,\n        eval_metric=\"logloss\",\n        random_state=42,\n        n_jobs=-1\n    )\n\n    model.fit(X_train_k, y_train)\n\n    # 2.3 Evaluate on test set\n    y_pred = model.predict(X_test_k)\n\n    acc = accuracy_score(y_test, y_pred)\n    prec = precision_score(y_test, y_pred, zero_division=0)\n    rec = recall_score(y_test, y_pred, zero_division=0)\n    f1 = f1_score(y_test, y_pred, zero_division=0)\n\n    print(f\"Test Accuracy : {acc:.6f}\")\n    print(f\"Precision     : {prec:.6f}\")\n    print(f\"Recall        : {rec:.6f}\")\n    print(f\"F1 Score      : {f1:.6f}\")\n\n    # store results for later comparison\n    results_rfe[k] = {\n        \"features\": selected_features,\n        \"acc\": acc,\n        \"prec\": prec,\n        \"rec\": rec,\n        \"f1\": f1\n    }\n\nprint(\"\\n=========== SUMMARY: RFE (XGBoost) ===========\")\nfor k, info in results_rfe.items():\n    print(f\"Top-{k} features -> F1={info['f1']:.6f}, Acc={info['acc']:.6f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T19:27:23.714135Z","iopub.execute_input":"2025-12-10T19:27:23.714980Z","iopub.status.idle":"2025-12-10T19:28:04.098734Z","shell.execute_reply.started":"2025-12-10T19:27:23.714951Z","shell.execute_reply":"2025-12-10T19:28:04.098183Z"}},"outputs":[{"name":"stdout","text":"Original dimension: 19\n\n============================================================\nRFE (XGBoost) with top-5 features\n============================================================\nSelected 5 features:\n['credit_score', 'defaults_on_file', 'delinquencies_last_2yrs', 'loan_intent', 'debt_to_income_ratio']\nTest Accuracy : 0.854900\nPrecision     : 0.850692\nRecall        : 0.893188\nF1 Score      : 0.871422\n\n============================================================\nRFE (XGBoost) with top-10 features\n============================================================\nSelected 10 features:\n['credit_score', 'credit_history_years', 'defaults_on_file', 'delinquencies_last_2yrs', 'derogatory_marks', 'product_type', 'loan_intent', 'debt_to_income_ratio', 'loan_to_income_ratio', 'payment_to_income_ratio']\nTest Accuracy : 0.912900\nPrecision     : 0.912131\nRecall        : 0.931517\nF1 Score      : 0.921722\n\n============================================================\nRFE (XGBoost) with top-15 features\n============================================================\nSelected 15 features:\n['age', 'occupation_status', 'years_employed', 'credit_score', 'credit_history_years', 'defaults_on_file', 'delinquencies_last_2yrs', 'derogatory_marks', 'product_type', 'loan_intent', 'loan_amount', 'interest_rate', 'debt_to_income_ratio', 'loan_to_income_ratio', 'payment_to_income_ratio']\nTest Accuracy : 0.926100\nPrecision     : 0.926297\nRecall        : 0.940599\nF1 Score      : 0.933393\n\n============================================================\nRFE (XGBoost) with top-11 features\n============================================================\nSelected 11 features:\n['years_employed', 'credit_score', 'credit_history_years', 'defaults_on_file', 'delinquencies_last_2yrs', 'derogatory_marks', 'product_type', 'loan_intent', 'debt_to_income_ratio', 'loan_to_income_ratio', 'payment_to_income_ratio']\nTest Accuracy : 0.920300\nPrecision     : 0.920357\nRecall        : 0.936240\nF1 Score      : 0.928231\n\n============================================================\nRFE (XGBoost) with top-13 features\n============================================================\nSelected 13 features:\n['years_employed', 'credit_score', 'credit_history_years', 'defaults_on_file', 'delinquencies_last_2yrs', 'derogatory_marks', 'product_type', 'loan_intent', 'loan_amount', 'interest_rate', 'debt_to_income_ratio', 'loan_to_income_ratio', 'payment_to_income_ratio']\nTest Accuracy : 0.926900\nPrecision     : 0.926250\nRecall        : 0.942234\nF1 Score      : 0.934174\n\n=========== SUMMARY: RFE (XGBoost) ===========\nTop-5 features -> F1=0.871422, Acc=0.854900\nTop-10 features -> F1=0.921722, Acc=0.912900\nTop-15 features -> F1=0.933393, Acc=0.926100\nTop-11 features -> F1=0.928231, Acc=0.920300\nTop-13 features -> F1=0.934174, Acc=0.926900\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"l1 lasso feature selection + xgboost","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom xgboost import XGBClassifier   # <-- changed\nimport numpy as np\nimport pandas as pd\n\n\nfeature_names = X.columns.tolist()\n\nprint(\"Original dimension:\", X.shape[1])\n\n# Train Logistic regression with L1 penalty to select features\nlog_reg = LogisticRegression(penalty='l1', solver='liblinear', C=0.3, max_iter=2000)\nlog_reg.fit(X, y)\n\n# Get non-zero coefficients\ncoef = log_reg.coef_[0]\nselected_idx = np.where(coef != 0)[0]\nselected_features = [feature_names[i] for i in selected_idx]\n\nprint(\"\\nSelected features using L1 (Lasso):\")\nprint(selected_features)\nprint(\"Total selected:\", len(selected_features))\n\n# ----- XGBoost on selected features -----\n\nX_sel = X[selected_features]\nX_train, X_test, y_train, y_test = train_test_split(\n    X_sel, y, test_size=0.20, stratify=y, random_state=42\n)\n\nmodel = XGBClassifier(\n    n_estimators=500,\n    learning_rate=0.05,\n    max_depth=6,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    objective=\"binary:logistic\",\n    use_label_encoder=False,   # suppress older xgboost label-encoder warning\n    eval_metric=\"logloss\",\n    random_state=42,\n    n_jobs=-1\n)\n\nmodel.fit(X_train, y_train)\n\n# Predictions\ny_pred = model.predict(X_test)\n\ntest_acc = accuracy_score(y_test, y_pred)\ntest_prec = precision_score(y_test, y_pred, zero_division=0)\ntest_rec = recall_score(y_test, y_pred, zero_division=0)\ntest_f1 = f1_score(y_test, y_pred, zero_division=0)\n\nprint(\"\\n===== L1 (Lasso) + XGBoost Results =====\")\nprint(f\"Test Accuracy : {test_acc:.8f}\")\nprint(f\"Precision     : {test_prec:.8f}\")\nprint(f\"Recall        : {test_rec:.8f}\")\nprint(f\"F1 Score      : {test_f1:.8f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T19:30:38.683916Z","iopub.execute_input":"2025-12-10T19:30:38.684312Z","iopub.status.idle":"2025-12-10T19:30:41.439495Z","shell.execute_reply.started":"2025-12-10T19:30:38.684285Z","shell.execute_reply":"2025-12-10T19:30:41.438621Z"}},"outputs":[{"name":"stdout","text":"Original dimension: 19\n\nSelected features using L1 (Lasso):\n['customer_id', 'age', 'occupation_status', 'years_employed', 'annual_income', 'credit_score', 'credit_history_years', 'savings_assets', 'current_debt', 'defaults_on_file', 'delinquencies_last_2yrs', 'derogatory_marks', 'product_type', 'loan_intent', 'loan_amount', 'interest_rate', 'debt_to_income_ratio', 'loan_to_income_ratio', 'payment_to_income_ratio']\nTotal selected: 19\n\n===== L1 (Lasso) + XGBoost Results =====\nTest Accuracy : 0.92720000\nPrecision     : 0.92613738\nRecall        : 0.94296094\nF1 Score      : 0.93447345\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"xgboost importance + xgboost baseline","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\nfrom xgboost import XGBClassifier\n\n# -------------------------\n# 1. Load data\n# -------------------------\n\nfeature_names = X.columns.tolist()\nprint(\"Original dimension:\", X.shape[1])\n\n# -------------------------\n# 2. Train XGBoost on ALL features to get importance\n# -------------------------\nbase_model = XGBClassifier(\n    n_estimators=500,\n    learning_rate=0.05,\n    max_depth=6,\n    objective=\"binary:logistic\",\n    use_label_encoder=False,\n    eval_metric=\"logloss\",\n    random_state=42,\n    n_jobs=-1\n)\n\nbase_model.fit(X, y)\n\n# Get feature importances from XGBoost\nimportances = base_model.feature_importances_\n# Sort indices by importance (descending)\nsorted_idx = np.argsort(importances)[::-1]\nsorted_features = [feature_names[i] for i in sorted_idx]\n\nprint(\"\\nTop 20 features by XGBoost importance:\")\nfor i in range(min(20, len(sorted_features))):\n    print(f\"{i+1:2d}. {sorted_features[i]}  (importance={importances[sorted_idx[i]]:.6f})\")\n\n# -------------------------\n# 3. Evaluate XGBoost using top-k important features\n# -------------------------\nK_values = [5, 10, 15, 11,13]\nresults_xgb_imp = {}\n\nprint(\"\\n============================================================\")\nprint(\"XGBOOST FEATURE IMPORTANCE + XGBOOST BASELINE\")\nprint(\"============================================================\")\n\nfor k in K_values:\n    top_k_features = sorted_features[:k]\n    X_sel = X[top_k_features]\n\n    # Same train-test strategy as other baselines\n    X_train, X_test, y_train, y_test = train_test_split(\n        X_sel, y,\n        test_size=0.20,\n        stratify=y,\n        random_state=42\n    )\n\n    model = XGBClassifier(\n        n_estimators=500,\n        learning_rate=0.05,\n        max_depth=6,\n        subsample=0.9,\n        colsample_bytree=0.9,\n        objective=\"binary:logistic\",\n        use_label_encoder=False,\n        eval_metric=\"logloss\",\n        random_state=42,\n        n_jobs=-1\n    )\n\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n\n    acc = accuracy_score(y_test, y_pred)\n    prec = precision_score(y_test, y_pred, zero_division=0)\n    rec = recall_score(y_test, y_pred, zero_division=0)\n    f1 = f1_score(y_test, y_pred, zero_division=0)\n\n    print(f\"\\n============================================================\")\n    print(f\"XGBoost-Importance + XGBoost with top-{k} features\")\n    print(\"============================================================\")\n    print(f\"Selected {k} features:\")\n    print(top_k_features)\n    print(f\"Test Accuracy : {acc:.8f}\")\n    print(f\"Precision     : {prec:.8f}\")\n    print(f\"Recall        : {rec:.8f}\")\n    print(f\"F1 Score      : {f1:.8f}\")\n\n    results_xgb_imp[k] = {\n        \"features\": top_k_features,\n        \"acc\": acc,\n        \"prec\": prec,\n        \"rec\": rec,\n        \"f1\": f1\n    }\n\nprint(\"\\n=========== SUMMARY: XGBoost Importance + XGBoost ===========\")\nfor k, info in results_xgb_imp.items():\n    print(f\"Top-{k} features -> F1={info['f1']:.8f}, Acc={info['acc']:.8f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T19:33:46.595101Z","iopub.execute_input":"2025-12-10T19:33:46.595950Z","iopub.status.idle":"2025-12-10T19:33:56.525204Z","shell.execute_reply.started":"2025-12-10T19:33:46.595919Z","shell.execute_reply":"2025-12-10T19:33:56.524228Z"}},"outputs":[{"name":"stdout","text":"Original dimension: 19\n\nTop 20 features by XGBoost importance:\n 1. defaults_on_file  (importance=0.199084)\n 2. credit_score  (importance=0.165578)\n 3. debt_to_income_ratio  (importance=0.103600)\n 4. delinquencies_last_2yrs  (importance=0.099482)\n 5. product_type  (importance=0.074467)\n 6. loan_intent  (importance=0.067603)\n 7. payment_to_income_ratio  (importance=0.060539)\n 8. credit_history_years  (importance=0.046555)\n 9. derogatory_marks  (importance=0.042836)\n10. loan_to_income_ratio  (importance=0.037167)\n11. years_employed  (importance=0.026586)\n12. loan_amount  (importance=0.016354)\n13. occupation_status  (importance=0.011668)\n14. interest_rate  (importance=0.011019)\n15. age  (importance=0.009721)\n16. savings_assets  (importance=0.008047)\n17. annual_income  (importance=0.007739)\n18. current_debt  (importance=0.006255)\n19. customer_id  (importance=0.005702)\n\n============================================================\nXGBOOST FEATURE IMPORTANCE + XGBOOST BASELINE\n============================================================\n\n============================================================\nXGBoost-Importance + XGBoost with top-5 features\n============================================================\nSelected 5 features:\n['defaults_on_file', 'credit_score', 'debt_to_income_ratio', 'delinquencies_last_2yrs', 'product_type']\nTest Accuracy : 0.83830000\nPrecision     : 0.83117547\nRecall        : 0.88628520\nF1 Score      : 0.85784615\n\n============================================================\nXGBoost-Importance + XGBoost with top-10 features\n============================================================\nSelected 10 features:\n['defaults_on_file', 'credit_score', 'debt_to_income_ratio', 'delinquencies_last_2yrs', 'product_type', 'loan_intent', 'payment_to_income_ratio', 'credit_history_years', 'derogatory_marks', 'loan_to_income_ratio']\nTest Accuracy : 0.91250000\nPrecision     : 0.91089812\nRecall        : 0.93224342\nF1 Score      : 0.92144717\n\n============================================================\nXGBoost-Importance + XGBoost with top-15 features\n============================================================\nSelected 15 features:\n['defaults_on_file', 'credit_score', 'debt_to_income_ratio', 'delinquencies_last_2yrs', 'product_type', 'loan_intent', 'payment_to_income_ratio', 'credit_history_years', 'derogatory_marks', 'loan_to_income_ratio', 'years_employed', 'loan_amount', 'occupation_status', 'interest_rate', 'age']\nTest Accuracy : 0.92690000\nPrecision     : 0.92747135\nRecall        : 0.94078111\nF1 Score      : 0.93407882\n\n============================================================\nXGBoost-Importance + XGBoost with top-11 features\n============================================================\nSelected 11 features:\n['defaults_on_file', 'credit_score', 'debt_to_income_ratio', 'delinquencies_last_2yrs', 'product_type', 'loan_intent', 'payment_to_income_ratio', 'credit_history_years', 'derogatory_marks', 'loan_to_income_ratio', 'years_employed']\nTest Accuracy : 0.92020000\nPrecision     : 0.92109501\nRecall        : 0.93514986\nF1 Score      : 0.92806923\n\n============================================================\nXGBoost-Importance + XGBoost with top-13 features\n============================================================\nSelected 13 features:\n['defaults_on_file', 'credit_score', 'debt_to_income_ratio', 'delinquencies_last_2yrs', 'product_type', 'loan_intent', 'payment_to_income_ratio', 'credit_history_years', 'derogatory_marks', 'loan_to_income_ratio', 'years_employed', 'loan_amount', 'occupation_status']\nTest Accuracy : 0.92680000\nPrecision     : 0.92608463\nRecall        : 0.94223433\nF1 Score      : 0.93408968\n\n=========== SUMMARY: XGBoost Importance + XGBoost ===========\nTop-5 features -> F1=0.85784615, Acc=0.83830000\nTop-10 features -> F1=0.92144717, Acc=0.91250000\nTop-15 features -> F1=0.93407882, Acc=0.92690000\nTop-11 features -> F1=0.92806923, Acc=0.92020000\nTop-13 features -> F1=0.93408968, Acc=0.92680000\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"heart attack\n","metadata":{}},{"cell_type":"markdown","source":"pca","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n# -----------------------------\n# 1Ô∏è‚É£ Load your dataset\n# -----------------------------\n\ndf = pd.read_csv(\"/kaggle/input/heart-attack-smoted/heart_attack_smoted.csv\")\n\nTARGET = \"Heart_Attack\"   # <-- change if your target column is different\n\nX = df.drop(TARGET, axis=1)\ny = df[TARGET].astype(int)\n\nprint(\"Original dimension:\", X.shape[1])\n\n# -----------------------------\n# 2Ô∏è‚É£ Train-test split\n# -----------------------------\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.20, stratify=y, random_state=42\n)\n\n# -----------------------------\n# 3Ô∏è‚É£ XGBoost Model (500 Iterations)\n# -----------------------------\nmodel = XGBClassifier(\n    n_estimators=500,          # <-- 500 boosting rounds\n    learning_rate=0.05,\n    max_depth=6,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    objective=\"binary:logistic\",\n    eval_metric=\"logloss\",\n    random_state=42,\n    n_jobs=-1\n)\n\nmodel.fit(X_train, y_train)\n\n# -----------------------------\n# 4Ô∏è‚É£ Predictions\n# -----------------------------\ny_pred = model.predict(X_test)\n\ntest_acc = accuracy_score(y_test, y_pred)\ntest_prec = precision_score(y_test, y_pred, zero_division=0)\ntest_rec = recall_score(y_test, y_pred, zero_division=0)\ntest_f1 = f1_score(y_test, y_pred, zero_division=0)\n\n# -----------------------------\n# 5Ô∏è‚É£ Results\n# -----------------------------\nprint(\"\\n===== XGBoost Results (500 Iterations) =====\")\nprint(f\"Test Accuracy : {test_acc:.8f}\")\nprint(f\"Precision     : {test_prec:.8f}\")\nprint(f\"Recall        : {test_rec:.8f}\")\nprint(f\"F1 Score      : {test_f1:.8f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T13:03:58.952449Z","iopub.execute_input":"2025-12-14T13:03:58.952747Z","iopub.status.idle":"2025-12-14T13:04:02.452826Z","shell.execute_reply.started":"2025-12-14T13:03:58.952718Z","shell.execute_reply":"2025-12-14T13:04:02.451742Z"}},"outputs":[{"name":"stdout","text":"Original dimension: 28\n\n===== XGBoost Results (500 Iterations) =====\nTest Accuracy : 0.92628060\nPrecision     : 1.00000000\nRecall        : 0.85256120\nF1 Score      : 0.92041353\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nimport numpy as np\nimport pandas as pd\nfrom xgboost import XGBClassifier   # üî• NEW\n\ndf = pd.read_csv(\"/kaggle/input/heart-attack-smoted/heart_attack_smoted.csv\")\n\nX = df.drop(\"Heart_Attack\",axis=1)\ny = df[\"Heart_Attack\"].astype(int)\n\n# 1) Scale\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# 2) PCA to retain 95% variance\npca = PCA(n_components=0.95)  \nX_pca = pca.fit_transform(X_scaled)\n\nprint(\"Original dimension:\", X.shape[1])\nprint(\"PCA dimension:\", X_pca.shape[1])\n\n# 3) Train/Test split\nX_train, X_test, y_train, y_test = train_test_split(\n    X_pca, y, test_size=0.20, stratify=y, random_state=42\n)\n\n# ---------------------------------------------------\n# 4) ‚ùó Replace CatBoost with XGBoost\n# ---------------------------------------------------\nmodel = XGBClassifier(\n    n_estimators=500,\n    learning_rate=0.05,\n    max_depth=6,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    objective=\"binary:logistic\",\n    eval_metric=\"logloss\",\n    tree_method=\"hist\"        # Fast training\n)\n\n# Fit\nmodel.fit(X_train, y_train)\n\n# Test prediction\ny_pred = model.predict(X_test)\n\n# Metrics\ntest_acc = accuracy_score(y_test, y_pred)\ntest_prec = precision_score(y_test, y_pred, zero_division=0)\ntest_rec = recall_score(y_test, y_pred, zero_division=0)\ntest_f1 = f1_score(y_test, y_pred, zero_division=0)\n\nprint(\"\\n=== PCA + XGBOOST MODEL RESULTS ===\")\nprint(\"Test Accuracy :\", test_acc)\nprint(\"Precision     :\", test_prec)\nprint(\"Recall        :\", test_rec)\nprint(\"F1 Score      :\", test_f1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T19:40:53.357738Z","iopub.execute_input":"2025-12-10T19:40:53.358137Z","iopub.status.idle":"2025-12-10T19:40:58.613780Z","shell.execute_reply.started":"2025-12-10T19:40:53.358111Z","shell.execute_reply":"2025-12-10T19:40:58.612873Z"}},"outputs":[{"name":"stdout","text":"Original dimension: 28\nPCA dimension: 27\n\n=== PCA + XGBOOST MODEL RESULTS ===\nTest Accuracy : 0.7612194016319129\nPrecision     : 0.7462606837606838\nRecall        : 0.7915911151405258\nF1 Score      : 0.7682578090629125\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"chi square","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\nfrom xgboost import XGBClassifier   # ‚Üê changed\n\n# -------------------------\n# 1. Prepare data\n# -------------------------\n\nprint(\"Original dimension:\", X.shape[1])\n\n# Train-test split (same style as your hybrid pipeline)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.20,\n    stratify=y,\n    random_state=42\n)\n\n# -------------------------\n# 2. Scale to non-negative for chi-square\n# -------------------------\nscaler = MinMaxScaler()   # maps features to [0, 1]\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# -------------------------\n# 3. Try different numbers of selected features\n# -------------------------\nk_values = [5, 10, 15, 20, 4,8]\n\nresults_chi2 = {}\n\nfor k in k_values:\n    print(\"\\n\" + \"=\"*60)\n    print(f\"CHI-SQUARE + XGBoost with top-{k} features\")\n    print(\"=\"*60)\n\n    # 3.1 Chi-Square feature selection\n    selector = SelectKBest(score_func=chi2, k=k)\n    X_train_k = selector.fit_transform(X_train_scaled, y_train)\n    X_test_k = selector.transform(X_test_scaled)\n\n    # Get selected feature names (from original X)\n    selected_mask = selector.get_support()\n    selected_features = X.columns[selected_mask].tolist()\n    print(f\"Selected {k} features:\")\n    print(selected_features)\n\n    # 3.2 Train XGBoost on selected features\n    model = XGBClassifier(\n        n_estimators=500,\n        learning_rate=0.05,\n        max_depth=6,\n        subsample=0.9,\n        colsample_bytree=0.9,\n        objective=\"binary:logistic\",\n        use_label_encoder=False,   # avoid label encoder warning\n        eval_metric=\"logloss\",\n        random_state=42,\n        n_jobs=-1\n    )\n\n    model.fit(X_train_k, y_train)\n\n    # 3.3 Evaluate on test set\n    y_pred = model.predict(X_test_k)\n\n    acc = accuracy_score(y_test, y_pred)\n    prec = precision_score(y_test, y_pred, zero_division=0)\n    rec = recall_score(y_test, y_pred, zero_division=0)\n    f1 = f1_score(y_test, y_pred, zero_division=0)\n\n    print(f\"Test Accuracy : {acc:.6f}\")\n    print(f\"Precision     : {prec:.6f}\")\n    print(f\"Recall        : {rec:.6f}\")\n    print(f\"F1 Score      : {f1:.6f}\")\n\n    # store results for later comparison\n    results_chi2[k] = {\n        \"features\": selected_features,\n        \"acc\": acc,\n        \"prec\": prec,\n        \"rec\": rec,\n        \"f1\": f1\n    }\n\nprint(\"\\n=========== SUMMARY: Chi-Square + XGBoost ===========\")\nfor k, info in results_chi2.items():\n    print(f\"Top-{k} features -> F1={info['f1']:.6f}, Acc={info['acc']:.6f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T19:42:43.422327Z","iopub.execute_input":"2025-12-10T19:42:43.422673Z","iopub.status.idle":"2025-12-10T19:42:53.508520Z","shell.execute_reply.started":"2025-12-10T19:42:43.422641Z","shell.execute_reply":"2025-12-10T19:42:53.507717Z"}},"outputs":[{"name":"stdout","text":"Original dimension: 28\n\n============================================================\nCHI-SQUARE + XGBoost with top-5 features\n============================================================\nSelected 5 features:\n['Diabetes', 'Angina', 'Heart_Disease_History', 'Medication', 'Obesity']\nTest Accuracy : 0.620014\nPrecision     : 0.611380\nRecall        : 0.658772\nF1 Score      : 0.634192\n\n============================================================\nCHI-SQUARE + XGBoost with top-10 features\n============================================================\nSelected 10 features:\n['Gender', 'Smoking', 'Diabetes', 'Family_History', 'Angina', 'Heart_Disease_History', 'Diet', 'Urban_Rural', 'Medication', 'Obesity']\nTest Accuracy : 0.636446\nPrecision     : 0.619944\nRecall        : 0.705236\nF1 Score      : 0.659845\n\n============================================================\nCHI-SQUARE + XGBoost with top-15 features\n============================================================\nSelected 15 features:\n['Gender', 'Region', 'Smoking', 'Diabetes', 'Family_History', 'Angina', 'Heart_Disease_History', 'Diet', 'Occupation', 'Income_Level', 'Education_Level', 'Marital_Status', 'Urban_Rural', 'Medication', 'Obesity']\nTest Accuracy : 0.692486\nPrecision     : 0.668352\nRecall        : 0.764166\nF1 Score      : 0.713055\n\n============================================================\nCHI-SQUARE + XGBoost with top-20 features\n============================================================\nSelected 20 features:\n['Gender', 'Region', 'Exercise_Level', 'Smoking', 'Alcohol_Consumption', 'Diabetes', 'Family_History', 'Angina', 'Heart_Disease_History', 'Diet', 'Occupation', 'Income_Level', 'Physical_Activity', 'Education_Level', 'Marital_Status', 'Urban_Rural', 'Medication', 'Health_Awareness', 'Daily_Water_Intake', 'Obesity']\nTest Accuracy : 0.904635\nPrecision     : 0.999860\nRecall        : 0.809383\nF1 Score      : 0.894595\n\n============================================================\nCHI-SQUARE + XGBoost with top-4 features\n============================================================\nSelected 4 features:\n['Diabetes', 'Angina', 'Heart_Disease_History', 'Medication']\nTest Accuracy : 0.609134\nPrecision     : 0.586182\nRecall        : 0.742294\nF1 Score      : 0.655066\n\n============================================================\nCHI-SQUARE + XGBoost with top-8 features\n============================================================\nSelected 8 features:\n['Gender', 'Smoking', 'Diabetes', 'Family_History', 'Angina', 'Heart_Disease_History', 'Medication', 'Obesity']\nTest Accuracy : 0.631743\nPrecision     : 0.619808\nRecall        : 0.681550\nF1 Score      : 0.649215\n\n=========== SUMMARY: Chi-Square + XGBoost ===========\nTop-5 features -> F1=0.634192, Acc=0.620014\nTop-10 features -> F1=0.659845, Acc=0.636446\nTop-15 features -> F1=0.713055, Acc=0.692486\nTop-20 features -> F1=0.894595, Acc=0.904635\nTop-4 features -> F1=0.655066, Acc=0.609134\nTop-8 features -> F1=0.649215, Acc=0.631743\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"mutual information","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import SelectKBest, mutual_info_classif\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\nfrom xgboost import XGBClassifier   # <-- changed\n\n# -------------------------\n# 1. Prepare data\n# -------------------------\n\nprint(\"Original dimension:\", X.shape[1])\n\n# Train-test split (keep same style as other experiments)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.20,\n    stratify=y,\n    random_state=42\n)\n\n# -------------------------\n# 2. Try different numbers of selected features\n# -------------------------\nk_values = [5,10,15,20,4,8]\n\nresults_mi = {}\n\nfor k in k_values:\n    print(\"\\n\" + \"=\"*60)\n    print(f\"MUTUAL INFORMATION + XGBoost with top-{k} features\")\n    print(\"=\"*60)\n\n    # 2.1 Mutual Information feature selection\n    selector = SelectKBest(score_func=mutual_info_classif, k=k)\n    X_train_k = selector.fit_transform(X_train, y_train)\n    X_test_k = selector.transform(X_test)\n\n    # Get selected feature names (from original X)\n    selected_mask = selector.get_support()\n    selected_features = X.columns[selected_mask].tolist()\n    print(f\"Selected {k} features:\")\n    print(selected_features)\n\n    # 2.2 Train XGBoost on selected features\n    model = XGBClassifier(\n        n_estimators=500,\n        learning_rate=0.05,\n        max_depth=6,\n        subsample=0.9,\n        colsample_bytree=0.9,\n        objective=\"binary:logistic\",\n        use_label_encoder=False,   # avoid label-encoder warning in older xgboost\n        eval_metric=\"logloss\",\n        random_state=42,\n        n_jobs=-1\n    )\n\n    model.fit(X_train_k, y_train)\n\n    # 2.3 Evaluate on test set\n    y_pred = model.predict(X_test_k)\n\n    acc = accuracy_score(y_test, y_pred)\n    prec = precision_score(y_test, y_pred, zero_division=0)\n    rec = recall_score(y_test, y_pred, zero_division=0)\n    f1 = f1_score(y_test, y_pred, zero_division=0)\n\n    print(f\"Test Accuracy : {acc:.6f}\")\n    print(f\"Precision     : {prec:.6f}\")\n    print(f\"Recall        : {rec:.6f}\")\n    print(f\"F1 Score      : {f1:.6f}\")\n\n    # store results for later comparison\n    results_mi[k] = {\n        \"features\": selected_features,\n        \"acc\": acc,\n        \"prec\": prec,\n        \"rec\": rec,\n        \"f1\": f1\n    }\n\nprint(\"\\n=========== SUMMARY: Mutual Information + XGBoost ===========\")\nfor k, info in results_mi.items():\n    print(f\"Top-{k} features -> F1={info['f1']:.6f}, Acc={info['acc']:.6f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T19:45:45.992265Z","iopub.execute_input":"2025-12-10T19:45:45.993198Z","iopub.status.idle":"2025-12-10T19:47:00.394995Z","shell.execute_reply.started":"2025-12-10T19:45:45.993170Z","shell.execute_reply":"2025-12-10T19:47:00.394130Z"}},"outputs":[{"name":"stdout","text":"Original dimension: 28\n\n============================================================\nMUTUAL INFORMATION + XGBoost with top-5 features\n============================================================\nSelected 5 features:\n['Age', 'BMI', 'Heart_Rate', 'Sleep_Hours', 'Daily_Water_Intake']\nTest Accuracy : 0.921691\nPrecision     : 1.000000\nRecall        : 0.843382\nF1 Score      : 0.915038\n\n============================================================\nMUTUAL INFORMATION + XGBoost with top-10 features\n============================================================\nSelected 10 features:\n['Age', 'Blood_Pressure', 'Cholesterol', 'BMI', 'Heart_Rate', 'Stress_Level', 'Sleep_Hours', 'Health_Awareness', 'Daily_Water_Intake', 'Mental_Health']\nTest Accuracy : 0.926167\nPrecision     : 0.999867\nRecall        : 0.852448\nF1 Score      : 0.920291\n\n============================================================\nMUTUAL INFORMATION + XGBoost with top-15 features\n============================================================\nSelected 15 features:\n['Age', 'Blood_Pressure', 'Cholesterol', 'BMI', 'Heart_Rate', 'Diabetes', 'Stress_Level', 'Heart_Disease_History', 'Sleep_Hours', 'Physical_Activity', 'Marital_Status', 'Medication', 'Health_Awareness', 'Daily_Water_Intake', 'Mental_Health']\nTest Accuracy : 0.926337\nPrecision     : 1.000000\nRecall        : 0.852675\nF1 Score      : 0.920480\n\n============================================================\nMUTUAL INFORMATION + XGBoost with top-20 features\n============================================================\nSelected 20 features:\n['Age', 'Gender', 'Blood_Pressure', 'Cholesterol', 'BMI', 'Heart_Rate', 'Smoking', 'Alcohol_Consumption', 'Diabetes', 'Family_History', 'Stress_Level', 'Angina', 'Diet', 'Sleep_Hours', 'Urban_Rural', 'Medication', 'Health_Awareness', 'Daily_Water_Intake', 'Mental_Health', 'Obesity']\nTest Accuracy : 0.926224\nPrecision     : 1.000000\nRecall        : 0.852448\nF1 Score      : 0.920347\n\n============================================================\nMUTUAL INFORMATION + XGBoost with top-4 features\n============================================================\nSelected 4 features:\n['Age', 'BMI', 'Sleep_Hours', 'Daily_Water_Intake']\nTest Accuracy : 0.913361\nPrecision     : 1.000000\nRecall        : 0.826723\nF1 Score      : 0.905143\n\n============================================================\nMUTUAL INFORMATION + XGBoost with top-8 features\n============================================================\nSelected 8 features:\n['Age', 'Blood_Pressure', 'BMI', 'Heart_Rate', 'Stress_Level', 'Sleep_Hours', 'Daily_Water_Intake', 'Mental_Health']\nTest Accuracy : 0.921974\nPrecision     : 1.000000\nRecall        : 0.843948\nF1 Score      : 0.915371\n\n=========== SUMMARY: Mutual Information + XGBoost ===========\nTop-5 features -> F1=0.915038, Acc=0.921691\nTop-10 features -> F1=0.920291, Acc=0.926167\nTop-15 features -> F1=0.920480, Acc=0.926337\nTop-20 features -> F1=0.920347, Acc=0.926224\nTop-4 features -> F1=0.905143, Acc=0.913361\nTop-8 features -> F1=0.915371, Acc=0.921974\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"recursive feature elimination","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import RFE\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\nfrom xgboost import XGBClassifier   # <-- changed\n\n# -------------------------\n# 1. Prepare data\n# -------------------------\n\nprint(\"Original dimension:\", X.shape[1])\n\n# Train-test split (same style as other baselines)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.20,\n    stratify=y,\n    random_state=42\n)\n\n# -------------------------\n# 2. Try different numbers of selected features with RFE\n# -------------------------\nk_values = [5,10,15,20,4,8]\n\nresults_rfe = {}\n\nfor k in k_values:\n    print(\"\\n\" + \"=\"*60)\n    print(f\"RFE (XGBoost) with top-{k} features\")\n    print(\"=\"*60)\n\n    # Base estimator for RFE (reduced iterations to save time)\n    base_model = XGBClassifier(\n        n_estimators=200,            # reduced for speed during RFE\n        learning_rate=0.05,\n        max_depth=6,\n        objective=\"binary:logistic\",\n        use_label_encoder=False,     # suppress older xgboost warning\n        eval_metric=\"logloss\",\n        random_state=42,\n        n_jobs=-1\n    )\n\n    # 2.1 RFE setup\n    selector = RFE(\n        estimator=base_model,\n        n_features_to_select=k,\n        step=1\n    )\n\n    # Fit RFE on training data\n    selector.fit(X_train, y_train)\n\n    # Transform train and test sets\n    X_train_k = selector.transform(X_train)\n    X_test_k = selector.transform(X_test)\n\n    # Get selected feature names\n    selected_mask = selector.get_support()\n    selected_features = X.columns[selected_mask].tolist()\n    print(f\"Selected {k} features:\")\n    print(selected_features)\n\n    # 2.2 Train a fresh XGBoost on the selected features (for fair comparison)\n    model = XGBClassifier(\n        n_estimators=500,\n        learning_rate=0.05,\n        max_depth=6,\n        subsample=0.9,\n        colsample_bytree=0.9,\n        objective=\"binary:logistic\",\n        use_label_encoder=False,\n        eval_metric=\"logloss\",\n        random_state=42,\n        n_jobs=-1\n    )\n\n    model.fit(X_train_k, y_train)\n\n    # 2.3 Evaluate on test set\n    y_pred = model.predict(X_test_k)\n\n    acc = accuracy_score(y_test, y_pred)\n    prec = precision_score(y_test, y_pred, zero_division=0)\n    rec = recall_score(y_test, y_pred, zero_division=0)\n    f1 = f1_score(y_test, y_pred, zero_division=0)\n\n    print(f\"Test Accuracy : {acc:.6f}\")\n    print(f\"Precision     : {prec:.6f}\")\n    print(f\"Recall        : {rec:.6f}\")\n    print(f\"F1 Score      : {f1:.6f}\")\n\n    # store results for later comparison\n    results_rfe[k] = {\n        \"features\": selected_features,\n        \"acc\": acc,\n        \"prec\": prec,\n        \"rec\": rec,\n        \"f1\": f1\n    }\n\nprint(\"\\n=========== SUMMARY: RFE (XGBoost) ===========\")\nfor k, info in results_rfe.items():\n    print(f\"Top-{k} features -> F1={info['f1']:.6f}, Acc={info['acc']:.6f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T19:48:46.978542Z","iopub.execute_input":"2025-12-10T19:48:46.978897Z","iopub.status.idle":"2025-12-10T19:50:30.486459Z","shell.execute_reply.started":"2025-12-10T19:48:46.978866Z","shell.execute_reply":"2025-12-10T19:50:30.485533Z"}},"outputs":[{"name":"stdout","text":"Original dimension: 28\n\n============================================================\nRFE (XGBoost) with top-5 features\n============================================================\nSelected 5 features:\n['Diabetes', 'Stress_Level', 'Heart_Disease_History', 'Health_Awareness', 'Mental_Health']\nTest Accuracy : 0.925714\nPrecision     : 1.000000\nRecall        : 0.851428\nF1 Score      : 0.919753\n\n============================================================\nRFE (XGBoost) with top-10 features\n============================================================\nSelected 10 features:\n['Gender', 'Diabetes', 'Stress_Level', 'Angina', 'Heart_Disease_History', 'Medication', 'Health_Awareness', 'Daily_Water_Intake', 'Mental_Health', 'Obesity']\nTest Accuracy : 0.925941\nPrecision     : 0.999867\nRecall        : 0.851995\nF1 Score      : 0.920027\n\n============================================================\nRFE (XGBoost) with top-15 features\n============================================================\nSelected 15 features:\n['Age', 'Gender', 'Heart_Rate', 'Smoking', 'Diabetes', 'Family_History', 'Stress_Level', 'Angina', 'Heart_Disease_History', 'Sleep_Hours', 'Medication', 'Health_Awareness', 'Daily_Water_Intake', 'Mental_Health', 'Obesity']\nTest Accuracy : 0.926734\nPrecision     : 1.000000\nRecall        : 0.853468\nF1 Score      : 0.920942\n\n============================================================\nRFE (XGBoost) with top-20 features\n============================================================\nSelected 20 features:\n['Age', 'Gender', 'BMI', 'Heart_Rate', 'Smoking', 'Diabetes', 'Family_History', 'Stress_Level', 'Angina', 'Heart_Disease_History', 'Diet', 'Sleep_Hours', 'Income_Level', 'Marital_Status', 'Urban_Rural', 'Medication', 'Health_Awareness', 'Daily_Water_Intake', 'Mental_Health', 'Obesity']\nTest Accuracy : 0.926224\nPrecision     : 0.999734\nRecall        : 0.852675\nF1 Score      : 0.920367\n\n============================================================\nRFE (XGBoost) with top-4 features\n============================================================\nSelected 4 features:\n['Diabetes', 'Stress_Level', 'Health_Awareness', 'Mental_Health']\nTest Accuracy : 0.925714\nPrecision     : 1.000000\nRecall        : 0.851428\nF1 Score      : 0.919753\n\n============================================================\nRFE (XGBoost) with top-8 features\n============================================================\nSelected 8 features:\n['Diabetes', 'Stress_Level', 'Angina', 'Heart_Disease_History', 'Medication', 'Health_Awareness', 'Mental_Health', 'Obesity']\nTest Accuracy : 0.925714\nPrecision     : 1.000000\nRecall        : 0.851428\nF1 Score      : 0.919753\n\n=========== SUMMARY: RFE (XGBoost) ===========\nTop-5 features -> F1=0.919753, Acc=0.925714\nTop-10 features -> F1=0.920027, Acc=0.925941\nTop-15 features -> F1=0.920942, Acc=0.926734\nTop-20 features -> F1=0.920367, Acc=0.926224\nTop-4 features -> F1=0.919753, Acc=0.925714\nTop-8 features -> F1=0.919753, Acc=0.925714\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"l1 lasso feature selection + xgboost","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom xgboost import XGBClassifier   # <-- changed\nimport numpy as np\nimport pandas as pd\n\n\nfeature_names = X.columns.tolist()\n\nprint(\"Original dimension:\", X.shape[1])\n\n# Train Logistic regression with L1 penalty to select features\nlog_reg = LogisticRegression(penalty='l1', solver='liblinear', C=0.3, max_iter=2000)\nlog_reg.fit(X, y)\n\n# Get non-zero coefficients\ncoef = log_reg.coef_[0]\nselected_idx = np.where(coef != 0)[0]\nselected_features = [feature_names[i] for i in selected_idx]\n\nprint(\"\\nSelected features using L1 (Lasso):\")\nprint(selected_features)\nprint(\"Total selected:\", len(selected_features))\n\n# ----- XGBoost on selected features -----\n\nX_sel = X[selected_features]\nX_train, X_test, y_train, y_test = train_test_split(\n    X_sel, y, test_size=0.20, stratify=y, random_state=42\n)\n\nmodel = XGBClassifier(\n    n_estimators=500,\n    learning_rate=0.05,\n    max_depth=6,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    objective=\"binary:logistic\",\n    use_label_encoder=False,   # suppress older xgboost label-encoder warning\n    eval_metric=\"logloss\",\n    random_state=42,\n    n_jobs=-1\n)\n\nmodel.fit(X_train, y_train)\n\n# Predictions\ny_pred = model.predict(X_test)\n\ntest_acc = accuracy_score(y_test, y_pred)\ntest_prec = precision_score(y_test, y_pred, zero_division=0)\ntest_rec = recall_score(y_test, y_pred, zero_division=0)\ntest_f1 = f1_score(y_test, y_pred, zero_division=0)\n\nprint(\"\\n===== L1 (Lasso) + XGBoost Results =====\")\nprint(f\"Test Accuracy : {test_acc:.8f}\")\nprint(f\"Precision     : {test_prec:.8f}\")\nprint(f\"Recall        : {test_rec:.8f}\")\nprint(f\"F1 Score      : {test_f1:.8f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T19:51:01.205921Z","iopub.execute_input":"2025-12-10T19:51:01.206578Z","iopub.status.idle":"2025-12-10T19:51:09.229236Z","shell.execute_reply.started":"2025-12-10T19:51:01.206551Z","shell.execute_reply":"2025-12-10T19:51:09.228385Z"}},"outputs":[{"name":"stdout","text":"Original dimension: 28\n\nSelected features using L1 (Lasso):\n['Age', 'Gender', 'Region', 'Blood_Pressure', 'Cholesterol', 'BMI', 'Heart_Rate', 'Exercise_Level', 'Smoking', 'Alcohol_Consumption', 'Diabetes', 'Family_History', 'Stress_Level', 'Angina', 'Heart_Disease_History', 'Diet', 'Sleep_Hours', 'Occupation', 'Income_Level', 'Physical_Activity', 'Education_Level', 'Marital_Status', 'Urban_Rural', 'Medication', 'Health_Awareness', 'Daily_Water_Intake', 'Mental_Health', 'Obesity']\nTotal selected: 28\n\n===== L1 (Lasso) + XGBoost Results =====\nTest Accuracy : 0.92628060\nPrecision     : 1.00000000\nRecall        : 0.85256120\nF1 Score      : 0.92041353\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"xgboost importance + xgboost","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\nfrom xgboost import XGBClassifier\n\n# -------------------------\n# 1. Load data\n# -------------------------\n\nfeature_names = X.columns.tolist()\nprint(\"Original dimension:\", X.shape[1])\n\n# -------------------------\n# 2. Train XGBoost on ALL features to get importance\n# -------------------------\nbase_model = XGBClassifier(\n    n_estimators=500,\n    learning_rate=0.05,\n    max_depth=6,\n    objective=\"binary:logistic\",\n    use_label_encoder=False,\n    eval_metric=\"logloss\",\n    random_state=42,\n    n_jobs=-1\n)\n\nbase_model.fit(X, y)\n\n# Get feature importances from XGBoost\nimportances = base_model.feature_importances_\n# Sort indices by importance (descending)\nsorted_idx = np.argsort(importances)[::-1]\nsorted_features = [feature_names[i] for i in sorted_idx]\n\nprint(\"\\nTop 20 features by XGBoost importance:\")\nfor i in range(min(20, len(sorted_features))):\n    print(f\"{i+1:2d}. {sorted_features[i]}  (importance={importances[sorted_idx[i]]:.6f})\")\n\n# -------------------------\n# 3. Evaluate XGBoost using top-k important features\n# -------------------------\nK_values = [5, 10, 15, 20,4,8]\nresults_xgb_imp = {}\n\nprint(\"\\n============================================================\")\nprint(\"XGBOOST FEATURE IMPORTANCE + XGBOOST BASELINE\")\nprint(\"============================================================\")\n\nfor k in K_values:\n    top_k_features = sorted_features[:k]\n    X_sel = X[top_k_features]\n\n    # Same train-test strategy as other baselines\n    X_train, X_test, y_train, y_test = train_test_split(\n        X_sel, y,\n        test_size=0.20,\n        stratify=y,\n        random_state=42\n    )\n\n    model = XGBClassifier(\n        n_estimators=500,\n        learning_rate=0.05,\n        max_depth=6,\n        subsample=0.9,\n        colsample_bytree=0.9,\n        objective=\"binary:logistic\",\n        use_label_encoder=False,\n        eval_metric=\"logloss\",\n        random_state=42,\n        n_jobs=-1\n    )\n\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n\n    acc = accuracy_score(y_test, y_pred)\n    prec = precision_score(y_test, y_pred, zero_division=0)\n    rec = recall_score(y_test, y_pred, zero_division=0)\n    f1 = f1_score(y_test, y_pred, zero_division=0)\n\n    print(f\"\\n============================================================\")\n    print(f\"XGBoost-Importance + XGBoost with top-{k} features\")\n    print(\"============================================================\")\n    print(f\"Selected {k} features:\")\n    print(top_k_features)\n    print(f\"Test Accuracy : {acc:.8f}\")\n    print(f\"Precision     : {prec:.8f}\")\n    print(f\"Recall        : {rec:.8f}\")\n    print(f\"F1 Score      : {f1:.8f}\")\n\n    results_xgb_imp[k] = {\n        \"features\": top_k_features,\n        \"acc\": acc,\n        \"prec\": prec,\n        \"rec\": rec,\n        \"f1\": f1\n    }\n\nprint(\"\\n=========== SUMMARY: XGBoost Importance + XGBoost ===========\")\nfor k, info in results_xgb_imp.items():\n    print(f\"Top-{k} features -> F1={info['f1']:.8f}, Acc={info['acc']:.8f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T19:54:17.400211Z","iopub.execute_input":"2025-12-10T19:54:17.400977Z","iopub.status.idle":"2025-12-10T19:54:30.501414Z","shell.execute_reply.started":"2025-12-10T19:54:17.400951Z","shell.execute_reply":"2025-12-10T19:54:30.500638Z"}},"outputs":[{"name":"stdout","text":"Original dimension: 28\n\nTop 20 features by XGBoost importance:\n 1. Health_Awareness  (importance=0.390064)\n 2. Stress_Level  (importance=0.152729)\n 3. Mental_Health  (importance=0.152697)\n 4. Diabetes  (importance=0.038637)\n 5. Heart_Disease_History  (importance=0.031578)\n 6. Medication  (importance=0.030690)\n 7. Angina  (importance=0.021057)\n 8. Obesity  (importance=0.019911)\n 9. Daily_Water_Intake  (importance=0.013725)\n10. Gender  (importance=0.013648)\n11. Heart_Rate  (importance=0.013400)\n12. Family_History  (importance=0.012155)\n13. Age  (importance=0.010369)\n14. Sleep_Hours  (importance=0.009569)\n15. Marital_Status  (importance=0.007509)\n16. Smoking  (importance=0.007483)\n17. Diet  (importance=0.006898)\n18. Income_Level  (importance=0.006882)\n19. Physical_Activity  (importance=0.006649)\n20. Education_Level  (importance=0.006305)\n\n============================================================\nXGBOOST FEATURE IMPORTANCE + XGBOOST BASELINE\n============================================================\n\n============================================================\nXGBoost-Importance + XGBoost with top-5 features\n============================================================\nSelected 5 features:\n['Health_Awareness', 'Stress_Level', 'Mental_Health', 'Diabetes', 'Heart_Disease_History']\nTest Accuracy : 0.92571396\nPrecision     : 1.00000000\nRecall        : 0.85142792\nF1 Score      : 0.91975271\n\n============================================================\nXGBoost-Importance + XGBoost with top-10 features\n============================================================\nSelected 10 features:\n['Health_Awareness', 'Stress_Level', 'Mental_Health', 'Diabetes', 'Heart_Disease_History', 'Medication', 'Angina', 'Obesity', 'Daily_Water_Intake', 'Gender']\nTest Accuracy : 0.92616727\nPrecision     : 0.99986707\nRecall        : 0.85244787\nF1 Score      : 0.92029118\n\n============================================================\nXGBoost-Importance + XGBoost with top-15 features\n============================================================\nSelected 15 features:\n['Health_Awareness', 'Stress_Level', 'Mental_Health', 'Diabetes', 'Heart_Disease_History', 'Medication', 'Angina', 'Obesity', 'Daily_Water_Intake', 'Gender', 'Heart_Rate', 'Family_History', 'Age', 'Sleep_Hours', 'Marital_Status']\nTest Accuracy : 0.92673391\nPrecision     : 0.99986725\nRecall        : 0.85358114\nF1 Score      : 0.92095127\n\n============================================================\nXGBoost-Importance + XGBoost with top-20 features\n============================================================\nSelected 20 features:\n['Health_Awareness', 'Stress_Level', 'Mental_Health', 'Diabetes', 'Heart_Disease_History', 'Medication', 'Angina', 'Obesity', 'Daily_Water_Intake', 'Gender', 'Heart_Rate', 'Family_History', 'Age', 'Sleep_Hours', 'Marital_Status', 'Smoking', 'Diet', 'Income_Level', 'Physical_Activity', 'Education_Level']\nTest Accuracy : 0.92673391\nPrecision     : 1.00000000\nRecall        : 0.85346782\nF1 Score      : 0.92094161\n\n============================================================\nXGBoost-Importance + XGBoost with top-4 features\n============================================================\nSelected 4 features:\n['Health_Awareness', 'Stress_Level', 'Mental_Health', 'Diabetes']\nTest Accuracy : 0.92571396\nPrecision     : 1.00000000\nRecall        : 0.85142792\nF1 Score      : 0.91975271\n\n============================================================\nXGBoost-Importance + XGBoost with top-8 features\n============================================================\nSelected 8 features:\n['Health_Awareness', 'Stress_Level', 'Mental_Health', 'Diabetes', 'Heart_Disease_History', 'Medication', 'Angina', 'Obesity']\nTest Accuracy : 0.92565730\nPrecision     : 0.99986692\nRecall        : 0.85142792\nF1 Score      : 0.91969641\n\n=========== SUMMARY: XGBoost Importance + XGBoost ===========\nTop-5 features -> F1=0.91975271, Acc=0.92571396\nTop-10 features -> F1=0.92029118, Acc=0.92616727\nTop-15 features -> F1=0.92095127, Acc=0.92673391\nTop-20 features -> F1=0.92094161, Acc=0.92673391\nTop-4 features -> F1=0.91975271, Acc=0.92571396\nTop-8 features -> F1=0.91969641, Acc=0.92565730\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"2019-ciciddos_full\n\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n# -----------------------------\n# 1Ô∏è‚É£ Load your dataset\n# -----------------------------\ndf = pd.read_csv(\"/kaggle/input/cicid-2019-cleaned/cicddos2019_cleaned_final.csv\")  # change file if needed\n\nTARGET = \"Label\"   # <-- change if your target column is different\n\nX = df.drop(TARGET, axis=1)\ny = df[TARGET].astype(int)\n\nprint(\"Original dimension:\", X.shape[1])\n\n# -----------------------------\n# 2Ô∏è‚É£ Train-test split\n# -----------------------------\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.20, stratify=y, random_state=42\n)\n\n# -----------------------------\n# 3Ô∏è‚É£ XGBoost Model (500 Iterations)\n# -----------------------------\nmodel = XGBClassifier(\n    n_estimators=500,          # <-- 500 boosting rounds\n    learning_rate=0.05,\n    max_depth=6,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    objective=\"binary:logistic\",\n    eval_metric=\"logloss\",\n    random_state=42,\n    n_jobs=-1\n)\n\nmodel.fit(X_train, y_train)\n\n# -----------------------------\n# 4Ô∏è‚É£ Predictions\n# -----------------------------\ny_pred = model.predict(X_test)\n\ntest_acc = accuracy_score(y_test, y_pred)\ntest_prec = precision_score(y_test, y_pred, zero_division=0)\ntest_rec = recall_score(y_test, y_pred, zero_division=0)\ntest_f1 = f1_score(y_test, y_pred, zero_division=0)\n\n# -----------------------------\n# 5Ô∏è‚É£ Results\n# -----------------------------\nprint(\"\\n===== XGBoost Results (500 Iterations) =====\")\nprint(f\"Test Accuracy : {test_acc:.8f}\")\nprint(f\"Precision     : {test_prec:.8f}\")\nprint(f\"Recall        : {test_rec:.8f}\")\nprint(f\"F1 Score      : {test_f1:.8f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T13:02:04.500082Z","iopub.execute_input":"2025-12-14T13:02:04.500409Z","iopub.status.idle":"2025-12-14T13:02:17.697069Z","shell.execute_reply.started":"2025-12-14T13:02:04.500360Z","shell.execute_reply":"2025-12-14T13:02:17.696158Z"}},"outputs":[{"name":"stdout","text":"Original dimension: 77\n\n===== XGBoost Results (500 Iterations) =====\nTest Accuracy : 0.99885000\nPrecision     : 0.99939934\nRecall        : 0.99830000\nF1 Score      : 0.99884937\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"pca","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nimport numpy as np\nimport pandas as pd\nfrom xgboost import XGBClassifier   # üî• NEW\n\ndf = pd.read_csv(\"/kaggle/input/cicid-2019-cleaned/cicddos2019_cleaned_final.csv\")\n\nX = df.drop(\"Label\",axis=1)\ny = df[\"Label\"].astype(int)\n\n# 1) Scale\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# 2) PCA to retain 95% variance\npca = PCA(n_components=0.95)  \nX_pca = pca.fit_transform(X_scaled)\n\nprint(\"Original dimension:\", X.shape[1])\nprint(\"PCA dimension:\", X_pca.shape[1])\n\n# 3) Train/Test split\nX_train, X_test, y_train, y_test = train_test_split(\n    X_pca, y, test_size=0.20, stratify=y, random_state=42\n)\n\n# ---------------------------------------------------\n# 4) ‚ùó Replace CatBoost with XGBoost\n# ---------------------------------------------------\nmodel = XGBClassifier(\n    n_estimators=500,\n    learning_rate=0.05,\n    max_depth=6,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    objective=\"binary:logistic\",\n    eval_metric=\"logloss\",\n    tree_method=\"hist\"        # Fast training\n)\n\n# Fit\nmodel.fit(X_train, y_train)\n\n# Test prediction\ny_pred = model.predict(X_test)\n\n# Metrics\ntest_acc = accuracy_score(y_test, y_pred)\ntest_prec = precision_score(y_test, y_pred, zero_division=0)\ntest_rec = recall_score(y_test, y_pred, zero_division=0)\ntest_f1 = f1_score(y_test, y_pred, zero_division=0)\n\nprint(\"\\n=== PCA + XGBOOST MODEL RESULTS ===\")\nprint(\"Test Accuracy :\", test_acc)\nprint(\"Precision     :\", test_prec)\nprint(\"Recall        :\", test_rec)\nprint(\"F1 Score      :\", test_f1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T20:32:13.177566Z","iopub.execute_input":"2025-12-10T20:32:13.177943Z","iopub.status.idle":"2025-12-10T20:32:18.904795Z","shell.execute_reply.started":"2025-12-10T20:32:13.177916Z","shell.execute_reply":"2025-12-10T20:32:18.903385Z"}},"outputs":[{"name":"stdout","text":"Original dimension: 77\nPCA dimension: 24\n\n=== PCA + XGBOOST MODEL RESULTS ===\nTest Accuracy : 0.9985\nPrecision     : 0.9987992795677406\nRecall        : 0.9982\nF1 Score      : 0.9984995498649595\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"chi square","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\nfrom xgboost import XGBClassifier   # ‚Üê changed\n\n# -------------------------\n# 1. Prepare data\n# -------------------------\n\nprint(\"Original dimension:\", X.shape[1])\n\n# Train-test split (same style as your hybrid pipeline)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.20,\n    stratify=y,\n    random_state=42\n)\n\n# -------------------------\n# 2. Scale to non-negative for chi-square\n# -------------------------\nscaler = MinMaxScaler()   # maps features to [0, 1]\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# -------------------------\n# 3. Try different numbers of selected features\n# -------------------------\nk_values = [5, 10, 15, 20, 30,31,9]\n\nresults_chi2 = {}\n\nfor k in k_values:\n    print(\"\\n\" + \"=\"*60)\n    print(f\"CHI-SQUARE + XGBoost with top-{k} features\")\n    print(\"=\"*60)\n\n    # 3.1 Chi-Square feature selection\n    selector = SelectKBest(score_func=chi2, k=k)\n    X_train_k = selector.fit_transform(X_train_scaled, y_train)\n    X_test_k = selector.transform(X_test_scaled)\n\n    # Get selected feature names (from original X)\n    selected_mask = selector.get_support()\n    selected_features = X.columns[selected_mask].tolist()\n    print(f\"Selected {k} features:\")\n    print(selected_features)\n\n    # 3.2 Train XGBoost on selected features\n    model = XGBClassifier(\n        n_estimators=500,\n        learning_rate=0.05,\n        max_depth=6,\n        subsample=0.9,\n        colsample_bytree=0.9,\n        objective=\"binary:logistic\",\n        use_label_encoder=False,   # avoid label encoder warning\n        eval_metric=\"logloss\",\n        random_state=42,\n        n_jobs=-1\n    )\n\n    model.fit(X_train_k, y_train)\n\n    # 3.3 Evaluate on test set\n    y_pred = model.predict(X_test_k)\n\n    acc = accuracy_score(y_test, y_pred)\n    prec = precision_score(y_test, y_pred, zero_division=0)\n    rec = recall_score(y_test, y_pred, zero_division=0)\n    f1 = f1_score(y_test, y_pred, zero_division=0)\n\n    print(f\"Test Accuracy : {acc:.6f}\")\n    print(f\"Precision     : {prec:.6f}\")\n    print(f\"Recall        : {rec:.6f}\")\n    print(f\"F1 Score      : {f1:.6f}\")\n\n    # store results for later comparison\n    results_chi2[k] = {\n        \"features\": selected_features,\n        \"acc\": acc,\n        \"prec\": prec,\n        \"rec\": rec,\n        \"f1\": f1\n    }\n\nprint(\"\\n=========== SUMMARY: Chi-Square + XGBoost ===========\")\nfor k, info in results_chi2.items():\n    print(f\"Top-{k} features -> F1={info['f1']:.6f}, Acc={info['acc']:.6f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T20:58:30.711948Z","iopub.execute_input":"2025-12-10T20:58:30.712287Z","iopub.status.idle":"2025-12-10T20:58:46.814392Z","shell.execute_reply.started":"2025-12-10T20:58:30.712265Z","shell.execute_reply":"2025-12-10T20:58:46.813521Z"}},"outputs":[{"name":"stdout","text":"Original dimension: 77\n\n============================================================\nCHI-SQUARE + XGBoost with top-5 features\n============================================================\nSelected 5 features:\n['Fwd Packet Length Min', 'Packet Length Min', 'URG Flag Count', 'CWE Flag Count', 'Avg Fwd Segment Size']\nTest Accuracy : 0.967400\nPrecision     : 0.945907\nRecall        : 0.991500\nF1 Score      : 0.968167\n\n============================================================\nCHI-SQUARE + XGBoost with top-10 features\n============================================================\nSelected 10 features:\n['Fwd Packet Length Min', 'Fwd Packet Length Mean', 'Flow Bytes/s', 'Fwd PSH Flags', 'Packet Length Min', 'RST Flag Count', 'URG Flag Count', 'CWE Flag Count', 'Avg Packet Size', 'Avg Fwd Segment Size']\nTest Accuracy : 0.970700\nPrecision     : 0.947178\nRecall        : 0.997000\nF1 Score      : 0.971451\n\n============================================================\nCHI-SQUARE + XGBoost with top-15 features\n============================================================\nSelected 15 features:\n['Protocol', 'Fwd Packet Length Min', 'Fwd Packet Length Mean', 'Flow Bytes/s', 'Flow Packets/s', 'Fwd PSH Flags', 'Fwd Packets/s', 'Packet Length Min', 'Packet Length Mean', 'RST Flag Count', 'URG Flag Count', 'CWE Flag Count', 'Avg Packet Size', 'Avg Fwd Segment Size', 'Init Fwd Win Bytes']\nTest Accuracy : 0.998550\nPrecision     : 0.998899\nRecall        : 0.998200\nF1 Score      : 0.998549\n\n============================================================\nCHI-SQUARE + XGBoost with top-20 features\n============================================================\nSelected 20 features:\n['Protocol', 'Fwd Packet Length Min', 'Fwd Packet Length Mean', 'Bwd Packet Length Min', 'Flow Bytes/s', 'Flow Packets/s', 'Bwd IAT Total', 'Fwd PSH Flags', 'Fwd Packets/s', 'Packet Length Min', 'Packet Length Mean', 'RST Flag Count', 'URG Flag Count', 'CWE Flag Count', 'Down/Up Ratio', 'Avg Packet Size', 'Avg Fwd Segment Size', 'Avg Bwd Segment Size', 'Init Fwd Win Bytes', 'Init Bwd Win Bytes']\nTest Accuracy : 0.998900\nPrecision     : 0.999399\nRecall        : 0.998400\nF1 Score      : 0.998899\n\n============================================================\nCHI-SQUARE + XGBoost with top-30 features\n============================================================\nSelected 30 features:\n['Protocol', 'Flow Duration', 'Fwd Packet Length Max', 'Fwd Packet Length Min', 'Fwd Packet Length Mean', 'Fwd Packet Length Std', 'Bwd Packet Length Max', 'Bwd Packet Length Min', 'Bwd Packet Length Mean', 'Bwd Packet Length Std', 'Flow Bytes/s', 'Flow Packets/s', 'Flow IAT Mean', 'Bwd IAT Total', 'Bwd IAT Max', 'Fwd PSH Flags', 'Fwd Packets/s', 'Packet Length Min', 'Packet Length Mean', 'Packet Length Std', 'RST Flag Count', 'URG Flag Count', 'CWE Flag Count', 'Down/Up Ratio', 'Avg Packet Size', 'Avg Fwd Segment Size', 'Avg Bwd Segment Size', 'Init Fwd Win Bytes', 'Init Bwd Win Bytes', 'Idle Min']\nTest Accuracy : 0.998850\nPrecision     : 0.999399\nRecall        : 0.998300\nF1 Score      : 0.998849\n\n============================================================\nCHI-SQUARE + XGBoost with top-31 features\n============================================================\nSelected 31 features:\n['Protocol', 'Flow Duration', 'Fwd Packet Length Max', 'Fwd Packet Length Min', 'Fwd Packet Length Mean', 'Fwd Packet Length Std', 'Bwd Packet Length Max', 'Bwd Packet Length Min', 'Bwd Packet Length Mean', 'Bwd Packet Length Std', 'Flow Bytes/s', 'Flow Packets/s', 'Flow IAT Mean', 'Bwd IAT Total', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Fwd Packets/s', 'Packet Length Min', 'Packet Length Mean', 'Packet Length Std', 'RST Flag Count', 'URG Flag Count', 'CWE Flag Count', 'Down/Up Ratio', 'Avg Packet Size', 'Avg Fwd Segment Size', 'Avg Bwd Segment Size', 'Init Fwd Win Bytes', 'Init Bwd Win Bytes', 'Idle Min']\nTest Accuracy : 0.998850\nPrecision     : 0.999399\nRecall        : 0.998300\nF1 Score      : 0.998849\n\n============================================================\nCHI-SQUARE + XGBoost with top-9 features\n============================================================\nSelected 9 features:\n['Fwd Packet Length Min', 'Fwd Packet Length Mean', 'Flow Bytes/s', 'Fwd PSH Flags', 'Packet Length Min', 'RST Flag Count', 'URG Flag Count', 'CWE Flag Count', 'Avg Fwd Segment Size']\nTest Accuracy : 0.970650\nPrecision     : 0.947173\nRecall        : 0.996900\nF1 Score      : 0.971401\n\n=========== SUMMARY: Chi-Square + XGBoost ===========\nTop-5 features -> F1=0.968167, Acc=0.967400\nTop-10 features -> F1=0.971451, Acc=0.970700\nTop-15 features -> F1=0.998549, Acc=0.998550\nTop-20 features -> F1=0.998899, Acc=0.998900\nTop-30 features -> F1=0.998849, Acc=0.998850\nTop-31 features -> F1=0.998849, Acc=0.998850\nTop-9 features -> F1=0.971401, Acc=0.970650\n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"mutual information","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import SelectKBest, mutual_info_classif\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\nfrom xgboost import XGBClassifier   # <-- changed\n\n# -------------------------\n# 1. Prepare data\n# -------------------------\n\nprint(\"Original dimension:\", X.shape[1])\n\n# Train-test split (keep same style as other experiments)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.20,\n    stratify=y,\n    random_state=42\n)\n\n# -------------------------\n# 2. Try different numbers of selected features\n# -------------------------\nk_values = [5, 10, 15, 20, 30,31,9]\n\nresults_mi = {}\n\nfor k in k_values:\n    print(\"\\n\" + \"=\"*60)\n    print(f\"MUTUAL INFORMATION + XGBoost with top-{k} features\")\n    print(\"=\"*60)\n\n    # 2.1 Mutual Information feature selection\n    selector = SelectKBest(score_func=mutual_info_classif, k=k)\n    X_train_k = selector.fit_transform(X_train, y_train)\n    X_test_k = selector.transform(X_test)\n\n    # Get selected feature names (from original X)\n    selected_mask = selector.get_support()\n    selected_features = X.columns[selected_mask].tolist()\n    print(f\"Selected {k} features:\")\n    print(selected_features)\n\n    # 2.2 Train XGBoost on selected features\n    model = XGBClassifier(\n        n_estimators=500,\n        learning_rate=0.05,\n        max_depth=6,\n        subsample=0.9,\n        colsample_bytree=0.9,\n        objective=\"binary:logistic\",\n        use_label_encoder=False,   # avoid label-encoder warning in older xgboost\n        eval_metric=\"logloss\",\n        random_state=42,\n        n_jobs=-1\n    )\n\n    model.fit(X_train_k, y_train)\n\n    # 2.3 Evaluate on test set\n    y_pred = model.predict(X_test_k)\n\n    acc = accuracy_score(y_test, y_pred)\n    prec = precision_score(y_test, y_pred, zero_division=0)\n    rec = recall_score(y_test, y_pred, zero_division=0)\n    f1 = f1_score(y_test, y_pred, zero_division=0)\n\n    print(f\"Test Accuracy : {acc:.6f}\")\n    print(f\"Precision     : {prec:.6f}\")\n    print(f\"Recall        : {rec:.6f}\")\n    print(f\"F1 Score      : {f1:.6f}\")\n\n    # store results for later comparison\n    results_mi[k] = {\n        \"features\": selected_features,\n        \"acc\": acc,\n        \"prec\": prec,\n        \"rec\": rec,\n        \"f1\": f1\n    }\n\nprint(\"\\n=========== SUMMARY: Mutual Information + XGBoost ===========\")\nfor k, info in results_mi.items():\n    print(f\"Top-{k} features -> F1={info['f1']:.6f}, Acc={info['acc']:.6f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T21:00:10.408797Z","iopub.execute_input":"2025-12-10T21:00:10.409616Z","iopub.status.idle":"2025-12-10T21:04:23.089449Z","shell.execute_reply.started":"2025-12-10T21:00:10.409590Z","shell.execute_reply":"2025-12-10T21:04:23.088594Z"}},"outputs":[{"name":"stdout","text":"Original dimension: 77\n\n============================================================\nMUTUAL INFORMATION + XGBoost with top-5 features\n============================================================\nSelected 5 features:\n['Fwd Packet Length Mean', 'Packet Length Min', 'Packet Length Mean', 'Avg Packet Size', 'Avg Fwd Segment Size']\nTest Accuracy : 0.958750\nPrecision     : 0.987669\nRecall        : 0.929100\nF1 Score      : 0.957490\n\n============================================================\nMUTUAL INFORMATION + XGBoost with top-10 features\n============================================================\nSelected 10 features:\n['Fwd Packets Length Total', 'Fwd Packet Length Min', 'Fwd Packet Length Mean', 'Flow Bytes/s', 'Packet Length Min', 'Packet Length Max', 'Packet Length Mean', 'Avg Packet Size', 'Avg Fwd Segment Size', 'Subflow Fwd Bytes']\nTest Accuracy : 0.964250\nPrecision     : 0.998604\nRecall        : 0.929800\nF1 Score      : 0.962974\n\n============================================================\nMUTUAL INFORMATION + XGBoost with top-15 features\n============================================================\nSelected 15 features:\n['Fwd Packets Length Total', 'Fwd Packet Length Max', 'Fwd Packet Length Min', 'Fwd Packet Length Mean', 'Flow Bytes/s', 'Flow Packets/s', 'Flow IAT Mean', 'Flow IAT Max', 'Fwd Packets/s', 'Packet Length Min', 'Packet Length Max', 'Packet Length Mean', 'Avg Packet Size', 'Avg Fwd Segment Size', 'Subflow Fwd Bytes']\nTest Accuracy : 0.995950\nPrecision     : 0.996496\nRecall        : 0.995400\nF1 Score      : 0.995948\n\n============================================================\nMUTUAL INFORMATION + XGBoost with top-20 features\n============================================================\nSelected 20 features:\n['Flow Duration', 'Fwd Packets Length Total', 'Fwd Packet Length Max', 'Fwd Packet Length Min', 'Fwd Packet Length Mean', 'Flow Bytes/s', 'Flow Packets/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Fwd IAT Max', 'Fwd Packets/s', 'Bwd Packets/s', 'Packet Length Min', 'Packet Length Max', 'Packet Length Mean', 'Packet Length Std', 'Avg Packet Size', 'Avg Fwd Segment Size', 'Subflow Fwd Bytes']\nTest Accuracy : 0.996400\nPrecision     : 0.996797\nRecall        : 0.996000\nF1 Score      : 0.996399\n\n============================================================\nMUTUAL INFORMATION + XGBoost with top-30 features\n============================================================\nSelected 30 features:\n['Flow Duration', 'Total Backward Packets', 'Fwd Packets Length Total', 'Fwd Packet Length Max', 'Fwd Packet Length Min', 'Fwd Packet Length Mean', 'Flow Bytes/s', 'Flow Packets/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Fwd IAT Total', 'Fwd IAT Mean', 'Fwd IAT Max', 'Bwd IAT Total', 'Bwd IAT Mean', 'Bwd IAT Max', 'Bwd Header Length', 'Fwd Packets/s', 'Bwd Packets/s', 'Packet Length Min', 'Packet Length Max', 'Packet Length Mean', 'Packet Length Std', 'Packet Length Variance', 'Avg Packet Size', 'Avg Fwd Segment Size', 'Subflow Fwd Bytes', 'Subflow Bwd Packets', 'Init Fwd Win Bytes']\nTest Accuracy : 0.998450\nPrecision     : 0.998699\nRecall        : 0.998200\nF1 Score      : 0.998450\n\n============================================================\nMUTUAL INFORMATION + XGBoost with top-31 features\n============================================================\nSelected 31 features:\n['Flow Duration', 'Total Backward Packets', 'Fwd Packets Length Total', 'Fwd Packet Length Max', 'Fwd Packet Length Min', 'Fwd Packet Length Mean', 'Flow Bytes/s', 'Flow Packets/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Fwd IAT Total', 'Fwd IAT Mean', 'Fwd IAT Max', 'Bwd IAT Total', 'Bwd IAT Mean', 'Bwd IAT Max', 'Bwd Header Length', 'Fwd Packets/s', 'Bwd Packets/s', 'Packet Length Min', 'Packet Length Max', 'Packet Length Mean', 'Packet Length Std', 'Packet Length Variance', 'Avg Packet Size', 'Avg Fwd Segment Size', 'Subflow Fwd Bytes', 'Subflow Bwd Packets', 'Subflow Bwd Bytes', 'Init Fwd Win Bytes']\nTest Accuracy : 0.998450\nPrecision     : 0.998699\nRecall        : 0.998200\nF1 Score      : 0.998450\n\n============================================================\nMUTUAL INFORMATION + XGBoost with top-9 features\n============================================================\nSelected 9 features:\n['Fwd Packet Length Min', 'Fwd Packet Length Mean', 'Flow Bytes/s', 'Packet Length Min', 'Packet Length Max', 'Packet Length Mean', 'Avg Packet Size', 'Avg Fwd Segment Size', 'Subflow Fwd Bytes']\nTest Accuracy : 0.964300\nPrecision     : 0.998711\nRecall        : 0.929800\nF1 Score      : 0.963024\n\n=========== SUMMARY: Mutual Information + XGBoost ===========\nTop-5 features -> F1=0.957490, Acc=0.958750\nTop-10 features -> F1=0.962974, Acc=0.964250\nTop-15 features -> F1=0.995948, Acc=0.995950\nTop-20 features -> F1=0.996399, Acc=0.996400\nTop-30 features -> F1=0.998450, Acc=0.998450\nTop-31 features -> F1=0.998450, Acc=0.998450\nTop-9 features -> F1=0.963024, Acc=0.964300\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"recursive feature elimination","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import RFE\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\nfrom xgboost import XGBClassifier   # <-- changed\n\n# -------------------------\n# 1. Prepare data\n# -------------------------\n\nprint(\"Original dimension:\", X.shape[1])\n\n# Train-test split (same style as other baselines)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.20,\n    stratify=y,\n    random_state=42\n)\n\n# -------------------------\n# 2. Try different numbers of selected features with RFE\n# -------------------------\nk_values = [5, 10, 15, 20, 30,31,9]\n\nresults_rfe = {}\n\nfor k in k_values:\n    print(\"\\n\" + \"=\"*60)\n    print(f\"RFE (XGBoost) with top-{k} features\")\n    print(\"=\"*60)\n\n    # Base estimator for RFE (reduced iterations to save time)\n    base_model = XGBClassifier(\n        n_estimators=200,            # reduced for speed during RFE\n        learning_rate=0.05,\n        max_depth=6,\n        objective=\"binary:logistic\",\n        use_label_encoder=False,     # suppress older xgboost warning\n        eval_metric=\"logloss\",\n        random_state=42,\n        n_jobs=-1\n    )\n\n    # 2.1 RFE setup\n    selector = RFE(\n        estimator=base_model,\n        n_features_to_select=k,\n        step=1\n    )\n\n    # Fit RFE on training data\n    selector.fit(X_train, y_train)\n\n    # Transform train and test sets\n    X_train_k = selector.transform(X_train)\n    X_test_k = selector.transform(X_test)\n\n    # Get selected feature names\n    selected_mask = selector.get_support()\n    selected_features = X.columns[selected_mask].tolist()\n    print(f\"Selected {k} features:\")\n    print(selected_features)\n\n    # 2.2 Train a fresh XGBoost on the selected features (for fair comparison)\n    model = XGBClassifier(\n        n_estimators=500,\n        learning_rate=0.05,\n        max_depth=6,\n        subsample=0.9,\n        colsample_bytree=0.9,\n        objective=\"binary:logistic\",\n        use_label_encoder=False,\n        eval_metric=\"logloss\",\n        random_state=42,\n        n_jobs=-1\n    )\n\n    model.fit(X_train_k, y_train)\n\n    # 2.3 Evaluate on test set\n    y_pred = model.predict(X_test_k)\n\n    acc = accuracy_score(y_test, y_pred)\n    prec = precision_score(y_test, y_pred, zero_division=0)\n    rec = recall_score(y_test, y_pred, zero_division=0)\n    f1 = f1_score(y_test, y_pred, zero_division=0)\n\n    print(f\"Test Accuracy : {acc:.6f}\")\n    print(f\"Precision     : {prec:.6f}\")\n    print(f\"Recall        : {rec:.6f}\")\n    print(f\"F1 Score      : {f1:.6f}\")\n\n    # store results for later comparison\n    results_rfe[k] = {\n        \"features\": selected_features,\n        \"acc\": acc,\n        \"prec\": prec,\n        \"rec\": rec,\n        \"f1\": f1\n    }\n\nprint(\"\\n=========== SUMMARY: RFE (XGBoost) ===========\")\nfor k, info in results_rfe.items():\n    print(f\"Top-{k} features -> F1={info['f1']:.6f}, Acc={info['acc']:.6f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T21:05:00.879868Z","iopub.execute_input":"2025-12-10T21:05:00.880231Z","iopub.status.idle":"2025-12-10T21:17:27.987203Z","shell.execute_reply.started":"2025-12-10T21:05:00.880205Z","shell.execute_reply":"2025-12-10T21:17:27.986254Z"}},"outputs":[{"name":"stdout","text":"Original dimension: 77\n\n============================================================\nRFE (XGBoost) with top-5 features\n============================================================\nSelected 5 features:\n['Flow IAT Mean', 'Packet Length Min', 'ACK Flag Count', 'URG Flag Count', 'Init Fwd Win Bytes']\nTest Accuracy : 0.998050\nPrecision     : 0.998499\nRecall        : 0.997600\nF1 Score      : 0.998049\n\n============================================================\nRFE (XGBoost) with top-10 features\n============================================================\nSelected 10 features:\n['Protocol', 'Fwd Packet Length Min', 'Flow IAT Mean', 'Flow IAT Min', 'Bwd Packets/s', 'Packet Length Min', 'Packet Length Std', 'ACK Flag Count', 'URG Flag Count', 'Init Fwd Win Bytes']\nTest Accuracy : 0.998650\nPrecision     : 0.998999\nRecall        : 0.998300\nF1 Score      : 0.998650\n\n============================================================\nRFE (XGBoost) with top-15 features\n============================================================\nSelected 15 features:\n['Protocol', 'Fwd Packet Length Min', 'Bwd Packet Length Max', 'Flow Bytes/s', 'Flow Packets/s', 'Flow IAT Mean', 'Flow IAT Min', 'Fwd Packets/s', 'Bwd Packets/s', 'Packet Length Min', 'Packet Length Std', 'ACK Flag Count', 'URG Flag Count', 'Init Fwd Win Bytes', 'Init Bwd Win Bytes']\nTest Accuracy : 0.998950\nPrecision     : 0.999399\nRecall        : 0.998500\nF1 Score      : 0.998950\n\n============================================================\nRFE (XGBoost) with top-20 features\n============================================================\nSelected 20 features:\n['Protocol', 'Bwd Packets Length Total', 'Fwd Packet Length Max', 'Fwd Packet Length Min', 'Fwd Packet Length Mean', 'Bwd Packet Length Max', 'Flow Bytes/s', 'Flow Packets/s', 'Flow IAT Mean', 'Flow IAT Max', 'Flow IAT Min', 'Fwd Packets/s', 'Bwd Packets/s', 'Packet Length Min', 'Packet Length Std', 'ACK Flag Count', 'URG Flag Count', 'Avg Packet Size', 'Init Fwd Win Bytes', 'Init Bwd Win Bytes']\nTest Accuracy : 0.998850\nPrecision     : 0.999399\nRecall        : 0.998300\nF1 Score      : 0.998849\n\n============================================================\nRFE (XGBoost) with top-30 features\n============================================================\nSelected 30 features:\n['Protocol', 'Total Fwd Packets', 'Total Backward Packets', 'Fwd Packets Length Total', 'Bwd Packets Length Total', 'Fwd Packet Length Max', 'Fwd Packet Length Min', 'Fwd Packet Length Mean', 'Bwd Packet Length Max', 'Bwd Packet Length Mean', 'Flow Bytes/s', 'Flow Packets/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Bwd IAT Max', 'Bwd Header Length', 'Fwd Packets/s', 'Bwd Packets/s', 'Packet Length Min', 'Packet Length Max', 'Packet Length Std', 'ACK Flag Count', 'URG Flag Count', 'Avg Packet Size', 'Init Fwd Win Bytes', 'Init Bwd Win Bytes', 'Active Max', 'Active Min']\nTest Accuracy : 0.998850\nPrecision     : 0.999399\nRecall        : 0.998300\nF1 Score      : 0.998849\n\n============================================================\nRFE (XGBoost) with top-31 features\n============================================================\nSelected 31 features:\n['Protocol', 'Total Fwd Packets', 'Total Backward Packets', 'Fwd Packets Length Total', 'Bwd Packets Length Total', 'Fwd Packet Length Max', 'Fwd Packet Length Min', 'Fwd Packet Length Mean', 'Bwd Packet Length Max', 'Bwd Packet Length Mean', 'Flow Bytes/s', 'Flow Packets/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Bwd IAT Max', 'Bwd Header Length', 'Fwd Packets/s', 'Bwd Packets/s', 'Packet Length Min', 'Packet Length Max', 'Packet Length Std', 'ACK Flag Count', 'URG Flag Count', 'Avg Packet Size', 'Init Fwd Win Bytes', 'Init Bwd Win Bytes', 'Fwd Seg Size Min', 'Active Max', 'Active Min']\nTest Accuracy : 0.998850\nPrecision     : 0.999399\nRecall        : 0.998300\nF1 Score      : 0.998849\n\n============================================================\nRFE (XGBoost) with top-9 features\n============================================================\nSelected 9 features:\n['Protocol', 'Fwd Packet Length Min', 'Flow IAT Mean', 'Bwd Packets/s', 'Packet Length Min', 'Packet Length Std', 'ACK Flag Count', 'URG Flag Count', 'Init Fwd Win Bytes']\nTest Accuracy : 0.998550\nPrecision     : 0.998999\nRecall        : 0.998100\nF1 Score      : 0.998549\n\n=========== SUMMARY: RFE (XGBoost) ===========\nTop-5 features -> F1=0.998049, Acc=0.998050\nTop-10 features -> F1=0.998650, Acc=0.998650\nTop-15 features -> F1=0.998950, Acc=0.998950\nTop-20 features -> F1=0.998849, Acc=0.998850\nTop-30 features -> F1=0.998849, Acc=0.998850\nTop-31 features -> F1=0.998849, Acc=0.998850\nTop-9 features -> F1=0.998549, Acc=0.998550\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"l1 lasso feature selection + xgboost","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom xgboost import XGBClassifier   # <-- changed\nimport numpy as np\nimport pandas as pd\n\n\nfeature_names = X.columns.tolist()\n\nprint(\"Original dimension:\", X.shape[1])\n\n# Train Logistic regression with L1 penalty to select features\nlog_reg = LogisticRegression(penalty='l1', solver='liblinear', C=0.3, max_iter=2000)\nlog_reg.fit(X, y)\n\n# Get non-zero coefficients\ncoef = log_reg.coef_[0]\nselected_idx = np.where(coef != 0)[0]\nselected_features = [feature_names[i] for i in selected_idx]\n\nprint(\"\\nSelected features using L1 (Lasso):\")\nprint(selected_features)\nprint(\"Total selected:\", len(selected_features))\n\n# ----- XGBoost on selected features -----\n\nX_sel = X[selected_features]\nX_train, X_test, y_train, y_test = train_test_split(\n    X_sel, y, test_size=0.20, stratify=y, random_state=42\n)\n\nmodel = XGBClassifier(\n    n_estimators=500,\n    learning_rate=0.05,\n    max_depth=6,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    objective=\"binary:logistic\",\n    use_label_encoder=False,   # suppress older xgboost label-encoder warning\n    eval_metric=\"logloss\",\n    random_state=42,\n    n_jobs=-1\n)\n\nmodel.fit(X_train, y_train)\n\n# Predictions\ny_pred = model.predict(X_test)\n\ntest_acc = accuracy_score(y_test, y_pred)\ntest_prec = precision_score(y_test, y_pred, zero_division=0)\ntest_rec = recall_score(y_test, y_pred, zero_division=0)\ntest_f1 = f1_score(y_test, y_pred, zero_division=0)\n\nprint(\"\\n===== L1 (Lasso) + XGBoost Results =====\")\nprint(f\"Test Accuracy : {test_acc:.8f}\")\nprint(f\"Precision     : {test_prec:.8f}\")\nprint(f\"Recall        : {test_rec:.8f}\")\nprint(f\"F1 Score      : {test_f1:.8f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T20:34:31.679585Z","iopub.execute_input":"2025-12-10T20:34:31.679978Z","iopub.status.idle":"2025-12-10T20:34:38.501508Z","shell.execute_reply.started":"2025-12-10T20:34:31.679954Z","shell.execute_reply":"2025-12-10T20:34:38.500659Z"}},"outputs":[{"name":"stdout","text":"Original dimension: 77\n\nSelected features using L1 (Lasso):\n['Protocol', 'Flow Duration', 'Fwd Packet Length Std', 'Bwd Packet Length Min', 'Bwd Packet Length Mean', 'Bwd Packet Length Std', 'Flow Packets/s', 'Flow IAT Std', 'Fwd IAT Mean', 'Fwd IAT Min', 'Bwd IAT Total', 'Bwd IAT Mean', 'Bwd IAT Max', 'Fwd Header Length', 'Bwd Header Length', 'Packet Length Min', 'RST Flag Count', 'ACK Flag Count', 'URG Flag Count', 'CWE Flag Count', 'Down/Up Ratio', 'Avg Packet Size', 'Avg Bwd Segment Size', 'Init Fwd Win Bytes', 'Init Bwd Win Bytes', 'Fwd Seg Size Min', 'Active Mean', 'Active Min', 'Idle Min']\nTotal selected: 29\n\n===== L1 (Lasso) + XGBoost Results =====\nTest Accuracy : 0.99895000\nPrecision     : 0.99939946\nRecall        : 0.99850000\nF1 Score      : 0.99894953\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"xgboost importance+baseline xgboost","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\nfrom xgboost import XGBClassifier\n\n# -------------------------\n# 1. Load data\n# -------------------------\n\nfeature_names = X.columns.tolist()\nprint(\"Original dimension:\", X.shape[1])\n\n# -------------------------\n# 2. Train XGBoost on ALL features to get importance\n# -------------------------\nbase_model = XGBClassifier(\n    n_estimators=500,\n    learning_rate=0.05,\n    max_depth=6,\n    objective=\"binary:logistic\",\n    use_label_encoder=False,\n    eval_metric=\"logloss\",\n    random_state=42,\n    n_jobs=-1\n)\n\nbase_model.fit(X, y)\n\n# Get feature importances from XGBoost\nimportances = base_model.feature_importances_\n# Sort indices by importance (descending)\nsorted_idx = np.argsort(importances)[::-1]\nsorted_features = [feature_names[i] for i in sorted_idx]\n\nprint(\"\\nTop 20 features by XGBoost importance:\")\nfor i in range(min(20, len(sorted_features))):\n    print(f\"{i+1:2d}. {sorted_features[i]}  (importance={importances[sorted_idx[i]]:.6f})\")\n\n# -------------------------\n# 3. Evaluate XGBoost using top-k important features\n# -------------------------\nK_values = [5, 10, 15, 20, 30,31,9]\nresults_xgb_imp = {}\n\nprint(\"\\n============================================================\")\nprint(\"XGBOOST FEATURE IMPORTANCE + XGBOOST BASELINE\")\nprint(\"============================================================\")\n\nfor k in K_values:\n    top_k_features = sorted_features[:k]\n    X_sel = X[top_k_features]\n\n    # Same train-test strategy as other baselines\n    X_train, X_test, y_train, y_test = train_test_split(\n        X_sel, y,\n        test_size=0.20,\n        stratify=y,\n        random_state=42\n    )\n\n    model = XGBClassifier(\n        n_estimators=500,\n        learning_rate=0.05,\n        max_depth=6,\n        subsample=0.9,\n        colsample_bytree=0.9,\n        objective=\"binary:logistic\",\n        use_label_encoder=False,\n        eval_metric=\"logloss\",\n        random_state=42,\n        n_jobs=-1\n    )\n\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n\n    acc = accuracy_score(y_test, y_pred)\n    prec = precision_score(y_test, y_pred, zero_division=0)\n    rec = recall_score(y_test, y_pred, zero_division=0)\n    f1 = f1_score(y_test, y_pred, zero_division=0)\n\n    print(f\"\\n============================================================\")\n    print(f\"XGBoost-Importance + XGBoost with top-{k} features\")\n    print(\"============================================================\")\n    print(f\"Selected {k} features:\")\n    print(top_k_features)\n    print(f\"Test Accuracy : {acc:.8f}\")\n    print(f\"Precision     : {prec:.8f}\")\n    print(f\"Recall        : {rec:.8f}\")\n    print(f\"F1 Score      : {f1:.8f}\")\n\n    results_xgb_imp[k] = {\n        \"features\": top_k_features,\n        \"acc\": acc,\n        \"prec\": prec,\n        \"rec\": rec,\n        \"f1\": f1\n    }\n\nprint(\"\\n=========== SUMMARY: XGBoost Importance + XGBoost ===========\")\nfor k, info in results_xgb_imp.items():\n    print(f\"Top-{k} features -> F1={info['f1']:.8f}, Acc={info['acc']:.8f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T21:18:03.519993Z","iopub.execute_input":"2025-12-10T21:18:03.520327Z","iopub.status.idle":"2025-12-10T21:18:26.432435Z","shell.execute_reply.started":"2025-12-10T21:18:03.520303Z","shell.execute_reply":"2025-12-10T21:18:26.431515Z"}},"outputs":[{"name":"stdout","text":"Original dimension: 77\n\nTop 20 features by XGBoost importance:\n 1. Packet Length Min  (importance=0.656341)\n 2. Flow IAT Mean  (importance=0.162800)\n 3. ACK Flag Count  (importance=0.047233)\n 4. URG Flag Count  (importance=0.037925)\n 5. Init Fwd Win Bytes  (importance=0.015062)\n 6. Flow Packets/s  (importance=0.011742)\n 7. Packet Length Std  (importance=0.010502)\n 8. Bwd Packets/s  (importance=0.009584)\n 9. Fwd Packet Length Min  (importance=0.008364)\n10. Flow IAT Min  (importance=0.003687)\n11. Bwd Packet Length Max  (importance=0.003624)\n12. Bwd Packets Length Total  (importance=0.002982)\n13. Flow Bytes/s  (importance=0.002641)\n14. Idle Std  (importance=0.001901)\n15. Protocol  (importance=0.001832)\n16. Idle Max  (importance=0.001560)\n17. Fwd Packets/s  (importance=0.001433)\n18. Flow IAT Std  (importance=0.001269)\n19. Flow IAT Max  (importance=0.001268)\n20. Fwd Packet Length Max  (importance=0.001200)\n\n============================================================\nXGBOOST FEATURE IMPORTANCE + XGBOOST BASELINE\n============================================================\n\n============================================================\nXGBoost-Importance + XGBoost with top-5 features\n============================================================\nSelected 5 features:\n['Packet Length Min', 'Flow IAT Mean', 'ACK Flag Count', 'URG Flag Count', 'Init Fwd Win Bytes']\nTest Accuracy : 0.99815000\nPrecision     : 0.99849895\nRecall        : 0.99780000\nF1 Score      : 0.99814935\n\n============================================================\nXGBoost-Importance + XGBoost with top-10 features\n============================================================\nSelected 10 features:\n['Packet Length Min', 'Flow IAT Mean', 'ACK Flag Count', 'URG Flag Count', 'Init Fwd Win Bytes', 'Flow Packets/s', 'Packet Length Std', 'Bwd Packets/s', 'Fwd Packet Length Min', 'Flow IAT Min']\nTest Accuracy : 0.99865000\nPrecision     : 0.99909919\nRecall        : 0.99820000\nF1 Score      : 0.99864939\n\n============================================================\nXGBoost-Importance + XGBoost with top-15 features\n============================================================\nSelected 15 features:\n['Packet Length Min', 'Flow IAT Mean', 'ACK Flag Count', 'URG Flag Count', 'Init Fwd Win Bytes', 'Flow Packets/s', 'Packet Length Std', 'Bwd Packets/s', 'Fwd Packet Length Min', 'Flow IAT Min', 'Bwd Packet Length Max', 'Bwd Packets Length Total', 'Flow Bytes/s', 'Idle Std', 'Protocol']\nTest Accuracy : 0.99860000\nPrecision     : 0.99909910\nRecall        : 0.99810000\nF1 Score      : 0.99859930\n\n============================================================\nXGBoost-Importance + XGBoost with top-20 features\n============================================================\nSelected 20 features:\n['Packet Length Min', 'Flow IAT Mean', 'ACK Flag Count', 'URG Flag Count', 'Init Fwd Win Bytes', 'Flow Packets/s', 'Packet Length Std', 'Bwd Packets/s', 'Fwd Packet Length Min', 'Flow IAT Min', 'Bwd Packet Length Max', 'Bwd Packets Length Total', 'Flow Bytes/s', 'Idle Std', 'Protocol', 'Idle Max', 'Fwd Packets/s', 'Flow IAT Std', 'Flow IAT Max', 'Fwd Packet Length Max']\nTest Accuracy : 0.99860000\nPrecision     : 0.99909910\nRecall        : 0.99810000\nF1 Score      : 0.99859930\n\n============================================================\nXGBoost-Importance + XGBoost with top-30 features\n============================================================\nSelected 30 features:\n['Packet Length Min', 'Flow IAT Mean', 'ACK Flag Count', 'URG Flag Count', 'Init Fwd Win Bytes', 'Flow Packets/s', 'Packet Length Std', 'Bwd Packets/s', 'Fwd Packet Length Min', 'Flow IAT Min', 'Bwd Packet Length Max', 'Bwd Packets Length Total', 'Flow Bytes/s', 'Idle Std', 'Protocol', 'Idle Max', 'Fwd Packets/s', 'Flow IAT Std', 'Flow IAT Max', 'Fwd Packet Length Max', 'Fwd Packet Length Mean', 'Active Max', 'Avg Packet Size', 'Init Bwd Win Bytes', 'Bwd Header Length', 'Fwd IAT Total', 'Total Backward Packets', 'Fwd IAT Max', 'Fwd Packets Length Total', 'Total Fwd Packets']\nTest Accuracy : 0.99895000\nPrecision     : 0.99939946\nRecall        : 0.99850000\nF1 Score      : 0.99894953\n\n============================================================\nXGBoost-Importance + XGBoost with top-31 features\n============================================================\nSelected 31 features:\n['Packet Length Min', 'Flow IAT Mean', 'ACK Flag Count', 'URG Flag Count', 'Init Fwd Win Bytes', 'Flow Packets/s', 'Packet Length Std', 'Bwd Packets/s', 'Fwd Packet Length Min', 'Flow IAT Min', 'Bwd Packet Length Max', 'Bwd Packets Length Total', 'Flow Bytes/s', 'Idle Std', 'Protocol', 'Idle Max', 'Fwd Packets/s', 'Flow IAT Std', 'Flow IAT Max', 'Fwd Packet Length Max', 'Fwd Packet Length Mean', 'Active Max', 'Avg Packet Size', 'Init Bwd Win Bytes', 'Bwd Header Length', 'Fwd IAT Total', 'Total Backward Packets', 'Fwd IAT Max', 'Fwd Packets Length Total', 'Total Fwd Packets', 'Bwd Packet Length Mean']\nTest Accuracy : 0.99890000\nPrecision     : 0.99939940\nRecall        : 0.99840000\nF1 Score      : 0.99889945\n\n============================================================\nXGBoost-Importance + XGBoost with top-9 features\n============================================================\nSelected 9 features:\n['Packet Length Min', 'Flow IAT Mean', 'ACK Flag Count', 'URG Flag Count', 'Init Fwd Win Bytes', 'Flow Packets/s', 'Packet Length Std', 'Bwd Packets/s', 'Fwd Packet Length Min']\nTest Accuracy : 0.99870000\nPrecision     : 0.99899940\nRecall        : 0.99840000\nF1 Score      : 0.99869961\n\n=========== SUMMARY: XGBoost Importance + XGBoost ===========\nTop-5 features -> F1=0.99814935, Acc=0.99815000\nTop-10 features -> F1=0.99864939, Acc=0.99865000\nTop-15 features -> F1=0.99859930, Acc=0.99860000\nTop-20 features -> F1=0.99859930, Acc=0.99860000\nTop-30 features -> F1=0.99894953, Acc=0.99895000\nTop-31 features -> F1=0.99889945, Acc=0.99890000\nTop-9 features -> F1=0.99869961, Acc=0.99870000\n","output_type":"stream"}],"execution_count":29}]}